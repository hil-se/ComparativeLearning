"issuekey","type","components","storypoint","title","description_text"
"NEXUS-10418","Improvement","Bootstrap",0.5,"automatically clear the karaf bundle cache on startup","""NEXUS-10014 has a good summary of the options for clearing bundle cache on startup.    The resolution of NEXUS-10014 meant:    - the UI installer touches a clean_cache file inside the chosen data directory so that the bundle cache is cleared on startup  - the archive installers include a clean_cache file inside their default ( but not optimal ) data subdirectory    According to some recent analysis we still have significant user base using the archives (zip/tgz) to install Nexus. In the use case of upgrade using the archive bundle, a user needs to edit bin/nexus.vmoptions and change the karaf.data property value to reference their existing, to be upgraded data directory. They then also need to manually create a clean_cache file there so that on startup the cache is cleared.    This issue is about eliminating the step of creating the clean_cache file and instead, configuring Nexus to always reload the bundle cache on startup. Other than some so far reportedly insignificant startup processing of bundle caching, I haven't heard of a good reason not to do this. Implementing this aims to make the upgrade process simpler.  """
"NEXUS-10421","Bug","Docker",3,"Docker pull through proxy repository fails if remote is not available","""Set up a docker proxy repository against dockerhub, pull an image through it.    Then disable outbound network access (for testing, I did this by setting an http proxy server that doesn't exist in the HTTP settings).    Then try to pull the image again.  This fails with a 502 error.    {quote}  docker pull 192.168.1.83:18887/tomcat  Using default tag: latest  Error response from daemon: Received unexpected HTTP status: 502 Bad Gateway  {quote}    The nexus.log shows what you would expect:    """
"NEXUS-10419","Bug","UI",2,"nested webapp context path Nexus 3 breaks the UI","""Edit $NEXUS_HOME/etc/org.sonatype.nexus.cfg, and set:        After doing this, the UI heartbeat does not work, the request is sent to """"http://localhost:8081/foo/service/extdirect"""" rather than """"http://localhost:8081/foo/nexus/service/extdirect"""".    This sort of context path works in Nexus 2.x.      """
"NEXUS-10423","Bug","NPM",2,"NPM proxy repository Timeout waiting for connection from pool","""npm install <package> is returning HTTP 500 errors.    In the logs we are seeing the following connection pool time outs. Restarting Nexus resolved the issue.    """
"NEXUS-10429","Bug","Scheduled Tasks",1,"Task produces WARN if ""incorrect"" repository is selected","""Running """"Remove snapshots from Maven repository"""" task with a proxy or hosted release repository selected fires the below WARN and errors the task.  Both proxy and hosted release repositories are available in the dropdown of repositories (and for maven configured by default).    I do not believe these failing repository types should be in the dropdown if they just cause errors.  This is borderline bug/improvement but I made a bug because of the error.    {quote}  016-07-01 14:40:55,715-0400 WARN  [quartz-2-thread-4] *SYSTEM org.sonatype.nexus.quartz.internal.task.QuartzTaskJob - Task 6b9d9397-3ed8-495d-923b-7c0646340fb7 : 'test2' [repository.maven.remove-snapshots] execution failure  java.lang.IllegalStateException: null   at com.google.common.base.Preconditions.checkState(Preconditions.java:158) [com.google.guava:18.0.0]   at org.sonatype.nexus.repository.RepositoryTaskSupport.findRepositories(RepositoryTaskSupport.java:83) [na:na]   at org.sonatype.nexus.repository.RepositoryTaskSupport.execute(RepositoryTaskSupport.java:53) [na:na]   at org.sonatype.nexus.scheduling.TaskSupport.call(TaskSupport.java:89) [org.sonatype.nexus.scheduling:3.1.0.SNAPSHOT]   at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.doExecute(QuartzTaskJob.java:144) [org.sonatype.nexus.quartz:3.1.0.SNAPSHOT]   at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.execute(QuartzTaskJob.java:105) [org.sonatype.nexus.quartz:3.1.0.SNAPSHOT]   at org.quartz.core.JobRunShell.run(JobRunShell.java:202) [org.quartz-scheduler.quartz:2.2.2]   at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40) [org.sonatype.nexus.thread:3.1.0.SNAPSHOT]   at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120) [org.apache.shiro.core:1.2.4]   at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108) [org.apache.shiro.core:1.2.4]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_40]   at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_40]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_40]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_40]   at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]  {quote}    Specifically seems to be happening because Repositories with a policy of RELEASE can be selected, even though they can't possibly contain SNAPSHOT jars. Note that this is stored in Repository attributes, which presently aren't exposed as a criteria to choose from in RepositoryComboBox definition."""
"NEXUS-10446","Story","Blobstore",5,"blob store names should be case insensitive","""Acceptance    * Blob store name uniqueness checks should be case insensitive"""
"NEXUS-10454","Story","Bootstrap|Installer",3,"Move the default location of the data directory in the tar/zip installs out of the install directory","""The Nexus installation is temporary, once you upgrade it becomes obsolete. The data directory is permanent, it needs to be preserved across all upgrades. The default data folder should be """"../sonatype-work/nexus3/data""""    We learned the hard way a very long time ago that it is a therefore good idea to have the default location of the data directory be outside of the installation directory (under """"../sonatype-work/""""). Doing this will prevent two common errors:    1. Users unpacking new installations directly on top of existing installations  2. Users mistakenly deleting or damaging work directories when they try to move them out of old installations.    An empty sonatype-work/nexus3 folder needs to be included in the bundle so the resulting folders have the correct permissions    We must call out in the release notes and upgrade notes that the data folder has been moved. At this time, we expect the move to be done one time manually by the end users, this is not being automated.    Users upgrading from pre-3.1 will need to either:    * manually move (recommended) or symlink the data folder to expose it to where pre-3.1 stored it   * configure Nexus 3.1 to point to the existing data folder inside the pre-3.1 layout (not recommended)    **Users:** Please consult the official upgrade documentation for upgrade instructions pertaining to this move"""
"NEXUS-10460","Bug","Scheduled Tasks",3,"Regression: Maven snapshot remover does not remove asc, md5, and sha1 files","""The snapshot remover no longer removes files with extensions of md5, sha1, and asc.     This is a regression which appears to have been introduced in 2.11.4.    """
"NEXUS-12531","Improvement","Browse Storage",8,"Allow browse of repository storage via HTML index directory listing","""Since the release of Nexus 3, some people still have expressed an interest in having a Browse Storage feature that works using basic HTML navigation.    Users of Nexus 2.x who just want to download a particular file for direct consumption often browse into the /content URL to locate what they want, click on it, and download it. This is a simple, very intuitive UI which can be used by end users who aren't very technical.    In Nexus 3 it is no longer possible to do this.    There are cases where you can download a file by requesting e.g.:   http://<nexus-hostname>/repository/<raw-repo-name/<filename>     This makes it intuitive if the directory listing would be given if you remove the filename from that url (However, currently that url returns 404)    Where it makes sense, consider allowing browsing of repositories via HTML directory listings.     A few repository formats where this likely does make sense are:    * Raw Repositories  * Maven  * Yum (future)     Additionally, the fact that Nexus 3 does not implement an HTML directory list, means implicitly **the Browse Remote feature of Nexus 2 will not work when a proxy is added against a Nexus 3 remote**."""
"NEXUS-10477","Bug","SSL",1,"SSL key/trust store is not thread-safe","""# Hook NX up for debugging and set a breakpoint at https://github.com/sonatype/nexus-internal/blob/738eb86c8013f524802cef87e2961975b1f5793a/components/nexus-ssl/src/main/java/org/sonatype/nexus/ssl/internal/geronimo/FileKeystoreInstance.java#L283  # Using two browser tabs, import a SSL certificate into the trust store  # Observe that the breakpoint is reached by both HTTP request threads concurrently    {{FileKeystoreInstance}} uses several basic collections ({{ArrayList}}, {{HashMap}}) that are not thread-safe by themselves and concurrent operations on the key store can cause corruption/failure. We should synchronize all access to {{FileKeystoreInstance}}, potentially even at a higher level like the methods in {{KeyStoreManagerImpl}}."""
"NEXUS-10473","Bug","Repository",2,"tmp files may be kept open after deletion","""Tmp files seem to kept open after deletion. Maybe they are not being closed properly. There are 380 of these deleted files that kept open in this example.        """
"NEXUS-10481","Bug","Docker",1,"outbound docker proxy requests always insert /library/ and commonly return 404 at the remote","""When nexus sends an outbound docker proxy request, the path gets manipulated on the fly to insert /library/.        If the remote is anything other than dockerhub, this can fail with 404, and therefore Nexus returns 404 to the docker client. If /library/ was not inserted, it might have succeeded    """
"NEXUS-10496","Improvement","Installer",2,"Warning when running installer as root.","""A warning should be given when running installer as root user to correct file permissions """"chown -R nexus-user:nexus-group data""""     Ideally it would be best if the install asked for nexus user and group  to correct permissions itself."""
"NEXUS-10505","Bug","Capabilities|Crowd",1,"Cannot create Crowd capability","""I find I am unable to create a Crowd capability using valid setup while I can create my Crowd configuration using the left nav Crowd item (which subsequently creates the capability with the same values).  Editing and deleting seems fine.    I found this testing HA, however, I was able to repro on latest 3.1 SNAPSHOT.  I feel like this is recent regression, however, I have not tested older 3.1s to verify (nor have I tested older 3x as 3.1 is the first """"official"""" pro release).  I also didn't check NX2.    Error (snipped; full nexus.log attached):  {quote}  2016-07-20 11:11:24,729-0400 ERROR [qtp585549155-319] admin org.sonatype.nexus.extdirect.internal.ExtDirectServlet - Failed to invoke action method: capability_Capability.create, java-method: org.sonatype.nexus.coreui.capability.CapabilityComponent.create  java.lang.NumberFormatException: null   at java.lang.Integer.parseInt(Integer.java:542) [na:1.8.0_40]   at java.lang.Integer.parseInt(Integer.java:615) [na:1.8.0_40]   at com.sonatype.nexus.crowd.CrowdConfiguration.<init>(CrowdConfiguration.java:74) [na:na]  ...  {quote}"""
"NEXUS-10503","Bug","Repository",2,"RepositoryManagerImpl is not thread-safe","""# Fire up NX3 with a debugger  # Set a breakpoint in {{RepositoryManagerImpl.track()}} (https://github.com/sonatype/nexus-internal/blob/8f1102843b5c02ad05a23e1d18e9ff2e24f27ee3/components/nexus-repository/src/main/java/org/sonatype/nexus/repository/manager/RepositoryManagerImpl.java#L158)  # Create a new repo, observe the breakpoint gets hit and leave the thread suspended at it  # Create another repo, observe the breakpoint gets hit again, i.e. two concurrent threads can execute that method which modifies {{repositories}}, a simple {{HashMap}}..."""
"NEXUS-10502","Bug","Staging",3,"nexus-staging-maven-plugin does not use proxy authentication credentials","""I am trying to publish a Java library to Maven Central using the nexus-staging-maven-plugin version 1.6.7 behind a corporate proxy.    In the end, I receive this error:     bq. Failed to execute goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.7:deploy (injected-nexus-deploy) on project exchange-model: Execution injected-nexus-deploy of goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.7:deploy failed: Nexus connection problem to URL https://oss.sonatype.org/: 407 - Proxy Authentication Required -> [Help 1]    It seems that the proxy authentication credentials are not picked up by the plugin, even though they are defined in my Maven settings.xml file on the proxy using the 'https' protocol.    Plugin configuration is:  {quote}  <plugin>                  <groupId>org.sonatype.plugins</groupId>                  <artifactId>nexus-staging-maven-plugin</artifactId>                  <version>1.6.7</version>                  <extensions>true</extensions>                  <configuration>                      <serverId>ossrh</serverId>                      <nexusUrl>https://oss.sonatype.org/</nexusUrl>                      <autoReleaseAfterClose>true</autoReleaseAfterClose>                  </configuration>   </plugin>  {quote}    Proxy config is:  {quote}<proxy>     <id>https_psbru</id>     <active>true</active>     <protocol>https</protocol>     <username>user</username>     <password>pass</password>     <host>psbru.cec.eu.int</host>     <port>8012</port>     <nonProxyHosts>localhost|127.0.0.1</nonProxyHosts>  </proxy>  {quote}    The proxy configuration is good since I can perform normal Maven builds and can download exte5rnal libraries. I was able to push the artefact by commenting the 'nexus-staging-maven-plugin' and just performing a 'mvn clean deploy'. My Maven version is 3.2.1.    """
"NEXUS-10518","Story","Database",5,"Upgrade to OrientDB 2.2.x","""We should upgrade to the latest release line of orientdb. This will get us a fix for a known issue: [http://www.prjhub.com/#/issues/7245]  It should also put us back on track for support until we re-up our contract in the September timeframe; plan there is to upgrade to Production support and avoid any issues about backwards support."""
"NEXUS-10533","Bug","LDAP",2,"EnterpriseLdapManager is not thread-safe","""{{EnterpriseLdapManager}} contains a mutable {{ArrayList}} of LDAP connectors (cf.  https://github.com/sonatype/nexus-internal/blob/5e34c8081fa48a0aeb5545c0c7f5aeb1d5d693a3/private/plugins/nexus-ldap-plugin/src/main/java/org/sonatype/nexus/ldap/internal/realms/EnterpriseLdapManager.java#L65). which is improperly managed for concurrent access.  # While {{getLdapConnectors()}} runs and rebuilds the list, nothing prevents/blocks a concurrent invocation of {{on(LdapClearCacheEvent)}}, giving rise to concurrent {{add()}} and {{clear()}} invocations on a non-concurrent list  # While methods like {{authenticateUser()}} iterate over the returned array list from {{getLdapConnectors()}}, nothing blocks a concurrent invocation of {{on(LdapClearCacheEvent)}}, causing {{ConcurrentModificationException}} or more cryptic failures from the iteration    """
"NEXUS-10540","Bug","Blobstore",1,"BlobStoreManagerImpl is not thread-safe","""The {{BlobStoreManagerImpl}} uses a simple {{HashMap}} for its {{stores}} member which isn't properly protected from concurrent access:  # Fire NX3 up with a debugger  # Set a breakpoint in {{BlobStoreManagerImpl.track()}} (https://github.com/sonatype/nexus-internal/blob/525334ca25c2463544b12dd49aa54ef55ee2a703/components/nexus-core/src/main/java/org/sonatype/nexus/internal/blobstore/BlobStoreManagerImpl.java#L207)  # In one browser tab, create a new blob store  # Observe breakpoint gets hit, leave thread suspended at it  # In another browser tab, create another blob store  # Observe breakpoint gets hit again, i.e. a concurrent thread attempts to modify the {{stores}}  """
"NEXUS-10565","Bug","LDAP",2,"LDAP group membership query can fail due to query size limits","""When we check for a user's LDAP group membership during authorization the query issued retrieves all groups a user is a member of:        This query (like any LDAP query that retrieves a set) can fail to retreive all results due to query result size limits in the LDAP server.  When this happens there is no workaround, the user's group mapping will not work if the group is not in the returned set.    We should either use a more targeted query, or we should be using a paged result set to retreive all results, rather than trying to get them all in one shot."""
"NEXUS-10564","Bug","Bower",2,"Bower proxy repository timeout waiting for connection from pool","""# Add a bower proxy for http://bower.herokuapp.com, e.g. repo name """"bower-proxy""""  # run {{curl -I http://localhost:8081/repository/bower-proxy/packages/search/foo}} about two dozen times (IIRC, 20 is the max number of connections per route/host of the shared HTTP connection pool, that needs to be exceeded), note that this makes HEAD requests, not GETs  # Eventually observe the exception below and the client receiving """"502 Bad Gateway""""        Part of the issue here maybe that {{DefaultHttpResponseSender}} does not consume/close the response payload for HEAD requests. If that payload is an instance of {{HttpEntityPayload}} as in this bower proxy scenario, the underlying HTTP connection is leaked."""
"NEXUS-10570","Story","Configuration|Documentation",3,"document best practice for url encoding/decoding through reverse proxies","""When npm clients send requests to the remote server ( ie. the registry URL ) it needlessly encodes slashs as %2f. Sonatype has filed an npm issue about this because it seems improper that npm client url encodes forward slashes: [https://github.com/npm/npm/issues/16380]    Other clients ( not npm ) may also send URLs with encoded slashes.    Unfortunately this encoding of slashes conflicts with the default settings of Apache httpd which by default DOES NOT ALLOW THEM and WILL RETURN a 404 response.    From [https://httpd.apache.org/docs/2.2/mod/core.html#allowencodedslashes]  {quote}With the default value, Off, such URLs are refused with a 404 (Not found) error.  {quote}  However, if you are running Apache httpd 2.0.52 to 2.2.8 and you set:  {noformat:title=DO NOT DO THIS in Apache httpd 2.0.52 to 2.2.8}  AllowEncodedSlashes On    *and*    The ProxyPass directive may also need nocanon option. From [https://httpd.apache.org/docs/2.2/mod/mod_proxy.html#proxypass] :  {quote}Normally, mod_proxy will canonicalise ProxyPassed URLs. But this may be incompatible with some backends, particularly those that make use of PATH_INFO. The optional nocanon keyword suppresses this and passes the URL path """"raw"""" to the backend. Note that this keyword may affect the security of your backend, as it removes the normal limited protection against URL-based attacks provided by the proxy.  {quote}  {noformat:title=DO THIS: Example use of nocanon option}  ProxyPass / http://localhost:8081/ nocanon  {noformat}      Basically Sonatype server products do not rely on Apache httpd to filter out suspect URLs containing path info with encoded values, so it is OK and sometimes required to let these through to the backend servers.     Additional Reference: [http://stackoverflow.com/a/9933890/235000]"""
"NEXUS-10599","Bug","Migration",1,"Staging repositories should not be included in the migration list","""Staging isn't currently supported in Nexus 3, we should detect staging repositories and grey them out, similar has been done for """"central-m1"""" in this screenshot.  This will let people who have staging know that it isn't in Nexus 3, giving them an opportunity to stop migration early, rather than wasting a large amount of time trying to migrate to Nexus 3.1."""
"NEXUS-10621","Bug","Capabilities",1,"DefaultCapabilityRegistry is not thread-safe","""The {{DefaultCapabilityRegistry}} uses a simple {{HashMap}} for its {{references}} field, a lock is used to protect that data structure from concurrent access. The {{getAll()}} method however hands out a direct reference to that very map (its value set actually but that's irrelevant here), exposing it to access that is not guarded by the lock, e.g. an iteration by {{get()}} or other callers which can then encounter exceptions like below.    https://github.com/sonatype/nexus-internal/blob/3ce6cd48c49603b70f3dc1e41efe53122e9ea033/components/nexus-core/src/main/java/org/sonatype/nexus/internal/capability/DefaultCapabilityRegistry.java#L362      """
"NEXUS-10633","Bug","UI",0.5,"Create Blobstore prompts to discard navigating away","""Just noticed I went to create a blobstore but immediately backed out and was prompted to discard.  This is inconsistent with our other create options however I suspect this is because the field """"Type"""" is auto filled with """"File"""".  I may be wrong because creating task Execute Script has """"Language"""" auto filled and does not exhibit the same behavior.  If not a bug, I still think worth documenting.  There are several of these discard bugs floating (most fixed) and it helps at least me to see it here rather than remembering if a bug or not.  At minimum bringing up for triage.    I didn't check older NX3 at this time.  NX2 did not have a Blobstore area so there is nothing to check there."""
"NEXUS-10631","Bug","Browse Storage|UI",3,"Show deployment user ID and IP address in component/asset attributes","""In Nexus 2.x you can see the user ID who deployed an artifact in the """"artifact"""" tab.    This information is not present in Nexus 3.  It should be.  Additionally, it would be good to show the IP address it was deployed from."""
"NEXUS-10630","Bug","Docker",2,"Unable to push image whenever it shares layers with another image already pushed by another docker client","""h1. Symptoms  When I try to push an image having layers used by other images already pushed onto the nexus docker registry (and pushed by another client) I got the following error message:   """"Upload failed, retrying: blob upload invalid: blob upload invalid""""    h1. Step to reproduce  Machine A: boot2docker (see details below)  Machine B: centos (see details below)    # Machine A: Create a Dockerfile (such as the one in attachment) set version in echo to v0.0  # Machine A: build tag and push it to a fresh nexus repository with tag 0.0  # Machine A: update dockerfile and set version in echo to 0.1  # Machine A: build tag and push it to nexus with tag 0.1  # Machine B: get Dockerfile and set version in echo to 1.0  # Machine B: build tag and push it to nexus with tag 1.0    h1. Configuration  h2. Nexus  * Nexus OSS 3.0.0-03  * Nexus running in a docker container.(sonatype/nexus3:3.0.0)  * running behind nginx (handling https)    h2. Clients  2 machines: a centos and a boot2docker hosted in windows 10. Both using the same version of docker  h3. Docker  1.12.0, build 8eab29e  h3. OS  boot2docker: docker-machine.exe version 0.8.0, build b85aac1  centos: CentOS Linux release 7.2.1511 (Core)  h1. Remarks  I tried with a very simple docker image (such as busybox instead of gocd/gocd-agent) and it seems to work."""
"NEXUS-10638","Improvement","Search",2,"add a search criteria for repository name","""It would be nice to have a repository name entry in the 'More criteria' section of the search.    Acceptance    * I am able to search within a specified repository"""
"NEXUS-10637","Story","Browse Storage|UI",1,"The URL and status fields in the Repository admin table should be cloned to browse mode","""We withhold important information about repositories from non-admins. Specifically, the repository status and URL fields.  * The repository status field is useful to see when a repository cannot be used by build tools.  * The URL field is necessary for folks to understand how to connect their build tools.   We should make both of these fields available in browse mode."""
"NEXUS-10654","Bug","UI",0.5,"Cancel button needs clicked twice, the first time validates","""While setting up an LDAP, near the end I clicked """"Verify login"""" but decided not to verify and instead clicked """"Cancel"""".  When I did so, I was informed that the name field was required via validation and had to click cancel again to close the modal.    This seemed familiar ref: https://issues.sonatype.org/browse/NEXUS-8430.  The fix may also be the same.  Interestingly esc and X close implemented in NEXUS-9670 both provide a workaround without actually clicking cancel as well.    I did not check older NX3 or NX2 at this time.  It seems likely based around NEXUS-8430 that this may have been lurking for a while."""
"NEXUS-10679","Bug","NPM",2,"NPM repos don't handle HEAD requests","""Looks to be a regression from NX2, and a deviation from the expected behaviour of an npm registry.  While NX2 and https://registry.npmjs.org both respond with 200/404 as expected, NX3 returns 400 in the event of all HEAD requests.   """
"NEXUS-10692","Bug","Security",5,"do not prompt for user credentials for RUT authenticated users","""If a user is authenticated via RUT authorization they should not be prompted for a password from Nexus under any circumstances, since they do not have one to provide.    This applies to at least the following:    * User Token Retrieval  * NuGet API Key Access  * Support Zip Download    All destructive operations should still require a confirmation dialog ( Yes or No - but not ask for credentials) - such as:    * resetting all user tokens  * resetting NuGet API key"""
"NEXUS-10750","Story","Search|UI",5,"limit displayed search criteria to the formats of configured searchable repositories","""In the UI, if you expand the Search option all types that Nexus supports are listed even if there are no repositories for a type configured.  I think it would be good if only those types that are are repos set up for in the instances are listed. Listing types that are not supported by this specific Nexus instance could possibly make the user think that there should be support. Which could cause support tickets for the Nexus admin.  For example, a user seeing """"Docker"""" listed could (as Docker is hype) think that it would proxy Docker hub and try to use it. When it doesn't work (as not proxy is set up) he/she could think something is wrong in this setup starting a internal support ticket to solve this.    Acceptance criteria:    * When no repositories have been configured for a given repository format, that format should not appear as a search option."""
"NEXUS-10759","Bug","Repository|Scalability",3,"Deleting a repository of non-trivial sizes lags and floods the log with exceptions","""I tried to delete the maven-central proxy out of my dev instance which contained 1000+ assets (from building goodies or nexus-internal). After clicking """"Delete repository"""" from the UI no immediate user feedback about the operation was provided, the repo screen was still there showing the deleted """"maven-central"""" repo. Meanwhile, the NX log got filled with the below exception, about every 5 seconds. After some time, the delete operations seems to finally have completed.        cf. https://github.com/sonatype/nexus-internal/blob/0a364dc7b5085ad1f02781a0752f23a268c5125d/components/nexus-repository/src/main/java/org/sonatype/nexus/repository/storage/StorageFacetImpl.java#L190"""
"NEXUS-10774","Story","UI",1,"the icon to collapse user interface feature menu can be easily confused for a back navigation button","""We’ve designed our UI to work at 1024x768. Thus, the ability to collapse the feature menu isn’t really needed, and can be disabled. This will also eliminate any confusion about how the collapse arrow relates to the breadcrumb.    !https://issues.sonatype.org/secure/attachment/72950/72950_Assets+-+Nexus+Repository+Manager+2016-08-31+16-58-57.png!"""
"NEXUS-10795","Bug","NPM",3,"500 response ORecordDuplicatedException when the same npm package metadata is requested concurrently","""While testing 3.0.2 npm, I noticed the below in the nexus.log when running the proxy tests.        I also noticed several of these (different packages) in the npm output, I believe related.      """
"NEXUS-10794","Bug","LDAP|RUT Auth|User Token",1,"user tokens do not work in combination with RUT Auth and LDAP realms","""h4. Setup Nexus    # Start with virgin Nexus 2.13.0-01  # Create a file at sonatype-work/nexus/conf/logback-overrides.xml with this content:    # (Optional) Add Nexus patches at https://issues.sonatype.org/browse/NEXUS-10431 and set {{nexus.usertoken.noPopUps=true}} in nexus.properties - **this step is optional to reproduce the underlying problem**  # Start Nexus  # Disable Anonymous Access  # Enable the RUT Auth capability with header value as {{REMOTE_USER}}  # Configure Enterprise LDAP to a server with at least one user in one ldap group. ( [can use Sonatype test LDAP|https://docs.sonatype.com/display/INSIGHT/Testing+IQ+LDAP+Integration] )  # Map a single external ldap role into Nexus with a role member of Nexus Administrator Role  # Configure Nexus 2.13.0-01 Realms as:  * RUT Auth  * User Token  * Xml Auth  * Xml Authz  * Enteprise LDAP    The direct Nexus URL in this case will be http://localhost:8081/nexus    **The attached nexus pro bundle has all of this configured already, with logs showing the problem**    h4. Setup Reverse Proxy    You are going to need a reverse proxy that sets REMOTE_USER header to the name of the LDAP user who is in the mapped LDAP role.    [Example using this support tool|https://github.com/sonatype/nexus-toolbox/tree/master/reverse-proxy]:  {{java -jar ./target/reverse-proxy-1.0-SNAPSHOT.jar -H """"REMOTE_USER:whitney.haig""""}}    The reverse proxy URL will be http://localhost:18081 in this case    h4. Perform Test    h5. Step 1  # Clean browser cache or open an incognito window  # Login to UI at direct URL: http://locahost:8081/nexus , using the LDAP username and password, for the user name you setup with the Reverse proxy      h5. Step 2  # Open in another browser or incognito window the reverse proxy URL http://locahost:18081/  - this should automatically log you in as the LDAP user in the REMOTE_USER header.  # Go to your profile in and Access your user token. Make note of this value.    h5. Step 3  Use curl to perform a basic auth request using the valid user token credentials obtained from step 2    Example:      **This fails with 401 instead of 200**    The Nexus log records this information:        Notice it said the token was deleted?    Now go back to the Step 2 browser window. Click Access User Token again. You still see your user token there and you can see in the logs that Nexus still gets this from the user token database.    h4. Problems    There are actually at least three bugs:    # Nexus detects a valid user token as stale when it is not stale  # Nexus claims to remove the stale user token, but it does not do this successfully, as later the same token can be retrieved from the usertoken db - it seems the query to DELETE tokens is broken  # Nexus uses the user token name code as the username to lookup in LDAP ( this is never expected to work )    h4. Expected    - a basic auth request with a valid user token should work with all the stated realms enabled.  - a request with a RUT Auth header of a valid user should authenticate and be properly authorized  - *a user account in LDAP should be able to authenticate to Nexus using either REMOTE_USER header or a valid user token name code, if both realms are enabled and set up correctly*"""
"NEXUS-10808","Bug","NuGet",3,"eager caching of nuget versions contributes to slow query performance","""External Report: *Paket*: [#1912](https://github.com/fsprojects/Paket/issues/1912)    I build our solutions with FAKE. In Interaction with Nexus 3 the `paket update` took very long, if I have FAKE in it (I think the problem here is on Nexus 3 side, because it tries to download all the versions of FAKE - and there are a lot of them). And it fails.         I prepared a simple project to reproduce the steps:    https://github.com/WebDucer/Nexus3PaketIssue    1. Install the Docker container for Nexus 3        2. Use the project above to execute `paket update` against nuget.org or Nexus3 as source (comment the unneded sources out).    Expected behavior    `paket update` should update quicker for packages with a lot of versions    Actual behavior    `paket update` need a lot of time, if package `FAKE` is in it.    Known workarounds    Using of two sources (nuget.org on the first place and Nexus 3 on the second).    Thread dump shows the thread getting all versions doing this:    """
"NEXUS-10813","Story","Docker|Documentation",8,"add anonymous read access support for docker repositories","""Users would like to have anonymous read (pull) access to docker repositories in Nexus. This helps consume and share docker images more easily by not requiring a specific login. This is analogous to the benefits offered by anonymous access by other formats.    As an end user, I don't want to have to configure authentication for read only access to docker repositories.  Docker hub does not require this, and neither should Nexus Repository Manager.    According to this comment from a Docker developer the correct way to do this would be to implement token authentication, and to have Nexus hand out tokens for anonymous access:    https://github.com/docker/docker/issues/24129#issuecomment-230610547    There might also be a simpler implementation that should be considered for blanket anonymous access to a repository.  """
"NEXUS-10817","Bug","NPM",1,"publishing npm packages with wrongly encoded ISO-8859-1 JSON fails with 400","""It is [common knowledge|https://issues.sonatype.org/browse/NEXUS-8043?focusedCommentId=298663&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-298663] that the official NPM registry contains packages that have wrongly encoded ( non UTF-8 ) pakage.json.     Nexus tries to handle JSON wrongly encoded as ISO-8859-1, by falling back to that encoding when parsing JSON as UTF-8 fails.    *It seems that the npm client may allow you to attempt publishing package.json that is not encoded properly - need to verify this*    The [code from here|https://github.com/sonatype/nexus-internal/blob/ee9501671d3d030a88b2bcf740538b6a706bd09b/private/plugins/nexus-repository-npm/src/main/java/com/sonatype/nexus/repository/npm/internal/NpmJsonUtils.java#L75-L75] shows how Nexus performs a fallback when UTF-8 parsing fails:    {code}  @Nonnull    static NestedAttributesMap parse(final Supplier<InputStream> streamSupplier) throws IOException {      try {        final Map<String, Object> backing =            mapper.<Map<String, Object>>readValue(streamSupplier.get(), rawMapJsonTypeRef);        return new NestedAttributesMap(String.valueOf(backing.get(NpmMetadataUtils.NAME)), backing);      }      catch (JsonParseException e) {        // fallback        if (e.getMessage().contains(""""Invalid UTF-8 middle byte"""")) {          // try again, but assume ISO8859-1 encoding now, that is illegal for JSON          final Map<String, Object> backing =              mapper.<Map<String, Object>>readValue(                  new InputStreamReader(streamSupplier.get(), Charsets.ISO_8859_1),                  rawMapJsonTypeRef              );          return new NestedAttributesMap(String.valueOf(backing.get(NpmMetadataUtils.NAME)), backing);        }        throw new InvalidContentException(""""Invalid JSON input"""", e);      }    }    {code}      However, parsing can fail with at least one other variation of exception message ( excluding any localized messages ):        """
"NEXUS-10829","Story","Configuration",5,"Store commonly customized configuration in the data folder","""There are commonly customized configuration options that are currently stored in the installation folder and a user needs to remember to copy these over every time.    As an admin, I want to be able to store the commonly customized property files inside the data folder so that I don't have to remember to reapply them or copy them every time I upgrade.    This should include things like the jetty config, port, webapp context, logging, jvm options, ha (hazelcast, orient), ehcache.xml etc.    Acceptance Criteria    * Create a single property file with commonly-changed properties  ** this lives in the data folder, which is read and pushed into other configuration via templates/property placeholders, so that (e.g.) jetty configuration files can still change structure version-to-version  ** a similarly structured default properties file provides values for properties not set by the user (e.g. in cases where Nexus 3.(n+1) adds a new user-configurable property)  ** When NX3 first boots, if the property override file does not exist, NX creates it  *** (No overrides are set in this NX-created file, but comments explain the available configuration properties)   * Subdivide the /etc directory into sub-directories to separate routinely changed configuration from rarely/never-changed configuration files   * Documentation exists to explain this, including the different manual steps needed to upgrade from pre-3.0 to 3.1   * Circulate the proposed layout with the NX team and Support prior to implementation"""
"NEXUS-10883","Bug","Logging",0.5,"DEBUG level logging should print HTTP response code Nexus is sending","""Nexus 3 has a debug log message indicating that it is sending a response, but it does not show what the response is:        You need to enable TRACE logging to see it:        Please just put the above at DEBUG, it's a PITA to have customers enable trace logging of any kind, because the noise makes it very difficult to go through logs."""
"NEXUS-10893","Bug","Outreach",1,"outreach outbound HEAD request triggers OutreachServlet WARN IOException Broken Pipe","""No idea why this happens, but something doesn't seem right here with this WARN message related to Outreach:        """
"NEXUS-10917","Bug","UI",0.5,"Repository Combobox Filtering not working","""It appears that ExtJS store filtering is not working properly for Repositories Combobox. It will properly filter when including a single Type/Format, but not with multiples, and also not when trying to exclude Type/Format. """
"NEXUS-10929","Bug","PyPI",2,"Unable to upload Python wheel using PyBuilder","""I am getting an error when I upload a wheel to Nexus using PyBuilder (http://pybuilder.github.io/). The same project can upload the wheel to pypi-server so I do not believe the problem is with the client. I can successfully install packages using pip and Nexus.    The error message in nexus log is:   """
"NEXUS-10945","Bug","Repository",1,"deleting a repository may not remove it as a member from group repository","""After deleting a -docker- repository (hosted or proxy) that belongs to a group, requests made through the group continue to try to access the deleted repository as seen in the logs.    If a new repository is made with the same name, it is automatically re-added to the group with the old configuration. If a bad configuration is deleted, then recreated with correct configuration, requests still fail. The work around is to remove the repo from the group, then delete it, then recreate it and add it to the group.    For example:    If npm-host-test repo is member of group npm-group-test    If you delete repository npm-host-test, from the UI npm-host-test is no longer a member of npm-group-test, but any requests made to that group generates the following warning:    Additionally the support zip will still show the repository as a member of the npm-group-test."""
"NEXUS-10957","Bug","NPM",3,"ETag 304 causes 404 response through npm proxy group","""I'm running the latest 3.x in Docker as an npm proxy.  I have two repos setup.  One is just a proxy of https://registry.npmjs.org.  The other is a npm group containing the proxy.    I found that {{jspm install npm:string_decoder}} was failing with a 404 response while {{npm install string_decoder}} was working.    The Nexus request log showed identical requests with one returning 404 and the other 200, so I enabled trace logging and observed that the jspm request was sending an {{If-None-Match: """"3SLQP4XEWUIBONOV855WOPR0B""""}} header while the npm request was not.  Here is the full request that was logged:        This seems to be the reason for the difference in the response.  Further in the log, I see:        I found that if I configured my npm registry to reference the proxy repository url directly, that jspm started to work.  It was only with the group repository that the ETag 304 response was being turned into a 404.  So it appears that something may be broken with how an npm group repository handles ETag headers.    A workaround is to just target the proxy repository directly or temporarily disable proxying through Nexus.  (jspm only seems to have this problem for node core modules; other npm requests came through the group just fine, but presumably w/o etags...no clue why it's requesting core modules at all, but it must have some added local metadata for them that it's using to produce the If-None-Match header for its request.)"""
"NEXUS-10961","Technical Debt","Documentation|Logging",1,"document logging configuration and output files","""https://books.sonatype.com/nexus-book/3.0/reference/install.html#directories    This section of the book makes no reference to where a user can find the Nexus log files, and what the log files contain.    This makes a poor user experience if a user is trying to debug a startup problem in particular, because they do not know where to look for logs or what the different log files could contain.    Acceptance:  - document log **configuration** file locations that are visible on disk  -- what are they used for internally  -- when should they be manually edited, if at all  -- possibly do not document at all files that should never be edited manually ( see NEXUS-9304 )  - document *log output* files that are actual logs containing output of the application, located in the $data-dir/log directory  -- jvm.log  -- nexus.log  -- karaf.log  -- request.log  -- archives of nexus and request log     Probably out of scope:  - document how to customize log rotation settings, including location for output  - document console commands like nexus:logger  """
"NEXUS-10982","Bug","UI",2,"Admin cog appears then disappears...then reappears","""Testing the past couple days, I've noticed that after signin the admin cog in the header appears briefly then disappears then reappears.  See attached.  It's just fast enough that I can't click it at first unless I try really hard/know it's coming.    This does not impact NX2 and also seems ok in NX3.0.2 and seems recent regression.  It doesn't break anything but could be polished up."""
"NEXUS-10981","Bug","Build",0.5,"Pro plugins leaked into OSS version and are shown as missing","""Starting a vanilla Nexus OSS shows in the plugin console that several of them are missing. Though, they are likely to be available in the Pro version only. Please see attached screenshot."""
"NEXUS-10978","Bug","Logging",1,"UnauthenticatedException should not be logged as ERROR","""When someone has an old browser session and they are viewing the Nexus UI, and they need to be authenticated again, Nexus can spit ERROR level log messages in the log related to org.apache.shiro.authz.UnauthenticatedException.        h4. Expected    - permission problems due to unauthenticated access should not be logged at ERROR level. This causes unwarranted concern from log scanners looking for more critical ERROR level server issues. It is completely normal in a server application that sessions sent from a browser can have been expired on the server. This is not an ERROR condition."""
"NEXUS-10987","Bug","Transport",2,"NoClassDefFoundError for SSLSocketImpl on non-Oracle JVM prevents proxying https remote","""Description    The fix for [NEXUS-6838] introduced a hard dependency on sun.security.ssl.SSLSocketImpl. JREs with alternative JSSE implementations like the IBM JRE do not ship this class.    This dependency leads to a NoClassDefFoundError on attempts to configure a proxy repository with a HTTPS URL, and thus renders the proxy repository feature unusable for https URLs *on non-Oracle JVMs*.    Symptoms    # Browsing remote for proxy repositories with https remote does not work  # Logfile shows        Steps to reproduce:    # download and install IBM JRE  # start nexus 2.x with IBM JRE  # configure a proxy repository with a https remote  # navigate to """"browse remote"""" and try to browse the content    Suggested fix    Use the plattform independent way to set the host for SNI as documented by Oracle: https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html#SNIExamples    Quick fix    Remove dependency on Oracle JRE by using reflection to access SSLSocketImpl.    The quick fix makes https usable for non-Oracle JREs, but removes SNI support for these JREs.    ---  ---  *The quick was implemented in this issue - SNI still does not work for any JVM other than Oracle JVMs. See NEXUS-6844.*  ---  ---"""
"NEXUS-11020","Bug","Logging",1,"too much DEBUG logging from com.orientechnologies.orient.core.storage.impl.local.paginated.OLocalPaginatedStorage","""On relatively calm instances, we ask customers to set their ROOT logger level to DEBUG to help diagnose problems they are experiencing.    But this turns on too much DEBUG logging at from orient for the simplest requests - for example the status resource the UI pings with.          Not only is it too verbose, it is relatively useless for human consumption.    Expected    - the above logging needs to be moved to TRACE in orient *and/or* we  need to add an explicit default log level for com.orientechnologies.orient.core.storage.impl.local.paginated.OLocalPaginatedStorage to level INFO  - the above logging is a huge performance suck and if enabled could contribute to system instability if enabled in a busy system.      """
"NEXUS-11001","Improvement","Proxy Repository|UI",0.5,"Get rid of ""connection pending..."" status message","""The """"connection pending..."""" message makes proxy repositories in Nexus 3 look broken.  We've seen this over and over now in support:    I think it would be better to change the status to """"OK"""" or something similar.   I get that we don't really know if it's OK until a connection attempt is made, but this message is too much detail. It just confuses users."""
"NEXUS-11027","Bug","UI",1,"accessibility problems with checkboxes","""I would like to report an accessibility bug.    Some things like a field that enables using emails are reported to my screenreader as checkbox. I assume this is not a <input type='checkbox'>. The problem is that pressing a space bar on any checkbox toggles it, but screenreader does not see the state change, and checkbox is always seen unchecked.  I assume you marked a checkbox with aria state properties, so it is actually needed to change the aria state when a real state changes."""
"NEXUS-11139","Bug","NPM",2,"ConcurrentModificationException when deleting NPM resource","""Some components in an npm repo will persistently throw ConcurrentModificationException on trying to delete them.  This seems to affect all versions of a component with a semver-patch range (every 1.1.x version of a component may be affected but 1.2.x will not be).    Using nexus 3.0.1-01 inside a docker container loosely based off the official sonatype nexus3 dockerfile.  Specifically,  https://download.sonatype.com/nexus/3/nexus-3.0.1-01-unix.tar.gz    The issue seems to be related to how the component is stored, as once it occurs it will persist across reboots.  There seems to be no way to get rid of it once it occurs, although I haven't tried dumping the repo entirely.    If the offending package is published to another npm repo on the same server, it usually is not affected, so I doubt it is related to the content of the package.  I seem to recall that some of the affected package version ranges used to delete just fine, but I can't be sure of that.    The issue happens fairly often to me, about 1 in every 5 semver-patch ranges seem to be affected.    My workflow may be causing this: I have redeploy off, but tend to play fast and loose when developing, and will commonly delete a semver-patch version if I discover a bug soon after publishing it.  Additionally, I tend to scan through the codebase and remove earlier patch versions that are no longer in use.    """
"NEXUS-11148","Bug","LDAP|UI",1,"Ldap Button ""View Certificate"" does not appear on Firefox","""When creating a LDAP connection, there is a display bug preventing the """"View Certificate"""" button to appear, thus preventing to store the certificate.    That makes it impossible to create a LDAPS connection using firefox.    The button appears correctly on Chrome.    The bug appears on the latest version of Firefox."""
"NEXUS-11141","Improvement","NuGet",1,"support for nuget repository ""package-versions"" endpoint","""Please support the nuget API endpoint """"package-versions"""" (e.g.: [https://www.nuget.org/api/v2/package-versions/NUnit?includePrerelease=true]) for faster version check."""
"NEXUS-11140","Bug","Search",2,"Elasticsearch (JDK) overflow on disks larger than 2^63 (Amazon EFS)","""I'm using Nexus OSS 3.0.2-02 in a docker container and the /nexus-data directory is and NFS mount (Amazon EFS). This version of Nexus uses Elasticsearch 2.2 which has a bug with handling large filesystems. Check references below:    https://discuss.elastic.co/t/elasticsearch-with-amazon-elastic-file-system/55867/5  https://github.com/elastic/elasticsearch/pull/20527  https://bugs.openjdk.java.net/browse/JDK-8162520  https://www.elastic.co/guide/en/elasticsearch/reference/2.4/release-notes-2.4.1.html    Any chance you can update nexus bundles org.apache.servicemix.bundles.elasticsearch and org.sonatype.nexus.elasticsearch  to use elasticsearch 2.4.1 ?"""
"NEXUS-11174","Bug","Maven|Migration|Search",1,"Upgraded (maven?) components do not show in format specific search","""An EA user noticed that, post-Upgrade he could keyword search for a component and it would show up but if he searched format specific (maven) that it would not.  I was able to duplicate this by performing the below steps:  1) In your NX2 add a maven artifact like http://localhost:8082/nexus/content/repositories/central/org/stagemonitor/stagemonitor-core/0.19.0/stagemonitor-core-0.19.0.pom  2) Setup and Upgrade NX2 to NX3  3) In your NX3 UI, go to search and expand the list and select Maven specific.  4) Enter artifact ID matching your artifact above (in our case """"stagemonitor-core"""").  BUG: No components found message back.    I also see the same issue with Group ID.    The components do show on the list unfiltered, so this seems to be a filtering issue.    After doing this, I added another version to NX3 and this I *was* able to see in format specific search (but still not the original) so this seems to be a problem with Upgrade.  {quote}  http://localhost:8081/repository/maven-central/org/stagemonitor/stagemonitor-core/0.25.0/stagemonitor-core-0.25.0.pom  {quote}  I also checked with a component with no dash in it in case it was a problem with the dash (like NEXUS-9493) but didn't seem to matter.    Upgrade is new to 3.1 so no older backchecking is needed."""
"NEXUS-11172","Story","UI",3,"Update license/eula shown in About dialog","""*Background*   When we created a single OSS/Pro binary, we inadvertently dropped all language associated with Nexus Repository OSS being a free product to use. This has caused user confusion and consternation.    *Acceptance*   * The about box should shows either the OSS or Pro license, depending on whether there is a license installed.   ** """"PRO OSS"""" licensed instances should show the Pro license.   * The OSS license should make it clear that Nexus Repo OSS is licensed under the EPL.   * The 'license installation' dialog should show the Pro license."""
"NEXUS-11179","Bug","Browse Storage|UI",1,"Error on login from anonymous from a browse details level","""I noticed an error firing when I was browsing the details of a component (or asset) and logged in.  See attached.  Subsequently, login does not give the admin cog.  You have to refresh the browser to see it.  This also causes the browse details to show.    Here's the detail from my console:      I did not check older NX3 or NX2 at this time."""
"NEXUS-11191","Bug","Docker",3,"Docker group with registry.access.redhat.com proxy as member does not work","""Following set up     - 1 proxy repo of DockerHub  - 1 proxy repo of https://registry.access.redhat.com/  to get access to OpenShift and other certified images  - 1 hosted repo for internal stuff    All of those repos work.    The repos are using a HTTP connector behind a F5 reverse proxy that terminates HTTPS. But all this is working so should not matter.    Now when I create a group and add the hosted repo and the DockerHub  proxy - everything still works.    However as soon as I add the RedHat repo the group repo seems to not  work anymore. I can no longer search or pull anything. The order of the repos does not seem to matter. But we desired order would be internal, redhat, dockerhub.    NXRM is running on RedHat with Oracle Java 8.    Docker client version used was 1.12"""
"NEXUS-11190","Bug","Support Tools",1,"java.nio.file.NoSuchFileException for inaccessible mounts prevents support zip generation","""It's not clear to me exactly what caused this, but this exception prevented a support zip from being generated on this ticket:     We should catch this exception, log it, and keep going with the system information retrieval.    Minimally we should catch this exception and allow the support zip generation to proceed, which appears to happen if a drive can't be queried for remaining size (dead mapped drive perhaps?)."""
"NEXUS-11219","Bug","Staging",2,"Drop Inactive Staging Repositories task aborts if a repository is not found","""If a staging repository cleanup task encounters a staging repository that is no longer on it's clean up list it aborts running completely.        It shouldn't do this.  Having this task complete is critical to end users who have large numbers of staging repositories, when the task doesn't run it can cause the entire instance to become unstable.  """
"NEXUS-11215","Bug","Repository",2,"valid .woff files fail Strict Content Type Validation with 400 response","""My Maven Plugin configuration is        When i execute the command          I'm getting the below error in the Maven build:          Nexus error log:      """
"NEXUS-11253","Bug","Smart Proxy|Yum",2,"repodata not updated in repository group after last staging repository is dropped or released","""I found out that Nexus Professional does not update Yum metadata after last staging repository present in repository group is dropped or released. Could you please report that to the development team?    Reproduction scenario on Nexus Professional 2.12.1-01:    1. Configure release repository for RPM packages (packages-el6 in our configuration) and a repository group grouping corresponding staging repositories (staged-packages-el6).   2. Configure a staging profile (packages-el6, id: 33ede191a55) that adds staged artifacts to the repository group and releases them to the release repository.   3. Configure Yum: Merge Metadata and Yum: Staging Generate Metadata capabilities for the repository group and staging profile.   4. Create and close two staging repositories containing some artifacts (packages-el6-1003 contained ns-repositories-4.1.0-1.el6; packages-el6-1004 contained ns-repositories-4.1.2-1.el6 and ns-repositories-4.1.3-1.el6).   5. Request repodata/repomd.xml from the repository group - this will merge metadata from the two staging repositories. The metadata should contain artifacts from both staging repositories.   6. Release the first staging repository (packages-el6-1003) and again request repodata/repomd.xml. The updated metadata should contain only artifacts from the remaining staging repository.   7. Release the second staging repository (packages-el6-1004) and request repodata/repomd.xml once more, so Nexus updates the metadata. Although the repository group is now empty, the yum metadata still contains artifacts from the last released staging repository.    """
"NEXUS-11283","Bug","Blobstore",2,"Blobstore counts inaccurate","""To replicate:  # Start Nexus 3, look at the blob store stats  # Migrate content from 2x, in a single repository (hard-linking)  # 832 changes are brought across  # Blob store stats show 2080 blobs (exactly 2.5x 832)  # In Nexus 3, delete the repository that was migrated  # Blob store stats now show 832 blobs (which is weird)  # Run the 'compact blobstore' scheduled task  # Blob store stats now show 832 blobs (still weird)  => They _should_ be zero."""
"NEXUS-11290","Story","NPM",2,"provide option to suppress merging metadata for the same npm package in different group members","""*Background*    Consider a hypothetical npm package 'bezor' version 1.0.0 (with no namespace), which lives in an npm hosted repo in Nexus.  Various client npm projects use this package, and the dependency version is """"~1.0.0"""" which signifies that the latest minor point release is acceptable.    The hosted repo is the first member of a group, which also contains a proxy for npmjs.org. One day, 'bezor' 1.0.5 shows up on npmjs.org, no relation to the hosted package.  Suddenly, builds start failing as the unrelated package is pulled down.    This is an inevitable consequence of a lack of namespacing with tilde-style dependency versioning, and it's also a blocker for a client adopting NX3 from a home-rolled npm repo.    *Acceptance*  * Administrators can set a system property that prevents npm groups from merging package metadata on a request-by-request basis.  * This only applies to the default namespace. Namespaced components continue to have their metadata merged, regardless of the system property's value.    If this property is set for the example, above, when a client requests metadata for package 'bezor' the client will only be shown metadata from the hosted repo. This contains 'bezor' 1.0.0 and nothing else.    *Notes*  This change won't affect the 'all' endpoint. Clients who request 'all' will still be able to see the multi-member merged metadata for a given package. We believe the exposure to this is minimal since the client doesn't use this to resolve dependencies."""
"NEXUS-11300","Bug","Docker",3,"Unable to connect to private repository on dockerhub","""Steps to reproduce:    * Set up a private repository on docker hub and add an image to it.  * Proxy Dockerhub, and add HTTP User authentication.   * Attempt to pull the image from your private repository, it fails:      If I add a basic auth header to the retrieve bearer token http request  here: https://github.com/sonatype/nexus-internal/blob/master/private/plugins/nexus-repository-docker/src/main/java/org/sonatype/nexus/repository/docker/internal/DockerProxyFacetImpl.java#L420 then the request succeeds."""
"NEXUS-11435","Story","Docker|Documentation",5,"Ability to clean up old docker images/layers from hosted repository","""There is currently no way to clean up old docker images from a hosted repository.    Docker images can be very large, and are often deployed in environments where access to older versions of an image is not necessary.    There should be a scheduled task to clean up old hosted docker images, and to also clean up layers which are no longer used by any hosted images.    *Acceptance Criteria*   * A user will be able to purge Docker images older than X days from a Docker hosted repository   ** Since Docker images share layers we can't remove all of the layers attached to the image, we have to remove them after we know they aren't used anymore   ** As a part of removing the old images, we should clean up all orphaned layers    NOTE:   * More info on how Docker is handling deletes: [https://github.com/docker/distribution/blob/master/ROADMAP.md#deletes]"""
"NEXUS-11301","Bug","Migration",1,"upgrade to Nexus 3 fails if Nexus 2 has no anonymous user defined","""If there is no """"anonymous"""" user in Nexus 2.x (that is, the user has been deleted and anonymous access is disabled) upgrade fails.    Nexus 3.1.0 log:        Nexus 2.x log:          """
"NEXUS-11518","Bug","Logging|Transport",0.5,"ProxyServiceException stack trace logged at WARN when remote responds with HTTP/1.1 401","""When the remote proxy repository responds with 401, a stack trace at WARN is on the logs.          This is not desireable. One does not need a full stack trace at WARN level for a 401 response from the remote.    Expected    Log a message at WARN level if the outbound request failed because the expected response code did not match the received response code, with a simple message showing expected code and received HTTP status line.  If the same Logger is at DEBUG levels, print the same message using the WARN logger, but also include the full stack trace.    """
"NEXUS-11616","Bug","Migration",3,"Upgrade does not rescan repositories on back and next","""I just noticed if I am Upgrading repositories and click Back to return to """"Repository Defaults"""", delete a repository and then click Next again, that repository is still on the list.  Similarly, while testing NEXUS-11530, I made a capability change that should impact the ability to Upgrade the repo but it still said un-Upgradable.    For the first case, if you click Next, it fails the Preview (with a 404).  In either case you can workaround by restarting Upgrade, however, this workaround will likely be unclear to anyone doing this.    Tested with Jelly SNAPSHOT and did not test older NX3 at this time, however, I have no reason to believe this works differently in 3.1."""
"NEXUS-11615","Bug","Browse Storage|UI",2,"Clicking on first (unlabeled) column in component browser gives an illegal argument exception","""If you click on the very first (unlabeled) column of the component browser you'll get an illegal argument exception.    The component browser is unusable after doing this until you sort by another column.    """
"NEXUS-11634","Bug","Content Selectors",1,"Search and Browse not handling leading slash properly for content selectors","""Very similar to NEXUS-11632 i have content selectors setup against   Which works great for content retrieval.    In order for search/browse to work, I need to also include a  (note the lack of leading slash) content selector"""
"NEXUS-11632","Bug","Content Selectors",1,"Content selector preview not handling leading slash properly","""Suppose i have some content in a docker repository (this most likely applies to other formats, this just happens to be the format i was using), and I want to restrict access to that content, I need to create content selectors that match against paths of """"/v2/blah"""".  These paths are now secured as i expect.    Problem is, when trying to test these queries using the content selector preview, I am forced to omit the leading slash to get any results.  We should be able to use the same path matching in both cases"""
"NEXUS-11639","Bug","Documentation",1,"legacy url mapping instructions are too confusing and not near enough to upgrade documentation","""The [Nexus Book chapter on legacy URLs|http://books.sonatype.com/nexus-book/3.1/reference/install.html#config-legacy-url] is a little confusing, I'm not sure I could understand it if I didn't already know what it meant.    I think we just need to say:  # Nexus 3 URLs are different than Nexus 2 URLs, here's an example of each  # If you want Nexus 3 URL to match what you were using in Nexus 2, then you need to do a few things:  #* Ensure the hostname, port, and context root are the same (how to do each of those is documented elsewhere)  #* Set this system property so that the repository URLs match Nexus 2.  # Please note, there's no repository directory browsing in Nexus 3.    ---  From NEXUS-11542 (merged into 1 effort):    Based on [this ticket|https://sonatype.zendesk.com/agent/tickets/16116]:     User could not locate how to enable legacy URL mapping.    While it is documented in the book - it is not in the context of the upgrade docs. The whole legacy URL enablement section here:    https://books.sonatype.com/nexus-book/reference3/install.html#config-legacy-url    Should directly be in the After Upgrade section of the book here:    https://books.sonatype.com/nexus-book/reference3/upgrading.html#_after_the_upgrade    Configuring the legacy url is only relevant for 2.x to 3.x upgrades - putting it in the generic section of the book does not make sense.    I realize there is a link out to it, but again, it is only relevant to upgrades, and should not be in the """"Configuring the Runtime Environment"""" section of the book."""
"NEXUS-11647","Bug","Logging",1,"Log spam...  HEAD requests for docker blobs that return 404 logged with warning","""Nexus 3.1.0 logs HEAD requests which are not found with WARN level, and """"Error"""" in the message:    {quote}  2016-11-16 21:26:55,574+0000 WARN \[qtp1729674283-259] someuser org.sonatype.nexus.repository.docker.internal.V2Handlers - Error: HEAD /v2/1114/sample-project/blobs/sha256:aa02b7a32ac3cc52451474c7b78f6aea3d2c20c92aff40dcc0c5244aea5ff3ad: 404 - org.sonatype.nexus.repository.docker.internal.V2Exception$BlobNotFound: blob unknown to registry  {quote}    It should not do this, this is not indicative of a problem.  For example, Docker will do a HEAD request before it does a put to see if a layer already exists:    {quote}  22.115.159.20 - someuser \[16/Nov/2016:21:26:55 +0000] """"HEAD /repository/1114/v2/1114/sample-project/blobs/sha256:aa02b7a32ac3cc52451474c7b78f6aea3d2c20c92aff40dcc0c5244aea5ff3ad HTTP/1.1"""" 404 0 19  22.115.159.20 - someuser \[16/Nov/2016:21:26:56 +0000] """"PUT /repository/1114/v2/1114/sample-project/blobs/uploads/0552bc82-77c6-4ee8-aa61-5c465968d4bf?digest=sha256%3Aaa02b7a32ac3cc52451474c7b78f6aea3d2c20c92aff40dcc0c5244aea5ff3ad HTTP/1.1"""" 201 0 229  22.115.159.20 - someuser \[16/Nov/2016:21:26:56 +0000] """"HEAD /repository/1114/v2/1114/sample-project/blobs/sha256:aa02b7a32ac3cc52451474c7b78f6aea3d2c20c92aff40dcc0c5244aea5ff3ad HTTP/1.1"""" 200 0 5  {quote}    We should log this at DEBUG, and remove """"Error"""" from the message.  """
"NEXUS-11670","Improvement","Audit",0.5,"show ldap server name instead of id in Audit context column","""While writing the audit documentation, I noticed that on the initial row in context, LDAP adjustments show ID not name.  ID isn't shown anywhere else in the UI, so this seems unhelpful for correlation.  You can see the name in the expanded row information so marking minor.    I didn't check older versions of NX3 however very little work has been done on this so I strongly suspect it's the same since implementation. NX2 doesn't have audit but has system feeds instead. I did not cross check against that either.    See attached, let me know if unclear."""
"NEXUS-11664","Story","Analytics|HA",2,"HA - add nodeId to analytics events","""The current analytics event data definition (which I believe is shared with some analytics tools hosted by OPS) doesn't include the nodeId where the UI event occurred. This story will look at adding nodeId to analytics events. Whoever works on this task should check with OPS to find out whether adding this new field will cause any issues with their downstream analytics tools.    Open questions:   * should we also look at versioning the analytics data definition?   * does the local sequence counter provide much value in HA?   * existing records should leave the nodeId unspecified"""
"NEXUS-11698","Bug","Docker",0.5,"browsing hosted docker repo may trigger UnrecognizedPropertyException Unrecognized field","""When browsing to attributes of the assets of a component in our hosted docker repo we get large stack traces like this as log spam. The UI and everything seems to work fine but there is a LOT of this filling up the log.    *********************  * Testing exception *  *********************  2016-11-23 18:00:09,578+0000 WARN  [qtp1616493320-1434] t841815 org.sonatype.nexus.repository.httpbridge.internal.ViewServlet - Service failure  com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field """"should_filter"""" (class org.sonatype.nexus.repository.docker.internal.V1SearchResult), not marked as ignorable (7 known properties: """"is_official"""", """"star_count"""", """"metaClass"""", """"name"""", """"description"""", """"is_trusted"""", """"is_automated""""])   at [Source: org.apache.http.conn.EofSensorInputStream@67527483; line: 1, column: 262] (through reference chain: org.sonatype.nexus.repository.docker.internal.V1SearchResults[""""results""""]->java.util.ArrayList[0]->org.sonatype.nexus.repository.docker.internal.V1SearchResult[""""should_filter""""])   at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:62) [na:na]   at com.fasterxml.jackson.databind.DeserializationContext.reportUnknownProperty(DeserializationContext.java:855) [na:na]   at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:1083) [na:na]   at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1389) [na:na]   at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1367) [na:na]   at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:266) [na:na]   at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:125) [na:na]   at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:277) [na:na]   at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:249) [na:na]   at com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:26) [na:na]   at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:490) [na:na]   at com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:95) [na:na]   at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:260) [na:na]   at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:125) [na:na]   at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3788) [na:na]   at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2849) [na:na]   at com.fasterxml.jackson.databind.ObjectMapper$readValue$5.call(Unknown Source) [na:na]   at org.sonatype.nexus.repository.docker.internal.V1GroupSearchHandler$_doGet_closure2.doCall(V1GroupSearchHandler.groovy:73) [na:na]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [na:1.8.0_101]   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) [na:1.8.0_101]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) [na:1.8.0_101]   at java.lang.reflect.Method.invoke(Method.java:498) [na:1.8.0_101]   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) [na:na]   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) [na:na]   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294) [na:na]   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1019) [na:na]   at groovy.lang.Closure.call(Closure.java:426) [na:na]   at org.codehaus.groovy.runtime.DefaultGroovyMethods.callClosureForMapEntry(DefaultGroovyMethods.java:5226) [na:na]   at org.codehaus.groovy.runtime.DefaultGroovyMethods.collect(DefaultGroovyMethods.java:3446) [na:na]   at org.codehaus.groovy.runtime.DefaultGroovyMethods.collect(DefaultGroovyMethods.java:3463) [na:na]   at org.codehaus.groovy.runtime.dgm$67.invoke(Unknown Source) [na:na]   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:274) [na:na]   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:56) [na:na]   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125) [na:na]   at org.sonatype.nexus.repository.docker.internal.V1GroupSearchHandler.doGet(V1GroupSearchHandler.groovy:72) [na:na]   at org.sonatype.nexus.repository.group.GroupHandler.handle(GroupHandler.java:79) [na:na]   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80) [na:na]   at org.sonatype.nexus.repository.security.SecurityHandler.handle(SecurityHandler.java:52) [na:na]   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80) [na:na]   at org.sonatype.nexus.repository.view.Context$proceed.call(Unknown Source) [na:na]   at org.sonatype.nexus.repository.docker.internal.V1Handlers$_closure17.doCall(V1Handlers.groovy:245) [na:na]   at sun.reflect.GeneratedMethodAccessor432.invoke(Unknown Source) [na:na]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) [na:1.8.0_101]   at java.lang.reflect.Method.invoke(Method.java:498) [na:1.8.0_101]   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) [na:na]   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) [na:na]   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294) [na:na]   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1019) [na:na]   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1084) [na:na]   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1019) [na:na]   at groovy.lang.Closure.call(Closure.java:426) [na:na]   at org.codehaus.groovy.runtime.ConvertedClosure.invokeCustom(ConvertedClosure.java:53) [na:na]   at org.codehaus.groovy.runtime.ConversionHandler.invoke(ConversionHandler.java:105) [na:na]   at com.sun.proxy.$Proxy115.handle(Unknown Source) [na:na]   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80) [na:na]   at org.sonatype.nexus.repository.view.handlers.TimingHandler.handle(TimingHandler.java:46) [na:na]   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80) [na:na]   at org.sonatype.nexus.repository.view.Context.start(Context.java:114) [na:na]   at org.sonatype.nexus.repository.view.Router.dispatch(Router.java:60) [na:na]   at org.sonatype.nexus.repository.view.ConfigurableViewFacet.dispatch(ConfigurableViewFacet.java:52) [na:na]   at org.sonatype.nexus.repository.view.ConfigurableViewFacet.dispatch(ConfigurableViewFacet.java:43) [na:na]   at org.sonatype.nexus.repository.httpbridge.internal.ViewServlet.dispatchAndSend(ViewServlet.java:198) [na:na]   at org.sonatype.nexus.repository.httpbridge.internal.ViewServlet.doService(ViewServlet.java:160) [na:na]   at org.sonatype.nexus.repository.httpbridge.internal.ViewServlet.service(ViewServlet.java:117) [na:na]   at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api:3.1.0]   at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:287) [com.google.inject:4.0.0]   at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:277) [com.google.inject:4.0.0]   at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:182) [com.google.inject:4.0.0]   at com.google.inject.servlet.DynamicServletPipeline.service(DynamicServletPipeline.java:71) [com.google.inject:4.0.0]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85) [com.google.inject:4.0.0]   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:112) [org.apache.shiro.web:1.3.2]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82) [com.google.inject:4.0.0]   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:112) [org.apache.shiro.web:1.3.2]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82) [com.google.inject:4.0.0]   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449) [org.apache.shiro.web:1.3.2]   at org.sonatype.nexus.security.SecurityFilter.executeChain(SecurityFilter.java:85) [org.sonatype.nexus.security:3.1.0.04]   at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365) [org.apache.shiro.web:1.3.2]   at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90) [org.apache.shiro.core:1.3.2]   at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83) [org.apache.shiro.core:1.3.2]   at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383) [org.apache.shiro.core:1.3.2]   at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362) [org.apache.shiro.web:1.3.2]   at org.sonatype.nexus.security.SecurityFilter.doFilterInternal(SecurityFilter.java:101) [org.sonatype.nexus.security:3.1.0.04]   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125) [org.apache.shiro.web:1.3.2]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82) [com.google.inject:4.0.0]   at com.sonatype.nexus.licensing.internal.LicensingRedirectFilter.doFilter(LicensingRedirectFilter.java:112) [com.sonatype.nexus.plugins.nexus-licensing-plugin:3.1.0.04]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82) [com.google.inject:4.0.0]   at com.codahale.metrics.servlet.AbstractInstrumentedFilter.doFilter(AbstractInstrumentedFilter.java:97) [com.codahale.metrics.servlet:3.0.2]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82) [com.google.inject:4.0.0]   at org.sonatype.nexus.internal.web.ErrorPageFilter.doFilter(ErrorPageFilter.java:63) [org.sonatype.nexus.base:3.1.0.04]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82) [com.google.inject:4.0.0]   at org.sonatype.nexus.internal.web.EnvironmentFilter.doFilter(EnvironmentFilter.java:97) [org.sonatype.nexus.base:3.1.0.04]   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82) [com.google.inject:4.0.0]   at com.google.inject.servlet.DynamicFilterPipeline.dispatch(DynamicFilterPipeline.java:104) [com.google.inject:4.0.0]   at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:133) [com.google.inject:4.0.0]   at com.google.inject.servlet.GuiceFilter$1.call(GuiceFilter.java:130) [com.google.inject:4.0.0]   at com.google.inject.servlet.GuiceFilter$Context.call(GuiceFilter.java:203) [com.google.inject:4.0.0]   at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:130) [com.google.inject:4.0.0]   at org.sonatype.nexus.bootstrap.osgi.DelegatingFilter.doFilter(DelegatingFilter.java:73) [org.sonatype.nexus.bootstrap:3.1.0.04]   at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1668) [org.eclipse.jetty.servlet:9.3.7.v20160115]   at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581) [org.eclipse.jetty.servlet:9.3.7.v20160115]   at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548) [org.eclipse.jetty.security:9.3.7.v20160115]   at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1158) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511) [org.eclipse.jetty.servlet:9.3.7.v20160115]   at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1090) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:119) [org.eclipse.jetty.server:9.3.7.v20160115]   at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:175) [com.codahale.metrics.jetty9:3.0.2]   at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:109) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:119) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.Server.handle(Server.java:517) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:308) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:242) [org.eclipse.jetty.server:9.3.7.v20160115]   at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273) [org.eclipse.jetty.io:9.3.7.v20160115]   at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95) [org.eclipse.jetty.io:9.3.7.v20160115]   at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:75) [org.eclipse.jetty.io:9.3.7.v20160115]   at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:213) [org.eclipse.jetty.util:9.3.7.v20160115]   at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:147) [org.eclipse.jetty.util:9.3.7.v20160115]   at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654) [org.eclipse.jetty.util:9.3.7.v20160115]   at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572) [org.eclipse.jetty.util:9.3.7.v20160115]   at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]"""
"NEXUS-11712","Bug","Repository",3,"RAR uploading fails - Detected content type ""x-zip-compressed"" but expected ""/x-rar""","""I tryed to upload a JCA RAR, but Nexus (OSS 3.0.2-02) throws the following error:    {quote}Failed to execute goal org.apache.maven.plugins:maven-deploy-plugin:2.4:deploy (default-deploy) on project jca.adapter: Failed to deploy artifacts: Could not transfer artifact de.jca:jca.adapter:rar:1.0.0 from/t  o nexus-deploy (http://x.x.x.x:8081/repository/releases/): Failed to transfer file: http://x.x.x.x:8081/repository/releases/jca/jca.adapter/1.0.0/jca.adapter-1.0.0.rar. Return code is: 400, Reason  Phrase: Detected content type [application/zip, application/x-zip-compressed], but expected [application/java-archive, application/x-rar-compressed, application/x-rar]: jca/jca.adapter/1.0.0/jca.adapter-1.0.0.rar.{quote}    This is how my pom file looks:  <project ...>   <modelVersion>4.0.0</modelVersion>   <artifactId>jca.adapter</artifactId>   <packaging>rar</packaging>     <dependencies ...>  </project>"""
"NEXUS-11726","Bug","Logging",0.5,"archived log file names should be named consistently","""By default the request.log and nexus.log files are archived every 24 hours.     nexus-yyyy-mmdd.log.gz  request-yyyy-mm-dd.log.gz    The file name formats are defined in logback.xml and logback-access.xml respectively.    Example from nexus-3.1.0-02:        There is a needless inconsistency. This also appears to be a regression - this was once fixed in : https://github.com/sonatype/nexus-oss/pull/1550/files     h3. Expected:    Change the name format of the nexus log to match the request log:        What happened to archiving the karaf.log? Any other log archives should be named similar to above pattern."""
"NEXUS-11739","Bug","Analytics|UI",2,"Clearing (analytics) Events on Page 2+ hides Events until browser refresh","""I noticed that if I have 2(+) pages of analytics events and I am on a page other than the first and clear, the pagination resets to 0 of 0 and I cannot see further events until a browser refresh.  Changing navigation or header refresh do not work.  This same behavior does not occur if you have 1 page or are on page 1.  See attached video.    I found if you were to wait until there was a page of events you were on it display (at page 1).    I didn't check older NX3 or NX2 at this time."""
"NEXUS-11733","Bug","Installer",0.5,"editing nexus-default.properties is not discouraged","""When you extract the nexus download archive, you notice a file at <install_dir>/etc/nexus-default.properties with uncommented property values.    Because this file is similar enough in name to nexus.properties ( from nexus 2 ) and the properties file we intend for them to customize is only created on startup at sonatype-work/nexus3/etc/nexus.properties , people are starting to edit the nexus-default.properties file instead of the nexus.properties file.    We keep seeing installations that have edited nexus-default.properties and not nexus.properties.  We want to discourage editing nexus-default.properties because the location of that file means that the custom values put there are not moved during upgrade, defeating the purpose of supporting a nexus.properties file in the work directory in the first place.    Acceptance criteria:     - keep nexus-default.properties  + add comments in the file explaining this file should not be edited   - ensure that comments about 'not editing' are not copied to nexus.properties    """
"NEXUS-11732","Bug","Migration",3,"prevent ClassCastException when handling StorageLinkItem during upgrade to Nexus 3","""Migration does not seem to handle finding a org.sonatype.nexus.proxy.item.DefaultStorageLinkItem in a maven2 hosted repo well. It throws a ClassCastException trying to cast it to StorageFileItem.        h3. Expected:    - migrate that item  - if an exception is thrown like this related to storage items migration, ALWAYS log the file name trying to be processed ( not sure if scouring code to ensure this does warrant a separate issue )"""
"NEXUS-11859","Bug","Migration",1,"Upgrade fails if Nexus 2 ""override local storage"" is not using a file URL","""If you have a repository in Nexus 2 which overrides local storage using a simple file path upgrade to Nexus 3 will fail.    {code:XML}    <localStorage>      <provider>file</provider>      <url>Z:\repositories\snapshots</url>    </localStorage>  {code}    The above is valid, the field takes either a file URL or a simple file path, see [here|https://github.com/sonatype/nexus-public/blob/477164f49c386e25a86c3866e4ac3a58c422b562/components/nexus-core/src/main/java/org/sonatype/nexus/proxy/storage/local/AbstractLocalRepositoryStorage.java#L137-L137].    But the upgrade fails with this:    """
"NEXUS-11869","Bug","Security|UI",2,"Repository Administration shows errors with limited privileges","""I made myself a role with nx-repository-admin-* and assigned to a user and when I logged in and navigated to repository admin, I got the attached error as well as the below from nexus.log.  I am suspect this is because I didn't have healthcheck privileges (as it says) and I know we want people to use healthcheck but I am skeptical we want them to use it so bad we throw errors down their throats.    This seemed familiar but I only saw one issue with this error and it was regarding anonymous access.    I didn't check NX2 at this time.  I checked also against NX3.1 and this is not recent regression.    Nexus.log snip:  """
"NEXUS-11868","Bug","UI",1,"Filtered items remain filtered after leaving the page but filter itself clears","""I noticed that when I filtered some privileges, moved to Roles and moved back to Privileges that the filter remained in effect but the filter field itself cleared.  This was momentarily confusing, I thought I might have erased something or been on the wrong page.  You can click the clear """"x"""" or refresh the browser as a workaround to restore the non-filtered list.  See attached vid.  I did not check vs NX2 at this time.  I did check 3.1.0 and this is not recent regression."""
"NEXUS-11872","Bug","Repository",1,"Deleting a repository with an item in it errors in nexus.log","""I deployed the attached pom to the default maven-snapshots repo and subsequently deleted it and later noticed the below error in the nexus.log.  I was able to repro this.  This does not occur when the snapshot repo is empty.    I did not check older NX3 or NX2 at this time however this reminded me of NEXUS-10759 where this doesn't occur (or at least the error is different) so there's either some subtly here or it's recent regression.  As with NEXUS-10759 the repo as far as I can tell does delete properly (definitely disappears from the UI and is able to be recreated), so marking minor for now despite the scary error.  Nothing bad shows on the UI side either.    """
"NEXUS-11874","Bug","Migration",1,"npm packages cannot be migrated due to IllegalStateException PackageVersion expected when contained in a rebuilt hosted repository","""Trying to migrate a npm repo from 2.14.1 to 3.1.    Error at Nexus 3.1 is      Error at Nexus 2.14.1 is  """
"NEXUS-11902","Story","REST",3,"REST Asset Search & Download Resource","""*Background*   This feature serves as a handy bridge between searching for things and downloading them. Having a download sub-resource means that clients can make simple HTTP calls for artifacts that may or may not exist, and download them without parsing any JSON.    *Acceptance*   * This sub-resource builds on NEXUS-14603 (asset search json api), but returns:   ** A 302 redirect to download the asset bytes   *** Rationale: This has the benefit of leaving a trail in build tools' logs that explains what specific artifact was suggested as the thing to download (e.g. in the case of a """"latest"""" search), rather than simply returning bytes.   *** It's fine to reuse existing format-specific content-serving endpoints, if that's expedient (as with the asset download sub-resource)   ** If there are no search results, HTTP 404   ** If the search isn't sufficiently precise to pick a winning asset (i.e. there's no sort order and there's more than one search result), then we should return HTTP 400 (client error)   ** Request logs should make it easy to tell whether users were searching for content vs. trying to download content (e.g. because downloading is a sub-endpoint or uses a 'download' param or whatever)    *Questions/Notes*   * Partial fetch/HEAD support is of course desirable, but that will be handled by the format-specific content bytes endpoint  """
"NEXUS-11888","Bug","Security",3,"http request header values should be checked for validity before allowing a request to proceed","""Certain HTTP request headers have well established value formats. Nexus should check the format of these header values and reject the request with a 400 response in such a case. Log a message in nexus.log in such a case at INFO level."""
"NEXUS-11911","Bug","Scheduled Tasks|Staging",5,"Stopping Nexus while a ""Drop Inactive Staging Repositories"" is running can lead to configuration inconsistencies","""Saw this in a customer log.  A """"drop inactive staging repositories"""" task was started:    {quote}  2016-12-09 18:00:02 INFO \[pxpool-1-thread-14\] *TASK com.sonatype.nexus.staging.internal.task.DropInactiveRepositoriesTask - Inactive repositories selected to drop: [build-50727, build-50728, build-50729, build-50730, build-50731, build-50732, build-50733, build-50734, build-50735, build-50736, build-50737, build-50738, build-50739, build-50740, build-50741, build-50742, build-50743, build-50744, build-50745, build-50746, build-50747, build-50748, build-50749, build-50750, build-50751, build-50752, build-50753, build-50754, build-50755, build-50756, build-50757, build-50758, build-50759, build-50760, build-50761, build-50762, build-50763, build-50764, build-50765, build-50766, build-50767, build-50768, build-50769, build-50770, build-50771, build-50772, build-50773, build-50774, build-50775, build-50776, build-50777, build-50778, build-50779, build-50780, build-50781, build-50782, build-50783, build-50784, build-50785, build-50786, build-50787, build-50788, build-50789, build-50790, build-50791, build-50792, build-50793, build-50794, build-50795, build-50796, build-50797, build-50798, build-50799, build-50800, build-50801, build-50802, build-50803, build-50804, build-50805, build-50806, build-50807, build-50808, build-50809, build-50810, build-50811, build-50812, build-50813, build-50814, build-50815, build-50816, build-50817, build-50818, build-50819, build-50820, build-50821, build-50822, build-50823, build-50824, build-50825, build-50826, build-50827, build-50828, build-50829, build-50830, build-50831, build-50832, build-50833, build-50834, build-50835, build-50836, build-50837, build-50838, build-50839, build-50840, build-50841, build-50842, build-50843, build-50844, build-50845, build-50846, build-50847, build-50848, build-50849, build-50850, build-50851, build-50852, build-50853, build-50854, build-50855, build-50856, build-50857, build-50858, build-50859, build-50860, build-50861, build-50862, build-50863, build-50864, build-50865, build-50866, build-50867, build-50868, build-50869, build-50870, build-50871, build-50872, build-50873, build-50874, build-50875, build-50876, build-50877, build-50878, build-50879, build-50880, build-50881, build-50882, build-50883, build-50884, build-50885, build-50886, build-50887, build-50888\]  {quote}    About a minute later Nexus was shut down:    {quote}  2016-12-09 18:01:07 INFO  \[WrapperListener_stop_runner] *SYSTEM org.sonatype.nexus.bootstrap.jsw.JswLauncher - Stopping with code: 0  2016-12-09 18:01:07 INFO  \[WrapperListener_stop_runner] *SYSTEM org.sonatype.nexus.bootstrap.jetty.JettyServer - Stopping  2016-12-09 18:01:07 INFO  \[WrapperListener_stop_runner] *SYSTEM org.sonatype.nexus.bootstrap.jetty.JettyServer - Stopping: org.eclipse.jetty.server.Server@695d7d60  2016-12-09 18:01:07 INFO  \[WrapperListener_stop_runner] *SYSTEM org.eclipse.jetty.server.Server - Graceful shutdown InstrumentedSelectChannelConnector@0.0.0.0:8080  2016-12-09 18:01:08 INFO  \[WrapperListener_stop_runner] *SYSTEM org.eclipse.jetty.server.Server - Graceful shutdown o.e.j.w.WebAppContext\{/nexus,file\:/usr/local/nexus-professional-2.12.0-01/nexus/},/usr/local/nexus-professional-2.12.0-01/nexus  {quote}    During shut down there are quite a few exceptions from the thread running the drop task.    And in the end the thread dropping the repositories is the last thing running, this is the end of the log:    {quote}  2016-12-09 18:01:17 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:17 WARN \[pxpool-1-thread-19] *TASK com.sonatype.nexus.staging.internal.task.RepositoryDropTask - One or more operations failed; aborting: com.sonatype.nexus.staging.StagingConfigurationException: org.sonatype.nexus.proxy.NoSuchRepositoryException: Repository with ID=""""build-50745"""" not found  2016-12-09 18:01:17 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:18 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:19 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  2016-12-09 18:01:20 INFO \[pxpool-1-thread-19] *TASK org.sonatype.nexus.configuration.ModelUtils - Saving model /usr/local/sonatype-work/nexus/conf/staging.xml  {quote}    After restart there are numerous inconsistencies between repositories in the nexus.xml and staging.xml files.  This resulted in the log getting enormous (20Gb), due to warnings, and also left a large number of staging repositories in a state where they couldn't be dropped.    """
"NEXUS-11910","Bug","NPM",2,"npm search against group repository fails with HTTP 500 due to not properly supporting /-/all resource","""Steps to reproduce:     * Create {{npm-internal}} (hosted) repository with *its own blob store*.   * Create {{registry.npmjs.org-proxy}} (proxy for https://registry.npmjs.org/) repository with *its own blob store*.   * Create {{npm-group}} repository containing the first two repos with *its own blobstore*.   * Update your {{.npmrc}} to set {{registry=http://<nexus host>/repository/npm-group}}   * Open a terminal and run {{npm search <some package> --verbose}}    The HTTP {{500}} response should be printed on the terminal and the following is observed in the Nexus logs:        It appears that the {{npm-group}} repository is attempting to find the {{npm-hosted/-/all}} asset *in its blobstore*.    I've confirmed something like this is happening by performing the steps outlined above but used _the same blobstore for all repositories_ and no error is observed."""
"NEXUS-11909","Story","Upgrade",3,"content requests to Nexus 2 by migration agent should avoid HTTP 404 Not Found caused by URL encoding","""*Background*  A number of customer bug reports have been traced to the fact that the NX2->NX3 migration REST API is using encoded slashes in requests. This causes problems with reverse proxies, and so far our recommendation is to fine tune the reverse proxy settings. This will continue to be problematic in the future, so let's adjust to eliminate the encoding.    The problem manifests itself when the HTTP download option is used during upgrade to Nexus 3, and Nexus 3 is sending requests for artifacts to Nexus 2. Nexus 3 sends requests to Nexus 2 with encoded slashes like   {{<base url>/service/siesta/migrationagent/repository-content/npmproxy/which%2F-%2Fwhich-1.0.9.tgz}}, those decoded slashes need to arrive at the Nexus 2 instance unaltered/undecoded in order for migration to work.    *Symptoms*    Requests by the Nexus migration agent to Nexus 2 will fail with 404 not found. Example stack trace from your nexus.log:        This can happen with any repository type.    *Acceptance*  * Remove slash encoding from the NX2 migration REST API so that we don't have problems with reverse proxies.  * Revert note added by https://github.com/sonatype/nexus-book-internal/pull/55 which will become obsolete    *Workaround*    - configure Nexus 3 to send requests to Nexus 2 directly instead of going through an intermediary reverse proxy  - configure the httpd in front of Nexus 2 with the directive {{AllowEncodedSlashes NoDecode}} and the option {{nocanon}} for your ProxyPass directive - do a similar change for nginx if using that server  - upgrade to 2.14.3/3.2.1 once released  """
"NEXUS-11925","Bug","Migration",2,"Nexus 2 to Nexus 3 upgrade may fail with NullPointerException Cannot invoke method extract while processing RepositoryChangelogResource","""I try to upgrade from 2.14.1-01 to 3.1.0-04. I selected single proxy repository and got the following:  !upgrade.JPG|thumbnail!    Nexus 2 log contains the following:        """
"NEXUS-11922","Bug","Documentation",0.5,"Book inaccurately states Group search criteria supports asterisk for pattern matching","""Description:   In Section 3.3.1 the book states """"Each criteria can be used with a search term and supports the * character (star, asterisk) for pattern matching. E.g., you could search with the *Group* search criteria and search for org.sonatype.nexus.*. This would return components with the group of org.sonatype.nexus, but also org.sonatype.nexus.plugins and many others.""""    But, applying the asterisk for the Keyword field seems to work.    Acceptance:  Update the example to Keyword search, instead of Group search.  """
"NEXUS-11937","Bug","Browse Storage",3,"privileges which allow reading repository content also expose all repository names when browsing assets / components ","""Anonymous user is assigned one role with only the following:  nexus:repository-view:maven2:maven-central:browse  nexus:repository-view:maven2:maven-central:read    There is a difference in behaviour between Nexus 3.0.2 and 3.1.0.    In Nexus 3.0.2 you only see maven-central in Browse Assets/Components.  !Browse 3.0.2.png!    In Nexus 3.1.0 you see all the repos in Browse Assets/Components. You do not see artifacts under repositories that you do not permissions for, but the issue is that the repository should not displayed   !Browse 3.1.0.png!    NOTE: This does not allow users to see the repository content, just the repository names."""
"NEXUS-11935","Story","Database|REST|Scheduled Tasks",3,"REST-Controlled Scheduled Tasks","""*Background*   Currently, there's a gap in terms of how NX3 backups can be orchestrated. We want admins to export the database (i.e. via the scheduled task), and keep the export files safe, and synchronize this with a blob store/rest of Nexus backup. This is tricky because the scheduled task is controlled inside Nexus, while blob stores are often backed up using an externally controlled rsync.    *Acceptance*   * There is a mechanism to list configured scheduled tasks, query their run status and invoke them (as if they were being manually invoked).    *Technical Notes*   * Perhaps the right way to do this is as part of beefing up the Provisioning API to cover scheduled tasks (including finding and triggering them).   * We'll defer configuration-specific endpoints (and this can currently be done with the provisioning API).   * Consider that some tasks may not come from our own plugins."""
"NEXUS-11947","Bug","Docker",0.5,"Deploying docker manifest which has unknown properties causes an entire docker package to become unusable.","""An end user uploaded some docker images using docker 1.13.0 RC which had manifests containing unsupported tags.    Now they can't seem to get out of the bad state, seems that deleting the bad manifests doesn't fix it, and they can't deploy new versions of this image using Docker 1.12.    {quote}  2016-12-15 14:51:06,613-0600 INFO \[qtp252865835-11141] snikifor org.sonatype.nexus.repository.storage.SingleAssetComponentMaintenance - Deleting component: Component\{metadata=AttachedEntityMetadata\{document=component#10:33165\{bucket:#9:9,format:docker,last_updated:Thu Dec 15 14:49:20 CST 2016,attributes:\[1],group:null,name:dock2box/centos7.2.1511,version:fb8c651} v1}, name=dock2box/centos7.2.1511, version=fb8c651, group=null}    2016-12-15 15:08:52,271-0600 WARN \[qtp252865835-11186] registry_svc org.sonatype.nexus.repository.docker.internal.V2Handlers - Error: HEAD /v2/stampede/centos7.2.1511/blobs/sha256:00e4abfa9a21665df632d0eaa741ba6dced8e8a3cf79722f85115d9d761125a9: 404 - org.sonatype.nexus.repository.docker.internal.V2Exception$BlobNotFound: blob unknown to registry  2016-12-15 15:09:05,293-0600 WARN \[qtp252865835-11201] snikifor org.sonatype.nexus.repository.docker.internal.V2Handlers - Error: GET /v2/stampede/centos7.2.1511/manifests/fb8c651  com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field """"comment"""" (class org.sonatype.nexus.repository.docker.internal.V2ManifestConfig), not marked as ignorable (11 known properties: """"container_config"""", """"container"""", """"config"""", """"architecture"""", """"os"""", """"created"""", """"docker_version"""", """"author"""", """"rootfs"""", """"metaClass"""", """"history""""])  at \[Source: java.io.BufferedInputStream@12f67dcc; line: 1, column: 36] (through reference chain: org.sonatype.nexus.repository.docker.internal.V2ManifestConfig\[""""comment""""])  at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:62) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.DeserializationContext.reportUnknownProperty(DeserializationContext.java:855) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:1083) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1389) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownVanilla(BeanDeserializerBase.java:1367) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:266) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:125) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3788) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2849) \[com.fasterxml.jackson.core.jackson-databind:2.7.1]  at com.fasterxml.jackson.databind.ObjectMapper$readValue$1.call(Unknown Source) \[na:na]  at org.sonatype.nexus.repository.docker.internal.V2ManifestUtilImpl.loadConfigurationFile(V2ManifestUtilImpl.groovy:209) \[na:na]  at org.sonatype.nexus.repository.docker.internal.V2ManifestUtilImpl$loadConfigurationFile$16.callStatic(Unknown Source) \[na:na]  at org.sonatype.nexus.repository.docker.internal.V2ManifestUtilImpl.mayDowngrade(V2ManifestUtilImpl.groovy:127) \[na:na]  at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:44) \[org.sonatype.nexus.transaction:3.0.2.02]  at org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl.getManifestByTag(DockerHostedFacetImpl.java:728) \[na:na]  at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:54) \[org.sonatype.nexus.transaction:3.0.2.02]  at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:53) \[org.sonatype.nexus.transaction:3.0.2.02]  at org.sonatype.nexus.repository.docker.internal.DockerHostedFacet$getManifestByTag.call(Unknown Source) \[na:na]  at org.sonatype.nexus.repository.docker.internal.V2Handlers$_closure14.doCall(V2Handlers.groovy:240) \[na:na]  at sun.reflect.GeneratedMethodAccessor296.invoke(Unknown Source) \[na:na]    {quote}  """
"NEXUS-11972","Bug","Database",3,"JobStoreImpl should skip over malformed records to allow nexus to start","""In the case where jobs end up malformed in the config database, as such:    3 item(s) found. Query executed in 0.003 sec(s).}}    and/or        The empty records (#28:3 and #42:3) caused an NPE which stopped nexus from starting:  """
"NEXUS-11965","Bug","NPM|Security",0.5,"npm install fails with 500 error when user has Group level privileges","""If a user has read privileges to a npm group, but not the underlying npm proxy, then the """"npm install <package>"""" will get a 500 response for any package that is not locally cached in the npm proxy.      Steps to reproduce:  1) Setup a npm group _npm-all_ that has npm proxy _npmjs_ as a member  2) Setup a user that has access to group repo _npm-all_ (privilege nx-repository-view-npm-npm-all).  3) Try to install a npm package that is not locally cached in  proxy _npmjs_     The npm install command will fail with something like the following:        Nexus logs will show the following:      *Workaround* If you give that user a read privilege to the proxy _npmjs_ (nx-repository-view-npm-npmjs-read) then the user should be able to install the package that is not already cached.    Group level permissions should be transitive, therefore use has the same privileges for all member repositories."""
"NEXUS-11988","Bug","NPM",3,"npm hosted repository package metadata tarball URLs incorrectly contain generated-on-request placeholder after upgrade","""NEXUS-11874 fixed an error which prevented some npm hosted packages from being migrated from 2.x to 3.x if the """"Rebuild npm hosted metadata"""" task had been run on the source repository.    Now an upgrade with Nexus Repository Manager 2.14.2 to 3.2.0, will migrate those hosted packages instead of failing the repository migration. However there is a new problem. When the package metadata for hosted npm packages are requested from Nexus 3, the tarball URLs may not be rewritten correctly and will contain invalid URLs which contain the internal placeholder term: """"generated-on-request"""".    1. Deploy an npm package to a Nexus Repository Manager 2.x npm hosted repository.  2. Schedule and run at least one """"Rebuild npm hosted metadata"""" task against the Nexus Repository Manager 2.x npm hosted repository.  3. Migrate the npm hosted repository to Nexus 3.2.0.  4. Request the root level package metadata of a migrated package. The tarball URLs will contain the term """"generated-on-request"""" instead of the expected tarball file name.    {noformat:title=Example bad metadata}  {""""maintainers"""":[{""""name"""":""""admin"""",""""email"""":""""my@example.com""""}],""""x-nx-rebuilt"""":""""2016-12-27T17:48:01.866-0400"""",""""keywords"""":[""""test1""""],""""dist-tags"""":{""""latest"""":""""0.0.1""""},""""versions"""":{""""0.0.1"""":{""""name"""":""""testproject1"""",""""version"""":""""0.0.1"""",""""description"""":""""Test Project 1"""",""""main"""":""""index.js"""",""""scripts"""":{""""test"""":""""echo \""""Error: no test specified\"""" && exit 0""""},""""dependencies"""":{""""commonjs"""":""""0.0.1""""},""""publishConfig"""":{""""registry"""":""""http://localhost:8081/nexus/content/repositories/npm-hosted/""""},""""keywords"""":[""""test1""""],""""author"""":{""""name"""":""""cstamas""""},""""license"""":""""ISC"""",""""readme"""":""""ERROR: No README data found!"""",""""_id"""":""""testproject1@0.0.1"""",""""_shasum"""":""""8bec15747a094d7720fc8a4d8e785b6ea5e23b85"""",""""_from"""":""""."""",""""_npmVersion"""":""""4.0.5"""",""""_nodeVersion"""":""""0.12.2"""",""""_npmUser"""":{""""name"""":""""admin"""",""""email"""":""""my@example.com""""},""""maintainers"""":[{""""name"""":""""admin"""",""""email"""":""""my@example.com""""}],""""dist"""":{""""shasum"""":""""8bec15747a094d7720fc8a4d8e785b6ea5e23b85"""",""""tarball"""":""""http://localhost:8091/repository/npm-hosted/testproject1/-/generated-on-request""""}}},""""name"""":""""testproject1"""",""""_rev"""":""""1"""",""""description"""":""""Test Project 1"""",""""_id"""":""""testproject1"""",""""readme"""":""""ERROR: No README data found!"""",""""time"""":{""""created"""":""""2016-12-27T21:34:40.348Z"""",""""modified"""":""""2016-12-27T21:48:01.870Z"""",""""0.0.1"""":""""2016-12-27T21:34:40.348Z""""}}  {noformat}    The invalid URLs will of course return 404 and therefore builds which previously worked using Nexus 2 will fail when used with Nexus 3.    h4. Expected    Package metadata migrated from Nexus 2 should render with proper tarball urls.    """
"NEXUS-11995","Bug","Upgrade",2,"Repository migration fails with ""com.fasterxml.jackson.databind.JsonMappingException: Invalid type marker byte 0xfa for expected field name (or END_OBJECT marker)""","""Upgrade fails with """"com.fasterxml.jackson.databind.JsonMappingException: Invalid type marker byte 0xfa for expected field name (or END_OBJECT marker)"""".    Initial investigation is showing that this may have something to do with migrating a proxy of the npm registry.    """
"NEXUS-11994","Bug","Maven",2,"SNAPSHOT requests are processed by Maven 2 repositories with policy Release","""h4. Nexus 2.x    1. Configure a Maven 2 proxy repo with policy RELEASE.  2. Add the Maven 2 proxy repo as a member to the public group.  3. Send the following request to a Nexus 2 public group containing a proxy repository member with policy RELEASE. Processing of the request is not handled by the RELEASE proxy repository as the path is detected as a SNAPSHOT request. Setting the logger {{remote.storage.outbound}} to DEBUG or processing the request with {{?describe}} proves this:    http://localhost:8081/nexus/content/groups/public/gov/utah/dws/erep/arch/base-pom/2.16-SNAPSHOT/maven-metadata.xml    h4. Nexus 3.2    1. Send the same request to a maven-public group with Release policy proxy repository ( ie. maven-central) as a group member and Nexus 3.2 sends the request outbound to the remote.  Setting the logger {{org.sonatype.nexus.httpclient.outbound}} to DEBUG proves this.    http://localhost:8081/repository/maven-public/gov/utah/dws/erep/arch/base-pom/2.16-SNAPSHOT/maven-metadata.xml      h3. Expected     For consistency - request paths that are detected as SNAPSHOT related in Nexus 2 should also be treated as SNAPSHOT related in Nexus 3 - unless Nexus 3 has an improvement to make over Nexus 2.     SNAPSHOT requests should not be processed AT ALL by RELEASE policy repository group members in Nexus 3.x, whether they are hosted or proxy repositories.    """
"NEXUS-11997","Story","Build|Documentation",1,"provide guidance on using Amazon EFS with Nexus 3","""*Background*  Customers want to use Amazon EFS with Nexus 3. Our performance testing has associated significant slowdowns with EFS, and ElasticSearch documentation recommends against it. (See below.)    *Acceptance*  * Update documentation and/or KB articles to make it clear EFS is potentially troublesome.  * Consider putting this on the Sonatype system requirements.        """
"NEXUS-12002","Bug","Maven|Search",2,"Maven publish indexes task fails to run if repository's layout policy is ""strict""","""Create a maven proxy repository in Nexus with the """"layout policy"""" of """"strict"""" (the default).    Then try and run a """"publish indexes"""" task agains it.  No outbound connections to the remote server will be seen.    Change the """"layout policy"""" to """"permissive"""".  Then run """"publish indexes"""" again.  This time it will work."""
"NEXUS-12010","Bug","Search|UI",2,"Logging in from top level search errors and hides admin menu button","""While on search either the top level """"Search"""" or one of the children (eg. """"Custom"""") if you login you get an error.  Login happens but subsequently (on post-login refresh), the admin menu button does not appear.  The error seems to be different depending on browser (I tried), but errors regardless.  See below.    This may seem familiar because of NEXUS-9087 but the result is different.  Since that issue was also noted in 3.2, I am filing distinctly for triage.  If they are found to be the same, we can merge them.  In my quick attempt, this prevents me from seeing NEXUS-9087 so I believe if not the same, this should be addressed first.    Workaround is to not login from search (from welcome or component browse).    JS Error from Chrome:      JS Errors from FF:  """
"NEXUS-12020","Bug","LDAP",3,"LDAP cache entries do not expire properly","""Map an LDAP user into Nexus 3, and log in as this user.    Change the user's LDAP password.  Don't log out of Nexus, instead restart the web browser.  Then wait 5 minutes for the cache entry to expire, and try the login again.    It won't work with the new password, you still need to use the old password.  Users have reported waiting over 24 hours, and still the old password works. It is necessary to manually clear out the LDAP cache in Nexus 3 to fix this.     Note that if you do log out of the UI the cache entry is cleared, and the new password will work the next time."""
"NEXUS-12017","Bug","Migration",2,"Upgrade fails with NPE if user has a group repository that contains only staging repositories","""A user has a group repository that contains only staging repositories:    {code:XML}      <repository>        <id>Test_Profile</id>        <name>Test Profile</name>        <providerRole>org.sonatype.nexus.proxy.repository.GroupRepository</providerRole>        <providerHint>maven2</providerHint>        <localStatus>IN_SERVICE</localStatus>        <notFoundCacheTTL>15</notFoundCacheTTL>        <userManaged>true</userManaged>        <exposed>true</exposed>        <browseable>true</browseable>        <writePolicy>READ_ONLY</writePolicy>        <indexable>true</indexable>        <localStorage>          <provider>file</provider>        </localStorage>        <externalConfiguration>          <memberRepositories>            <memberRepository>test-1001</memberRepository>          </memberRepositories>        </externalConfiguration>      </repository>    {code}    The upgrade wizard allows this group repository to be selected (see screenshot).    And then the upgrade subsequently fails with an NPE because the group repository cannot be found.     Note: I had to patch the server to add extra logging to find out what repository caused this problem, because the repository ID was not printed in the log.    """
"NEXUS-12030","Bug","Scheduled Tasks|UI",1,"Schedule task prompts for discarding changes after change of day","""I noticed that if I have a weekly scheduled task saved if I go in to edit the day it runs, when I save, the changes save however, the discard button is still enabled and when I try and navigate away I get prompted to discard.  If I do or click the discard button, it reverts the change, however, when I reload the task the change is saved.  Since the save works, am marking minor, however, it is pretty confusing.    Repro steps:  1) Create scheduled task (any afaik) filling in required fields.  For frequency select weekly, any start time and pick a day (any afaik).  Save.  2) Reselect the task (it goes to the list).  3) Edit the day from whatever you picked to another day.  Save.  BUG: Discard button is still enabled.  4) Click away, for example to """"Nodes"""".  BUG: Prompt occurs for discard or go back.    I didn't check older NX3 or NX2 at this time."""
"NEXUS-12036","Bug","SSL",3,"NXRM3 does not use its SSL truststore for redirects","""Configure Nexus to go through an http proxy server that rewrites remote site certificates using a private certificate.    Trust the remote certificate in Nexus ( add to SSL certificates UI as a trusted cert ).    When a request reaches the remote, the response may ask for a 302 redirect to a different host.    Nexus trusts the SSL certificate of the first host. When Nexus follows the 302 redirect, and gets a response, it will not trust the response and fail with PKIX errors as expected.    Go to the SSL UI, and add the certificate being returned from the second host manually using """"load from server"""" and the host name of the redirect (in my case this was dseasb33srnrn.cloudfront.net).    The outbound requests will still fail with SSL handshake errors because the truststore is not used for the 302 redirect.    Example remote hosts that perform 302 redirects as normal course are as [https://registry-1.docker.io|https://registry-1.docker.io/] or [https://www.nuget.org|https://www.nuget.org/] - so at least docker and nuget proxy repositories are affected.  h4. Diagnosis    Set the logger     org.sonatype.nexus.httpclient.outbound    to level    DEBUG    Repeat the build requests into nexus that cause this problem. The logger will show you the different hosts that outbound requests are being 3xx redirected to.     Compare the serial number of certificates already added in your SSL Certificates truststore to the certificates of all the redirected hostnames related to the PKIX error. If there are differences then you have verified this is the problem. A command to do this is:    h4. Expected    If the Use Nexus Truststore option is enabled for a repository, then all TLS based hosts that might be encountered for 302/301/307 redirects fetching content from a proxy repository remote URL should consult the Nexus truststore for certificate verification, even if the hostname and port is different from the initial request.  h4. Workaround    It seems you can avoid the problem by importing the HTTP proxy server root certificate or redirect host certificate into a custom truststore. Then Nexus will implicitly trust any remote signed by that certificate, including redirected hosts.    Follow the directions here: https://help.sonatype.com/repomanager3/security/configuring-ssl#ConfiguringSSL-OutboundSSL-TrustingSSLCertificatesUsingKeytool    1. Make a copy of JRE cacerts file   2. Import the certificates from the redirect hosts into it as trusted cert entries. Determine the redirect hosts by examining the DEBUG log statements from the org.sonatype.nexus.httpclient.outbound logger when the PKIX errors are reproduced inside of nexus.log   3. Specify to use the modified cacerts file as the JVM truststore for Nexus by specifying system properties inside bin/nexus.vmoptions.  h4. Known SSL Certificates That May Require Explicit Import Into A Custom JVM Truststore  h5. Nuget.org   - [https://az320820.vo.msecnd.net|https://az320820.vo.msecnd.net/] or [https://api.nuget.org|https://api.nuget.org/]    h5. Pypi   - [https://files.pythonhosted.org|https://files.pythonhosted.org/]    h5. Docker  - https://production.cloudflare.docker.com/  - cloudflare.net  - auth.docker.io"""
"NEXUS-12057","Bug","Scheduled Tasks",3,"Poor performance from Nexus 3 ""purge unused snapshots"" task","""The purge unused snapshot task runs very slowly against large numbers of components.    This has caused database query timeouts, and triggered a secondary bug in the handling of those timeouts:  NEXUS-12040    This may be similar to NEXUS-11614, so it's possible the fixes for that issue may be relevant to fixing this one.      """
"NEXUS-12040","Bug","Database",2,"Faulty handling of query timeouts in OrientAsyncHelper.QueueConsumingIterable","""https://github.com/sonatype/nexus-internal/blob/c341bcc9311c122cc17d51df0abb30920b2fe33f/components/nexus-repository/src/main/java/org/sonatype/nexus/repository/storage/OrientAsyncHelper.java#L184    The {{queue.poll()}} invocation there yields no exception but {{null}} when the timeout elapses. This causes the following issues:  # {{hasNext()}} returning {{true}} when in reality no element is available  # {{next()}} returning a non-deterministic number of {{null}} elements tripping up processing of the query results, e.g.    # {{next()}} failing with {{NoSuchElementException}} although the caller previously checked {{hasNext() == true}} ({{next()}} calls {{hasNext()}} again and can observe a different value than the caller, like the end-of-query sentinel), e.g.  """
"NEXUS-12064","Bug","NuGet",1,"JPQLGenerator.toJpqlLiteral NullPointerException for NuGet /Packages() resource as submitted by OctopusDeploy","""An end user has reported that OctopusDeploy submitted the following query to their NuGet repository in Nexus 3:        This results in an NPE (see below).  I'm not sure if something is wrong with this query or not, but even if something is wrong  Nexus should not get an NPE.    This problem can be reproduced by sending the above query to any NuGet repository in Nexus 3.2.    """
"NEXUS-12062","Bug","NPM",2,"If-Modified-Since header fails with NPM proxy repository","""The Nexus Repository OSS 3 (in Version 3.0.1-01) fails with the common HTTP header 'If-Modified-Since' for NPM repositories.    The official NPM registry does support this header, thus Nexus should also support it. (see example requests in the attachment)    Even when Nexus does not support a '304 Not Modified' response for this case, the normal '200 OK' response should be returned instead of a '404 Not Found'.    This header appears when for example a HTTP proxy is used between the NPM client and Nexus repository, that tries to do HTTP caching.    (I did not see any mentions of this in the Release Notes for the newer 3.x releases, thus I did not yet try the newer versions.)"""
"NEXUS-12073","Bug","Docker",2,"Pulling from Docker group generates error unless read access assigned directly to member","""Create a docker group repository, and put a docker hub proxy repository in this group.    Disable anonymous access in Nexus 3.  Then create a test user, and grant them read access to the docker group repository, but not to the docker hub proxy repository.    They will not be able to pull any docker images through the group, they'll get a 404 response every time. If you grant them direct read access to the docker hub proxy then requests through the docker group repository will start working.    I also tested this scenario with Maven repositories, and for those transitive privileges worked.    I've attached a support zip file with my setup.        """
"NEXUS-12081","Bug","Migration",1,"Upgrade never completes if source repository has zero length files in it","""If a source repository has zero length files in it upgrade never completes, the """"continue"""" button at the end of migration is never enabled.    Zero length files can be created in a Nexus 2 repository if the disk fills up, or the process runs out of file handles.      On the Nexus 3 side we see:        On Nexus 2 the request log shows it is asking for the zero length file over and over again:        Deleting the zero length files from repository storage in Nexus 2 allows the upgrade to continue.    h3. Expected    In most cases, a zero length file is not valid ( created due to out of disk for example, or some other error )    However we cannot guess why a zero length file exists. Maybe it is a marker file of some sort.    The consensus seems to be that zero length files should be migrated and Nexus 3 should handle them.    Add a single INFO log message per migrated component/asset on the nexus 3 side stating the full path and repo being processed, and that that path was zero-length.    Do not confuse a zero length file in nexus 2 with a failure to download all bytes from Nexus 2.  """
"NEXUS-12077","Bug","Repository|Transport",1,"Auto-blocked proxy repository logs gigantic stack trace, doesn't say what was blocked, or why","""When a repository is auto-blocked in Nexus 3 a very large stack trace is logged.  The log message does not indicate why it was auto-blocked, nor does it indicate what repository was blocked.  So it's pretty useless.  Furthermore, this WARN with the stack is logged every single time a request comes into the proxy repository, which fills up the log.    The log message should indicate which repository was auto-blocked.  It should only be logged when the repository is first auto-blocked.   The stack trace should be logged at DEBUG.    """
"NEXUS-12076","Bug","Upgrade",1,"Upgrade wizard checks for hard link capability even though a different ingest method is chosen","""If you choose the Download ingest method during migration, Nexus 3 still seems to perform filesystem checks for hardlinking. If the hardlinking is not possible, this fills the Nexus 3 log with stack traces for potentially every repo being migrated ( if the storage is not hard link capable).    h3. Expected    - do not perform hard-link checking if a different ingest method is chosen    h3. Symptoms:    Stack traces in the nexus 3 log look something like this for every repo:    """
"NEXUS-12075","Bug","PyPI",2,"PyPi packages are case sensitive and not correctly redirected","""Having uploaded all the Flask package to my PyPi repository and its dependencies I am unable to use easy_install to install Flask.    The following  {code:}  > easy_install Flask  Searching for Flask  Reading http://hostname/repository/my-pypi/simple/Flask/  Authenticating as admin for  http://hostname/repository/my-pypi/simple/Flask/ (from .pypirc)  Couldn't find index page for 'Flask' (maybe misspelled?)  Scanning index of all packages (this may take a while)  Reading http://hostname/repository/my-pypi/simple/  Authenticating as admin for http://hostname/repository/my-pypi/simple/ (from .pypirc)  No local packages or working download links found for Flask  error: Could not find suitable distribution for Requirement.parse('Flask')      will start to work as it can find the module as the search url has the lowercase """"f"""". This will however eventually fail when it tries to install the dependency of Jinja2 due to the same reason as above. i.e it is looking for Jinja2 with a capital 'J', as Jinja2 is supplied as a dependency it is not easy to change it to ask for the lowercase j.     Easy Install works when pointing to pypi.python.org.    Pip works ok and can deal with the case issues, but the requirement to use easy_install is due to the integration with setuptools which relies on easy_install.    This is using nexus 3.2.0-01.  """
"NEXUS-12091","Bug","Transport|UI",2,"HTTP Proxy host name setting accepts invalid characters such as space which can prevent server start","""The host name filed of Nexus 3 http proxy settings accepts spaces. It should not.    Worse, once you've set a host name with spaces in it it prevents startup of the server.    """
"NEXUS-12108","Bug","NPM",5,"npm@4: searching a hosted repo a second time causes npm error","""While reviewing npm@4, I noticed that I could search hosted repos and find results via the CLI (ie npm search --registry=http://localhost:8081/repository/npm-hosted/ whatever) however, after the first time I was getting an npm error not in the nexus.log but from npm.        Of note:  1) Resultant searches do show results (see above) before the error.  However, searches with no results JUST show the error which is confusing.  Marked minor because of this.  2) If you clear the cache (ie sudo rm -R ~/.npm) first this error does not occur.    This occurs in npm 4.0.0, 4.1.1 and 4.1.2.  It does not occur in 3.10.3.  I actually misanalyzed it as not occurring in 4.0.0 because I went from 3.10.3 to 4.0.0 without clearing cache (see #2 above).  This may be why this has not been seen much in the field.    I did not check older NX3 or NX2 at this time, however, because it's an npm CLI command, I doubt it'd make much difference.  I can check if desired."""
"NEXUS-12099","Bug","Upgrade",3,"gradually slowing upgrade of Nexus 2 site repositories to Nexus 3 raw repositories","""Migration of large Nexus 2 site repositories containing many small files synonymous of typical published Maven 2 html/css/js sites, may gradually slow to a crawl when migrating into Nexus 3 raw repositories.    The slowness was seen with a Nexus 2 site repository of 485000 files ( double that if you include the related ./nexus/attributes files ).  The slowness is expected to be present during future publishing to the Nexus 3 RAW repo, even if the upgrade is eventually completed successfully.    Here are sample threads taken from a thread dump where the slowness was detected to eventually reach approx 1 hard linked item migrated per second. No disk issues were noticed to slow the hard linking, so the assumption is there is slowness in the code doing the migration.    {noformat:title=WAITING threads}  plan-executor-8-thread-1 id=270 state=WAITING      - waiting on <0x21e1bb52> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)      - locked <0x21e1bb52> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)      at sun.misc.Unsafe.park(Native Method)      at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)      at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)      at com.google.common.util.concurrent.Monitor.await(Monitor.java:1082)      at com.google.common.util.concurrent.Monitor.waitFor(Monitor.java:738)      at com.google.common.util.concurrent.Monitor$waitFor$0.call(Unknown Source)      at com.sonatype.nexus.migration.plan.StepExecutor.run(StepExecutor.groovy:111)      at com.sonatype.nexus.migration.plan.StepExecutor$run.call(Unknown Source)      at com.sonatype.nexus.migration.plan.Plan.runSteps(Plan.groovy:253)      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)      at java.lang.reflect.Method.invoke(Method.java:498)      at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)      at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)      at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:384)      at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1024)      at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:69)      at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166)      at com.sonatype.nexus.migration.plan.Plan$_doBegin_closure6.doCall(Plan.groovy:238)      at com.sonatype.nexus.migration.plan.Plan$_doBegin_closure6.doCall(Plan.groovy)      at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)      at java.lang.reflect.Method.invoke(Method.java:498)      at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)      at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)      at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294)      at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1024)      at groovy.lang.Closure.call(Closure.java:414)      at groovy.lang.Closure.call(Closure.java:408)      at groovy.lang.Closure.run(Closure.java:495)      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)      at java.util.concurrent.FutureTask.run(FutureTask.java:266)      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)      at java.lang.Thread.run(Thread.java:745)        Locked synchronizers: count = 1        - java.util.concurrent.ThreadPoolExecutor$Worker@2a92b49    plan-executor-8-thread-2 id=271 state=WAITING      - waiting on <0x43ca5a69> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)      - locked <0x43ca5a69> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)      at sun.misc.Unsafe.park(Native Method)      at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)      at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)      at java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:403)      at java_util_concurrent_BlockingQueue$take.call(Unknown Source)      at com.sonatype.nexus.migration.repository.ProcessChangesStep.sync(ProcessChangesStep.groovy:260)      at com.sonatype.nexus.migration.repository.ProcessChangesStep$sync.callCurrent(Unknown Source)      at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52)      at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154)      at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:158)      at com.sonatype.nexus.migration.repository.ProcessChangesStep.doRun(ProcessChangesStep.groovy:147)      at com.sonatype.nexus.migration.plan.Step.run(Step.groovy:271)      at com.sonatype.nexus.migration.plan.Step$run$1.call(Unknown Source)      at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48)      at com.sonatype.nexus.migration.plan.Step$run$1.call(Unknown Source)      at com.sonatype.nexus.migration.plan.StepExecutor.runSync(StepExecutor.groovy:168)      at sun.reflect.GeneratedMethodAccessor351.invoke(Unknown Source)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)      at java.lang.reflect.Method.invoke(Method.java:498)      at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)      at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)      at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:384)      at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1024)      at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:69)      at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:166)      at com.sonatype.nexus.migration.plan.StepExecutor$_runAsync_closure1.doCall(StepExecutor.groovy:197)      at com.sonatype.nexus.migration.plan.StepExecutor$_runAsync_closure1.doCall(StepExecutor.groovy)      at sun.reflect.GeneratedMethodAccessor395.invoke(Unknown Source)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)      at java.lang.reflect.Method.invoke(Method.java:498)      at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)      at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)      at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294)      at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1024)      at groovy.lang.Closure.call(Closure.java:414)      at groovy.lang.Closure.call(Closure.java:408)      at groovy.lang.Closure.run(Closure.java:495)      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)      at java.util.concurrent.FutureTask.run(FutureTask.java:266)      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)      at java.lang.Thread.run(Thread.java:745)        Locked synchronizers: count = 1        - java.util.concurrent.ThreadPoolExecutor$Worker@3f5912e2    change-processing-9-thread-1 id=274 state=WAITING      - waiting on <0x7a65c73d> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)      - locked <0x7a65c73d> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)      at sun.misc.Unsafe.park(Native Method)      at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)      at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)      at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)      at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)      at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)      at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)      at com.orientechnologies.common.collection.closabledictionary.OClosableEntry.acquireStateLock(OClosableEntry.java:84)      at com.orientechnologies.common.collection.closabledictionary.OClosableLinkedContainer.acquire(OClosableLinkedContainer.java:267)      at com.orientechnologies.orient.core.storage.cache.local.OWOWCache.getFilledUpTo(OWOWCache.java:731)      at com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.getFilledUpTo(ODurableComponent.java:135)      at com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.readRecord(OPaginatedCluster.java:649)      at com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.readRecord(OPaginatedCluster.java:628)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.doReadRecord(OAbstractPaginatedStorage.java:3234)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.readRecord(OAbstractPaginatedStorage.java:2864)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.readRecord(OAbstractPaginatedStorage.java:1091)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx$SimpleRecordReader.readRecord(ODatabaseDocumentTx.java:3227)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.executeReadRecord(ODatabaseDocumentTx.java:1921)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.load(ODatabaseDocumentTx.java:649)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.load(ODatabaseDocumentTx.java:102)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearchRecord(OCommandExecutorSQLSelect.java:588)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.serialIterator(OCommandExecutorSQLSelect.java:1619)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.fetchFromTarget(OCommandExecutorSQLSelect.java:1566)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.fetchValuesFromIndexCursor(OCommandExecutorSQLSelect.java:2450)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.searchForIndexes(OCommandExecutorSQLSelect.java:2265)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.searchInClasses(OCommandExecutorSQLSelect.java:1001)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLResultsetAbstract.assignTarget(OCommandExecutorSQLResultsetAbstract.java:209)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.assignTarget(OCommandExecutorSQLSelect.java:530)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearch(OCommandExecutorSQLSelect.java:512)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.execute(OCommandExecutorSQLSelect.java:488)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelegate.execute(OCommandExecutorSQLDelegate.java:74)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:2577)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.command(OAbstractPaginatedStorage.java:2523)      at com.orientechnologies.orient.core.command.OCommandRequestTextAbstract.execute(OCommandRequestTextAbstract.java:69)      at org.sonatype.nexus.repository.storage.MetadataNodeEntityAdapter.findByProperty(MetadataNodeEntityAdapter.java:147)      at org.sonatype.nexus.repository.storage.StorageTxImpl.findComponentWithProperty(StorageTxImpl.java:367)      at sun.reflect.GeneratedMethodAccessor463.invoke(Unknown Source)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)      at java.lang.reflect.Method.invoke(Method.java:498)      at org.sonatype.nexus.common.stateguard.SimpleMethodInvocation.proceed(SimpleMethodInvocation.java:53)      at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)      at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:270)      at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)      at org.sonatype.nexus.common.stateguard.StateGuardAspect$1.invoke(StateGuardAspect.java:63)      at com.sun.proxy.$Proxy206.findComponentWithProperty(Unknown Source)      at org.sonatype.nexus.repository.raw.internal.RawContentFacetImpl.getOrCreateAsset(RawContentFacetImpl.java:131)      at org.sonatype.nexus.repository.raw.internal.RawContentFacetImpl$$EnhancerByGuice$$3286c0b4.CGLIB$getOrCreateAsset$2(<generated>)      at org.sonatype.nexus.repository.raw.internal.RawContentFacetImpl$$EnhancerByGuice$$3286c0b4$$FastClassByGuice$$af5c3f69.invoke(<generated>)      at com.google.inject.internal.cglib.proxy.$MethodProxy.invokeSuper(MethodProxy.java:228)      at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:76)      at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:45)      at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)      at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)      at org.sonatype.nexus.repository.raw.internal.RawContentFacetImpl$$EnhancerByGuice$$3286c0b4.getOrCreateAsset(<generated>)      at com.sonatype.nexus.migration.repository.migrators.RawHostedRepositoryMigrator.lambda$0(RawHostedRepositoryMigrator.java:47)      at com.sonatype.nexus.migration.repository.migrators.RawHostedRepositoryMigrator$$Lambda$149/1969362139.call(Unknown Source)      at org.sonatype.nexus.transaction.OperationPoint.proceed(OperationPoint.java:64)      at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)      at org.sonatype.nexus.transaction.Operations.transactional(Operations.java:200)      at org.sonatype.nexus.transaction.Operations.call(Operations.java:146)      at com.sonatype.nexus.migration.repository.migrators.RepositoryMigratorSupport.inStorageTx(RepositoryMigratorSupport.java:199)      at com.sonatype.nexus.migration.repository.migrators.RawHostedRepositoryMigrator.recordMetadata(RawHostedRepositoryMigrator.java:44)      at com.sonatype.nexus.migration.repository.migrators.RepositoryMigratorSupport.processChange(RepositoryMigratorSupport.java:135)      at com.sonatype.nexus.migration.repository.RepositoryMigrator$processChange$2.call(Unknown Source)      at com.sonatype.nexus.migration.repository.ProcessChangesStep$_submit_closure2.doCall(ProcessChangesStep.groovy:317)      at com.sonatype.nexus.migration.repository.ProcessChangesStep$_submit_closure2.doCall(ProcessChangesStep.groovy)      at sun.reflect.GeneratedMethodAccessor460.invoke(Unknown Source)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)      at java.lang.reflect.Method.invoke(Method.java:498)      at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)      at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)      at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294)      at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1024)      at groovy.lang.Closure.call(Closure.java:414)      at groovy.lang.Closure.call(Closure.java:408)      at groovy.lang.Closure.run(Closure.java:495)      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)      at java.util.concurrent.FutureTask.run(FutureTask.java:266)      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)      at java.lang.Thread.run(Thread.java:745)        Locked synchronizers: count = 1        - java.util.concurrent.ThreadPoolExecutor$Worker@2a8387f1    """
"NEXUS-12222","Bug","Migration|Security",2,"NXRM2 repository view privileges are not migrated to NXRM3 browse privileges during upgrade","""A role in Nexus 2 may be assigned a repository view privilege - an example entry would take the form:    {{<repoid> - (view)}}    Where repoid is replaced with the actual repository identifier. Example:    {{company_repo-repo - (view)}}    When such a role is migrated to Nexus 3 by the upgrade process, this view privilege is omitted, resulting in users assigned that role to not be able to browse the contents of the affected repositories.  ----  In attached screenshots the """"somegroup"""" role has view and read privileges to the """"somegroup"""" group repository.    After upgrade, the """"somegroup"""" role only has a read privilege. It should also have a """"nx-repository-view-<format>-somegroup-browse"""" privilege for this group.  h4. Workaround    If you have a small number of affected roles, then assign the missing privilege manually to the affected Nexus 3 roles.    The missing privilege takes the form of this naming pattern:    {{nx-repository-view-<repo-format>-<repoid>-browse}}    Example:    {{nx-repository-view-maven2-company_repo-browse}}    Or you can assign view privileges to all repositories by using:    {{nx-repository-view-\*-\*-browse}}    If you have a large number of affected roles, then we advise waiting until we fix this issue before upgrading.               """
"NEXUS-12216","Improvement","Docker",2,"Support pushing Docker Windows Container images and loosen manifest validation to allow for 'foreign-layers'","""I followed the instructions for setting up a Hosted Docker repository in Nexus. I'm using Windows containers on my Windows 10 machine. I created an image based off microsoft/nanoserver, which I try to push to the Hosted Docker repo in Nexus, but I am met with the following error: """"blob unknown: blob unknown to registry"""".     I switch Docker on my machine to use Linux containers, create an image, upload that image to the Hosted Docker repo in Nexus, and it succeeds.     My conclusion is that the Hosted Docker repo in Nexus is unable to store Windows Container images and there is no obvious place to configure the Nexus repo to host Windows Container images.    *Solution*  Minimally NXRM should be able to validate the pushed Manifest, taking into account that the 'foreign-layers' will never be in NXRM directly."""
"NEXUS-12230","Bug","Crowd|LDAP|User Token",2,"User token is deleted if external server cannot be reached","""An error occurred when trying to contact a Crowd server to look up the user ID associated with a user token:    {quote}  2017-01-17 17:59:19,293-0600 WARN \[qtp1976642001-174945] *UNKNOWN com.atlassian.crowd.integration.rest.service.RestExecutor - The following URL does not specify a valid Crowd User Management REST service: https://com.server.com/crowd/rest/usermanagement/1/user?username=USERNAME  2017-01-17 17:59:19,355-0600 WARN \[qtp1976642001-174945] *UNKNOWN com.sonatype.nexus.crowd.internal.CrowdUserManager - Unable to look up Crowd user USERNAME due to javax.xml.bind.DataBindingException/javax.xml.bind.UnmarshalException  - with linked exception:  \[org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Premature end of file.]  2017-01-17 17:59:19,355-0600 DEBUG \[qtp1976642001-174945] *UNKNOWN com.sonatype.nexus.usertoken.plugin.realm.UserTokenRealm - Removing stale user-token, target principals are no longer valid  2017-01-17 17:59:19,355-0600 DEBUG \[qtp1976642001-174945] *UNKNOWN com.sonatype.nexus.usertoken.plugin.internal.UserTokenServiceImpl - Removing record for: USERNAME  2017-01-17 17:59:19,371-0600 TRACE \[qtp1976642001-174945] *UNKNOWN org.sonatype.security.internal.UserIdMdcHelper - Set: USERNAME  {quote}    In response to this Nexus removed the user token.      Expected:    1. Nexus should not remove the token unless the server can be reached, and the user can't be found  2. Nexus should return a 401 if the server can't be reached.    """
"NEXUS-12238","Story","Content Selectors|Documentation|Maven",3,"Partial coordinates for maven metadata","""*Background*  1. A user creates a hosted repository, and a content selector permission based on a groupId, e.g. {{format == """"maven2"""" and coordinate.groupId =^ """"my.test""""}}  2. The user attempt to upload a .jar, via    3. Deployment fails:      The reason for this is that maven-metadata.xml files don't have any coordinates associated with them for the purposes of content selector evaluation. The workaround is that users can create a more complicated content selector that ORs the groupId with a path, but that's unnecessarily fiddly.    *Acceptance*  * maven-metadata.xml files have partial component coordinates assigned to them (just {{groupId}} and {{artifactId}}, I guess?) so that the scenario, above, works correctly and the client can update metadata."""
"NEXUS-12250","Bug","LDAP",0.5,"""Generic LDAP Server"" UI configuration template should not have password attribute set by default","""The """"Generic LDAP Server"""" template has a value in the """"password attribute"""" field.    Setting the password attribute is almost never needed, and in fact, it usually causes login failures.  We should remove this value from our Generic LDAP Server"""" template.      When the password attribute is not set an LDAP bind will be done, which is what you want 99.9% of the time.    """
"NEXUS-12242","Bug","Logging",1,"repository requests to paths containing certain characters may fail with status 500 ""Illegal character in path at index""","""*Acceptance*  Repository path parsing needs to be made more robust as currently it can fail for paths containing spaces. Secondly when a path cannot be parsed it should send back a 400 response (bad request) instead of letting the parse exception propagate all the way back to the view servlet.    *Example*  """
"NEXUS-12255","Bug","Documentation",1,"book mentions upgrade options that are not available","""https://books.sonatype.com/nexus-book/reference3/upgrading.html#upgrade-content    This section of the book implies that detailed parts of configuration, content and security can be selectively migrated to Nexus 3.    In fact that is not true anymore, due to the complexity that can entail. During initial upgrade testing and design, it was determined this level of flexibility was too error prone and potentially confusing for end users.    Now there are only two options:    - upgrade repositories and content ( and user accounts and associated security )  - Server configuration    The whole section of the book should be revised to match the current implementation.  """
"NEXUS-12254","Improvement","Documentation",0.5,"clarify that migrating repositories and content also migrates user accounts","""A user was recently surprised that Nexus 2 user accounts are also migrated to Nexus 3, even if the  """"Server Configuration"""" option was not checked during upgrade.    In fact migrating user accounts like this was deliberate, due to complex interactions that can occur between user accounts and custom roles and privileges possible in Nexus 2.    The book should clarify that migrating user accounts will happen, even if the Server configuration option is not checked.    """
"NEXUS-12253","Bug","UI",5,"Order by version incorrect","""Ordering components on a repository does not use numerical order but instead does string comparison.    Therefore when sorting I see:  packagename-1.2  packagename-1.10  packagename-1.0    instead of  packagename-1.10  packagename-1.2  packagename-1.0    This is such a simple change and yet has such a large impact on usability that it should just be done. Especially since it's been dragging along for quite a long time."""
"NEXUS-12276","Improvement","Repository",2,"reduce JVM heap memory consumed by group repository 404 not found responses","""AbstractGroupRepository collects any throwables thrown from its member repositories when requesting content and stores them in a map (memberThrowables) which it then propagates upstream inside a GroupItemNotFoundException. The stored throwables are only used when handling 'describe' requests.    For complex setups with lots of nested groups and many concurrent requests these throwables can end up taking a significant amount of heap, only never to be used because the requests are almost exclusively non-describe requests.    By skipping collection of these throwables for non-describe requests we should reduce pressure on the heap for all users (those with complex setups should see the most benefit)."""
"NEXUS-12285","Bug","Configuration|UI",0.5,"Remote Storage URL should be a required for proxy repository configuration","""Just noticed that you can freely create a proxy repository without entering a Remote Storage url.  Seeing as this is a rather important part of a proxy repository, feels like this should be a required field.    Note: JoeT also validated that was an issue in 3.2.0, so this is NOT a regression"""
"NEXUS-12304","Bug","NPM",2,"`npm publish` on an already published package version does not update all changed package metadata","""I use the Nexus Repository for private NPM repository with allow-redeploy enabled. And the problem is that the system is not deleting dependencies in package.json at same module version number republished.    For example:   I have published package at version 0.0.2-SNAPSHOT  http://localhost:8081/nexus/content/repositories/npm-private/fooPackage/    I will get the metadata descriptor:       Then i will change dependencies for version 0.0.2-SNAPSHOT to this      after npm publish I will get from http://localhost:8081/nexus/content/repositories/npm-private/fooPackage/        I replicated this bug at latest docker images sonatype/nexus:oss and sonatype/nexus3.    I know that republishing in NPM is considered as an anti-pattern. But you are allowing this option in private repo and it is very usefull, because we using the snapshot versions in development cycle at it is a bit pain to change the version everytime if there is any update in snapshot.    I'm using this as a workaround now : https://support.sonatype.com/hc/en-us/articles/221433608-Deleting-a-specific-npm-package-version-in-Nexus-Repository-Manager-2-x    Thanks"""
"NEXUS-12322","Bug","Documentation|Migration",0.5,"Storage consideration when upgrading from 2.x to 3.x","""The upgrade documentation should mention that you need double the storage as the data is duplicated when using file copy or downloading method to migrate.    [20.5.7. Data Transfer Methods|https://books.sonatype.com/nexus-book/reference3/upgrading.html#upgrade-methods]"""
"NEXUS-12339","Bug","NuGet",2,"Faulty result ordering for NuGet searches","""As found in NEXUS-12028, NuGet searches translate to db queries like    Note that the {{ORDER BY}} clause refers to the non-existing asset properties {{id}} and {{version}} instead of {{attributes.nuget.id|version}}.    At first glance, the cause might be https://github.com/sonatype/nexus-internal/blob/84a2d0537cb61ea16318e2748019f197bfb6b800/private/plugins/nexus-repository-nuget/src/main/java/com/sonatype/nexus/repository/nuget/odata/ODataUtils.java#L182 which doesn't consider the {{COLUMN_ALIASES}}."""
"NEXUS-12338","Bug","NuGet",1,"query parameter names for NuGet search requests are not case-insensitive","""Fallout from NEXUS-12028: The request log from that ticket suggests that NuGet searches employ case-insensitive query parameters, i.e. {{/FindPackagesById()?Id=foo}} vs {{/FindPackagesById()?id=foo}}. Our current code however expects case-sensitive query parameters (cf. https://github.com/sonatype/nexus-internal/blob/84a2d0537cb61ea16318e2748019f197bfb6b800/private/plugins/nexus-repository-nuget/src/main/java/com/sonatype/nexus/repository/nuget/odata/ODataUtils.java#L127). This leads to wrong database queries if the parameter case in the request doesn't match the code's expectations, e.g.  - {{select from asset where (LOWER(attributes.nuget.id) = :p0) and (bucket=#25:0) ORDER BY attributes.nuget.download_count DESC, id asc, version asc, parameters: \[p0=foo\]}} vs  - {{select from asset where (bucket=#25:0) ORDER BY attributes.nuget.download_count DESC, id asc, version asc, parameters: \[\]}} (missing id criteria)  """
"NEXUS-12337","Bug","NuGet",8,"NuGet queries against asset attributes can be slow due to non-optimized indexes","""Certain NuGet query/download/update requests from visual studio 2015 are reported to be slow in Nexus 3.1    Nexus 2.14 does not exhibit the slowness.    The following NuGet related log message from a customer indicates slow query performance:        asset.attributes is a distinct data structure from other properties like asset.name, asset.format etc. [ code reference|https://github.com/sonatype/nexus-internal/blob/e28b4254688e26ea6daae74b67ad85c3936db85e/components/nexus-repository/src/main/java/org/sonatype/nexus/repository/storage/MetadataNodeEntityAdapter.java#L53] - therefore this excludes this issue being a duplicate of NEXUS-12310 where a missing index on component name was the problem.     'attributes.nuget.id' is not handled with an index    Also there are no indices around attributes.nuget.id in AssetEntityAdapter, so these kind of queries are non-performant because they do table scans.    For NuGet, users can essentially write their own queries where they can do pretty much anything non-performant, but we should ensure the sunny day uses cases by common clients are supported in a performant manner."""
"NEXUS-12334","Bug","Logging",1,"Log spam when a user's session expires while viewing the repositories UI","""When you're looking at the repository list in the nexus UI it polls the repository status using this call:    {quote}  127.0.0.1 - - \[07/Feb/2017:12:38:36 -0500\] """"GET /service/extdirect/poll/coreui_Repository_readStatus?_dc=1486489116483 HTTP/1.1"""" 200 189 8  {quote}    When a user's session expires, the poll request fails. This is logged at ERROR level with a  stack trace.      This should not be logged at ERROR, this is a normal occurrence. -The message should be logged at INFO, and the stack trace logged at DEBUG.- The message should be logged at DEBUG with no stack trace (and should _definitely_ not be at ERROR level).     """
"NEXUS-12361","Story","CMA|Documentation|Scheduled Tasks",3,"Task to populate blobCreated dates and asset owner fields","""*Background*  Once we've built NEXUS-12360, we need a way to backfill data from the (hidden) blob creation dates and [asset owner fields|https://issues.sonatype.org/browse/PE-165] in the blob store.    *Acceptance*  * There is a way for an admin user to populate Asset.blobCreated from the blob created dates in the blob store.  * {{blobUpdated}} should be set to the same date (since we don't know if it's been updated since then).  * Since one-off, user-visible, version-specific upgrade tasks aren't good, consider rolling this behavior into a general metadata/blob store reconciliation task (e.g. the same task from NEXUS-11213).  * Update documentation to describe the new scheduled task/behavior of the scheduled task."""
"NEXUS-12360","Story","Browse Storage",3,"expose blob created and updated dates to avoid confusion with last updated date","""*Background*  The 'last updated' field of Asset is confusing users, who think it shows when the content arrived. It doesn't, it indicates when the record itself was last updated (just like the same field on Component).    The information they care about isn't actually available - NXRM has it, but it's buried in blob store metadata.    *Acceptance*  * Remove the 'lastUpdated' field from the Asset metadata summary. This is an internal field that tracks when the metadata record has changed, and users don't care about it.  * Add two new Date fields to asset:  ** {{blobCreated}} - touched when a blob is first attached to the asset  ** {{blobUpdated}} - touched on creation, and when a new blob (i.e. different hash) is attached to the asset  ** Rename {{lastAccessed}} to {{lastDownloaded}} for clarity (e.g. it refers to when the content was downloaded, not when someone last had a look at the asset metadata through search/browse)  * Add the two new fields to the Asset summary.  * Update the book documentation to explain the fields.  ** Currently the [field descriptions|https://books.sonatype.com/nexus-book/reference3/using.html#_search_results] don't add value.    *Notes*  * Looking ahead to fabric, these fields still are 1:1 with the Asset - multiple blob_refs don't require us to have multiple created/updated fields, because the expectation is that the blobs will all be identical (barring corruption). These dates represent when they arrived in NXRM, rather than in a particular blob store. (The blob stores will continue to record that.)    *Technical Notes*  * blobCreated and blobUpdated will need to be nullable for situations like NuGet assets that don't have blobs.  """
"NEXUS-12355","Bug","Maven",1,"MavenModels throws an IOException when attempting to parse an empty InputStream","""The Maven facet attempts to fill in information about a pom using the {{MavenModels.readModel}} function. If the supplied input stream is empty, this throws an exception and causes issues upstream when we (for example) attempt to migrate a zero-length pom or deploy one to a hosted repository (see stack trace of resulting HTTP 500 error below).    The javadoc for the method in question says that it should """"return \{@code null\} if input not parsable"""". It seems more consistent to return null when the input stream is empty instead of throwing an EOFException, but what would that cause unintended side-effects?    For reference, NX2 does not reject deployments of empty POMs.    """
"NEXUS-12367","Bug","Repository Health Check",1,"UI Danger error message when enabling RHC on a Maven Snapshot Repo.","""When clicking on """"Enable Health Check"""" on a Maven snapshot repository, the following """"Danger"""" error is displayed.    !Enable RHC on snapshot repo .png!    Although a RHC is not supported for Maven snapshot repositories, either a more informative message should be given or the """"Enable Health Check"""" button should be disabled."""
"NEXUS-12373","Bug","RubyGems",1,"parsing dates in some gemspec files could fail with IllegalArgumentException: Invalid format","""Nexus 3.2 is not able to serve the gem `barcode v2.0` via a rubygems proxy repo. It looks like it is having issue parsing a date.            actual barcode-0.2 gemspec:       This same request works in Nexus 2.14.2.    """
"NEXUS-12390","Story","Scheduled Tasks",3,"add a dry run mode to the Restore Asset/Component metadata from Blob Store task","""*Background*   The repair/rebuild task first introduced in NEXUS-11213 has the potential to make quite a few changes to data and/or blobs, especially in a post-restore situation.    *Acceptance*   * Add a mode to the rebuild/repair task that causes it to log the changes it would make, but not actually make any changes to blobs and/or metadata.   * Line items should still be made in a long-running task log for the run.   * Each task log entry should clearly indicate it was possible but not actually performed. (Some clear prefix?)"""
"NEXUS-12389","Story","Blobstore",3,"if the component database references a soft-deleted blob then prevent blob store compaction task from hard deleting the blob","""*Background*   An unwise sequence of backup and restore actions can produce a database that refers to missing blobs. (Example, drop a repository, restore the component database to an earlier state, start the system.)    *Acceptance*   * Blob store compaction does not delete blobs that are still referenced by component metadata.   * Mass deletions/compactions (e.g. dropping a repo) should still be performant.   * Make an entry in the long-running task log to record:   ** Every blob deletion that happens in compact   ** Every blob we choose NOT to delete due to a reference from component metadata"""
"NEXUS-12388","Story","Proxy Repository",3,"attempt to refetch proxy repository content from remote when a referenced local blob is missing","""*Background*   In the case of a damaged/ill-timed blobstore backup/restore, it's possible there are some missing blobs.    *Acceptance*   * If a blob in a proxy repo is missing, rather than immediately returning a 500 error, Repo attempts to refetch it.   * If the content is available in the remote, it is downloaded.    *Notes*   We should ensure we're only trying this for 'missing blob' situations that correspond to proxied artifacts. If it's a missing blob for cached metadata, there will be nothing to refetch."""
"NEXUS-12405","Bug","Crowd",2,"Crowd realm is missing 'Clear Cache' option like the LDAP realm has","""It appears the caching used in Shiro could allow for a 'Clear Cache' button for Crowd Realms, just as 'Clear Cache' is available for LDAP Realms.     The inability to manually clear credentials cached from Crowd is a problem when trying to workaround and caching problems.    This also relates to the configuration of realm specific cache timeout settings in: https://issues.sonatype.org/browse/NEXUS-9754, as if it is determined we are caching Crowd credentials via Shiro, then we should also allow configuration of the Crowd cache timeout."""
"NEXUS-12404","Bug","Crowd",0.5,"Crowd cache entries do not expire properly","""1. Create a Crowd Realm in Nexus 3. Add a user in crowd and verify you can login to Nexus with that user.    2. Change the Crowd users password via the Crowd admin pages.    3. Close the browser used to login to Nexus, restart the browser and attempt to login to Nexus. The old password will still work, and the new (current) password does not work win Nexus for the crowd user.    * Note that if you do log out of the UI the cache entry is cleared, and the new password will work the next time.      It appears this is related to how we use Shiro caching realms."""
"NEXUS-12397","Bug","Build",3,"nexus-public base template binary fails to start due to DependencyResolver$UnresolvedDependencyException","""While testing 3.2.1, I ran ./run.sh and forgot to designate pro or oss.  This ends up running base.  However, I noticed that my run did not succeed, it errored and shut down with the below.    On repro attempt, I did notice that if you run oss/pro first, it failed with different errors (I didn't write them down).  However, if you do a fresh build then run ./run.sh this seems reproducable.      Attaching full log as well in case I missed something.    As far as I know this is not intentional.  I am pretty sure I've run base in the past without modifications, however, it stopped being useful to me so haven't done so in a while.  If intentional, it'd be good to know what to do to get it working (as well as to know that).    Acceptance criteria:  the base template distribution will start without error"""
"NEXUS-12434","Bug","Bootstrap",1,"NEXUS-10154 is not fixed in our official docker images","""The fix for NEXUS-10154 was to add this system property to $INSTALL_DIR/conf/nexus.properties:        But the Nexus 2 docker images don't use our bootstrapper, so this file isn't processed.  Therefore these docker images are susceptible to NEXUS-10154.    We should modify our docker images to set this system property."""
"NEXUS-12443","Bug","Raw",1,"GET request to Nexus  Pro raw format group repository trigger 500 MissingFacetException No facet of type AttributesFacet","""in Nexus pro 3.2.1 with *a license installed that activates the IQ Quarantine support*:  1. Create a RAW proxy repository to https://services.gradle.org/ with default options.  2. Configure a RAW group repository with the RAW proxy as the lone member  3. Do not have IQ Server configured ( if configured may not matter, but it can happen if not configured).  4. send a GET request to the RAW group like this:    5. The request responds with:    6. The nexus log contains a stack trace such as this:        h2. Expected    - the zip file is downloaded and cached as expected through the proxy repo  - given the response reason phrase includes the exception message, might be worth verifying that response reason phrase text is parsed to confirm to the spec, which is _recommended_ up to 8000 octets long and a _required_ certain subset of characters.  https://tools.ietf.org/html/rfc7230#section-3.1 reason-phrase  = \*( HTAB / SP / VCHAR / obs-text ) - *Note: the reason phrase in this case appears to comply just fine.*    h2. Workaround    - request the file directly through the proxy repository instead, by passing the group repo, works fine.  - Nexus OSS does not have this problem          """
"NEXUS-12452","Bug","Bower|Security",1,"bower install fails when user has only group level privileges","""If a user has read privileges to a bower group, but not the underlying bower proxy, then the """"bower install <package>"""" will fail for any package that is not locally cached.    Steps to reproduce:  # Setup a bower group {{bower-group}} that has a bower proxy {{bower-proxy}} as a member  # Setup a user ({{boweruser}}) that only has view access to {{bower-group}} (i.e. {{nx-repository-view-bower-bower-group-*}})  # Try to install a bower package that is not locally cached in proxy {{bower-proxy}}  # Configure {{.bowerrc}} to use the {{boweruser}} user:      The bower install command will fail like the following:  """
"NEXUS-12450","Bug","Transport",2,"The ""https.protocols"" system property does not work for Nexus transport","""See here:    https://issues.apache.org/jira/browse/HTTPCLIENT-1595    The """"https.protocols"""" java system property does not work with httpclient 4.3.6, which is used by Nexus 2.14.3 as it's outbound transport.    This also affects clients built on the nexus-client-core, including (in particular) the nexus-staging-maven-plugin.  This means that if you've got builds running that require Java 7, and they are talking to a Nexus that uses newer TLS protocols the builds will not work.    We will need to bump up our httpclient version to fix this problem."""
"NEXUS-12458","Improvement","REST",1,"allow HTTP POST for uploading scripts to scripting API","""https://books.sonatype.com/nexus-book/reference3/scripting.html#scripting-configuration    In order to add a script into Nexus, you need to wrap the script in JSON first, then upload JSON using PUT.    This involves two steps:    1) package script as JSON  2) use a client like curl or wget to upload it    Wrapping a large script in json is cumbersome. You need to be familiar with JSON escaping rules.    It would be user friendly to allow POST *of* the groovy script as a file param as well as the other json payload attributes as POST parameters.    Idea from multiple customer feedback they would find it handy to avoid needing step 1.  """
"NEXUS-12457","Bug","NPM",2,"npm proxy receiving connection reset responds to client with status 500 instead of 404","""If an npm proxy repository receives a connection reset when connecting to it's remote a 500 response is returned by Nexus.  This is true even if the inbound request is made through a group repository.   Also, the proxy is not auto-blocked when this happens.      Moving the affected proxy to the bottom of an npm group repository does not provide a workaround, inbound metadata queries result in outbound queries to all members of the group.    Expected:    1. The proxy should not return a 500 response when the remote can't be reached, it should return 404, and log the connection problem.  2. The proxy should auto-block when it's remote can't be reached.    Testing note... I used """"cynic"""" to set this up, so I could quickly get the """"connection reset"""" error happening.  Note that """"cynic"""" does not run on Mac OSX, so it's necessary to use Linux if you want to use this method.    https://pypi.python.org/pypi/cynic/1.0"""
"NEXUS-12456","Story","NPM|UI",3,"add support for ""npm login"" bearer token authentication to proxied upstream NPM private repositories","""When configuring an NPM proxy repository """"HTTP Authentication"""" allows for """"Authentication type"""" of """"Username"""" (I guess that means HTTP Basic Auth) and """"Windows NTLM"""".   It would be useful to also support the NPM specific """"npm login"""" AKA """"npm Bearer Token"""" authentication method.    *Acceptance*   * NXRM administrators can configure npm proxy repos to use _npm bearer token_ the authentication method with the upstream repository"""
"NEXUS-12477","Bug","NPM|Repository Health Check",2,"Repository health check fails for angular/core npm package.","""Trying to run repository health check on npm package @angular/core/-/core-2.4.8.tgz results in a failure:    {quote}  Unable to identify component: Malformed npm component with path '/@angular/core/-/core-2.4.8.tgz'. If this is packageRoot, try selecting the tarball instead  {quote}    The log shows:    """
"NEXUS-12474","Bug","Database",8,"Last downloaded date is updated on upload","""We renamed the last_accessed field to last_downloaded as part of the work on NEXUS-12360, but the existing semantics don't entirely match what one would expect given the rename. In particular, there are a few situations where the Last Downloaded timestamp is updated when content is uploaded, and this doesn't match the expectations for it not to be.    Overall there are a couple of obvious solutions:    # We can rename the field (again) to something more neutral.  # We can stop updating this field for upload operations.    Given the expectations in NEXUS-12360 and conversations since then, I'm assuming we'll want to stop updating the field for upload operations and the like."""
"NEXUS-12486","Bug","Documentation|Scripting",0.5,"Book example fails to download required dependencies","""Looks like between 3.2.0 and 3.2.1 something in the dependency chain here has changed substantially enough to make this example fail. Looks suspiciously like one of the libraries we use was upgraded and somehow we no longer have concrete versions for several dependencies, therefore attempting to download 'working' versions of these libs. See attached output for more(captured with these params while running the script: -Dgroovy.grape.report.downloads=true -Divy.message.logger.level=4 ).  Likely will require updating the dependency info in the script, but not necessarily in a trivial way if the problem really is dependency poms missing concrete version info.    https://github.com/sonatype/nexus-book-examples/blob/055bfa74f6c2761e234834537c3d4e89bc06afb9/scripting/complex-script/addUpdateScript.groovy    """
"NEXUS-12485","Bug","Repository Health Check|Security",2,"add privilege that controls access to health check summary report","""I've granted every healthcheck and iq privilege there is to a user, and they still do not have permissions to view the healthcheck summary report.    It should be possible for a non-admin user to view both the summary and the detail health check report in Nexus 3.    This was tested in both Nexus 3.2.1 and 3.3.0-SNAPSHOT.    A support zip with my test configuration is attached, the test user has credentials """"test:test"""".    """
"NEXUS-12484","Bug","NuGet",1,"targetFramework attribute in NuGet nuspec file is rendered as Unsupported","""Nexus does not correctly parse the targetFramework attribute from the nuspec file when we push an artifact to the nuget feed hosted on Nexus. When we check the dependency attribute on the uploaded artifact, it shows """"Unsupported"""". E.g. the following values are shown in the dependency attribute:     NETStandard.Library:1.6.0:Unsupported|System.Collections.Specialized:4.0.1:Unsupported|System.Runtime.Serialization.Primitives:4.1.1:Unsupported|Microsoft.CSharp:4.0.1:Unsupported|System.Dynamic.Runtime:4.0.11:Unsupported|System.Reflection.TypeExtensions:4.1.0:Unsupported|System.ComponentModel.TypeConverter:4.1.0:Unsupported    When downloading the artifact with Nexus as a proxy, it works fine what means that there are no Unsupported values but the real version mentioned there.    For me, it looks like the functionality that was built in NEXUS-6158 is broken as it doesn't just take the information that is provided from the nuspec file but is replacing that by """"Unsupported"""" in the feed. Or is this expected behavior? It's not about having this information searchable via the API or so, it's about that it's just not there."""
"NEXUS-12483","Bug","Migration|NuGet",2,"invalid nexus 2.x NuGet repository files will cause nexus 3.x upgrade to fail with NullPointerException","""Migration to Nexus 3 failed with following NullPointerException in the Nexus 2 instance.  From the stack trace we can see that the migration of Nuget repo is involved, but unsure of which nuget repo or exact cause of issue:    """
"NEXUS-12482","Bug","Maven",2,"Inconsistent behaviour with upload to snapshot repository","""The following two snapshot paths are invalid and should really be rejected with 400 response.    com/sonatype/test/testapp/0.0-dev-SNAPSHOT/testapp-0.0-dev-1487857435-ecfcead.tgz   com/sonatype/test/testapp/0.0-dev-SNAPSHOT/testapp-0.0-dev-1488295749-d11e956.tgz    I am seeing inconsistent behaviour in both Nexus 2 and Nexus 3 when using a direct deploy method like below.    com/sonatype/test/testapp/0.0-dev-SNAPSHOT/testapp-0.0-dev-1487857435-ecfcead.tgz   * Nexus 2 uploads with 201 response   * Nexus 3 fails with 400 response    com/sonatype/test/testapp/0.0-dev-SNAPSHOT/testapp-0.0-dev-1488295749-d11e956.tgz   * Nexus 2 fails with 400 response   * Nexus 3 works with 201 response    h4. Expected    Nexus 2.x: We will not be making a change to Nexus 2.x codebase due to the potential regression risks related to users deploying or retrieving non-timestamped SNAPSHOT versions, which Apache Maven 2.x and Maven 1.x did allow.    Nexus 3.x: Nexus 3.x has dropped Maven 1.x support.    Nexus 3.x must allow deployments and retrieval of two types of SNAPSHOT versioning schemes:   - example-1.0-SNAPSHOT.jar ( deprecated and not recommended in industry )   - example-1.0-20171208.202054-1.jar ( preferred and modern )    All other invalid paths should be rejected for STRICT policy. Keep in mind there is a very strict layout for Maven 2 format repos.    The literal {{example-1.0-SNAPSHOT.jar}} form of file name is possible in Maven 2.x or versions of Apache IVY. Sonatype does not recommend using non-timestamped snapshot versioning schemes, and we do not optimize for that use case, however if you do it should work."""
"NEXUS-12481","Bug","Scheduled Tasks",1,"NullPointerException while rebuilding maven metadata if database operations timeout",""""""
"NEXUS-12488","Bug","SSL",3,"remote https repository with TLS client certificate loaded in NXRM JVM keystore not trusted","""When a remote repository requires SSL client certificate authentication, Nexus 3 (3.2.0-01) does not match the client certificate present in the keyStore. With SSL debugging enabled, it logs:  {{2017-03-01 16:38:02,934+0100 INFO  [qtp1053574947-168] adm_lop sun.security.ssl.ClientHandshaker - Warning: no suitable certificate found - continuing without client authentication}}  As a result, we can't see the client certificate being sent to the remote repository in a tcpdump.  In the end Nexus throws an exception:  {{java.io.IOException: Received fatal alert: handshake_failure}}    The keyStore and trustStore's are the same as the previous old version 2 (nexus-2.14.2-01) where it was working well. A test with SSLPoke with the same setting and stores works fine. I attached log files for the working SSLPoke and the non-working Nexus."""
"NEXUS-12496","Bug","Blobstore",1,"FileBlobStore error handling makes it impossible to see what blob causes a runtime exception","""A user has a snapshot removal task failing due to a problem with a file in the blobstore:        There is a problem with the creation timestamp:    https://github.com/sonatype/nexus-internal/blob/release-3.2.1-01/components/nexus-blobstore-file/src/main/java/org/sonatype/nexus/blobstore/file/internal/BlobAttributes.java#L117    But we only catch IOException, so it is now impossible to see what blob caused this:    https://github.com/sonatype/nexus-internal/blob/release-3.2.1-01/components/nexus-blobstore-file/src/main/java/org/sonatype/nexus/blobstore/file/internal/FileBlobStore.java#L378    Is there a reason not to catch """"Exception"""" there, and then wrap it up in """"BlobStoreException""""?  We need see what blob caused the issue whenever any exception is thrown.    """
"NEXUS-12508","Bug","Proxy Repository",2,"raw proxy repository negative not found cache does not cache correctly","""While testing caching, I checked NFC in the raw repository by performing the below steps.  I was unable to get it working.  npm did work in 3.2.1 using the same procedure so raw, at least more recently, just doesn't seem to be working.    Steps I took:  1) Create raw hosted repo  2) Create raw proxy repo (of #1). Have NFC timeout as 5 min (from default 1440 min).  3) Hit the url for package on proxy (e.g. http://localhost:8081/repository/rawproxy1/ticketlist.txt). Should return not found.  4) Push the file (I used curl; curl -v --user 'admin:admin123' --upload-file ./ticketlist.txt http://localhost:8081/repository/rawhosted1/ticketlist.txt).  5) Before 5 minutes hit the url for package on proxy (e.g. http://localhost:8081/repository/rawproxy1/ticketlist.txt). EXPECTED not found but BUG/Test Concern #1: It loads."""
"NEXUS-12513","Bug","Migration",1,"invalid nexus 2.x npm packages will fail the upgrade to nexus 3.x with 500 internal server error","""An invalid npm package causes the migration to Nexus 3 fail with a 500 error.        The cause of issue is shown in the Nexus 2 logs      Cause of the issue is the following file:    sonatype-work/nexus/storage/npm-internal/somecompany-stilguide/-rev/undefined    This should really not cause a failure. The migration should just log a warning and skip this invalid component.  """
"NEXUS-12523","Improvement","LDAP",2,"add support for more secure and salted LDAP PasswordEncoders such as SHA-256 SHA-384 SHA-512","""Failed to connect to LDAP Server: Password encoding: SSHA512 has no associated PasswordEncoder.    2017-03-22 16:49:37,575+0100 INFO UNKNOWN org.sonatype.nexus.ldap.internal.connector.dao.password.DefaultPasswordEncoderManager - Verifying password with encoding: SSHA512 (encoder: null)."""
"NEXUS-12520","Bug","Search",3,"assets visible by browsing are not available when searching due to non-optimized elasticsearch configuration rebuilding indexes","""Hi,  Nexus OSS 3.1.0-04, Linux    Some maven releases  artefacts are missing when performing a search but I can see them when browsing.    Steps to reproduce :  - In the top global search textfield, I type the name of the artefact : nxxxx, then I press Enter  <2 secs>  - I see 2 releases (1.0.3 and 1.0.4) and 3 snapshots  release 1.0.1 is missing (but I'm sure it exists as a mvn deploy complain it already exist and the repository is configured in no redeploy)    Notes:  - adding quotes around the searched keywork produces the same results  - Being logged-in or not has no effect on the issue    Then, I browse:  - I click on """"Components""""  - I select the (Release) repository of the artefact  - I insert nxxxx in the right-side filter  [very long search : 20 secs+]  - I can see all the three releases : 1.0.1, 1.0.3 and 1.0.4    Reading the doc, I pretty sure than I don't need to run the Nexus rebuild index task (as all artefacts are deployed through Nexus with mvn deploy) but in case off, I run it : sometimes missing artefacts in the search reappear, sometimes others disappear again, so I disabled it again.    What do you think ?    Thanks"""
"NEXUS-12535","Bug","UI",1,"Manage Privileges search dialog loses cursor focus","""The search dialog in the Manage Privileges view loses cursor focus on the search text box while completing the auto-filter action after a user has begun to type search criteria.    It is likely the user will need to continue entering their search criteria to further narrow the results, causing the user to have to go through a process of selecting the search text box and typing, possibly several times.    This behavior is inconsistent within other UI search dialogs and results in a frustrating user experience."""
"NEXUS-12540","Bug","Build",2,"Unable to Start Nexus-public","""I am following directions at [https://www.github.com/sonatype/nexus-public]. I am running on OS X with:            Nexus is unable to start with the following stacktrace:      Has anyone encountered this? Is there a way through?"""
"NEXUS-12655","Story","Backup",3,"HTTP requests which attempt modification should receive 503 when server is read-only mode","""(This should probably get broken into a couple of issues.)    *Acceptance*    During read-only mode, NXRM won't accept writes. In this situation, it would be ideal if:   * Rejected requests return 503 specifically (rather than other 500-level responses).   * The reason code should specify that the cluster is in read-only mode   * This covers REST endpoints, and repository write access"""
"NEXUS-12651","Bug","NuGet|UI",3,"Some Nuget targetFramework metadata does not have the same makeup as others","""While testing NEXUS-12484, I loaded https://www.nuget.org/packages/Elasticsearch.Net/ (5.2.0) into the default nuget.org-proxy.  I noticed the targetFramework metadata displayed link this:    {quote}  ::net45|::net46|NETStandard.Library:1.6.0:netstandard1.3|System.Collections.Specialized:4.0.1:netstandard1.3|System.Runtime.Serialization.Primitives:4.1.1:netstandard1.3|Microsoft.CSharp:4.0.1:netstandard1.3|System.Dynamic.Runtime:4.0.11:netstandard1.3|System.Reflection.TypeExtensions:4.1.0:netstandard1.3|System.ComponentModel.TypeConverter:4.1.0:netstandard1.3  {quote}    However, this seems to be missing the >= markup present in other packages.  For example, """"NETStandard.Library (>= 1.6.0)"""" is listed and we show """"NETStandard.Library:1.6.0:netstandard1.3"""" but I feel that should be """"NETStandard.Library:[1.6.0, ):netstandard1.3"""".    Compare with https://www.nuget.org/packages/AutoMapper/4.2.0  {quote}  ::portable45-net45+win8+wpa81|::net45|Microsoft.CSharp:[4.0.0, ):dotnet5.1|System.Collections:[4.0.10, ):dotnet5.1|System.Collections.Concurrent:[4.0.10, ):dotnet5.1|System.Collections.Specialized:[4.0.0, ):dotnet5.1|System.ComponentModel.TypeConverter:[4.0.0, ):dotnet5.1|System.Diagnostics.Debug:[4.0.10, ):dotnet5.1|System.Dynamic.Runtime:[4.0.10, ):dotnet5.1|System.Globalization:[4.0.10, ):dotnet5.1|System.Linq:[4.0.0, ):dotnet5.1|System.Linq.Expressions:[4.0.10, ):dotnet5.1|System.Linq.Queryable:[4.0.0, ):dotnet5.1|System.ObjectModel:[4.0.10, ):dotnet5.1|System.Reflection:[4.0.10, ):dotnet5.1|System.Reflection.Emit:[4.0.0, ):dotnet5.1|System.Reflection.Emit.ILGeneration:[4.0.0, ):dotnet5.1|System.Reflection.Extensions:[4.0.0, ):dotnet5.1|System.Reflection.Primitives:[4.0.0, ):dotnet5.1|System.Reflection.TypeExtensions:[4.0.0, ):dotnet5.1|System.Runtime:[4.0.20, ):dotnet5.1|System.Runtime.Extensions:[4.0.10, ):dotnet5.1|System.Text.RegularExpressions:[4.0.10, ):dotnet5.1|System.Threading:[4.0.10, ):dotnet5.1|System.Threading.Tasks:[4.0.10, ):dotnet5.1  {quote}    I did not check older NX3 at this time because of NEXUS-12484, I do not believe I could (would show as """"Unsupported"""")."""
"NEXUS-12680","Bug","Staging",2,"nexus-staging-maven-plugin does not work with SNI","""The nexus-staging-maven-plugin does not work with SSL enabled servers that use SNI.    This can be easily reproduced by setting the """"nexusUrl"""" parameter in the plugin configuration as follows:    {code:XML}                     <nexusUrl>https://nexus.opendaylight.org/</nexusUrl>  {code}    This results in:    {quote}  \[ERROR\] Failed to execute goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.8:deploy (injected-nexus-deploy) on project parent: Execution injected-nexus-deploy of goal org.sonatype.plugins:nexus-staging-maven-plugin:1.6.8:deploy failed: Nexus connection problem to URL [https://nexus.opendaylight.org/ ]: com.sun.jersey.api.client.ClientHandlerException: javax.net.ssl.SSLException: hostname in certificate didn't match: <nexus.opendaylight.org> != <logs.opendaylight.org> OR <logs.opendaylight.org> -> \[Help 1\]  {quote}      """
"NEXUS-12677","Bug","Proxy Repository",2,"proxy repository default negative cache size is too low to be effective","""Nexus 2 had a global not found cache of 100000 entries shared across all repositories.    When we suspected large customers were overflowing this 100000 max entry limit, we would ask them to up this to 1 million and this resulted in noticeable performance improvements ( throughput)    Nexus 3 has a per repository not found cache of 1000 entries.    In the majority of cases the Nexus 3 default cache size is  too low and forces Nexus to go remote more often than desired.      """
"NEXUS-12673","Story","UI",1,"display given roles in alphabetical order by name instead of arbitrary order","""The role definition screen appears to display the contained roles in an arbitrary order (maybe db insertion order). This makes them hard to grok, and is a usability regression from NX2.    Acceptance  * Order the roles in the role list alphabetically """
"NEXUS-12684","Bug","Docker",3,"HEAD request to /v2/<name>/manifests/<reference> results in 404 error","""When you specify a HEAD request to /v2/<name>/manifests/<reference> for an existing docker image in a Nexus docker repository, a 404 error will occur.    For example, I have tried to send a HEAD request to something like [http://host:8081/respository/dockerRepo/v2/test/alpine/manifests/latest] and I get back a 404.    However, if I try a GET request to [http://host:8081/respository/dockerRepo/v2/test/alpine/manifests/latest], I get back a response with the details that I am looking for.    In contrast, in a Docker registry, a HEAD request to [http://host:8081/v2/test/alpine/manifests/latest] works properly.    For more details about the API call, you can go to """"https://docs.docker.com/registry/spec/api/#pulling-an-image"""" and look for """"Existing Manifests""""    The HEAD request should be supported in Nexus docker repositories and not be a 404 error."""
"NEXUS-12693","Bug","UI",2,"Create wildcard privilege form does not redirect to list view on success","""Observed on master (d5e04b386).    1. Log in as admin.  2. Navigate to Security -> Privileges  3. Click Create privilege  4. Pick wildcard.  5. Enter valid name and pattern.  6. Submit ('create privilege').    Expect: to be taken back to the privileges list.  Observe: remain on Create wildcard privilege form. Form submit button still enabled.    There are 2 SecurityIT tests affected by this bug:    * Can add, edit, and delete wildcard privileges  * Can add, edit, and delete content selector privileges        """
"NEXUS-12691","Bug","Search|Security",5,"group privileges insufficient to search for member content","""Logging in as a registered user which has BROWSE and READ privileges to the GROUP containing several private repos (and also nx-search-read).     The user DOES NOT have BROWSE or READ privileges to the individual group repository member repos in the GROUP, only the GROUP repository  (which works fine as maven2 repos).     Doing a search through the GUI does not yield any result at all, which is not what I expected.    The user can browse to the component and download, but is unable to search this component    Acceptance criteria:    * If the user has BROWSE permissions to the group, they should be able to search for components in the member repositories of that group (even if they don't have explicit permissions for those repositories). They should also be able to view the component/asset details,  though not download the assets (that requires the READ permission).  * Make sure that component-level permissions are still applied."""
"NEXUS-12711","Bug","Docker",3,"Nexus docker registry delete REST api partially deletes an image","""Deleted a docker image through docker registry REST api. The image was deleted successfully from Nexus UI, but still lists in the GET /v2/<name>/tags/list API.   No workaround exists.    Steps:  *Get image digest:*        From response header:  Docker-Content-Digest: sha256:0c2e8350966123c27427152c66024d280e8d60a01ca37b3bac84b61882b6f832    *Delete image*      get response: HTTP/1.1 202 Accepted    At this point image has been deleted from Nexus UI. But below list tags call still return the version        """
"NEXUS-12716","Bug","NPM",3,"NullPointerException when running npm search","""When trying npm search it fails with a NullPointerException.    I have reproduced this issue on 3.2.1. It seems to be related to searching across multiple blob stores. The npm search is run using the npm-group    npm-group using npm-group-blobstore  npm-host using npm-host-blobstore  npm-proxy using npm-proxy-blobstore    When all three are on the same blobstore, I am unable reproduce.    """
"NEXUS-12780","Bug","Scheduled Tasks",3,"task scheduler threads may deadlock at QuartzTaskJob.mayBlock() when more than 20 blocking tasks are encountered","""During some stress testing earlier, I added 100 Maven2 proxy repos and enabled """"IQ: Audit and Quarantine"""" on them. Upon restarting Nexus, I was presented with 100 """"Repository Audit"""" tasks that wouldn't move forward.    From the thread dump:    Per default configuration, 20 threads are allocated for Quartz, all of them are stuck like that in {{QuartzTaskJob.mayBlock()}}."""
"NEXUS-12794","Bug","Repository Health Check|Transport",1,"Repository Health Check RHC only detects HTTP proxy server settings changes on server restart","""1. Start up Nexus 3.2.1/3.4.0  2. Run repository health check against one repository  3. Configure an HTTP proxy server in Nexus  4. Run repository health check against another repository ( manually running the Health Check task is equivalent )  5. Problem: The second health check request will not be routed through the HTTP proxy server.  It is necessary to restart the server in order to fix this.    1. Remove an HTTP proxy server from Nexus  2. manually run a Health Check task  3. Problem: Requests to rhc-pro.sonatype.com or rhc.sonatype.com continue to try an go through the previously configured HTTP proxy server that was deleted.    h4. Expected    The repository health check feature should detect all relevant HTTP proxy server changes and use them immediately, it should not be necessary to restart the server or make other configuration changes.    h4. Workaround    After changing HTTP proxy server settings, a Nexus restart is required in order for Repository Health Check to detect and use the correct HTTP proxy server settings.  """
"NEXUS-12793","Bug","Logging",1,"if java.lang.Error is thrown during request processing it may not be logged at default log levels","""A report was made recently where an NPM package deployment was failing with a 500 response.    Analysis was performed on the Nexus 3 logs    {noformat:title=request.log}  172.17.25.121 - admin [22/Mar/2017:23:11:27 +0000] """"PUT /repository/uwp/@itg%2fwebstack HTTP/1.1"""" 500 0 4624 """"npm/4.4.1 node/v6.9.5 win32 x64""""      After changing the ROOT logger level to DEBUG the problem was reproduced and it was determined an OutOfMemoryError was being thrown:        h3. Expected    - Nexus should ALWAYS attempt to log java.lang.Error derivatives at ERROR log levels at the shallowest stack depth  - logging java.lang.Error should not depend on some higher level log statement  - one should not have to enable DEBUG log levels to notice an OutOfMemoryError  """
"NEXUS-12828","Bug","Scheduled Tasks",3,"submitting more than 20 tasks at once causes ERROR QuartzSchedulerThread - ThreadPool.runInThread() return false! for some tasks","""Submitting considerably more than 20 tasks at once causes a rejection error for some tasks. These tasks do not run, instead they hang in a """"Waiting"""" state. You can see an error in the logs reported from quartz:    To reproduce, you can run the test here: [https://github.com/sonatype/nexus-internal/pull/1671/files#diff-f47ab1796bb11e077d08f418a8fcf42e]    Changing the maximum tasks to 50 instead of 21.    Also, you can use this groovy script to schedule a large number of tasks to run at once:     """
"NEXUS-12844","Improvement","Maven",1,"Upgrade Apache Tika dependency to 1.14","""As part of investigating 10087 I found there was a slight improvement in the behaviour of Tika:         1.14 still wrongly identifies as text/html if the xml contains any tag starting with {{<html..}}, however if you have a comment at the start of your xml file, like the pom in question (Note: works even if it's empty {{<!-- -->}}) then it correctly identifies as text/xml.         To test:   # Create a proxy repo called """"RSO"""" and point it to [https://repository.sonatype.org/service/local/repositories/sonatype-internal/content/]   # Request [http://localhost:8081/repository/RSO/com/sonatype/insight/ci/insight-ci-parent/2.14.4/insight-ci-parent-2.14.4.pom]    # You should be presented with a POM rather than a 404.     """
"NEXUS-12849","Story","REST",2,"When a REST API request is made for assets or components in a repository that a user cannot access, the appropriate response code should be returned","""Currently if a user makes a REST API request for components or assets in a repository they cannot access, they get an empty set. They should probably receive an error code. This occurs with the following resources:   * GET {{/service/siesta/rest/v1/assets}}   * GET {{/service/siesta/rest/v1/components}}    Technical note: What error code do we want to return? 403 vs 404."""
"NEXUS-12856","Bug","Docker-Nexus",0.5,"Docker-nexus3 - readme incorrectly describes how data volumes are used","""readme contains:  {quote}""""Since data volumes are persistent until no containers use them,""""  {quote}  This statement appears wrong.    Data volumes are not deleted when no container uses them. In fact data volumes are never removed by docker when containers are deleted.    From [https://docs.docker.com/engine/tutorials/dockervolumes/#data-volumes] :  {quote}Data volumes persist even if the container itself is deleted.  Data volumes are designed to persist data, independent of the container’s lifecycle. Docker therefore never automatically deletes volumes when you remove a container, nor will it “garbage collect” volumes that are no longer referenced by a container.  {quote}"""
"NEXUS-12855","Bug","Webhooks",0.5,"Only one capability of type 'Webhook: Repository' can be created","""Users are prevented from creating more than one Repository webhook by the Capability rules, but need to be allowed to create for multiple Repositories."""
"NEXUS-12869","Bug","NPM",3,"npm publish a large package may cause java.lang.OutOfMemoryError: Java heap space when parsing the JSON payload","""If one uploads a large npm package using npm publish, this could cause Nexus to run out of memory as it parses the POSTed JSON payload.         Reproduce with the following:    Artificially reduce the Nexus 3 max heap size to 256mb    Artificially create a large npm package to publish:   # Create a 25mb file ([http://unix.stackexchange.com/q/33629])   head -c 25M </dev/urandom > big.file   # Include big file in package.json   $ cat package.json | grep files   """"files"""" : [ """"big.file"""" ]         Attempt to npm publish while the Nexus 3.3 and below has the logger     at DEBUG.         You will see:            h3. Expected   - large npm package uploads should not cause Nexus to run out of memory - stream the JSON instead of loading it entirely into memory"""
"NEXUS-12907","Bug","Upgrade",3,"Upgrade from 2.x to 3.x Hangs on Group Repos containing only Staging Repos","""My Nexus 2.14.4 has 2 group repositories containing only Staging repositories. Going through the upgrade steps in Nexus 3.3.0, I can select those groups, but the Staging Repos are not able to be selected. Also, I have an empty group, which is not able to be selected. See Screenshot. !image-2017-04-13-10-29-26-293.png!         During the Synchronizing step, it processes 19 of the 21 repositories, and the """"Process Changes"""" completes and waits for changes on the Nexus 2 side, but the last 2 Repositories never complete """"Scanning"""". The message in the log is shown below as well as screenshot.    Rerunning the upgrade without selecting these two groups is successful.    !image-2017-04-13-10-31-52-430.png!"""
"NEXUS-12972","Bug","Content Selectors|Logging",1,"Warning in log seen when using content selector with ""coordinate.extension""","""Define a JEXL expression for a content selector like this:       {code:java}  format == """"maven2"""" and coordinate.extension == """"war""""  {code}       Hit the """"preview"""" button in the content select or editor.  Observe that this works, only war files are shown in the preview.         But the nexus.log is filled with warnings:       """
"NEXUS-12968","Bug","Logging",2,"Uninformative log message in ProxyFacetSupport - Content not present for throwing exception","""Nexus 3.3 may log messages such as these:         Where did it throw the exception? What was the exception? """"Content not present for"""" means nothing. What content where? On disk? In an http response from the proxy? Headers?    These log levels MAY have been overridden at the time in logback-overrides.xml, but these loggers should not prevent seeing a more useful log message:          """
"NEXUS-12962","Bug","Docker|Scheduled Tasks",3,"Exception while executing ""Purge unused docker manifests and images"" task","""I have created a task of the type """"Purge unused docker manifests and images"""" and when I execute it I always get this exception:      Before launching the task i have deleted some image from components UI"""
"NEXUS-12942","Bug","Repository",2,"possible to create a repository without a valid blobstore reference","""# In browser window A, create a blobstore named test   # In browser window B start creating a repository that refers to blobstore test, but do not save it yet   # In browser window A, delete the blobstore test   # In browser window B, save the repository. It will save successfully, referring to non-existent blobstore test   # Try to delete the repository, you cannot - the delete will fail because the blobstore does not exist    The same thing can be done with a groovy script when calling RepositoryManager.create(config) API directly. There is not validation on save of a repo that a blobstore exists by that name.    Example groovy script that can also put Nexus in this state:       h3. Workaround    In order to fix the invalid state:   * create blobstore name that matches the name referenced by the repo   * *restart Nexus* ( without restart, repo delete still fails with)            * delete repository   * delete blobstore"""
"NEXUS-12983","Bug","NuGet",3,"NuGet FindPackagesById queries may perform slowly possibly leading to general non-responsiveness","""Below example shows that the NuGet query is taking nearly 5 minutes to respond.     request.log      nexus.log      With many of theses type of request, causes Nexus hang and requires restart to recover. This issue is related to NEXUS-12337 where a fix is included in Nexus 3.3. Unfortunately it has not resolved this issue.    h4. Cause    The internal database queries backing the NuGet queries are being slowed down by the lowercasing of the Nuget package name, bypassing some important database indexes."""
"NEXUS-13032","Improvement","CMA",5,"eliminate soft-deleted blobs and reduce transaction retries for identical proxy repository asset requests","""*Background*    In highly concurrent situations, multiple clients may request the same content from an NXRM proxy repository. In these cases, the parallel requests significant churn on the Asset record, with unnecessarily large numbers of blobs created as a result of transaction retries fighting over which blob to attach to it. This could occur in multiple HA nodes using the same blobstore, or on a single node (multiple threads). Because the soft-deleted blobs from the unsuccessful tries linger until the blob store is compacted, this can also waste storage space.    *Acceptance*    Detect when a transaction is attempting to attach a blob to an Asset when an identical blob already exists on that Asset. In those cases, ignore the attach request, and retain the existing blob.     """
"NEXUS-13030","Improvement","Blobstore",2,"automatically retry blob creation when a UUID collision is detected","""BlobIds are based on randomly generated UUIDs. While the chance of a collision is extremely rare (https://en.wikipedia.org/wiki/Universally_unique_identifier#Collisions) if it does ever happen then we should automatically retry the create blob operation with a fresh randomly generated id."""
"NEXUS-13057","Bug","UI",2,"""Uncaught TypeError"" UI errors","""While pre-testing 3.3.1, I unpackaged the pro bundle and set it up to perform Migration.  I then ran migration from 2.14.4.  After doing such, I accessed """"Repositories"""" to see if everything came across and noticed the attached errors.  [~ccarlucci] had reported this in the support room several days prior.    Workaround appears to be loading in incognito window or clearing browser cache.    Before this, I had cleared my browser cache testing the 3.3.1 OSS bundle so this seems either related to cache differences between OSS and PRO or something related to Migration."""
"NEXUS-13071","Bug","LDAP|UI",2,"Unfiltered LDAP user search will retrieve all users from an LDAP server, which can result in an OOM","""Reproduce steps:   # Go to """"security/users"""" in the UI   # Change the """"source"""" to LDAP   # Hit enter in the """"filter by user ID"""" box without inputting any data         Observe that _all_ user ID's from the LDAP server are retrieved, and for each one the full user record is retrieved.    This will result in a UI timeout if the LDAP server has a large number of users, or possibly even an OOM.           Expected:  The UI should limit the number of users retrieved in some way.   This could be a paged result set.  Or it could be as simple as only retrieving at most some arbitrary number o users, such as 1000.               """
"NEXUS-13092","Bug","Scheduled Tasks",5,"slow performance and metadata rebuild failures when running ""Remove snapshots from Maven repository"" against large datasets","""Looking at the logs we can see that the task """"Deleted 576 components from 522 distinct GAVs"""", 13 minutes after task 'Remove2 old snapshot from Amundi Snapshots' has started.      Then there is a 4+ hours gap before a next log entry on 42 GAVs found that have already been released.      Rebuild of metadata has now started.      After 9 hours of rebuilding metadata you get the following failure.     """
"NEXUS-13085","Bug","Maven",2,"IllegalArgumentException Version mismatch may be logged when GA maven-metadata.xml versions are merged in a group repository request ","""Download for artifact appears to be working, but getting error in log and causing some concern over conflict or if something has been inproperly loaded.    [^nexus-logviewer-VersionMismatch.txt]        Maven central has npanday custom-lifecycle-maven-plugin  latest release 1.4.0-Incubating    We have the npanday custom-lifecycle-maven-plugin  built locally to have a 'pre release' release of 1.5.0-NSL  which was captured to host on a Nexus 2.11  3rd Party repo    Central and 3rd party proxies are included within a maven2 group  type where it is downloaded from.    when looking at pom.xml in various locations all appears well that I thought to check so far.     """
"NEXUS-13099","Bug","Outreach",1,"outreach capability Base URL is editable","""Nexus 2 allowed setting an override URL for outreach for testing purposes.    The Base URL is for display purposes only and should not be editable. We display it so that in case you do override it, you can easily revert to the default value.    Nexus 3 incorrectly allows editing and saving the BaseURL field    Since Base URL is named the same as the Base URL field they are used to seeing in Nexus 2 and the Nexus 3 Base URL Capability is hidden by default, we have had at least one customer set the Base URL of outreach to the base URL of their Nexus instance expecting that to work.    *Expected*   - make Base URL for outreach capability not editable   - -if on upgrade it is changed from it's default value, reset it in the database-   - document this change in release notes in case the user used the Base URL field to perform an override instead of the override field   - Change of Labels and Help text (came out of PR review)   ** Change the field label {{Base URL *to* Default Outreach Content URL:}}.   ** Change the help text for that field from {{URL *to* use for querying page bundles.}} to {{Default external URL for downloading new Outreach content.}}   ** Change the field label {{Override URL:}} *to* read {{Override Outreach Content URL:}}   ** Change the help text for that field from {{Retrieve the page bundle from a specific URL.}} *to* {{Override external URL for downloading new Outreach content.}}"""
"NEXUS-13098","Improvement","Bootstrap",1,"log and prevent startup when the started Nexus version is using a data directory from a newer Nexus version","""Nexus 2 detected and failed startup in most cases where configuration file compatibility versions were conflicting.    Some Nexus 3 users accidentally or intentionally startup old versions of Nexus 3 against newer upgraded versions of Nexus 3 data directories. This may lead to potential problems and is certainly a state which is considered """"unknown"""". This complicates trying to solve any other reported problem with a running instance.    h4. Expected  - Nexus 3 should detect startup against incompatible data directories/files (ie. those with later schema versions compared to the installation attempting to run), fail, and log a SEVERE message indicating the problem into the nexus.log"""
"NEXUS-13096","Bug","Logging",2,"com.orientechnologies.common.profiler.OAbstractProfiler$MemoryChecker log spam every 2 minutes","""Some nexus.log files are filling with thousands of these log messages ( counted over 5000 in one 13 hr period, printed every 2 minutes ):        h4. Expected    - document what this means and what actions might or might not be taken  - provide a different message that does not refer to files/settings that are not applicable to the orient database being embedded inside nexus  - do not spam logs constantly if this INFO message is not important or not actionable"""
"NEXUS-13095","Bug","Browse Storage",5,"Browse components of large repositories fails with IllegalStateException Timed out reading query result from queue","""We are seeing the warning message """"*Operation failed as server could not be contacted*"""" when browsing components of large repository as non-admin/admin users . This only happens when browsing components of huge repositories. Browsing """"assets"""" of these repositories are fine. Browsing """"components"""" of other smaller repositories are fine as well.     Can confirm that the same is also happening for browsing groups with huge repos in assets page for both admin/non admin users.        With Debug switched on, we see this error:       """
"NEXUS-13121","Bug","Scheduled Tasks",3,"tasks may appear as Starting or Cancelling indefinitely and cannot be stopped cancelled or deleted","""After restarting Nexus repository manager that has a large number of tasks, one or more tasks may   - transition to Starting state before Nexus boots  - never leaves Starting state to Started state  - the task has the Stop button enabled  - after clicking the Stop button the task is indefinitely in Cancelling state  - after the clicking the task Delete button, the task is indefinitely in Cancelling state    The nexus.log may show the task transitions from WAITING to RUNNING before Nexus reports it has fully booted - example: Rebuild Indexes:        A thread dump from the server may show the task thread stuck as follows:        h4. Workaround    A user may want to know how to delete the stuck task. The only way is to explicitly delete the task from the configuration database using a special OrientDB console command. Please contact support if you need the additional information to delete a similarly stuck task."""
"NEXUS-13127","Bug","User Token",2,"Anonymous access to repositories does not work when ""Require user tokens for repository authentication"" is set","""Configure user token access in the server, and enable """"Require user tokens for repository authentication"""".  Then run a Maven build against the server against a repository that allows anonymous read access.         The build will fail, a 401 response is sent.         The """"Require user tokens for repository authentication"""" feature should not block anonymous requests.  It should only block requests that are sent with credentials that are not user tokens."""
"NEXUS-13126","Bug","User Token",1,"When ""Require user tokens for repository authentication"" is set nexus does not send an authorization header","""Configure user token access in the server, and enable """"Require user tokens for repository authentication"""".  Then run a Maven build against the server.  Artifact downloads will fail with 401 even if a valid token is used.    The reason for this is that Maven by default sends requests with non-preemptive authentication, and will only send credentials in response to a 401 response with a challenge header.  The challenge header is missing in the response.  Nexus should be setting something like:       """
"NEXUS-13130","Bug","Scheduled Tasks",5,"Timed out reading query result from queue when running purge unused snapshots task","""     The purge unused snapshots task fails with query timeouts when run against large data sets.           The blob store this was running against is used only for the snapshot repository the task is running against.  It has:    blob count: 3752052   total size: 828.9 GB         The machine has plenty of RAM (32Gb).  14Gb of space was allocated for OrientDB cache, 4Gb for heap.            """
"NEXUS-13141","Bug","Logging",0.5,"when merging maven-metadata.xml for a group request fails the repository ID containing the bad metadata is not logged","""If a maven-metadata.xml file merge fails in Nexus 3 the repository ID with bad metadata is not logged, even at ROOT DEBUG.  This makes it extremely difficult to find where the problematic file is.               """
"NEXUS-13140","Improvement","Browse Storage",2,"when browsing assets in a group repository the asset summary UI should display the containing member repository name ","""If you use """"browse assets"""" on a group repository you can find the files in the group.  But there is no way to know what repository these files actually live in.       This is a critical diagnostic tool needed by support.  As an example, a customer here had bad metadata files in their group repository which was causing merges to fail.    There was no way to find out what repository actually had the bad files.  Note that """"?describe"""" diagnostics are not user friendly and would not work in this case since it would end up running a merge, which would fail.    h4. Expected    - The asset details screen should show what proxy or hosted group member containing repository the asset resides in"""
"NEXUS-13137","Improvement","Bower|PyPI",0.5,"Purge unused components and assets task should support PyPi and Bower proxy repositories","""Currently the task """"Purge unused components and assets task"""" is not configurable against and no-op for PyPi and Bower format proxy repositories.    h4. Expected    - PyPi and Bower proxy repositories should be selectable in the repository list shown on the task configiuration  - when the task is configured for All Repositories, PyPi and Bower proxy repositories should not be a no-op, but instead be processed     """
"NEXUS-13136","Bug","Transport",1,"NullPointerException is thrown when user-agent header value is not present","""Discovered as part of NEXUS-13048 where a user was doing an upload to a raw repository and seeing an NPE in the logs. The user was using a Groovy library to perform the upload which was not sending the user-agent header which caused the NPE.    Acceptance criteria: No NPE"""
"NEXUS-13148","Bug","Database|Maven|Scheduled Tasks",5,"Query timeout when running Publish Maven Indexes task.","""The Publish Maven Indexes task fails with query timeouts when run against large data sets.    The machine has plenty of RAM (32Gb).  14Gb of space was allocated for OrientDB cache, 4Gb for heap.        The hosted repo orbis-dev is using a custom blobstore that no other repo uses.    orbis-dev-2016 blob store:   Blob count: 2922750   Total size: 562.1 GB"""
"NEXUS-13145","Bug","NPM",2,"Rejecting requests including an Authorization header for unrestricted resoruces","""This is a bug that bit me hard while trying to install scoped packages ([#6857|https://issues.sonatype.org/browse/NEXUS-6857]) with [yarn|https://yarnpkg.com/en/]. It is a new popular client for npm.    When requesting scoped packages, yarn always includes an Authorization header, if the user previously logged in to the npm registry.    This request is a standard non-scoped package and Nexus responds correctly.        This request is for a scoped package and includes the Authorization header.        Nexus responds with a 401.        Versus a yarn request without having logged in:            Expected behavior: Ignore the Authorization header for unrestricted packages instead of issuing a 401."""
"NEXUS-13173","Bug","RUT Auth|UI",1,"Search UI does not show up when using RUT authentication","""When a user with """"nx-admin"""" role is authenticated using RUT authorization the repository search UI does not show up.         To reproduce this set up remote user authorization:    [https://books.sonatype.com/nexus-book/3.3/reference/security.html#remote-user-token]    [https://support.sonatype.com/hc/en-us/articles/214942368-How-to-Configure-Request-Header-Authentication-in-Nexus-with-Apache]         Then simply authenticate through Apache using a user that has """"nx-admin"""" role.  You will see that the search UI is missing on the landing page."""
"NEXUS-13171","Bug","RUT Auth|UI",2,"Administration UI ""cog"" does not show up when using RUT authentication","""When a user with """"nx-admin"""" role is authenticated using RUT authorization the """"cog"""" icon which lets you into the user administration UI for Nexus does not show up.         To reproduce this set up remote user authorization:    [https://books.sonatype.com/nexus-book/3.3/reference/security.html#remote-user-token]    [https://support.sonatype.com/hc/en-us/articles/214942368-How-to-Configure-Request-Header-Authentication-in-Nexus-with-Apache]         Then simply authenticate through Apache using a user that has """"nx-admin"""" role.  You will see that the administration cog is not visible.         Direct access to the administration UI through URL's like [http://localhost:8081/#admin/security/users] does work.     """
"NEXUS-13168","Bug","NPM",2,"NullPointerException on npm search when invalidating cache","""While investigating https://issues.sonatype.org/browse/NEXUS-12716 I found another NPE when running npm search.    When npm search is run against a group containing a proxy the following stack is outputted but the search runs successfully:    This has something to do with the fact that the proxy fires the search index invalidation event while the group is in the middle of building its search index, although it is not immediately obvious how the repository field could ever be null."""
"NEXUS-13163","Improvement","Search",2,"Use bulk API for incremental Elasticsearch updates","""Incremental Elasticsearch index updates, such as those triggered by component and asset entity events, currently use the standard put and delete API. When a massive insert of assets occurs this could cause index updates to fail for the same reason as NEXUS-12520    To get the same benefits as NEXUS-12520 (automatic retries, etc.) we should also use the bulk API for incremental updates."""
"NEXUS-13178","Bug","RubyGems",3,"gem development dependencies are treated as runtime dependencies","""1. publish a gem with dev dependencies to nx3 hosted repo  2. when you install the gem from the hosted repo, nx3 reports the dev dependencies as main dependencies (downloads them when it should not ) - this is different than how rubygems.org and nexus 2 works    Ruby gems dependency metadata returned from /api/v1/dependencies incorrectly identifies development dependencies as runtime dependencies - this can cause builds to fail due to conflicts resolving dependency versions.    Expected  - make Nexus 3 treat dev dependencies like rubygems.org and nexus 2 do"""
"NEXUS-13200","Improvement","Docker",2,"Deleting docker image leaves dangling image of same ID","""When I have 2 images with different tags but delete one using curl, it removes the sha file and the latest image but leaves the other image there.  This was intentional after NEXUS-12711 but further work could be done so delete both images.  The workaround is run the delete again and it will remove the second image.    Steps to repro:  1) docker pull hello-world  2) docker tag hello-world <hosted:port>/hello-world  3) docker push <hosted:port>/hello-world  4) docker tag hello-world <hosted:port>/hello-world:second  5) docker push <hosted:port>/hello-world:second  6) curl -v -u 'admin:admin123' -H 'Accept:application/vnd.docker.distribution.manifest.v2+json' https://<hosted:port>/v2/hello-world/manifests/latest -k  7) curl -v -X DELETE -u 'admin:admin123' https://<hosted:port>/v2/hello-world/manifests/sha256:<Docker-Content-Digest> -k  8) curl -u 'admin:admin123' https://<hosted:port>/v2/hello-world/tags/list -k  shows """"second"""". UI also shows image and manifest for second.  WEIRD9) curl -v -u 'admin:admin123' -H 'Accept:application/vnd.docker.distribution.manifest.v2+json' https://<hosted:port>/v2/hello-world/manifests/second -k  shows the <Docker-Content-Digest> same as latest. So seems an associated file is now missing (on push, both the manifest and sha file are created)."""
"NEXUS-13207","Bug","NPM",2,"NPM group search document is not invalidated when member search documents change","""While testing NPM (mostly group search) I noticed this exception a few times.  I could not find a reproducable case nor am I sure any adverse behavior but am filing after discussion with the team and also after seeing in 3.3.1, knowing it's not (recent) regression.    """
"NEXUS-13228","Bug","Logging",2,"race condition writing to request.log under high load may cause dead lock, thread pool exhaustion and CPU spike","""Nexus 2.x uses version 1.1.2 of logback-access,logback-core,logback-classic. As such it may be affected by a race condition that has been reported and described in https://jira.qos.ch/browse/LOGBACK-1270.    Nexus uses a custom PatternLayoutEncoder defined inside the logback-access.xml file so that the request log can print the user name for each authenticated request log line.    Recently the LogBack developers released a backported fix for version 1.1.x line of releases. Nexus 2.14.x uses version 1.1.2 of logback.    h4. Symptoms    The main symptom is a single thread was blocking many others from writing to the request.log.    {noformat:title=Example Blocking Thread}  """"qtp2074628523-9375"""" #9375 prio=5 os_prio=0 tid=0x00007f8cb0404800 nid=0x2014 waiting for monitor entry [0x00007f8d4466b000]     java.lang.Thread.State: BLOCKED (on object monitor)      at ch.qos.logback.core.pattern.PatternLayoutBase.writeLoopOnConverters(PatternLayoutBase.java:116)      at ch.qos.logback.access.PatternLayout.doLayout(PatternLayout.java:196)      at ch.qos.logback.access.PatternLayout.doLayout(PatternLayout.java:65)      at ch.qos.logback.core.encoder.LayoutWrappingEncoder.doEncode(LayoutWrappingEncoder.java:134)      at ch.qos.logback.core.OutputStreamAppender.writeOut(OutputStreamAppender.java:194)      at ch.qos.logback.core.FileAppender.writeOut(FileAppender.java:209)      at ch.qos.logback.core.OutputStreamAppender.subAppend(OutputStreamAppender.java:219)      at ch.qos.logback.core.rolling.RollingFileAppender.subAppend(RollingFileAppender.java:182)      at ch.qos.logback.core.OutputStreamAppender.append(OutputStreamAppender.java:103)      at ch.qos.logback.core.UnsynchronizedAppenderBase.doAppend(UnsynchronizedAppenderBase.java:88)      at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:48)      at ch.qos.logback.access.jetty.RequestLogImpl.log(RequestLogImpl.java:142)      at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:92)      at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)      at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)      at org.eclipse.jetty.server.Server.handle(Server.java:370)      at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)      at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)      at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)      at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)      at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)      at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)      at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)      at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)      at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)      at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)      at java.lang.Thread.run(Unknown Source)       Locked ownable synchronizers:      - <0x00000006c8e2e578> (a java.util.concurrent.locks.ReentrantLock$FairSync)          h4. Workaround    A restart will be required to escape the dead lock.    """
"NEXUS-13227","Bug","Search|UI",2,"Deleting last asset from search leaves component until refresh","""While testing, I found if I deleted the last asset of a component that I had searched, I was returned to the search list however the component was still there.  If you manually enter the component, it has no assets.  If you refresh the search again, the component disappears.  While I was looking to see if this was filed (seemed familiar), I noticed browse does NOT act the same way.  Thus I am filing a bug, as I had suspsected it might be intentional, but cannot rationalize why Search would work this way and Browse not (Browse works as I expected).  Marking minor since the workaround is to refresh."""
"NEXUS-13304","Bug","IQ Integration",3,"500 responses from Nexus after enabling quarantine on NuGet proxy repository","""Reproduce steps:   # Configure Nexus 3.3.1 to use an IQ server   # Set up audit/quarantine capability for a proxy of """"https://www.nuget.org/api/v2/"""".  Enable quarantine for this capability   # Request a NuGet package through the server, such as""""http://localhost:8081/repository/nuget.org-proxy/Newtonsoft.Json/10.0.2""""          You'll get a 500 response from the server.   Note that this affects all new component requests, not just the first one.    Not all repository formats are affected by this bug.  I was not able to reproduce this using Maven, for example.     When the failure occurs the log shows:              When we confirm the fix, we need to make sure that we haven't left a whole bunch of artifacts in pending status."""
"NEXUS-13303","Bug","Configuration",2,"email non-ssl fails when 'use Nexus truststore is selected'","""When none of the SSL options are selected for email server, but """"Use the Nexus Trust Store"""" is then email connection fails with the following error.        Although the """"Use the Nexus Trust Store"""" does not make sense when connecting to non-ssl email server, it should not fail a and use plain text connect.    A valid use case would be to have """"Enable STARTTLS support for insecure connections"""" selected, so if SSL is available it would use it, otherwise use plain text. But with  """"Use the Nexus Trust Store"""" select it will for it to use SSL."""
"NEXUS-13329","Bug","Migration",3,"Maven artifact whose case does not match the version folder they reside in are not migrated","""If you have a Maven artifacts in Nexus 2 whose artifact ID does not match the ID of the artifact folder they reside in these artifacts will not be migrated to Nexus 3 on upgrade.    See attached screenshots for an example of this.    This is a critical bug, it is unfortunately not at all uncommon for users who are running Nexus 2.x on Windows to have artifacts whose case changes over time.  These will be lost on upgrade.    I've attached the work directory from Nexus 2.x to make this easy to reproduce.          """
"NEXUS-13333","Bug","Maven",2,"400 bad request when deploying a release versioned Maven artifact with an artifactId containing the word ""SNAPSHOT""","""My artifact name is """"A_INITIAL_SNAPSHOT_DWSCHEMA_TABLES_00"""" (I know this is ugly but it is my company's naming convention)    When I try to release my artifact I get the following error. What I am uploading is a release (version 0.1) so it should not be refused. If I just change the name of my artifact removing SNAPSHOT everything works fine so I guess instead of just checking the version number, Nexus is wrongly checking if the String """"SNAPSHOT"""" is present anywhere in the artifact description.    Here is the exception:  """
"NEXUS-13363","Bug","Docker",3,"Conditional GET requests for Docker image layers always download the layer when proxying another Nexus","""Tested this with 3.2.1, although the problem has also been reported against 3.3.1 as well.   # (Optional) Keep things simple and use --insecure-registry and plain http ports for the docker repos In this example I will use plain http ports.   # (Optional) use https ports and have your docker daemon trust the Nexus certificate.   # Configure Nexus A with a docker hosted repo. ( 192.168.2.97:14440 )     # Configure Nexus B with a docker proxy repo which proxies Nexus A hosted repo. ( 192.168.2.97:13350 )     # Configure the following loggers in Nexus B so that you can monitor the outbound HTTP headers and requests:   ## org.apache.http at DEBUG   # Docker Login to both proxy and hosted repos so that docker v2 API works and Nexus does not fall back automatically to using /v1 API ( which would otherwise void this test).   ## docker login 192.168.2.97:14440   ## docker login 192.168.2.97:13350   # Find and remove all images with the same name to remove all the layers attached to those images - in this example I will re-use the sonatype/nexus 3 image:   ## docker images ( list ALL images based on sonatype/nexus3 )   ## docker rmi <fill sonatype/nexus3 image name as reported under REPOSITORY column of previous command> - there may be more than one image, delete them all to make sure subsequent pulls actually download layers not already cached   # tag and push an image to Nexus A docker hosted.   ## docker pull sonatype/nexus3 ( download from official docker registry )   ## docker tag sonatype/nexus3 192.168.2.97:14440/sonatype/nexus3 ( tag the nexus 3 image against the hosted repo )   ## docker push 192.168.2.97:14440/sonatype/nexus3 ( push the image and layers to hosted repo )   # remove the local image you just tagged from the docker client cache. This should also remove dependent layers forcing them to re-download on the next pull.   ## docker rmi 192.168.2.97:14440/sonatype/nexus3   # request the image through the proxy repository which will in turn download from the hosted repo   ## docker pull 192.168.2.97:13350/sonatype/nexus3   # Examine the nexus.log of Nexus B. There you will find outbound requests to the hosted repo, downloading the layers as expected.   # Delete the cached image and layers from the proxy now and then pull again   ## docker rmi 192.168.2.97:13350/sonatype/nexus3   ## docker pull 192.168.2.97:13350/sonatype/nexus3   # Examine the nexus.log of Nexus B. There you will find identical outbound requests to the hosted repo again. The manifest request will return 304 indicating it is not changed. {color:#ff0000}*However all the conditional GET requests for the layers will return 200 and the full content of the image.*{color}    h4. Problem    The images are always re-downloaded from the remote Nexus, even if the hash/content was unchanged. The conditional GET does not work as expected - 304 status code and no content was expected.    For comparison, proxying the official docker registry at [https://registry-1.docker.io|https://registry-1.docker.io/] from Nexus 3.2.1 does work correctly in that the official registry returns 304 for unchanged layers.    h4. Workaround    Set Component Max Age to {{-1}} as image layers never change hash identity anyways. This prevents already cached image layers from being requested from the remote."""
"NEXUS-13355","Story","Documentation|Scheduled Tasks",8,"Regenerate CMA from Blobs - Docker","""*Acceptance*    Augment the self-repair/consistency task so that it will re-create component metadata entries for this format.   * Every operation should be logged to the new long-running task log.   ** Creating a new component db record   ** Conflicts (e.g. there's an unreferenced blob, but there's already an asset record that exist at the implied coordinate, but it refers to a different blob)   * Conflicts never overwrite existing Orient metadata; they're only logged.   * All operations should have a 'do no work' mode as per ---NEXUS-12390---   * -Restore should work if only components are dropped- (while true we have a ticket for this elsewhere)   * Restore should work if only assets are dropped   * Restore should work if both component and asset are dropped"""
"NEXUS-13352","Story","Scheduled Tasks",3,"add per task log files for easier task auditing","""*Background*    There are a number of long-running tasks whose specifics we need to audit (self-repair tasks being a good example, but also purging releases, snapshots, etc.), but we don't want to a) fill the nexus log with a million line items, or b) lose information at the whim of a customer's log levels. Support frequently gets requests from users asking to understand what happened to specific artifacts.    *Acceptance*   * Create a service that can accept all these detailed actions, in large volumes   * One log file per run of a given scheduled task   * Output files should be dated, clearly indicate which task was run   * Emit the log file location into the log at the start and end of the scheduled task   * Include task logs in the support tools zip """
"NEXUS-13371","Bug","Logging",1,"BlobAttributes deletedReason is logged as reason: null instead of the actual reason","""BlobAttributes deletedReason is persisted, but is not restored on load    This leads to the following non-helpful messages:      Instead of:    """
"NEXUS-13378","Improvement","Proxy Repository",5,"Limit multiple outbound upstream requests for the same proxied asset","""NX3 currently allows multiple requests for the same proxied asset to go upstream, one of which will be the """"winner"""" when the storage transaction is committed. This avoids any upfront locking, improving scalability - especially in a distributed setup. The downside is the potential for duplicated work, which will end up being thrown away for any """"losing"""" threads.    This story will attempt to limit the amount duplicated proxy work, while maintaining scalability."""
"NEXUS-13385","Bug","Docker",0.5,"java.util.ConcurrentModificationException possible with Docker UploadManager during POST to blobs/uploads","""A POST request to a Docker repository at endpoint /v2/*/blobs/uploads/ may result in a ConcurrentModificationException and 500 HTTP status code response.    request.log    {code:java}   172.20.9.5 - user [12/Jun/2017:07:15:59 -0700] """"POST /repository/docker-internal/v2/twistlock/blobs/uploads/ HTTP/1.1"""" 500 65 31 """"docker/17.06.0-ce-rc2 go/go1.8.3 git-commit/402dd4a kernel/4.9.30-moby os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce-rc2 (darwin))""""  {code}    nexus.log:        h4. Diagnosis    Failure to follow the fully advertised Collections.synchronizedList() contract, leading to potential for ConcurrentModificationException.  """
"NEXUS-13393","Bug","Docker",3,"Docker V2 API returns incorrect list of tags for a repository","""I use {{GET /v2/_catalog}} Docker API call to get a list of repositories, then request a list of tags as {{GET /v2/<name>/tags/list}} and the response differs from Nexus WebUI, see below:    There are *45+* tags returned from Docker API call, but Nexus WebUI knows only about *16*.    I used Docker API to remove all these extra/obsolete layers/images by issuing {{DELETE /v2/<name>/manifests/<reference>}} calls before.    Seems like Nexus is aware these objects are no longer valid, but Nexus Docker Registry API still announces them.    I expect Registry API and Nexus WebUI show the same objects."""
"NEXUS-13391","Bug","Documentation|Security",1,"non-migration of admin credentials isn't documented","""The Admin user credentials from NXRM 2 are not migrated to NXRM 3. Although this may not be a bug and is intentional, it should be mentioned in the following section of documentation.       [20.5.3. What Is Not Upgraded|http://books.sonatype.com/nexus-book/reference3/upgrading.html#not-upgraded]"""
"NEXUS-13422","Bug","Docker",2,"HEAD WARN when pushing docker images","""I noticed if I push an image to docker hosted, I am getting back messages like this:  {quote}  2017-06-16 15:01:34,297-0400 WARN [qtp31792032-299] admin org.sonatype.nexus.repository.docker.internal.V2Handlers - Error: HEAD /v2/mongo/blobs/sha256:4a3c5460d81c6322019bdbe40d0921c4c97f425506306d8b6708b4a614665863: 404 - org.sonatype.nexus.repository.docker.internal.V2Exception$BlobNotFound: blob unknown to registry  {quote}    As far as I can tell everything is functioning correctly, so does not seem WARN worthy at least.  This reminds me of NEXUS-10517 but is definitely pushing not pulling from group."""
"NEXUS-13426","Bug","NuGet",3,"Infinite loop when using $filter (the $skip-Parameter is ignored)","""Please see the comment from Sebastian Jancke."""
"NEXUS-13440","Bug","Maven|NPM|Scheduled Tasks",3,"RemoveSnapshotsTask has threading issues and incorrectly processes npm repositories","""Nexus 3.3.1 was configured with two Snapshot removal tasks that started at the same time:    See attached text file for play-by-play.    h4. Problems  - quartz-1-thread-11 task is only configured to process agfa-snapshots but logs it is also processing orbis-snapshots:  2017-06-17 03:00:00,141+0200 INFO  [quartz-1-thread-11] *SYSTEM org.sonatype.nexus.repository.maven.internal.RemoveSnapshotsFacetImpl - Removing snapshots on repository orbis-snapshots with configuration: org.sonatype.nexus.repository.maven.tasks.RemoveSnapshotsConfig(2, 7, true, 365)  - the ending task thread is clearly processing **npm** assets when it should only be processing Maven repositories  - one of tasks times out and fails performing a query - the other one takes an extremely long time with very little progress logging."""
"NEXUS-13465","Bug","Database|Licensing",2,"Accesslog database cleanup fails with BufferUnderflowException","""We should add some limits to this deletion to ensure that it doesn't suffer from what are presumably issues deleting large amounts of data at once.   """
"NEXUS-13462","Improvement","HA|Proxy Repository",5,"Extend proxy cooperation across HA nodes","""NEXUS-13378 introduced proxy cooperation between threads in the same node, to avoid the same content being downloaded multiple times.    This story will look at supporting proxy cooperation across HA nodes, potentially using Hazelcast to perform the cooperation. Since hazelcast is only available in HA this will likely involve different implementations for the local and HA case, sharing the same common interface."""
"NEXUS-13486","Story","Backup",5,"prevent restoring database backups with mismatched versions","""*Background*    As discovered in the field, some users have occasionally restored some databases, but not all of them. In other cases, restores were carried out using a set of export files taken from different exports. This can lead to unusable NXRM instances with an incoherent state.    *Acceptance*   * Modify the filenames of database exports so it's clear whether they were generated   * Disallow restores from incomplete sets, or sets with mismatched export times"""
"NEXUS-13485","Improvement","Bootstrap",1,"warn in UI when ulimit < 65536 on Linux or OSX","""As per -NEXUS-12041-, Nexus 3.x process user should have a minimum max ulimit of 65536 file handles. Having less could potentially lead to a critical failure.  h4.      *Acceptance*   * When NXRM detects that it has insufficient file handles   ** a prominent user interface warning is shown to administrators (indicating NXRM failed a health check, pls check the logs)   ** an ERROR appears in the log"""
"NEXUS-13483","Improvement","LDAP",5,"Re-introduce LDAP user caching","""{{EnterpriseLdapManager}} in NX2 used to provide user-level caching to avoid repeated requests for the same information from going upstream to the LDAP server. This was necessary to avoid a flood of upstream LDAP requests from token-based realms such as NuGet or Usertoken.    When the LDAP codebase was ported to NX3 this caching appears to have been removed from {{EnterpriseLdapManager}}. This story will look at re-introducing user-level caching to the NX3 LDAP codebase, so all token-based realms (and other users of LDAP-based information) can take advantage of this cache and don't have to implement their own caching.    See https://issues.sonatype.org/browse/NEXUS-13467?focusedCommentId=415462&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-415462 for more background."""
"NEXUS-13554","Bug","Migration|NuGet",2,"A NuGet package that is in Nexus 2.x storage but not in its database causes a NullPointerException on migration to Nexus 3","""If a NuGet package is in repository storage but not in the Nexus 2.x database you will get an NPE in Nexus 3 when it migrates it.    To reproduce this, just place a NuGet package into directly into storage in the right spot in Nexus 2, and run an upgrade to Nexus 3.         *This may also affect npm*."""
"NEXUS-13566","Bug","Blobstore|Database",5,"OOM may lead to blobstore/database inconsistencies","""We had a user here who had thousands of blobs that were referenced in their database, but are not in blob storage.    The cause seems to be that they ran out of heap space, and this caused CPU starvation of threads writing to the blob store. These threads reached their timeout, and the blobs were not written.  An example of this is below.  """
"NEXUS-13591","Story","Licensing",3,"Ingest product license file on server startup via a system property","""*Acceptance*   * We want a clean system with no blob stores or repositories, starting up a Nexus Pro with clustered=true from a fresh install   * The ability for NXRM to ingest a license file on startup (e.g. by specifying where the license file is)   * Use the {{nexus.licenseFile}} nexus property to point to the license file   * Fails startup if {{nexus.licenseFile}} is set but that license file is invalid"""
"NEXUS-13586","Bug","NuGet",2,"non-standard date format in NuGet ODATA feed causes IllegalArgumentException Invalid format","""After an export/import operation is performed to repair a Nexus Repository Manager 3 database, (c.f. NEXUS-12064), NXRM can generate NuGet feeds with a non-standard date format, causing client exceptions and 500 responses from Nexus while retrieving those packages.     Date format before:  {code:java}  <d:Published m:type=""""Edm.DateTime"""">2017-04-02T13:37:50.910Z</d:Published>  {code}  Date format now (triggering error):  {code:java}  <d:Published m:type=""""Edm.DateTime"""">1480511591411</d:Published>{code}    Example Log message:          h4. Cause    We believe this is caused by an export / import of the database. The date-times in question are stored in a part of the database that is schema-less and therefore loses the information that a timestamp was actually a date object.    h4. Mitigation    The linked ticket also suggests that this only affects older versions of the client and is fixed in Visual Studio 2015 and later (see https://issues.sonatype.org/browse/NEXUS-12064?focusedCommentId=412345&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-412345). If you are using an older version of Visual Studio and are affected by this bug then we suggest upgrading NXRM to the fix version listed in this ticket or upgrading to 3.12.1 and applying the patch attached to this ticket for that specific version.      """
"NEXUS-13626","Improvement","Security|UI",1,"Privilege name does not fit in list of Privileges box","""When assigning privileges to roles the list containing """"Available"""" and """"Given"""" does not have room horizontally for a lot of the privileges especially when the scrollbar is visible. It makes it ever so hard to get an overview of what privileges are given.    Could you please either increase the width or make them resizeable?"""
"NEXUS-13639","Bug","LDAP|Migration|User Token",3,"User tokens not migrated if LDAP user ID case does not match login case","""User token lookups in Nexus 3 from user ID to token are done using a case sensitive match.  This causes a problem after upgrading from Nexus 2.x to 3.x because Nexus 2.x uses the case a user logs in with when creating a token, and Nexus 3 uses the case of the ID stored in the LDAP server.    Reproduce steps:   # Create an upper case LDAP user ID in an LDAP system.   # Map this user into roles in Nexus 2.x that allow access to user tokens   # Enable user token access in Nexus 2.x   # Log in as the LDAP user using a lower case login ID, and access the user token   # Upgrade to Nexus 3.4.0   # Log in as the LDAP user using a lower case login ID, and access the user token         You will see that the token in Nexus 3 does not match the one in Nexus 2.    *Notes*   * LDAP user IDs are not case sensitive. This should be preserved in NXRM 3's behavior.   * This will likely require an upgrade step that cleans up duplicate user tokens that only differ by case.     * This problem may also affect default realm user ids."""
"NEXUS-13766","Bug","NuGet",2,"NuGet metadata can incorrectly display pre-release as blank","""I noticed that if I downloaded https://www.nuget.org/packages/bootstrap/4.0.0-alpha6 into my NXRM3 that the is_prerelease field in the metadata shows as blank.  However, on nuget.org it's clearly marked is a pre-release.  I verified this as a non-regression from what I was testing so am filing seperately.  While reproducable I noticed the same behavior with HtmlAgilityPack/1.5.2-beta2 in case you need a second source."""
"NEXUS-13780","Bug","UI",2,"Danger Uncaught TypeError Cannot read property 'load' of undefined when reloading UI after server disconnect","""See attached video for reproduce case.    Basically  1. Login as admin and load a restricted config page ( example security/realms )  2. Stop nexus  3. Wait for UI to report server is disconnected  4. Start nexus  5. Retry UI loading with the dialog  6. Login again  7. See Danger message after login and also you apparently don't have access to the UI that admin should have access to.  8. refresh the browser page - realms are visible.    This was verified using Chrome."""
"NEXUS-13777","Bug","Logging",1,"JVM optimizations may log exceptions without stack traces by default due to OmitStackTraceInFastThrow ","""We have seen at least two instances where a java.lang.NullPointerException is thrown unintentionally due to programming error and yet no stack trace is logged to help us determine and fix the root cause of the bug.    This seems to be due to a JVM optimization according to: http://www.oracle.com/technetwork/java/javase/relnotes-139183.html    bq. The compiler in the server VM now provides correct stack backtraces for all """"cold"""" built-in exceptions. For performance purposes, when such an exception is thrown a few times, the method may be recompiled. After recompilation, the compiler may choose a faster tactic using preallocated exceptions that do not provide a stack trace. To disable completely the use of preallocated exceptions, use this new flag: -XX:-OmitStackTraceInFastThrow     h4. Expected    - prevent critical exceptions to be logged without without a stack trace, by turning off the JVM optimization and setting {{-XX:-OmitStackTraceInFastThrow}} by default    Additional Reference: https://stackoverflow.com/a/27214402/235000    h4. Workaround    A user may wish to disable this JVM optimization themselves until it is made the default option. They can edit bin/nexus.vmoptions and add the following line and restart Nexus:    """
"NEXUS-13866","Bug","Outreach|UI",1,"IE11: Outreach wraps despite there being space","""Noticed in IE11, that Outreach will wrap around despite there being space for both columns.  I made the left navigation as small as possible and still doesn't not wrap.  I don't have this issue in Chrome or FF."""
"NEXUS-13861","Bug","Security|UI",2,"Leading/trailing whitespace not trimmed from role ID's","""When creating a new role ID in Nexus 3 leading and trailing spaces are not trimmed.  So you can create roles that have whitespace preceding and trailing the role ID.         *To Reproduce*   # Navigate to <your:server>/#admin/security/roles   # {color:#ff0000}!https://issues.sonatype.org/images/icons/emoticons/add.png|width=16,height=16!  {color}Create Role > Nexus Role   # Add an ID with leading or trailing spaces    *Expected*    Field to show an validation message when a Role ID has leading or trailing spaces    *Actual*    Role is able to be saved without out any challenge or failure"""
"NEXUS-13882","Bug","Search",2,"If an old indexing context is found when creating a new repository it should be deleted","""When a repository is deleted in Nexus the search index context is not always deleted properly.      If you later try to recreate the repository the old search index context will be found, and this will result in an error.  The only way to recover from this is to shut the server down. manually remove the old index context from disk.  Then restart and rebuild the index for the affected repository.    Expected: If an old (stale) indexing context is present on disk when a repository is being created we should delete it, and create a new one.    """
"NEXUS-14039","Bug","RubyGems",0.5,"NPE in RubygemsContentFacetImpl while handling asset download count","""While doing upgrade testing in NEXUS-13735 I saw this NPE while running a Ruby {{bundle install}} command.    Saw in 3.4, but also reproduced in 3.5. I was just following the instructions at [https://docs.sonatype.com/display/Nexus/RubyGems+Testing+-+NX3.] The final test is to download a custom asset from a hosted repository."""
"NEXUS-13894","Bug","Docker",1,"Docker unauthenticated access shows ""unknown: unknown"" console output","""*Description*    When having configured a docker proxy without or with invalid HTTP Authentication we expect a similar or at least close to output message as docker hub would give us.    *To reproduce*   # Setup a private repo on Docker Hub (see [here|https://issues.sonatype.org/browse/NEXUS-11300?focusedCommentId=425137&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-425137] for some instructions on a setup)   # Setup a docker proxy, do not give it any HTTP Authentication   # On the command do a pull   ## docker pull <ip:port>/<username>/<reponame>    *Actual*    *Expected*  """
"NEXUS-13905","Improvement","Docker-Nexus",1,"Docker image should set java preference store location","""Our Docker images should override the Java preferences store location to be somewhere under the work directory.  This way an installed license will not be lost when a new Docker image is deployed.          """
"NEXUS-13900","Story","Yum",8,"add YUM proxy repository support","""As a systems developer, so that I can minimize downtime, I want to be able to access remote yum packages if the remote yum repository goes down    Acceptance Criteria:    * Access to metadata in SQLLite database format should be blocked   * If proxied metadata is changed, the metadata in the proxy repository will be updated when a package is requested   * Proxy repository must function if the remote repository goes down  ** The proxy will serve what cached components are currently available   * A user with sufficient privilege will be able to run the following commands:   ** yum install _<package name/s>_  ** yum update _<package name/s>_  ** yum check-update  ** yum remove _<package name/s>_  ** yum search _<keyword>_  ** yum list   * Deleting a yum component should function the same as yum remove, removing the dependencies as well     """
"NEXUS-13897","Bug","UI",1,"static css files are not using a cache buster which may cause unexpected UI rendering","""When you load the Nexus UI, requests are made to URLs like this:    [http://localhost:8081/static/rapture/resources/baseapp-prod_01.css]   [http://localhost:8081/static/rapture/resources/baseapp-prod_02.css]   [http://localhost:8081/static/rapture/resources/baseapp-prod_03.css]    It seems that the browser will cache these and then during upgrade of Nexus to a new version with the same files, may read the file from the browser cache instead of asking for a new version from Nexus.    This could lead to artifacts like this:    !image-2017-08-03-02-02-03-687.png!    Instead of the expected:    !image-2017-08-03-02-02-28-285.png!  h4. Expected    The static resource urls should be handled in such a way that makes them unique per Nexus release and so that a user does not need to clear their browser cache to load new UI elements as intended.    h4. Workaround    Manually clear your browser cache and restart the browser, then load the new version of Nexus.   """
"NEXUS-13916","Bug","Maven|Scheduled Tasks",3,"Generation of maven-metadata.xml representing a group of plugins adds an extra ""groupId"" tag, which to be used for artifacts","""When using “Rebuild Maven repository metadata” task, maven-metadata.xml that represents plugin groups, has and extra “groupId” tag. As maven documentation states here ([http://maven.apache.org/ref/3.5.0/maven-repository-metadata/repository-metadata.html]) there are two types of matadata content: artifacts /artifact versions (groupId, artifactId, version, versioning) and group of plugins (plugins).    Metadata generated by Nexus 3    Metadata in Maven Central ([http://repo1.maven.org/maven2/org/codehaus/mojo/maven-metadata.xml])     When using the maven repository directly that is not really an issue but when that repository participates in maven repository group is one.    Current Nexus 3 matadata merge (RepositoryMetadataMerger.merge) has a validation bug so when merging metadata for plugins from different repositories may result in a incomplete result."""
"NEXUS-13915","Bug","NPM|Transport",2,"conditional GET to npm group repository returns status 404 instead of 304 or 200","""When a Nexus 3 instance proxy's a repository it sends conditional GET requests for npm metadata to the remote.  These fail if the remote is a group repository in another Nexus 3 instance.    Reproduce case:   # Configure an npm proxy repository to https:registry.npmjs.org in Nexus 3.5.0   # Create an npm group repository, add the proxy repo into it as a member   # Request npm metadata through the group, such as [http://localhost:8081/repository/npm-group/ansi-regex]   # Now request it again with a conditional GET - That will return a 404 response:      h4. Expected    Nexus should return 304 (not modified) IF   the combined/merged metadata for all group repository members is no different ( not updated ) as compared to the conditional get conditions as applied by general standards.    -[https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.25]- OBSOLETE   [https://tools.ietf.org/html/rfc7232] - CURRENT conditional gets   [https://tools.ietf.org/html/rfc7230#section-3.2] - message headers general    Note: Sending the same conditional GET directly to the remote proxy returns 304.    h4. Outcome    Group repo requests will return 200 status + the most recent package metadata content instead of 404 now.  Proper 304 response to conditional gets is a desired future improvement."""
"NEXUS-13921","Bug","Yum",1,"yum proxy xml re-encoded and truncated",""".xml and .xml.gz files proxied and cached by the nexus Yum Proxy are being re-encoded. Additionally .xml.gz files are being truncated.              whereas downloading from the remote repository source directly:         Using sha256sum on the two downloaded urls give me different checksums as well. Interestingly, the sha256sum of the file I download from the nexus3.5.0 container matches the asset checksum sha245 attribute of the file (via the browser). This leads me to believe that as the yum proxy attempts to fetch the file, it is saving it with an incorrect length.         You can see more evidence of the munging if you expand and diff the two xml files:         to reproduce, setup a yum proxy repository with a name of frYum6 and a source of [http://centos.mirror.iweb.ca/6.9/os/x86_64/]         Then compare the checksum of the downloaded file:    $ curl -o nexus.primary.xml.gz [http://your.nexus3.test.server/repository/frYum6/repodata/ed2b2d4ac98d774d4cd3e91467e1532f7e8b0275cfc91a0d214b532dcaf1e979-primary.xml.gz]         with the source:    $ curl -o mirror.primary.xml.gz  http://centos.mirror.iweb.ca/6/os/x86_64/repodata/ed2b2d4ac98d774d4cd3e91467e1532f7e8b0275cfc91a0d214b532dcaf1e979-primary.xml.gz    $ sha256sum *gz  ed2b2d4ac98d774d4cd3e91467e1532f7e8b0275cfc91a0d214b532dcaf1e979 mirror.primary.xml.gz  c24f8d887a2a374469ad5276d9c14f72b93abc7c3b6bd1437482aa832dd1d330 nexus.primary.xml.gz     """
"NEXUS-13929","Bug","Logging",1,"help link printed in log file for open file descriptor limit minimums does not match the link used in the UI","""The link in the log file is different than the UI warning link at the top of the UI:    http://links.sonatype.com/products/nexus/system-reqs#filehandles    h4. Expected    Make the link in the log use the links.sonatype.com link instead of the support.sonatype.com link     """
"NEXUS-13939","Bug","Logging",1,"default task log progress ---- Mark ---- messages are missing helpful context and therefore should be removed","""Starting in 3.5.0, the task specific logs ( not nexus.log ) may print messages such as this ( specific concern is about these ---- Mark ---- messages ):        These Mark messages are not helpful because:    - they lack context  - they fill the logs without telling us what the task is doing  - the thread id is different from the task thread doing the actual work  - a thread dump can tell use where in the code the task thread is doing work specificly  - a leading informative log message ahead of beginning any major work is a better way to summarize what work a task is about to begin  - an end user can view task state ( ie. Running,waiting, etc) for end user visible tasks in the tasks UI    Some tasks take days to run, and could leave 1440 of these in the log per day for no discernable value.    h4. Expected    For now the simplest thing is to remove the default progress logging which prints ---- Mark ---- as a log message. In other words, there should be no periodic progress logging unless explicitly coded into a task and that log message includes task specific progress data. Any work to add more explicit progress logging should be done via another ticket."""
"NEXUS-13949","Bug","Scheduled Tasks",5,"Remove snapshots from Maven repository remove if released option may progress slowly","""The maven snapshot removal task is doing more work than expected. Expected that it would look at the snapshots in the snapshot repo and then check whether those exist in release repos. Instead it seems to be looking in other repos and checking whether those components exist in the snapshot repo.    Steps to reproduce:    Create a new maven snapshot repo and add one 0.0.1-SNAPSHOT version of a component.    Configure the snapshot removal task with minimum snapshot count 1 and remove if released enabled with a grace period 1 day.    Run the task with debug logging enabled.    Expected:    The snapshot would not be removed since it was just added, and there would be very little activity recorded in the log since there is just one snapshot in the repo and nothing to clean up.    Actual:    A whole bunch of debug log messages were recorded looking for components that do not exist in the snapshot repository. From the logs it looks like the task is iterating over other repositories, such as the central proxy, and checking whether those components exist in the snapshot repo.    Example log entry:    Searching for nexus-buildsupport only exists in Central proxy repo.     """
"NEXUS-13948","Bug","REST",1,"/swagger-ui prompts to save a useless file instead of redirecting to something useful","""Load this in a browser:    http://localhost:8081/swagger-ui    You are prompted to save a file. Saving this file on a local machine takes about 15 seconds. When it is done, all this file contains is this text:        h4. Expected    redirect to proper swagger-ui url of http://localhost:8081/swagger-ui/#/ and load that instead.    Users are not going to anticipate /swagger-ui/#/ and are going to frequently type /swagger-ui and expect something more useful to happen. When it doesn't, they will file support questions."""
"NEXUS-13947","Bug","Logging",2,"Expand Snapshot Remover task log messages with context","""Per --NEXUS-13580-- - Tasks that can support additional task log progress information need to be updated to provide  {quote} # At the start and end of each major area of work performed by the task, add a contextual INFO log message to the nexus.log and task specific log indicating what work is being/been performed   # At regular intervals during work, log progress with contextual data about the actual progress of that specific work{quote}  AC for the Snapshot Remover task:   # Major areas of work: Effectively each major method in {{RemoveSnapshotsFacetImpl}} can have a log.info. A number already do (e.g. {{processSnapshots}}), but some do not (e.g. {{processRepository}} and {{findReleasedSnapshots}}). There are many possible places of 'major work' in this task (compared to all others) so this may require some iteration with feedback from support.   # Contextual data: depends on the major stage we are in, but there are a number of iterators in the task that can be leveraged.    _Note: the exact items logged may get clearer as the work is done. Above AC is from a quick analysis of the code_    Original description (above description added to be inline with [the other task log context updates|https://issues.sonatype.org/issues/?jql=issue%20in%20linkedIssues(NEXUS-13580)%20AND%20summary%20~%20%22with%20context%22%20ORDER%20BY%20key%20ASC]:  ----  In contrast to NEXUS-13939 which deals with Mark messages only across all tasks, this is a more specific issue about snapshot removal task.  h4. Expected   - At the start and end of each major area of work performed by the task, add a contextual INFO log message to the nexus.log and task specific log indicating what work is being/been performed   - at regular intervals during work, log progress with contextual data about the actual progress of that specific work   - looking at a thread dump should not be the only way to tell what the task is doing at any given moment.    Attached Example task log output which is not adequate.    When the log was captured, a thread dump shows the task is busy trying to find released snapshots, but there is no indication in the log that this is what it is actually doing or what progress it has made so that one can have a better sense of when it might complete or move on to the next piece of work.  """
"NEXUS-14038","Bug","Repository|Scripting",1,"Group repositories created through the Provisioning API do not preserve member order","""Two problems introduced by the fix for NEXUS-13064:  - Jackson serializes LinkedHashSet to a HashSet when transmitting to the client, losing the ordering  - Orientdb serializes LinkedHashSet to OTrackedSet when converting the mapped attributes on a Repository configuration, so even though the member order is initially correct with the in-memory representation, restarting the server will reload from the db and therefore lose the order    NOTE:  - for the small number of Repositories that are likely affected by this, the manual remediation after this fix is to load the Group repo in the UI, reorder the members as desired and save; from this point forward the order should be consistent  """
"NEXUS-14034","Bug","Database|NuGet",2,"NuGet ""filter=(tolower(Id))"" queries don't use a database index in Nexus 2.x, causing severe performance issues in large instances","""Nexus 2.14.4 has experienced some severe performance problems with certain NuGet builds. Queries issued sporadically slow down, and they frequently get complete failures due to the h2 connection pool being exhausted.    They have builds that are making large numbers of queries like this:    Investigation has shown the cause was queries with a e.g. """"((LOWER(ID) = 'nunit')"""" in them. It turns out these will not use the index, and will result in a full table scan being done.     *Acceptance*   * Queries with a """"LOWER"""" will use the index"""
"NEXUS-14029","Bug","NuGet",2,"Support Nuget ""NormalizedVersion"" queries","""Newer versions of NuGet make queries with a """"NormalizedVersion"""" field.    [https://github.com/NuGet/NuGetGallery/issues/2944]    Example:         When Nexus 2.x receives one of these queries it results in an error:         The query is then re-issued by NuGet.exe with just """"version"""", so this isn't visible to the end user.  But it creates a *ton* of log noise, and no doubt slows builds down."""
"NEXUS-14048","Bug","Browse Storage|UI",2,"Component/Asset browsing UI gets JavaScript errors in Edge browser","""Reproduce steps:   # Load the Nexus 3.5 UI in the Edge browser on Windows 10   # Click on the asset browser   # Click on the component browser    You'll get a JavaScript error (see screenshot).    I reproduced this using an Edge browser which had a completely clear cache."""
"NEXUS-14062","Bug","Migration|NPM",5,"ERROR OConcurrentModificationException Cannot UPDATE the record because the version is not the latest during upgrade of npm repositories ","""During upgrade from Nexus 2.x to Nexus 3.x, Nexus 3 logs may report the following type of ERROR:        This looks like a problem where multiple ProcessChangesStep threads in Nexus 3 could be attempting to update the same npm package metadata for different tarballs. The concern is that some npm package metadata tarballs _may_ be lost on upgrade to Nexus 3 when encountering this type of ERROR.    In the case of a proxy repository, does not appear to be serious. In the case of this happening with a hosted repository, this is considerably more serious.      h4. Workaround    In Nexus 3, edit ./bin/nexus.vmoptions  Add this new line at the end of that file and save:        This attempts to reduce the migration concurrency to avoid the problem of different threads trying to update the same record.    Restart the Nexus 3 migration using a completely empty karaf.data directory"""
"NEXUS-14058","Story","Documentation|Yum",5,"configurable repodata depth for yum proxy repos","""*Background*    The yum client commands and yum configuration files [support dynamic variables|https://www.centos.org/docs/5/html/5.2/Deployment_Guide/s1-yum-useful-variables.html]:    $releasever   This is replaced with the package's version, as listed in distroverpkg. This defaults to the version of the redhat-release package.    $arch   This is replaced with your system's architecture, as listed by os.uname() in Python.    $basearch   This is replaced with your base architecture. For example, if $arch=i686 then $basearch=i386.    The yum client can use these variables to dynamically construct the final URL of the repository from which to download yum metadata - for example in yum.conf:    The advantage from a client perspective is a more flexible configuration.    Using Nexus 2.x Yum support or Nexus 3.5.0 Yum Proxy support, a Nexus administrator needs to create a single Yum repository for every combination of dynamic variable. This is because Nexus will only expect repodata at the root of the remote URL value of a proxy repo or at the base of a hosted repo.    This is not ideal since repositories are heavyweight and the management of these many repositories is not scalable if a repository is made for every unique variable combination.    Instead it should be possible to create a single Nexus repository that supports any unique combination of URL form used by the client *at a specific depth*. The base URL from the Nexus administrator perspective would be simply [http://nexus:8081/repository/yum-proxy/] and the administrator would configure a depth at which they expect the metadata about what is in the repository will be stored/calculated.    Example:   Client configures baseurl=[http://nexus:8081/repository/yum-proxy/$releasever/os/$basearch/]   Nexus has single yum proxy repo at [http://nexus:8081/repository/yum-proxy/] with a repodata metadata depth of '3' ( 3 path parts ). So metadata will be accessible under [http://nexus:8081/repository/yum-proxy/$releasever/os/$basearch/repodata/*]    Example:   Client configures baseurl=[http://nexus:8081/repository/yum-proxy/$basearch/$releasever]   Nexus has single yum proxy repo at [http://nexus:8081/repository/yum-proxy/] with a repodata metadata depth of '2'. ( 2 path parts ). So metadata will be accessible under [http://nexus:8081/repository/yum-proxy/$basearch/$releasever/repodata/*]    Note: this scheme does not mean repodata within a single Nexus repository can be at multiple depths - you would still need to create more than one repository per depth    *Acceptance*   * Support repos at a configurable depth inside a single aggregating repo   ** When we generate metadata, it's at the correct depth   ** Ensure we're only invalidating the appropriate metadata when content changes """
"NEXUS-14075","Bug","Migration|Repository Health Check",3,"Migrated proxy does not migrate analyzed","""I migrated over my Central proxy and noticed that while the contents migrated and the Health Check task migrated, the health check column in the repository list still had the analyze button (the NXRM2 one had report results).  I tried running the task and still nothing.  I may be missing some subtlty (or maybe itetentional) but it seems like this aspect is not migrated to me.  Workaround is click the analyze button and enable as usual (NOTE: Doing this results in """"Insufficient Trend Data"""" but still it's enabled).  I left major as I have no clue if the trend data is gathering while not analyzing.  If it is, then this seems minor."""
"NEXUS-14105","Bug","RubyGems",5,"potential OutOfMemory or poor performance retrieving specs.4.8.gz from a rubygems repository","""Make a RubyGems group. ( gem-group )  Put a RubyGems proxy to https://rubygems.org in the group as the only member.    Make a HEAD request for http://localhost:8081/repository/gem-group/specs.4.8.gz - HEAD with return no content as expected, status 200.  Make a GET request for http://localhost:8081/repository/gem-group/specs.4.8.gz - GET will return content and status 200.    A local test on Nexus with no load showed consistent 6-7 second response times infinitely for both types of requests with a single user making the request.    The time required for Nexus to begin a HTTP response is identical for both HTTP methods and much slower than sending the same request to rubygems.org.    Repeating the requests over and over even though no content has changed, does not improve the response time.    A thread dump while a HEAD request is active shows the following work being performed:        As load increases ( increase concurrent threads requesting these files ), the time to respond increases exponentially.    Further it is very easy to trigger OutOfMemory when multiple threads request this file at the same time using either method type.      """
"NEXUS-14174","Bug","Database",5,"Component Database got corrupted after OOM exception.","""After an OutOfMemoryError, the component database was corrupted. A """"{{repair database --fix-links"""" and """"rebuild index *}}"""" was required to fix the database.      There have been many timeout errors throughout the day:  """
