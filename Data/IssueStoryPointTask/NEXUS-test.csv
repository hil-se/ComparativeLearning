"issuekey","type","components","storypoint","title","description_text"
"NEXUS-14198","Bug","Transport",1,"outbound proxy repository requests for identical files which take more than 10 minutes duplicate work and disk space","""Configure a Proxy repository which points to a remote server hosting a large file that will take over 10 minutes to download.    Send 2 or more identical requests into the proxy repository for that large file.    One request will begin downloading the remote file. The other requests will be put into a queue waiting for it to complete.    The first request starts writing the remote file into the blobstore as a temporary file. It may look like this:        After 10 minutes, the initial request is not done. The second thread that was in the queue will get tired of waiting and begin downloading the same file. Now two threads are actively downloading the same file from the remote and writing the content to the blobstore to a temporary file.        This pattern continues for as long as there are identical inbound requests for the same file in a queue - every 10 minutes, a new outbound request for the same file begins. The disk holding the repository blobstore location becomes more full.    In the case of    - the remote file is large ( less common multi-GB )   - the network is comparatively slow and/or bandwidth starved ( common )  - requests for identical files at the same time come into Nexus concurrently ( common ie. chef )  - the blobstore does not have considerable free space larger than (concurrent requests \* file size)     there becomes a high probability that the blobstore may fill with these temporary files, overflowing the disk. Running out of disk is a critical event.    h4. Expected    - the common cases described should be better mitigated against to prevent high disk usage and otherwise useless outbound requests and negative side effects of this behaviour    h4. Workaround    A Nexus administrator needs to guestimate the longest time the largest file should take to be downloaded from a proxy repository remote. Knowing this value, they can change the default 10 minute wait period to this new value.    For example, if the longest remote request can take up to 8 hrs, then:    Edit {{sonatype-work/nexus3/etc/nexus.properties}}    Add a new line like this to the file:        or        or          """
"NEXUS-14196","Improvement","Docker-Nexus",1,"Nexus 3 docker image should allow tweaking the ""-XX:MaxDirectMemorySize""","""Our [installation requirements|https://help.sonatype.com/display/NXRM3/System+Requirements#SystemRequirements-GeneralMemoryGuidelines] indicate that """"-XX:MaxDirectMemorySize"""" should be increased on systems that have sufficient RAM to support doing so.  But our Nexus 3 docker image currently provides no way to change this value.    We should change our Docker image to allow customization of that falue."""
"NEXUS-14211","Bug","NuGet",2,"Certain OData requests can cause pathological regex behavior in Nuget implementation","""Certain Nuget OData requests do not play well with the regex-based query rewriting introduced in 3.5.0 causing poor performance.     This can be confirmed by trying the following request:    I have also been able to confirm that this issue does not occur if the regexes are not in play."""
"NEXUS-14227","Bug","Scheduled Tasks",2,"Thread count increases linearly with scheduled task execution","""Starting with 3.5.0, a new thread appears to be created each time a scheduled task executes, but is left in the WAITING state at the conclusion of the task. The overall thread count for the Nexus process increases indefinitely until the max user processes limit is reached (ulimit -u). Once this occurs, Nexus becomes mostly unresponsive.    The behavior was first observed in a regular installation of 3.5.0 on RHEL 6, where the default ulimit -u was set to 1024. Increasing the """"max user processes"""" limit extends the time before restarting the Nexus process is required.    The issue of threads linearly increasing was reproduced using the sonatype/nexus3 Docker image on a macOS host using the following steps:   # Create and run Nexus in new Docker container with defaults:  {code:none}  docker run -d -p 8081:8081 --name nexus sonatype/nexus3:3.5.1  {code}     # Login as admin, navigate to """"Metrics"""", download """"Thread dump"""" (result attached: 01-threads-start.txt)   # Create scheduled task, select """"Advanced"""" for """"Task frequency"""", and provide """"* * * * * ?"""" for cron expression to run every second.   # Wait 30 minutes (approx. 1800 tasks run, 1800+ threads).   # Navigate to """"Metrics"""", download """"Thread dump"""" (result attached: 02-threads-1800-tasks-run.txt)    Thread count never exceeds 120 when following the same steps on released versions 3.3.0 - 3.4.1. """
"NEXUS-14312","Bug","Yum",0.5,"Yum proxy of mirrors.centos.org fails with encoding error ","""Reproduce steps...    # Create a yum proxy repository in Nexus with remote set to http://mirror.centos.org/centos/7/os/x86_64  # Configure yum in CentOS 7.3 to use it    This fails with an error complaining that the encoding has not been set in the primary.xml file.    Sure enough, if you check the file from RedHat it has:        Our generated XML is missing the encoding:        Full error log is below:    """
"NEXUS-14315","Story","S3",2,"S3 Blobstore - provide a blobstore 'prefix' option","""As a nexus administrator    I want to have multiple blobstores in one S3 bucket    so that I can use 1 S3 buckets for all my blobstores    *Assumptions*    At time of writing, the S3 Blobstore assumes that the bucket is exclusive to the blobstore (the content is stored at the root of the bucket).    If we provide administrators the ability to supply a """"prefix"""" for all content in the S3 blobstore, this will allow them to have multiple blobstores sitting in one S3 bucket.    *Acceptance Criteria*    * The UI presents an option, empty by default, for a content prefix to use. When set, all content for the blobstore is stored under the prefix.    * Using the prefix allows multiple separate blobstores to co-exist in the same S3 bucket."""
"NEXUS-14387","Bug","UI",3,"Repository creation fails often with Javascript errors in Edge Browser","""*To reproduce*   # On a Windows 10 machine (can be a vm) start up NXRM, preferably the latest 3.* version   # Login with Admin credentials   # Navigate to Repositories   # Click create Repository   # Select for example yum-proxy    *Expected*   * Create Yum Proxy repository page    *Actual*   * Empty page stuck at """"Repositories / Select Recipe"""", showing a red danger popup saying :      *Side affect*   * After above error, click on the left hand side """"Repository -> Repositories"""" link will show the same error with strange screen layout (see attached screen capture)    Note appears to be similar as NEXUS-14048"""
"NEXUS-14385","Bug","NPM",1,"Possible regression, publishing npm package fails on 3.5.2, same package works on 3.1.0","""Paypal is getting an error when publishing the attached npm package into Nexus 3.5.2.The same package can be published into Nexus 3.1.0 without any problem.           This is caused by the """"scope"""": """"null"""" line here:        The """"_args"""" is added by the npm installer when it locally installs a package in a node_modules directory:    https://github.com/npm/npm/issues/10393    And it seems like it was only put in when using npm versions prior to 5, although I'm not completely certain about that.    I'm not sure about whether publishing the package.json produced by a local install is a good idea or not, but we should at least be able to handle these uploads, especially because we used to do that in version 3.1.0. (We may want to be ignoring these fields being put in by the npm install altogether, but this particular issue is related to a parsing regression.)"""
"NEXUS-14384","Bug","Logging|Repository",0.5,"No INFO log message when repositories are created and deleted","""When a user creates or deletes a repository in Nexus 3 nothing is logged at all to record these actions in the nexus.log file.    This is very important information, without it we will get reports of repositories """"disappearing"""", and no way to investigate them.  There should be an INFO level log message for both of these events."""
"NEXUS-14381","Bug","Scheduled Tasks",1,"Schedule task compact blob store failed with BlobId: null","""Hi,    since we upgraded in 3.5.2 the scheduled task compact blob store failed.    2017-09-18 07:19:21,342+0000 WARN [quartz-1-thread-19] *SYSTEM org.sonatype.nexus.quartz.internal.task.QuartzTaskJob - Task 8d20bb2e-f599-4977-97f0-3970fc912118 : 'maven' [blobstore.compact] execution failure   org.sonatype.nexus.blobstore.api.BlobStoreException: BlobId: null, java.lang.NullPointerException   at org.sonatype.nexus.blobstore.file.FileBlobStore.compact(FileBlobStore.java:575)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:270)   at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)   at org.sonatype.nexus.blobstore.compact.internal.CompactBlobStoreTask.execute(CompactBlobStoreTask.java:79)   at org.sonatype.nexus.scheduling.TaskSupport.call(TaskSupport.java:92)   at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.doExecute(QuartzTaskJob.java:145)   at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.execute(QuartzTaskJob.java:108)   at org.quartz.core.JobRunShell.run(JobRunShell.java:202)   at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)   at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)   at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)   at java.util.concurrent.FutureTask.run(FutureTask.java:266)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)   at java.lang.Thread.run(Thread.java:745)   Caused by: java.lang.NullPointerException: null   at org.sonatype.nexus.blobstore.file.FileBlobStore.maybeCompactBlob(FileBlobStore.java:583)   at org.sonatype.nexus.blobstore.file.FileBlobStore.compact(FileBlobStore.java:563)   ... 16 common frames omitted         Thx"""
"NEXUS-14398","Bug","Content Selectors|Documentation",0.5,"Content selector with ' in path errors","""One of the comments on https://docs.sonatype.com/display/Nexus/Content+Selector+Testing is  """"String literals containing """" or ' will be rejected"""".  I attempted _path =^ """"/jtt/jtza'pp/""""_ and got back the below error both in the UI and in the nexus.log:    Full here: https://gist.github.com/joedragons/6c052e86c4c19a45a4c569b41b845a15    So it is rejected but filing in case we want to handle the error (I suspect we normally do, tho this may be an exception).    I checked 3.5.2 and this is not new to CSEL and not regression."""
"NEXUS-14414","Bug","Security|UI",1,"Privileges warn they're read only twice","""On our default uneditable privilieges I noticed that the message that they cannot be edited appears twice.    See attached (by the discard button).    I suspect this is regression but haven't back checked at this time."""
"NEXUS-14407","Story","REST",8,"REST Search & Download by 'Latest'","""*Acceptance*   * Extend the search & download service so that users can sort the search results by 'latest version' (see below)   ** In the case of more than one, return the first-ordered result (since the 'best' result is unambiguous).   * There multiple standard """"latest"""" orderings   ** format-specific ordering   *** maven2 ordering of component.version (as implemented by [Apache Maven Artifact Resolver|https://maven.apache.org/resolver/] ( formerly [Eclipse Aether|https://projects.eclipse.org/projects/technology.aether] which is being [migrated to maven-resolver|http://maven.apache.org/aether.html] )   *** semver ordering of component.version     """
"NEXUS-14419","Bug","Licensing",0.5,"Access log output not sorted as expected","""Came up with static analysis of the codebase that the Comparable impl here was never implemented correctly."""
"NEXUS-14441","Bug","Repository|Scheduled Tasks",3,"Race condition can cause component upload to fail.","""If a large file is uploaded while a snapshot removal task is running it can fail with a file not found error.  This is a rare race condition, but it can happen.    This is because the directories needed in the target repository are made here:    https://github.com/sonatype/nexus-public/blob/release-2.14.5-02/components/nexus-core/src/main/java/org/sonatype/nexus/proxy/storage/local/fs/DefaultFSPeer.java#L159    Only after this is the file actually stored, and then after it is stored Nexus attempts to rename it to it's final destination:    https://github.com/sonatype/nexus-public/blob/release-2.14.5-02/components/nexus-core/src/main/java/org/sonatype/nexus/proxy/storage/local/fs/DefaultFSPeer.java#L190    But at that point a snapshot removal task may have deleted it's destination directory.    This was actually observed in a user's log files. They had an upload that finished at """"26/Sep/2017:07:39:42 -0700"""", and ran for 790 seconds.    A snapshot removal task finished at 07:35:32:        And then a few minutes later the upload failed because the destination directory didn't exist anymore.      """
"NEXUS-14439","Bug","Database|REST",3,"/service/siesta/rest/beta/search IllegalStateException current database instance is not active on the current thread while under stress","""Under stress ( 700-900 requests per second of various types ) the /service/siesta/rest/beta/search resource may trigger the following message in the nexus.log:        **Note**: When this exception happens, the request.log will not show any 4xx or 5xx status codes for this request, only 200 - this seems wrong as it sounds like a pretty severe error and may suggest the REST resource needs better error handling. Unclear what the response payload did contain ( and error payload with message and status code 200???? )"""
"NEXUS-14459","Bug","NPM",3,"npm logout throws a 403 error","""the npm login command works with no permissions, but to do a logout you get a 403 error.  Once the -delete permission is added to the user/role you are allowed to.     Nexus 3.5.2        """
"NEXUS-14465","Bug","PyPI",1,"PyPI hosted repository does not send etag header","""Seems like the response from Nexus when downloading a hosted PyPI artifact doesn't include an etag or expires header.  This prevents pip from caching the artifact.  I see Maven handles this better by synthesizing an etag on upload if none is set:  [MavenFacetUtils.jav|https://github.com/sonatype/nexus-public/blob/1c8eff7e75a95cd96f4152e3f649245f21a76c66/plugins/nexus-repository-maven/src/main/java/org/sonatype/nexus/repository/maven/internal/MavenFacetUtils.java#L149]a    You can see [here|https://github.com/pypa/pip/blob/a9d56c7734fd465d01437d61f632749a293e7805/src/pip/_vendor/cachecontrol/controller.py#L273-L279] for how pip handles the caching, and why an etag or expires header is desirable.     The response looks like:     HTTP/1.1 200 OK    Date: Mon, 02 Oct 2017 16:49:10 GMT    Server: Nexus/3.5.0-02 (OSS)    X-Frame-Options: SAMEORIGIN    X-Content-Type-Options: nosniff    Last-Modified: Mon, 25 Sep 2017 22:13:17 GMT    Content-Type: application/zip    Content-Length: 51711"""
"NEXUS-14484","Story","Documentation|Yum",0.5,"Add support for proxying yum repositories that have sha1 checksums","""If you proxy a yum repository in Nexus 3 whose remote has """"sha"""" style checksums it will fail.         An example of this style can be seen here:    [http://yum.spacewalkproject.org/2.5-client/RHEL/7/x86_64/repodata/]         As noted in the createrepo documentation:         We should consider adding support for this style of checksum, since an and user is often not in control of the checksum format that is used on the remote of a proxy repository."""
"NEXUS-14512","Improvement","Docker|Documentation",1,"add anonymous search for docker repositories","""As an extension to providing anonymous docker pull access we would also like to provide search capability."""
"NEXUS-14511","Bug","Backup",1,"ERROR failed to release FreezeRequest from DatabaseBackupTask","""Database backup task may indicate successful completion, but also report """"fail to release FreezeRequest"""".        The message implies the database may be left """"frozen"""".    h4. Expected    The ERROR message consequences need to be better understood and prevented if possible.  If there is a marker file stored someplace and its presence changes behavior, then we need to capture that fact in a support zip in some place other than a log file ( ie. sysinfo.json? ) - -I suggest sysinfo.json because we cannot infinitely grow the support zip with included files- sounds like the actual contents of frozen.marker may be relevant, therefore will be relevant to include the actual file if present, in the support zip."""
"NEXUS-14493","Bug","Backup|Documentation",2,"directory to restore backups from should not be named ""backup""","""The Database backup scheduled task asks for a directory to store database backups within.    Naturally some users would want to store this some place under their existing karaf.data directory in a directory named 'backup.    Unfortunately Nexus allows this, which means next time they try to start nexus, an automatic restore will be attempted.    Worse, if there are more than two backups there, Nexus will not even start.  h4. Expected   * explicitly do not allow the task which performs backup to accept a target location which is the same location from which a restore will automatically take place   * change the default restore location from $\{karaf.data}/backup to something much more obvious, like $\{karaf.data}/restore-from-backup    * do not worry about backwards restore compatibility to the old restore location - it was just a bad idea"""
"NEXUS-14488","Bug","Docker",1,"Cannot perform docker pull against some Docker-Hub proxy","""Hi,   I have Nexus OSS 3.6.0-02 installed running on the context path [http://10.105.139.17:8082/nexus]   I have:   - created a docker proxy repository to docker hub listening on 10.105.139.17:18001   - configured the docker client to allow insecure calls to the docker repository 10.105.139.17:18001    I can perform a search for jenkins docker images and get a resultset via:           but cannot perform a docker login or docker pull           My docker client details are:       Could you please confirm if this is a bug related to running Nexus OSS 3.6 on a context path /nexus or if this is a mis-configuration error.    Many Thanks        Chris"""
"NEXUS-14521","Bug","PyPI",3,"Pypi proxying is broken between 2 NXRM instances","""We have several nexuses. The two in question are set up such that:   * `A` nexus has a pypi group `pypi`, with several constituent repositories   * `B` nexus has a pypi proxy repository pypi-upstream, configured with an upstream url of `https://A.example.com/repository/pypi/`    We're seeing it fail to proxy artifacts correctly.         If we view `https://B.example.com/repository/pypi-upstream/simple/python-dateutil/` it shows no links. Invalidating the cache or rebuilding the index doesn't help.    A tcpdump on nexus A shows that it is indeed sending a request, however the request it sends has an If-Modified-Since header, which consequently returns no results.         Nexus B accepts this, fills it's cache with a blank result (which is nonsense), and proxying is thus failing.         (Querying nexus A using curl presents many links for `https://A.example.com/repository/pypi/simple/python-dateutil/`, the packages are definitely there)     """
"NEXUS-14520","Bug","Repository Health Check",1,"Nexus unresponsive due to healthcheck asset download count","""Nexus becomes unresponsive and CPU is high.    Following threads are seen in the thread dump.      See the following errors in the log:        Workaround is to disable asset download count.    [How do I disable recording download asset counts?|https://support.sonatype.com/hc/en-us/articles/115006490287-How-do-I-disable-recording-download-asset-counts-]"""
"NEXUS-14528","Bug","Logging|Repository",1,"INFO log message is missing when a repository is deleted/created/changed","""- -creating a repository-   - -deleting a repository-   - changing settings of a repository    Are system changes and all system level changes need to be logged visibly at default log levels.    For group repositories, the group members added or removed need to be logged explicitly. A concern is that logging the entire active group member list ( even the first level ) would create a very verbose log statement in some circumstances - this is why only first level additions and removals are requested. Yes - when the repo is first created, listing all the first level member ids is expected."""
"NEXUS-14593","Bug","UI",1,"Errors reported when accessing NXRM3 via index.html","""Intermittant errors have been reported when accessing NXRM3 via index.html (aka localhost:8081/index.html). See attached.   No errors have been reported when using the site without index.html.   Reportedly, this was carryover from NXRM2 (and was a result of old bookmarking).    *Steps to Reproduce*   * Edit _nexus.properties_ and set _nexus-context-path=/nexus/_   * Start nxrm and go to [http://localhost:8081/nexus/index.html]"""
"NEXUS-14604","Bug","Yum",2,"Yum proxy repository with relative paths in ""location"" cannot be proxied","""This yum repository:    [https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64]    has relative paths in it's """"location"""" tags in the primary.xml.gz file:    It is not possible to create a proxy repository of this location in Nexus 3, because the """"location"""" tags are not rewritten.  The yum client attempts to make a request to the """"../../pool"""" location against nexus, and this is not valid."""
"NEXUS-14603","Story","REST",5,"REST Asset Search JSON API","""*Acceptance*   * This sub-resource accepts the same inputs as NEXUS-11745 (search REST endpoint), except it replies with an ordered JSON list of assets   * Until NEXUS-11744 (order by 'latest') is implemented, it's fine if the ordering is unspecified (e.g. it emerges from Elastic Search scoring)    *Non-Requirements*    (Just for clarity)   * We're trying to avoid format-specific logic   ** For maven, if people want the 'jar' (which was a default in NXRM2), then they specify that as a search parameter. maven.extension-jar   ** Primary maven artifacts, as designated in POMs, aren't considered. Users have to ask for what they want.   ** Similarly for PyPI, it's users' responsibility to know if they want an egg, a wheel, if they care, etc. and to specify the search accordingly"""
"NEXUS-14598","Bug","Documentation|NPM|User Token",3,"Setting ""require user tokens for repository authentication"" prevents npm bearer tokens from working","""If you enable  """"require user tokens for repository authentication"""" under """"security/user tokens"""" in the UI then publishing npm packages using npm bearer tokens no longer works.    Expected:  Npm bearer tokens should continue to work regardless of the user token required setting. Bearer tokens are just as secure as user tokens.    Also note that you cannot execute """"npm login"""" using a user token, npm won't allow it due to the charachters in the username. So that doesn't provide a workaround:     """
"NEXUS-14639","Improvement","REST",1,"Add sha1 to asset resource on REST API","""One outcome of NEXUS-14631, all agreed that sha1 (a universal attribute on all formats) should be part of the asset resource response in the API    Acceptance   * Add checksum to asset resource (AssetXO)   * Should apply to /components, /assets, and /search"""
"NEXUS-14636","Improvement","Maven",1,"allow customizing maven metadata rebuild db query buffer size and timeout values to mitigate IllegalStateException Timed out reading result from queue","""Customers with large databases can have issues with the maven metadata rebuild with timeouts. Maven metadata can be rebuilt with the explicit task or as a side effect of snapshot removal tasks.    Currently this implementation uses an asynchronous Orient query where Orient is feeding results into the backing queue faster than the metadata code can process them. Ultimately once the limits of the backing queue are hit (defaults are size 128, timeout 1 minute) then an error is thrown:    One of the approaches to alleviate this issue without looking at larger re-writes is that we want to make these values configurable    *Acceptance*:   * The {{MetadataRebuilder.browseGAVs}} query needs to ultimately use {{OrientAsyncHelper.asyncIterable}} with the {{bufferSize}} and {{timeoutSeconds}} options. This will require adding a {{StorageTx.browse}} that exposes these two params.   * Configuration option for the buffer size in {{browseGAVs}} query. Default should be 1000   * Configuration option for the timeout in the {{browseGAVs}} query. Default should be 60 seconds"""
"NEXUS-14653","Bug","Security|UI",1,"UI session time out not respected","""Admin user login.   Edits UI: Settings capability from default values.   Authenticated User polling interval was set to 2 minutes   Session timeout set to 3 minutes.   Save capability.   Logout.   Login as admin user and wait on welcome screen.    Stay on welcome screen.    After approx *5* minutes ( not the expected 3) , UI timed out and prompts user that session will timeout in 30 seconds and begins count down.    Allow the session to timeout. The UI sends an explicit request ( not user initiated ) to delete the session.    h4. Expected    The UI session timeout after inactivity should be respected for any new sessions while the regular polling the UI does should not be classified as 'activity'.  h4. Reference    https://groups.google.com/a/glists.sonatype.com/d/msgid/nexus-users/68d7050b-18bb-4793-b23b-f2efd8ec6306%40glists.sonatype.com?utm_medium=email&utm_source=footer     """
"NEXUS-14646","Bug","Repository",0,"Etag is missing in http response for an artifact download from a proxy repository nexus3","""Etag is missing in the http response for an artifact download from a proxy repository, however it works with hosted repository    Nexus: 3.6.0-02    Repository type: proxy    Format: maven2    Proxy: [http://repo1.maven.org/maven2/]         The http response in nexus2x is returned with Etag """
"NEXUS-14671","Bug","Bootstrap|Repository",1,"Missing repository recipe prevents startup","""If a repository recipe cannot be found it prevents the startup Nexus.  This is a problem, because third party plugins often create new repository recipes, and it is very easy to forget to re-install a third party plugin during an upgrade.      Expected: An error should be logged if a repository recipe cannot be found, and the repository should either not be loaded, or should be disabled.  Nexus should still be able to start.  """
"NEXUS-14729","Bug","Transport",2,"Regression: Nexus 3 returns 501, 400 and 204 responses for MKCOL requests","""Nexus 3 returns 501, 400, and 204 responses for MKCOL requests:       {quote}127.0.0.1 - admin [01/Nov/2017:13:37:07 -0500] """"MKCOL /repository/ HTTP/1.1"""" 400 0 1 """"Jakarta Commons-HttpClient/3.1""""    127.0.0.1 - admin [01/Nov/2017:13:37:07 -0500] """"MKCOL /repository/raw/site/ HTTP/1.1"""" 204 0 1 """"Jakarta Commons-HttpClient/3.1""""    127.0.0.1 - admin [01/Nov/2017:13:37:07 -0500] """"MKCOL /repository/raw/ HTTP/1.1"""" 400 0 1 """"Jakarta Commons-HttpClient/3.1""""    127.0.0.1 - admin [01/Nov/2017:13:37:07 -0500] """"MKCOL /repository/ HTTP/1.1"""" 400 0 2 """"Jakarta Commons-HttpClient/3.1""""    127.0.0.1 - - [01/Nov/2017:13:37:07 -0500] """"MKCOL / HTTP/1.1"""" 501 0 1 """"Jakarta Commons-HttpClient/3.1""""  {quote}       It should be returning 405. This is covered here for Nexus 2.x:    https://issues.sonatype.org/browse/NEXUS-6169    It is currently returning 400 for MKCOL within a repository, and 501 for MKCOL to the context root, and 204 for other directory creation requests.         This error response can break builds, and cause HTTP monitoring alarms to be raised when they should not be.    The 204 responses are causing the maven-site-plugin to make separate MKCOL requests to create each parent directory individually, rather than giving up after the first 405 is received.  This causes site deploys to Nexus Repo 3 to take much longer than they did for Nexus Repo 2, even though the time for processing the PUT requests is faster in Repo 3.    Here is a sample of requests for Nexus Repo 2:  {quote}123.123.123.123 - admin [13/Jun/2019:14:29:18 -0400] """"MKCOL /nexus/content/sites/site/org/foo/bar/fubar/tools/blah/project/0.0.1-SNAPSHOT/images/ HTTP/1.1"""" 405 654 4   123.123.123.123 - admin [13/Jun/2019:14:29:18 -0400] """"PUT /nexus/content/sites/site/org/foo/bar/fubar/tools/blah/project/0.0.1-SNAPSHOT/./images/icon_warning.gif HTTP/1.1"""" 201 0 7  {quote}  And Nexus Repo 3:  {quote}123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/bar/fubar/tools/blah/project/0.0.1-SNAPSHOT/images/ HTTP/1.1"""" 204 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/bar/fubar/tools/blah/project/0.0.1-SNAPSHOT/ HTTP/1.1"""" 204 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/bar/fubar/tools/blah/project/ HTTP/1.1"""" 204 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/bar/fubar/tools/blah/ HTTP/1.1"""" 204 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/bar/fubar/tools/ HTTP/1.1"""" 204 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/bar/fubar/ HTTP/1.1"""" 204 0 0 2   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/bar/ HTTP/1.1"""" 204 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/foo/ HTTP/1.1"""" 204 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/org/ HTTP/1.1"""" 204 0 0 2   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/site/ HTTP/1.1"""" 400 0 0 2   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/repository/ HTTP/1.1"""" 400 0 0 1   123.123.123.123 - - [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/ HTTP/1.1"""" 501 0 0 0   123.123.123.123 - - [13/Jun/2019:14:06:46 -0400] """"MKCOL /nxrm/ HTTP/1.1"""" 501 0 0 1   123.123.123.123 - admin [13/Jun/2019:14:06:46 -0400] """"PUT /nexus/content/sites/site/org/foo/bar/fubar/tools/blah/project/0.0.1-SNAPSHOT/./images/icon_warning.gif HTTP/1.1"""" 201 0 7  {quote}  As you can imagine, for a large site deploy all those extra MKCOL requests become very time consuming.    *Expected*: Nexus Repo 3 should return HTTP 405 (method not allowed) for all MKCOL requests, just as Nexus Repo 2 does.    """
"NEXUS-14838","Bug","Docker",3,"Random version deleted if there are multiple versions with same digest","""Issue: If multiple image version exist with same digest/sha value, nexus randomly picks and deletes any one version.    Usecase:    Developers build snapshots as part of CI/CD process during development phase.    Final snapshot is tagged as release version so digest stays the same.    Developer tries to delete the snapshot version using the API but nexus randomly deletes any version that has the same digest.    Command used to delete :   curl -v -X DELETE -u '<userid>:<passwd>' https://<host>:<port>/v2/<image name>/manifests/<digest>        Fix Suggestion:    Option1 :    Digest should be unique per version    Option2:    Expose delete API that accepts version in the input request so clients know which exact version will be deleted."""
"NEXUS-14837","Story","Blobstore",2,"Script to report blob and repository size and space reclaimable by Compact Blobstore task","""Customers are trying to understand how NXRM is using their disk space; currently it's hard for them to ascribe the size of a blob store to particular repositories.    This is a simpler version of NEXUS-13056.  h4. Acceptance   * Admins can run a script to read the properties files of a blob store to summarize which repositories are using the blob store, and how much space each is consuming   * Tabulate both the total size, and the size that could be reclaimed by compacting (i.e. soft-deleted blobs)    h4. Solution    We have a script that iterates over all files in a single blobstore and reports the total size of all files per repository in that blobstore and the amount of disk that could be reclaimed by running a compact blobstore task on that blobstore.   # Visit this support page: [https://support.sonatype.com/hc/en-us/articles/115009519847]   # Follow the instructions for *Listing the Size of File-based Repositories and Blobstores*"""
"NEXUS-14839","Bug","NuGet",2," Hosted nuget repository ignore the framework version, always returns the .NET Core Version","""As per issue reported here:     [https://groups.google.com/a/glists.sonatype.com/forum/?utm_medium=email&utm_source=footer#!msg/nexus-users/f-CDUV-OqAQ/H2EdCL1pBgAJ]    the behavior when installing a package from a hosted repo is different to that from nuget.org, a nexus proxy to nuget.org, using klondike, or using nugetserver.org's implementation.    In all cases except the nexus hosted repo, the behavior is the same - the others all return the .NET Framework version and it's dependencies whereas the hosted nexus repo returns the .NETCore version and it's dependencies.    The referenced thread contains a project which will demonstrate the problem but as a minimal example of the problem, create a new empty nexus repo and put *only* System.Threading.Tasks 4.3.0 in it - you can get that from here: [https://www.nuget.org/packages/System.Threading.Tasks/]    You can see that the .Net framework version has no dependencies at all, whereas the .NET Core version has dependencies.    Now create an empty folder somewhere and in that folder, try to install System.Threading.Tasks and you will see the output as per the nuget-nexus.jpg image I attached.  The package failed to install because of the .NET Core dependencies which are not available in this repo.    Then try the same thing with a local klondike instance - or indeed with nuget.org, though of course this will have all the dependencies available so you aren't really comparing like with like (ie repositories containing only this single package).    The attached nuget-klondike.jpg image shows that using the klondike repo, the package was successfully installed.    I was using the latest nexus-3.6.0-02 from your website.    https://docs.microsoft.com/en-us/nuget/schema/target-frameworks     """
"NEXUS-14860","Bug","Scheduled Tasks",3,"Rebuild maven repository metadata task bloats blobstore","""h2. Description   # Start up Nexus with a clean blob store   # Deploy a Maven project into a hosted release repository   # Scan the blobstore with """"find . -name """"*.properties""""|xargs grep deleted=true"""", observe there are no deleted blobs   # Schedule and run a rebuild repository metadata task against the repository you deployed into   # Scan the blobstore again, observer there are now 3 deleted blobs for each maven-metadata.xml file   # Run the task again   # Scan the blobstore again, observe there are now an additional 3 deleted blobs for each maven-metadata.xml file    h2. Expected    Rebuilt maven-metadata.xml files should only be written out if they have changed from the previous version.  This means that it is expected that the first run of the task will delete all the existing maven-metadata.xml files and their checksums, since Nexus does not build the files in the same format as Maven.    The consequence of this behavior is that blob storage gets unnecessarily full of deleted files, consuming way more space than is needed, and running a compact blob store task can take a very long time to complete.    The hope is two things:   * avoid replacing the blob with an identical blob - if the hashes match, leave the original in place (and see if there's a way to purge the rejected candidate replacement)   * another thing to consider would be hard-deleting maven-metadata files immediately instead of soft-deleting them (as long as that's not a weird special case down in generic asset code); this would stop blobstores getting bloated with soft-deleted derived content"""
"NEXUS-14869","Bug","NuGet",3,"nuget list against a www.nuget.org/api/v2/ proxy repository returns partial results","""Tested with NuGet CLI 4.3.0    nuget list -Verbosity detailed jquery -Source http://localhost:8081/repository/nuget.org-proxy/     pages results 3 times    nuget list -Verbosity detailed jquery -Source https://www.nuget.org/api/v2/     pages results 40 times    Attached is a charles capture showing both commands for comparison against Nexus 3.6.0.    h4. Expected    There should be no difference in results paged through for identical queries.  """
"NEXUS-14878","Bug","Maven",1,"Artifact with no maven group is redeployed when ""Disable redeploy"" is set on maven hosted repo","""It is possible to redeploy artifacts when Deployment Policy is set to """"Disable redeploy"""". This only happens when the artifact has no maven group.    Steps to reproduce:    1) Create a maven hosted repo with the following setting:   Layout Policy : Permissive   Deployment Policy: Disable redeploy.    2) Upload a file to    [http://localhost:8081/repository/maven-hosted/TestZip/1.0.0/TestZip-1.0.0.zip]    3) Upload again to the same path. This should not be allowed.    [http://localhost:8081/repository/maven-hosted/TestZip/1.0.0/TestZip-1.0.0.zip]    If a maven group is present in the path then the redeploy is prevented         *Acceptance*    Attempts to redeploy should be prevented."""
"NEXUS-14885","Improvement","Documentation|RubyGems",2,"Remove support for the non-gzipped specs 4.8 from Rubygems","""Remove support for rubygems specs 4.8 file for performance reasons and deprecated repository format feature.    This file is an uncompressed repository-wide index and Rubygems.org no longer supports it since it has been replaced by other more efficient indexes."""
"NEXUS-14940","Story","REST",3,"deprecate /service/siesta part of REST API urls","""*Background*    This is seemingly cosmetic issue for the REST API. I think it was discussed in the past, as we were planning development of long term public REST API for Nexus 3, but I don't see an issue to track it.    The REST APIs are being mounted under   - /service/siesta/rest/beta/tasks   - /service/siesta/rest/v1/tasks    The /service/siesta part was not intended to be a long term mount point. To my knowledge it was only kept to make getting /service/siesta/rest/v1/script out the door quicker.    *Acceptance*   * Expose the NXRM 3 public REST api under /service/rest/v1   * This includes the existing scripting endpoint, so update the examples for the scripting API   * the previous /service/siesta/rest/v1/script endpoint has been moved to /service/rest/v1/script  - there is no shim - release notes will announce scripts depending on the old URL will need to be updated """
"NEXUS-14969","Bug","HA",3,"HA-C nodes do not rejoin their cluster after cluster shutdown","""Given an HA-C cluster, when nodes that were previously members of the cluster have been shut down, the nodes will not always properly rejoin the cluster. The following errors have been encountered in the log files:    {code}  2017-11-21 07:18:04,448+0000 ERROR [FelixStartLevel] *SYSTEM com.sonatype.nexus.hazelcast.internal.orient.SharedHazelcastPlugin - [F7E2EC33-6D42028D-074D8BDF-2EED45CB-ABC97A45] No LSN found for delta sync for database 'accesslog'. Asking for full database sync...  {code}    Additionally, there may be error messages similar to the following:    {code}  Caused by: com.orientechnologies.orient.server.distributed.ODistributedException: Quorum (1) cannot be reached on server 'XXXXXXXX-XXXXXXXX-XXXXXXXX-XXXXXXXX-XXXXXXXX' database 'config' because it is major than the nodes in quorum (0)  {code}    h4. Reproduce    To reproduce, here is a brief description of what was done:    1) Start up Nexus1 in the 3-node cluster   2) Start up Nexus2 in the 3-node cluster   3) Start up Nexus3 in the 3-node cluster   4) Add some config to Nexus like new repositories, new users, etc.   5) Stop Nexus1   6) Stop Nexus2   7) Stop Nexus3   8) Try to start up Nexus1 or Nexus2 but an error will occur:    9) If you try to start Nexus3, then it starts up.    h4. Expected    There should not be a defined order to which nodes must rejoin the cluster after being shutdown.    h4. Workaround    The only workaround seems to be to abandon the two nodes that won't start, and add two new nodes to the one working node.    """
"NEXUS-15090","Bug","Search",1,"Asset search does not treat short and long parameter names the same","""Using the shortened parameter names produces the expected result:      But using the long name for extension does not:    These two searches should produce the same result, since {{maven.extension}} is mapped to {{assets.attributes.maven2.extension}}."""
"NEXUS-15089","Bug","REST|UI",0.5,"Error response code 204 not listed in REST API codes for component and asset delete","""While testing, I entered a ID into the REST API via the UI and submitted it and was returned (the expected) 204 response code. I noticed it was not listed in the status code list however.        Acceptance   * Add Swagger doc for 204 response for successful component or asset delete"""
"NEXUS-15088","Bug","REST|UI",0.5,"Incorrect error response code 406 for bad ID in DELETE /component","""If you take a real, but incorrect ID (such as an asset ID) and use it as a component ID in {{DELETE /component/\{id}}}, you get a 406 response.    Acceptance    * Should be considered a malformed ID and return a 422 response"""
"NEXUS-15105","Bug","Documentation|Yum",1,"No Yum specific search fields defined in Documentation","""I noticed that the yum specific search fields are not defined in the documentation (specifically https://help.sonatype.com/display/NXRM3/Searching+for+Components#SearchingforComponents-SearchCriteriaandComponentAttributes).  This seems an oversight."""
"NEXUS-15095","Bug","Blobstore|Maven|Scheduled Tasks",3,"MissingBlobException can occur when publishing maven index","""Hello    I configured   * a blobstore 'central' with   ** proxy-repo for maven-central   * a blobstore 'test-index' with   ** a hosted-repo 'test-index-hosted'   ** a group-repo 'test-index' which groups 'central' and 'test-index-hosted    The indices for 'maven-central' and 'test-index-hosted' are created and published    When i execute the task 'Publish Maven indexes of test-index', it finishes with an error and I get the following stacktrace         The attatched picture shows  that the missing file in the hosted repo is the '.index/nexus-maven-repository-index.properties' from the 'maven-central' proxy-repo which is stored inside the 'central'-blobstore.    When I browse the asset inside 'maven-central' then i can download it.    When I browse the asset inside 'test-index' then i get a http-404. All other maven artifacts are downloadable.         When I remove 'maven-central' from 'test-index' or store it inside the same blobstore then there are no issues and everything works fine          """
"NEXUS-15131","Bug","Yum",3,"Component naming for Yum Proxy does not match RPM header","""Yum Proxy does not use the correct naming conventions for components. This is inconsistent and could cause problems in the future. Also, the versioning does not include the release number.    *Example*  For the package GeoIP-1.5.0-11.el7.x86_64.rpm    Component name for proxy = GeoIP-1.5.0-11.el7.x86_64  Expected component name = GeoIP  Version field for both = 1.5.0    *Acceptance Criteria*  * Yum proxy should use the name from the RPM header (i.e. GeoIP).  * Existing Yum Proxy repositories should be upgraded so those already fetched components use the new naming convention.  * Yum proxy should include the release number in the version (1.5.0-11.el7).  * Existing components in Yum Proxy repositories should be upgraded to use the new versioning."""
"NEXUS-15136","Story","REST",3,"Repository List REST endpoint","""Create a new REST endpoint for Repository interaction; initial phase will only support a small number of operations    *Acceptance*   * User has the ability to retrieve the list of configured repositories in NXRM3   * Returned information should include, at a minimum, the unique repository name, format, and location (URL)   ** Consider including additional metadata during implementation as desired     * Visibility of repos through this endpoint should match the visibility of repos on the UI - it should abide by the same permission scheme    *Notes*   * 'Bucket' doesn't have format on it, so unlike the other REST endpoints this will probably be driven by the config db."""
"NEXUS-15148","Bug","Docker|Documentation",2,"docker anonymous pull configuration confusing","""Users find the docker anonymous pull option confusing.    Change:    Force basic authentication:  [ ] Disable to allow anonymous pull (Note: also requires Docker Bearer Token Realm to be activated)    to:    Anonymous docker pull:  [ ] - Allow anonymous docker pull ( Docker Bearer Token Realm required )    NOTE: This will be an inversion of the current flag in the UI."""
"NEXUS-15147","Bug","LDAP",2,"Multiple edits of user roles fails with: ConcurrentModificationException: User-role mapping","""When adding multiple roles to an LDAP user in multiple steps, the second role will fail to save and the UI shows an error like: """"Warning: user-role mapping 'USERID' updated in the meantime""""     Steps to reproduce.   # Setup LDAP   # Search for (via Source: LDAP, and search box) and Select an LDAP user.    # Add a role from the """"Available"""" list to the """"Granted"""" list. Click Save.   # Add a second role from the """"Available"""" list to the """"Granted"""" list. Click Save. Boom! The warning above is shown, and the change is not saved, nor will retries save the change.    Example:    After saving first role:    !RoleAdd1.png!         Failed save of second role:    !RoleAdd2Error.png!    Support.zip from reproduce case:    [^support-20171207-145031-1.zip]    Stack trace:      h4. Short term workaround for affected versions    A short term workaround should allow you to make role assignment changes if you can't immediately upgrade:    # Make a note of all the currently assigned Roles for the user. Remove all assigned roles, click 'Save'. The user should have no roles at this point.  # Add all Roles you want for the user, click 'Save'. There should be no errors.  """
"NEXUS-15177","Bug","Security|UI",2,"Security -> Realms UI shows incorrect saved realm order for Active realms","""Steps to reproduce:   # Add a new Realm to the Active list (LDAP in this example), and move the new realm below the existing Nexus Local realms. The screenshot below shows the order just before Saving:   !PreSaveOrder.png!   # Click the Save button, and the screen is refreshed and the Active realm order is changed and no longer matches what was saved. In the example below, the LDAP realm is shown first in the list.   !PostSaveOrderWrong.png!         Note:  - despite the different displayed order after clicking Save, the database will store the realm order that was displayed before clicking Save  - if you move the order around, the """"Save"""" button """"disables"""" itself when the order is changed to match the last order that was actually saved"""
"NEXUS-15175","Bug","Bootstrap",1,"Nexus 3 service will not start if /tmp is not writable","""By default Nexus 3 sets the java.io.tmpdir to a location other than /tmp, but Install4J still attempts to write a file to /tmp and the startup fails if the nexus user cannot write to /tmp, either due to permissions or if a file of the same name it is trying to write, already exists - for example one left over from an unclean shutdown.    h4. Workaround     A workaround is documented here: https://help.sonatype.com/repomanager3/installation/run-as-a-service#RunasaService-PIDFile    h4. Expected    Perhaps this should be added to the default configuration?    h4. Reproduce    To test, set permissions on tmp to only allow the root user to write to it. Attempt to start nexus 3 not as root user. Startup should silently fail."""
"NEXUS-15202","Bug","REST",2,"No way to download a jar through /rest/beta/search/assets/download if a jar with classifier for same GAV exists","""I'm trying to use the /rest/beta/search/assets/download to download a jar file:    org/foo/project/1.0.0/project-1.0.0.jar    To do this, I used:    [http://localhost:8081/nexus/service/siesta/rest/beta/search/assets/downlload?repository=maven-releases&maven.groupId=org.foo&maven.artifactId=project&maven.baseVersion=1.0.0&maven.extension=jar]    But this always fails with a 400 response. The logs show that the reason for this is multiple files are found. Using  the/rest/beta/search/assets endpoit I find that the reason is there is also a jar with a classifier:    I can find no way to create a search that does not match the classifier.  So I always get two results, and I cannot download this file directly by REST API.    This is either a bug in the API, or a bug in the documentation.  If the latter, the REST API documentation should show how to search for an empty classifier.     """
"NEXUS-15195","Bug","Search|UI",2,"Logging in from anonymous search causes exception","""Steps to reproduce:    * Sign out of the UI  * Search for a component (e.g. aether)  * Click one of the results (e.g. aether-impl)  * Sign in    After a short delay, an exception pops up and the search feature grays out."""
"NEXUS-15282","Bug","NPM",1,"NPM allows redeploys despite Deploy Policy","""We no longer get any type of HTTP error message when redeploying an NPM package to a hosted repo, with the Deploy Policy set to *Disable Redeploy.*    A HTTP 200 is returned..     """
"NEXUS-15278","Bug","Browse Storage",2,"repositories marked not online can prevent tree and html view from displaying all content","""If a Nexus 2 or 3 instance is upgraded to 3.7.0, and that original instance contained any repository marked offline or out of service, then the background task which builds the Tree View / HTML view may fail to index existing cached items in ANY other repository. The result is there is no way to Browse those repositories that were not indexed by the background task. The exact repositories affected is indeterminate.    Offline means:   * Nexus 3.x: The Online checkbox is not selected.   * Nexus 2.x: The repository has been """"Put out of service""""    This bug does not fail builds which download components or prevent UI search from working, but will hide repository items from the tree view and HTML view for some repositories.    {noformat:title=Example nexus.log ERROR message that will be reported in this scenario}  2017-12-21 09:08:16,900-0600 ERROR [quartz-3-thread-5] *SYSTEM org.sonatype.nexus.repository.browse.internal.RebuildBrowseNodesTask - Could not re-create browse nodes for repository: RepositoryImpl$$EnhancerByGuice$$c53e9f01{type=hosted, format=nuget, name='nuget-hosted'}  org.sonatype.nexus.scheduling.TaskInterruptedException: Repository nuget-hosted is offline, rebuilding browse nodes was cancelled  at org.sonatype.nexus.repository.browse.internal.RebuildBrowseNodesTask.checkContinuation(RebuildBrowseNodesTask.java:144)  at org.sonatype.nexus.repository.browse.internal.RebuildBrowseNodesTask.execute(RebuildBrowseNodesTask.java:101)  at org.sonatype.nexus.repository.RepositoryTaskSupport.execute(RepositoryTaskSupport.java:69)  at org.sonatype.nexus.scheduling.TaskSupport.call(TaskSupport.java:93)  at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.doExecute(QuartzTaskJob.java:145)  at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.execute(QuartzTaskJob.java:108)  at org.quartz.core.JobRunShell.run(JobRunShell.java:202)  at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)  at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)  at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)  at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)  at java.util.concurrent.FutureTask.run(Unknown Source)  at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)  at java.lang.Thread.run(Unknown Source)  {noformat}    h4. Workaround   * if you have repositories marked offline or out of service, do not upgrade to 3.7.0   * if you require access to another bug fix in 3.7.0 before this issue is fixed in a newer release, then contact [support@sonatype.com|mailto:support@sonatype.com] to learn about your options    h4. Expected    All cached repository content should be viewable in tree view, regardless whether a repository is offline or online. Also, a single offline repo should not fail fast the RebuildBrowseNodesTask at all."""
"NEXUS-15364","Bug","Logging",2,"logging from different task threads may log to the same task log if tasks are started within the same second","""It was noticed that the log messages from two different distinct task threads processing different repositories may log to the same task specific log.  h4. Expected    Each scheduled task should log to its own uniquely named task log.     """
"NEXUS-15363","Bug","Migration",2,"Not Found Cache TTL timeout setting of ""-1"" fails upgrade from Nexus 2x to 3x","""In Nexus 2.x instance, if the Not Found Cache TTL timeout is set to -1, then this fails the migration.    You see the following warning in the logs.     Also caused the migration to hang on a group repo containing the above repo."""
"NEXUS-15372","Bug","Security|Smart Proxy",2,"""Smart Proxy - Publish/Subscribe Configuration"" privileges don't work","""The """"Smart Proxy - Publish/Subscribe Configuration"""" privileges don't work, event after granting them to a user they will still receive a 403 response when trying to access the """"/service/local/smartproxy/pub-sub"""" resource:    127.0.0.1 - deployment [02/Jan/2018:08:36:19 -0600] """"GET /nexus/service/local/smartproxy/pub-sub/amd-core-rm-Snapshots?_dc=1514903779880 HTTP/1.1"""" 403 581 3     """
"NEXUS-15422","Bug","RUT Auth|Scheduled Tasks|Security",2,"NuGet API Key generated by a user authenticated with RUT will get purged by Purge Orphaned API Keys task","""h4. Setup    - Nexus 3.7.1  - NuGet API Key realm is active  - RUT Auth realm is active  - RUT Auth Capability is enabled    h4. Problem Reproduce    # Authenticate to Nexus 3 UI using RUT Auth ( your user account can be ldap, crowd or default realm user account )  # Go to your profile and generate a NuGet API Key by clicking Access Key. ( You will be prompted for authentication due to NEXUS-10692 ). When prompted for password, enter your password. Record the displayed API Key.  # Schedule and run a Purge Orphaned  API Key task. After the task completes, your Nuget API Key previously displayed will no longer be valid as the task considered it orphaned and removed it.    Debug logging while the task is run shows why the task considered it orphaned:        h4. Expected    When a RUT authed user creates an API key ( for NuGet or any other key realm ) the API key should be associated to the realm that actually stores the user's account for the purposes of authentication as determined by the order of realms in the active realms list ( default, ldap, crowd), instead of {{rutauth-realm}}. In this way, the key will not normally be considered orphaned and not be removed by the the purge task.    h4. Regression    Nexus 2.x is not known to have this same problem so this issue may affect upgraded configurations from Nexus 2x to 3x."""
"NEXUS-15418","Bug","Logging",1,"include edition and version in logged uptime message","""When Nexus is shutdown, the Uptime is logged - example:        If the shutdown immediately precedes a Nexus upgrade, it is useful to understand what edition and version was shut down immediately preceding the upgrade.    h4. Expected    - include the last active edition and version that was shutdown, in the uptime message:    Example: {{Uptime: 22 hours, 21 minutes, 28 seconds and 576 milliseconds (nexus-pro-edition/3.7.1.02)}}    - make sure that if there is a license change during the last run of the server, the reported edition accurately reflects that last change    *Note:* The edition version string example suggested here is the same one seen during these messages on startup:        Another format of edition version could be acceptable as well, open to suggestions.  """
"NEXUS-15425","Bug","NPM",2,"Asset not updated when a npm package is republished","""I have a npm hosted repository with a deployment policy = """"Allow redeploy"""" in order to simulate snapshot versions.   After the second publish, I can't install the dependency anymore because there is a failure when checksums are compared (tested with npm 5.6.0 and yarn 1.3.2).       The problem appears in Nexus 3.7.0. I don't have this problem with Nexus 3.6.2.        Steps to reproduce the problem:   * Create a npm hosted repository with a deployment policy = """"Allow redeploy""""   * Create a directory with only this package.json:       * Publish to the npm hosted repository       * Create a file README.md   * Publish again to the npm hosted repository   * At this step, you can already see there is a problem in the Nexus UI (asset not updated)   * Create another directory with only this package.json:       * Get the dependencies       * It fails    """
"NEXUS-15467","Bug","Blobstore|UI",1,"Blob store type field is editable... it shouldn't be","""The blobstore type field is editable.  If you put in a value that is not in the list and save you get an NPE (see screenshot).               """
"NEXUS-15461","Technical Debt","Scheduled Tasks",8,"ensure scheduled tasks are cancelable (part 2)","""From execution of --NEXUS-13208-- there are additional scheduled tasks which, as of 3.7, do not support cancellation. For the same reasons as the aforementioned JIRA these tasks should be allowed to be canceled where possible.         * CompactBlobStoreTask   * DockerUploadPurgetask   * PurgeApiKeysTask   * -RebuildAssetUploadMetadataTask- (completed previously)   * -RebuildBrowseNodesTask- (completed previously)   * RebuildIndexTask   * -YumCreateRepoTask- (not addressing at this time)   * -RepairYumProxyComponentsTask- (class no longer exists)"""
"NEXUS-15470","Story","Database",2,"Reduce the default frequency of updating asset last access time","""*Background*    Currently the asset last access time is updated every 1 minute at the most frequent and this is not adjustable. See NEXUS-15469. This can cause excessive updates to the database triggered by asset downloads.    *Acceptance*   * Make the default 'last accessed time' update rate 12 hours (not 1 minute).   * Modify the UI so that it only displays the day (and leaves off hours/minutes/seconds) for this date.   * Download count will not be impacted."""
"NEXUS-15561","Bug","Logging|NPM",2,"GET /-/v1/search java.lang.IllegalArgumentException Invalid query WARN message with no context","""Customer logs contain verbose WARN messages such as this using 3.7.1:        The problem is the large stack trace for a WARN message for a query that at first glance does not appear to be invalid. There is also absolutely no log message information about WHY it is not valid. The log message and spam it creates it not helpful unless more context is added - hiding the context at DEBUG level is NOT helpful.    """
"NEXUS-15582","Bug","Docker",1,"docker proxy repository does not work for container-registry.oracle.com","""I am trying to proxy the oracle container registry, which requires a username/password. It seems there is an error in the callback during auth as it has a space in it.    [https://blogs.oracle.com/weblogicserver/the-oracle-container-registry-has-gone-live]    i get the following error;        h4. Cause    In DockerProxyFacetImpl.retrieveBearerToken we do        The URL is not properly encoded.      h4. Reproduce    # Sign up at https://container-registry.oracle.com   # Create a Docker proxy against that (with authentication)  # pull something with docker against this repo. ie. {{docker pull localhost:12344/database/enterprise:12.2.0.1}}"""
"NEXUS-15714","Bug","NPM",5,"Continue to serve locally cached proxied npm packages that are unpublished on the remote","""It is rare, but technically unpublishing npm packages is possible at a remote npm registry.    When Nexus is proxying a remote npm repository, a specific package version may get locally cached. Then the remote may unpublish that same package, thus altering the package metadata which lists the versions of the package at the remote site.    The next time Nexus gets the remote metadata for that package, the metadata is replaced, not merged, with its own local metadata. See https://issues.sonatype.org/browse/NEXUS-8624.    If metadata age is set as -1, then Nexus will never get new metadata, so will not know about new versions that get published.    If it is set to anything greater, then the new metadata from the remote may get downloaded and will not refer to the already locally cached package, it will replace the local metadata and Nexus will not serve the already cached unpublished package to inbound requests.    There is a use case for continuing to serve the already cached package, if we have a project depending on that package, we don't want our project to be affected if this module is unpublished at the remote.    Nexus should also provide an option to continue serving the cached packages, while at the same time have a method to reconcile packages/metadata which are explicitly deleted from the local cache or remote site."""
"NEXUS-15722","Bug","Docker|HA",3,"Docker push can fail in HA Environment","""Within a HA Environment, a Docker push of an image with several layers can fail due to the upload requests landing on different nodes within the cluster.    For example, Node A handles the initial POST and PATCH requests, but the PUT is handled by a different node (Node B) which doesn't appear to be aware of the upload and ends up throwing a """"java.lang.IllegalStateException: Missing upload with uuid:"""" and returning a 404:  {quote}org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl - Failed to complete upload   java.lang.IllegalStateException: Missing upload with uuid: f26e5743-c7c3-4300-ae69-00166e49b9c7   at org.sonatype.nexus.repository.docker.internal.UploadManagerImpl.ensureGetUpload(UploadManagerImpl.java:134)   at org.sonatype.nexus.repository.docker.internal.UploadManagerImpl.complete(UploadManagerImpl.java:94)   at org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl.completeBlobUpload(DockerHostedFacetImpl.java:574)  {quote}"""
"NEXUS-15727","Bug","NPM",1,"Error when maintainers is shortened string","""When a npm hosted package.json contains a maintainers attribute that is of a shortened type an exception is thrown:          To replicate the problem for npm-hosted on NX3 follow the same instructions as [here|https://issues.sonatype.org/browse/NEXUS-12347?focusedCommentId=451315&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-451315] but adding a maintainers attribute to the package.json. For example    The above example in my opinion does not match the specification but it has been noted that some packages are using this format (i.e. not an array of maintainers)."""
"NEXUS-15734","Bug","Yum",5,"Proxied yum packages can become undiscoverable","""NXRM Proxy repos should continue to serve packages even if they go missing upstream. However, since we proxy the metadata listing from upstream we're vulnerable to effectively propagating upstream deletions.    # Create a yum *hosted* repo with two RPMs in it.  # Wait for the hosted repository to generate the metadata (this defaults to 60 seconds)  # Create a yum *proxy* of the hosted repo.  # Fetch the metadata through the *proxy*, specifically repomd.xml which will automatically fetch primary.xml.gz.  # Fetch both the RPMs through the *proxy* so that they are cached.  # Delete one of the RPMs from the hosted repo and wait for the metadata to be regenerated. The primary.xml.gz file in the hosted repository should now only list one RPM.  # Invalidate the cache on the proxy and then fetch the repomd.xml again.  # The primary.xml.gz file in the proxy will now only list one RPM but both RPMs will be available in the repository.    (Note another way to reproduce this would using a Centos 6 Docker image and a Centos 7 Docker image and then to point a proxy at a centos 6 remote http://mirror.centos.org/centos-6/6.9/os/x86_64/ install some packages, change the remote url to centos 7 install some packages http://mirror.centos.org/centos-7/7/os/x86_64/ the go back and try and install the 6 packages, which will fail)    This issue is the Yum version of the NPM-specific NEXUS-15714."""
"NEXUS-15745","Bug","Yum",2,"Yum proxy is not able to remove absolute URLs for metadata files that aren't at the root of a repository.","""When repomd.xml is fetched through a yum proxy then Nexus automatically fetches the primary.xml.gz file so that it can remove any absolute urls, and update the checksum and size attributes that are stored in repomd.xml.    This works fine when the metadata is at the root of the repository. (i.e. remote url of http://mirror.centos.org/centos/7/os/x86_64 and a request url of http://localhost:8081/repository/yum-proxy/repodata/repomd.xml).    It fails when metadata is not at the root because it uses the path directly from the repomd.xml.    # Create a yum proxy repository with remote url http://mirror.centos.org/centos  # Fetch repomd.xml via http://localhost:8081/repository/yum-proxy/7/os/x86_64/repodata/repomd.xml  # The following message will be logged    {quote}  2018-01-30 10:53:43,834-0400 WARN  [qtp732641399-75] admin org.sonatype.nexus.repository.yum.internal.proxy.YumProxyFacetImpl - Failed to fetch metadata file for path repodata/b686d3a0f337323e656d9387b9a76ce6808b26255fc3a138b1a87d3b1cb95ed5-primary.xml.gz and repository yum-proxy-97. Received status code 404  {quote}    To fix this we need to prepend the /7/os/x86_64 part of the request url onto the url that is extracted from repomd.xml"""
"NEXUS-15765","Bug","Repository|UI",2,"Group members appear in seemingly random order","""While testing bower, I added 2 proxy repos and a hosted repo (in that order) and went to put them in a group.  I noticed the order displayed on the Available list was: proxy, hosted, proxy2.  This was not in the creation order or alphabetical order.    I can imagine this might be a problem with more than 3 repos.  However, workaround SHOULD be filtering, assuming the names arn't too similar and long.    I doubt this is regression, tho I haven't checked at this time.  I also doubt this is bower specific, tho I also didn't check."""
"NEXUS-15762","Bug","Repository|UI",2,"Repository HTTP configuration not available in Safari","""In the edit screen for a proxy repository the http configuration sections are not displayed when viewed in Safari. Works fine in Firefox    !Screen Shot 2018-01-31 at 17.01.31.png|thumbnail!         This issue isn't just affecting the HTTP Configuration section, all the subheadings on the new Repository form page are now hidden. See screenshot below:    !Safari - Broken Proxy Repo Page.png|thumbnail!    It also affects other pages with similar elements. e.g.     !Safari - Broken HTTP Page.png|thumbnail!        What's more, after the ExtJS upgrade, the style of the subheadings on all browsers is off. They are supposed to be larger than the form fields as shown in the Chrome screenshot below:     !Chrome - Proxy Repo Subheading Styles Pre Upgrade.png|thumbnail!     But after the upgrade, the structure of the DOM nodes that ExtJS generates was modified and the custom CSS style rule for subheadings is no longer matching. Therefore the subheadings look like all the other text on that page as can be seen on Chrome below:     !Chrome - Proxy Repo Subheading Styles Off.png|thumbnail!   """
"NEXUS-15768","Improvement","Logging|Yum",1,"Yum hosted should log more information when the metadata is being generated","""A message is logged when createrepo starts but not during the generation or when it finishes, making it unclear whether the metadata has been updated."""
"NEXUS-15795","Bug","Yum",1,"Yum Hosted Recipe should not add the negative cache handler","""If the yum hosted metadata is fetched before it is generated then a 404 will be returned. Because the negative cache handler has been incorrectly implemented on hosted the 404 will still be returned after the metadata has been generate. This is true for RPMs as well (fetched before upload and fetched after upload). There is no way to clear the negative cache without waiting for it to expire or restarting the server.    The Yum hosted recipe is adding the NegativeCacheHandler (i.e. [https://github.com/sonatype/nexus-internal/blob/30c875f80f2406800b8ec37cc618bee0cfd47897/private/plugins/nexus-repository-yum/src/main/java/org/sonatype/nexus/repository/yum/internal/hosted/YumHostedRecipe.groovy#L90)]       """
"NEXUS-15878","Bug","Security|UI",5,"UI poll requests iterates over all privileges.","""Every time the UI polls Nexus it iterates over all privileges:    [https://github.com/sonatype/nexus-internal/blob/release-3.8.0-02/components/nexus-rapture/src/main/java/org/sonatype/nexus/rapture/internal/security/SecurityComponent.java#L197]    This is similar to an issue we fixed in Nexus 2.x (NEXUS-5713).    The processing of these privileges looks very expensive:         h4. Mitigation    Reducing the frequency the UI makes poll requests will in turn reduce the total processing of these requests by UI users. By default, poll requests are sent every 5 seconds.    Go to """"system/capabilities"""" in the administration UI, and click on the """"UI: Settings"""" capability. In the settings tab, change the """"authenticated user status interval"""" to something like 120 seconds. Make sure the standard request timeout value is higher than 120 seconds or whatever you sent the value to.    """
"NEXUS-15816","Improvement","LDAP",2,"Provide a means to disable paged result sets in LDAP searches","""Some LDAP servers have paged result sets disabled for performance reasons (1).    These LDAP servers no longer work with Nexus versions 3.6.0 and higher due to the fix for -NEXUS-10565-.    We should consider adding a way to to disable paged results sets in our LDAP searches to restore compatibility with these servers.    1) [https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.1.0/com.ibm.zos.v2r1.glpa200/glpa200_Paged_search_considerations.htm]"""
"NEXUS-15815","Improvement","Proxy Repository|Yum",1,"Yum Proxy implement Condition GET","""When requesting from a Yum Proxy we should provided a proper response for a conditional get with the {{If-Modified-Since}} header.    *Acceptance Criteria*   * Add for the URI patterns in {{YumProxyRecipe}} to add a Conditional Request Handler, affected URL's should return a 304 http status if data was not modified before the date time given in the {{If-Modified-Since}} header.    ** /repomd.xml   ** /repodata/*   ** *.rpm   ** *.xml.asc    *Example call*     """
"NEXUS-16067","Bug","IQ Integration|Security",3,"Add privilege that controls access to IQ component details","""I've granted every privilege in Nexus 3 that there is to a user, and they still do not have permissions to view IQ component details.    It should be possible for a non-admin user to view the IQ component details when looking at a component.  Currently the only way I can get access is to give the user the """"nx-admin"""" role.    This was tested in both Nexus 3.8.0.    A support zip with my test configuration is attached, the test user has credentials """"test:test""""."""
"NEXUS-16066","Improvement","Yum",1,"Yum Hosted implement Condition GET for remaining endpoints","""When requesting from a Yum Hosted we should provided a proper response for a conditional get with the {{If-Modified-Since}} header.    *Acceptance Criteria*   * Add for the URI patterns in {{YumHostedRecipe}} to add a Conditional Request Handler, affected URL's should return a 304 http status if data was not modified before the date time given in the {{If-Modified-Since}} header.    ** /repomd.xml   ** /repodata/*   ** *.rpm   ** **.**    *Example call*     """
"NEXUS-16057","Story","Upload UI|Yum",3,"Add UI upload for Yum","""*Acceptance criteria*   * A user can select an RPM and upload it to a yum hosted repository.   * An RPM can be uploaded to any path in the repository that is at or deeper than the configured repodata depth. """
"NEXUS-16082","Bug","Licensing",3,"Nexus Pro reverts to Nexus OSS if license is removed","""If you remove the Nexus Repo Manager license from the Java preference store Nexus will revert to OSS mode.    This is of particular concern to Nexus Firewall users, when running in OSS mode quarantine is not enforced.    Also, there is a bug in recent Java 8 versions, a Java update can wipe out the user preference store. We've encountered this on multiple support tickets recently:    [https://support.sonatype.com/hc/en-us/articles/360000093547-Java-update-in-Windows-can-remove-Nexus-license]    Firewall users who encounter the above issue will find that quarantine is no longer enforced, and will not be until they re-install their license and restart Nexus.    h4. Symptom on Upgrading Nexus Repository Manager Where OSS Mode is Enabled Trying to Load formerly PRO database Model        *Expected*    Once an instance has had a Pro license, it should not downgrade to OSS, but should present the 'please supply a license' screen."""
"NEXUS-16077","Bug","PyPI|Security",2,"pip search on group requires read privilege on member repositories","""On my production installation, I only grant anonymous access to a set of specific repositories/groups. This is the situation for pypi:   * a 'pypi' proxy repository pointing to [https://pypi.python.org/|https://pypi.python.org/] which is not browsable/readable for anonymous   * a group repository 'pypi-public' containing the above proxy repository. This one is the default the anonymous user must use with browse/read permissions granted    Using the 'pypi-public' group to install packages from pip command line is behaving as expected (packages get downloaded and installed)    Using the pip command line to search packages against the group fires a login prompt. To enable anonymous search your have to specifically grant the read access on the 'pypi' member repository. This is annoying as the admin must remember to grant read access to new member repository. The read permission on the group should be enough to complete the search."""
"NEXUS-16159","Bug","APT|Bower|cocoapods|Docker|User Token",0,"""Require user tokens for repository authentication"" now enforced properly","""When User token is enabled and the Require user tokens for repository authentication is also enabled, docker requests are still allowed to use plain text credentials instead of being forced to use user token credentials.    The changes made by NEXUS-11231 seem to only affect Maven and Raw format repositories.    h4. Expected    When Require user tokens for repository authentication is enabled, all non user-token credentials should be blocked for all repository formats."""
"NEXUS-16227","Bug","Security",2,"Assigned privileges are restored when repository is deleted and created again","""Assigned privileges are restored when repository is deleted and created again with same repository id.    1) Have an existing proxy """"nuget-test"""".  2) Assign """"nx-repository-view-nuget-nuget-test-_"""" to a Role._  _3) Delete repo """"nuget-test""""._  _4) Check that """"nx-repository-view-nuget-nuget-test-_"""" is removed from the Role.  5) Create proxy """"nuget-test"""".  6) Check that """"nx-repository-view-nuget-nuget-test-*"""" is added back to the Role.    Not sure if this is intentional, but does indicate that role privilege are not cleaned up on deletion on a repo"""
"NEXUS-16225","Bug","REST|UI",2,"Swagger UI caching causing load problems on upgrade","""URLs to load parts of the swagger-ui via the http://localhost:8081/#admin/system/api link do not include the typical cache busting mechanism employed by other URLS {{?_v=<productversion>}}. The URLs include a Cache-Control: max-age=2592000 header in the response which causes the generic URL content to be cached for 30 days in the browser.    We have had several reports of problems loading the in product swagger-ui because of this caching issue.      {noformat:title=Request}    GET /swagger-ui/ HTTP/1.1  Host: localhost:8081  Connection: keep-alive  Upgrade-Insecure-Requests: 1  User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.167 Safari/537.36  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8  Referer: http://localhost:8081/  Accept-Encoding: gzip, deflate, br  Accept-Language: en-US,en;q=0.9  Cookie: NXSESSIONID=b1639abe-c8f8-4e43-b1dc-04828c8713f2      The broken interface leads to support tickets and users mailing list questions.    h4. Expected    Include cache buster query param in all UI URLs. UI users should not be forced to clear browser cache and restart their browsers on upgrading Nexus.    h4. Workaround    Clear browser cache and restart web browsers after upgrading Nexus. Some reports of force refreshing using CTRL-F5 have worked as well.  """
"NEXUS-16303","Bug","Maven|Scheduled Tasks",1,"PublishMavenIndexTask fails to republish maven Index if hosted Maven repository has write policy of ALLOW_ONCE","""On locally hosted repositories, Maven indexes are not published for the second running of the task for those repositories marked as Release.    I expected that if the repositories marked as release had not changed, then no update to the index would be attempted and the logs would indicate that. Instead a error occurred stating the following:    org.sonatype.nexus.repository.IllegalOperationException: Repository does not allow updating assets: releases    was present in the task log, and the task manager showed that the task had failed with an error.    ----    The releases repository has a writePolicy of """"ALLOW_ONCE"""". That policy is intended to prevent republishing/overwriting the same artifact into the repository.    It is not intended to prevent the publish indexes task from running repeatedly and updating.                 Also, when the task fails in some circumstances it can leave blob references in the database that are deleted from the blob store. Future runs of the task may throw MissingBlobException - see NEXUS-16560  """
"NEXUS-16249","Bug","Outreach",0.5,"Nexus welcome page announces Nexus Repository 2.14.8 is now available when it is not","""Right after updating to 2.14.7, Nexus promotes a new Update on its landing page:      Nexus Repository 2.14.8 is now available with support for ...    But following the link only leads to the 2.14.7 version. 2.14.8 does not seem to be available."""
"NEXUS-16319","Bug","Database|Scheduled Tasks",8,"Publishing maven indexes against large repository has poor performance, gets OrientDB profiler warning","""Publishing maven indexes against a large repository can run very slowly.    The OrientDb profiler emits a warning when the task is run indicating that the query being run is returning a result set that is too large:     {quote}  2018-02-23 08:20:16,493-0700 INFO [Thread-41067] *SYSTEM com.orientechnologies.common.profiler.OProfilerStub - $ANSI\{green \{db=component}} [TIP] Query 'SELECT last_updated AS lastModified, component.group AS groupId, component.name AS artifactId, component.attributes.maven2.baseVersion AS version, component.attributes.maven2.packaging AS packaging, component.attributes.maven2.pom_name AS pom_name, component.attributes.maven2.pom_description AS pom_description, attributes.maven2.classifier AS classifier, name AS path, attributes.content.last_modified AS contentLastModified, size AS contentSize, attributes.checksum.sha1 AS sha1 FROM asset WHERE bucket = [#129|https://sonatype.zendesk.com/agent/tickets/129]:1 AND attributes.maven2.asset_kind = """"ARTIFACT"""" AND component IS NOT NULL' returned a result set with more than 10000 records. Check if you really need all these records, or reduce the resultset by using a LIMIT to improve both performance and used RAM  {quote}    """
"NEXUS-16317","Bug","NuGet",3,"NuGet group repository package requests may respond with URLs to member repositories","""From: https://groups.google.com/a/glists.sonatype.com/d/msgid/nexus-users/f2f39b37-7d39-45b1-96c1-a9dd5677672f%40glists.sonatype.com?utm_medium=email&utm_source=footer    I have multiple repositories:  nuget-dotnet-firstparty  nuget-dotnet-thirdparty  nuget-dotnet-generated  etc  and a group repository called:  nuget-dotnet  which brings all of the above together.    This is all on the server """"DepServer"""".    When I try to do a """"nuget restore"""" when the client has access to nuget-dotnet (the group repository) only I get the following:  WARNING: Error downloading 'MyPackage.1.2.3' from 'https://DepServer:8443/repository/nuget-dotnet-firstparty/MyPackage/1.2.3'.  The HTTP request to 'GET https://DepServer:8443/repository/nuget-dotnet-firstparty/MyPackage/1.2.3' has timed out after 100000ms    Now, the client has no references to """"nuget-dotnet-firstparty"""", and so no security access set up for it.  But it shouldn't need it, because it has access to the group.    I couldn't work out why it was that it was having this issue, so I went looking through the information on the server via the nuget protocol.  What I found was:    https://DepServer:8443/repository/nuget-dotnet/  points to  https://DepServer:8443/repository/nuget-dotnet/Packages  Each entry has a link with a title of V2FeedPackage an an href of """"Packages(Id='MyPackage',Version='1.2.3')""""  Following one of those URLs shows that a line with  - content type=""""application/zip"""" and a src of  """"https://DepServer:8443/repository/nuget-dotnet-firstparty/MyPackage/1.2.3""""    However, as the client has no access to the """"nuget-dotnet-firstparty"""" repository, only to the group repository, that's when the error occurs.      h4. Expected    Responses from group requests should only reference urls to the group repo, not group members.  """
"NEXUS-16312","Bug","NPM",3,"Metadata for NPM group considers pre-release version higher than actual version","""[https://semver.org/#spec-item-11] it says:    Example: 1.0.0-alpha < 1.0.0-alpha.1 < 1.0.0-alpha.beta < 1.0.0-beta < 1.0.0-beta.2 < 1.0.0-beta.11 < 1.0.0-rc.1 < 1.0.0.    So, semantic versioning considers anything with “-” a prerelease so 1.0.0-SNAPSHOT is considered prerelease and 1.0.0 is bigger than 1.0.0-SNAPSHOT.    1) Upload a package to hosted npm repo npm-snapshot with version 1.0.0-SNAPSHOT    2) Upload a package to hosted npm repo npm-release with version 1.0.0.    For repo npm-snapshot 1.0.0-SNAPSHOT is latest    For repo npm-release 1.0.0 is latest    3) Look at the group metadata, you will notice that it sets 1.0.0-SNAPSHOT as the latest         """"npm install testdeploy""""  from group repo will return 1.0.0-SNAPSHOT, as 1.0.0-SNAPSHOT set as latest.    Even if look at the deploy times 1.0.0 is deployed later than 1.0.0-SNAPSHOT.    So there may be some issue with npm group logic on picking which version should be latest as there is a latest tag on each member npm repos.         A workaround is do a """"{{npm install package@*""""}} which will exclude prereleases."""
"NEXUS-16387","Bug","Scheduled Tasks",2,"Attempting to rebuild browse nodes for deleted repositories results in NullPointerException on startup","""When a repository is deleted, it is marked as such by renaming it to an obscure name in the database. This renaming will allow an end user to avoid referential integrity issues should they wish to create a new repository with the same name.    When scheduled tasks run against """"All Repositories"""", the list may include these obscurely named deleted repos. An example where this was seen in practice is in the following logging in Nexus 3.7.1 at startup:        h4. Expected     Mainly the NullPointerException should not happen.    Since deleted repos are effectively """"gone"""" - tasks should not be able to retrieve a list of repositories that includes deleted repositories by default. If there is a valid use case for processing deleted repos with a task, the task should explicitly handle asking for a list which may include them ( although I am unaware of such use case at this time ).      """
"NEXUS-16393","Bug","Maven",2,"NullPointerException when merging group GAV maven-metadata.xml for  non-timestamped snapshot","""Nexus 3.8.0 gets an NPE when merging GAV level maven-metadata.xml files from non-timestamped snapshot deploys.     Example maven-metadata.xml file:  {code:java}  <?xml version=""""1.0"""" encoding=""""UTF-8""""?>  <metadata>    <groupId>org.sonatype</groupId>    <artifactId>project</artifactId>    <version>1.1.5-SNAPSHOT</version>    <versioning>      <snapshot>        <buildNumber>1</buildNumber>      </snapshot>      <lastUpdated>20180227155805</lastUpdated>    </versioning>  </metadata>  {code}  When this file is merged with another repository's maven-metadata.xml file at the group level you get an NPE, and a 500 response:         *Reproduce Case*:  Create two snapshot repositories, put them in a group repository.  Deploy a normal timestamped snapshot into the first one.  For the second one, put """"<uniqueVersion>false</uniqueVersion>"""" in the distributionManagement section in the pom file, and deploy using 2.2.1.      Then attempt to retreive the GAV level maven-metadata.xml file through the group repository.         *Expected #1*: Nexus should be able to handle files like these, there are still commonly used tools  (Ivy) that can't deploy timestamped snapshots.    *Expected #2*: In the more general case, if Nexus encounters a maven-metadata.xml file it can't parse during merge it should log a warning, and ignore the bad file.     """
"NEXUS-16392","Bug","NuGet",0,"Nexus does not interpret + signs at URLs correctly","""See https://github.com/NuGet/Home/issues/3858 , I have the same issue. What I observe when calling NCR's Nexus installation:    ```  > http://***.com/nexus/service/local/nuget/repo-name/Nuget.Name/1.2.3+09d6b7c  404 Path /Nuget.Name/1.2.3 09d6b7c/Nuget.Name-1.2.3 09d6b7c.nupkg not found in local storage of repository """"Repository"""" [id=repo-name]  ```    Note the `+` sign missing in the response. Now when I escape manually:    ```  > http://***.com/nexus/service/local/nuget/repo-name/Nuget.Name/1.2.3%2B09d6b7c  [download offered]  ```    According sources for simpletons like me like ( https://stackoverflow.com/a/2678602/577067 ), it seems to me that you should not replace + by space in the path component of URLs. You should present me with the NuGet package for the first query.    Until this is fixed, we can't use SemVer 2 with Nexus."""
"NEXUS-16401","Bug","PyPI|REST|Search",3,"PyPi hosted repository packages can only be searched by pep-0503 normalized name","""Use twine to upload a python package with a special character in its name, such as """"jyt.python""""    into a hosted pypi repository:    1. python setup.py sdist --formats=gztar    2. twine upload FILENAME.tar.gz --repository-url NEXUS_PYPI_URL    Then try to use the REST API or UI search to find this package by its name. Searching using """"jyt.python"""" will not work, but """"jyt_python"""" will work. This appears to be due to the normalization of names applied by Nexus as defined by [https://www.python.org/dev/peps/pep-0503/#normalized-names] .    Contrary, when using a PyPi proxy to a python package with """"."""" in its name, Nexus stores this name NOT normalized and searching without an underscore will work.  h4. Expected   - one should be able to find PyPi packages using REST or UI search in a consistent way regardless of name normalization or the repository type containing the package   - Updated: a discussion on the implications of this ticket was held on 11/08/18. To fully implement this ticket would require an upgrade step. This was felt unnecessary at the moment, therefore only new uploaded packages will be able to search on both normalized and non-normalized package names."""
"NEXUS-16409","Improvement","Yum",2,"Ability to DELETE from a yum hosted repository","""When testing yum repositories it was realized that there is currently no implementation of http DELETE to allow deletion of rpms    While there is a way to delete via the REST API ( [see mailing list answer|https://groups.google.com/a/glists.sonatype.com/d/msg/nexus-users/bVLQHtkDYAU/dx4c4g3pDAAJ] ) - a standard HTTP DELETE to the path of the RPM file should be allowed as well, similar to RAW or Maven repos. Nexus 2.x supported this and there are no YUM repository format rules that disallow this.    *Acceptance*   # RPMs can be deleted by making a http DELETE request. Updating yum metadata asynchronously from this action  should be handled as well.( ie. do not block the delete waiting for metadata to update )   # Attempting a DELETE of metadata will return a http 405 (NOT_ALLOWED) as these files will be updated automatically.   # Maven files (e.g. pom.xml) and other file types in PERMISSIVE repositories can be deleted by making a http DELETE request."""
"NEXUS-16425","Bug","REST",1,"Download Endpoint Returns Checksum Files if Maven Classifier Param is Set","""When calling /beta/search/assets or /beta/search/assets/download with both the maven classifier and maven extension parameters, the response returns the checksum files along with the asset specified in the extension.    For example calling:    [http://localhost:8081/service/rest/beta/search/assets?repository=maven-releases&version=1.0.1&maven.groupId=org.foo&maven.artifactId=bar.project&maven.extension=jar&maven.classifier=blah|http://localhost:8081/service/rest/beta/search/assets?repository=maven-releases&version=1.0.1&maven.groupId=org.foo&maven.artifactId=bar.project&maven.extension=zip&maven.classifier=blah]    Will result in the jar asset as well as its checksum files being returned. It is expected that only the jar will be returned.    Omitting the classifier parameter results in only the jar being returned as expected.          """
"NEXUS-16457","Bug","UI",1,"Help - Issue tracker link currently defunct","""In the help menu, if you click on """"Issue Tracker"""" the link currently goes to links.sonatype.com/products/nexus/issues which redirects to https://support.sonatype.com/hc/en-us/articles/213466158-Sonatype-Nexus-Issues which is a non-existant page.  I suspect the link broke and this affects all versions but I haven't checked back at this time."""
"NEXUS-16453","Bug","Search|UI",1,"Search field clearing for /#security-users","""If I use the X next to the search button to clear the field, it disappears and I'm not able to clear the field again this way until I refresh my Nexus tab or re-login. Happens in https://oss.sonatype.org/#security-users and with """"All Authorized Users"""" in the drop-down. Have not reproduced elsewhere."""
"NEXUS-16450","Story","Database",1,"Upgrade to OrientDB 2.2.34 to pick up HA sync and backup fixes","""Changes since last release we picked up (2.2.31):    https://github.com/orientechnologies/orientdb/wiki/OrientDB-2.2-Release-Notes#2234---april-12-2018  https://github.com/orientechnologies/orientdb/wiki/OrientDB-2.2-Release-Notes#2233---march-5-2018  https://github.com/orientechnologies/orientdb/wiki/OrientDB-2.2-Release-Notes#2232---february-5-2018    Note: 2.2.33 introduced an HA regression around sync'ing of delta changes which was fixed in 2.2.34"""
"NEXUS-16449","Bug","Security",1,"Role to privilege cache gets cleared out at inappropriate times due to use of weak references","""     The default security realm in Nexus holds a cache of roles to privileges using a map that has weakly referenced values.     [https://github.com/sonatype/nexus-public/blob/release-2.14.4-03/components/nexus-security-realms/src/main/java/org/sonatype/security/realms/XmlRolePermissionResolver.java#L79]    This means that there is the potential for this map to lose all it's values every time Java runs garbage collection.  Rebuilding this map can be expensive on a heavily loaded instance, because the requests threads can experience considerable lock contention as they try to put new values into the map.    In Nexus 3.x we changed this map to use soft references. This prevents the values from being reclaimed unless heap memory is actually low.  We should make the same change in Nexus 2."""
"NEXUS-16430","Bug","Maven",1,"Connection reset when uploading large file using Apache Ivy","""Not entierly sure if this is bug or not.    We had a build job which uploaded a few fairly large (~200Mb) artifacts to artifacts to nexus 2 with apache ivy. After upgrade this build job will not work any more.    The reason is that Ivy cannot be set to authenticate preemptively to nexus. So what happens is that ivy immediately sends the request without authentication header to nexus and eventually nexus times out with a """"connection reset"""" error. Nexus is trying to read the request to be able to reply back with a WWW-Authenticate response, but instead just resets the connection.    If however the same request is made with authentication header, then the nexus will not time out, and everything will work as expected.    It seems to me that there are two different timeouts being used here. And no apparent way to configure any of them.         Using Nexus 3.7.1"""
"NEXUS-16461","Bug","Blobstore|RubyGems",1,"New ruby gems dependency files are cached in blob storage every time Nexus requests them from a proxy repository remote","""After installing bundler using the """"gem"""" command, I have these two files in my blobstore:    After this, I expired cache in my rubygems.org proxy repository, and then requested this (just once):    [http://localhost:8081/repository/rubygems-proxy/api/v1/dependencies?gems=bundle]    And now I have:    Those older blobs are not marked for deletion, running compact blobstore does not remove them.    Every time cache expires and the dependency request is made two more dependency files are cached. Over time, these will consume quite a lot of space.  h5. Expected:    # Older dependency files should be deleted as new ones are downloaded from the remote.    # Ideally, if the content of the file has not changed, no new file will be stored.   # We will need to publish a groovy script that can find the """"orphaned"""" dependency blobs created by this bug and delete them."""
"NEXUS-16476","Bug","NuGet|User Token",2,"API key changes when the regular account password changes","""NuGet API keys should be tied to identity ( username + realm identity), and identity should not include profile attributes such as regular password, user first name, last name, or email.    Nexus 3.x: Change the regular password of a repository manager user account. This will change the NuGet API key as well. This should not happen.    Nexus 2.x does not change the NuGet API key when password or any other account profile data changes.    Related mailing list exchange: https://groups.google.com/a/glists.sonatype.com/d/msgid/nexus-users/19330e21-3372-418f-9cd3-0b4c8fa89f69%40glists.sonatype.com?utm_medium=email&utm_source=footer  """
"NEXUS-16475","Bug","REST|Yum",3,"DELETE asset REST endpoint for yum should not delete component","""I noticed that when a delete by asset is done on a yum hosted repository that it also deletes the component, this in turn will delete the remaining assets of the component.     *To reproduce:*   # Upload two rpms for different architectures, I used bakefile from [http://mirror.centos.org/centos/6/extras/i386/Packages/bakefile-0.2.8-3.el6.centos.i686.rpm] and [http://mirror.centos.org/centos/6/extras/x86_64/Packages/bakefile-0.2.8-3.el6.centos.x86_64.rpm]   # Use the rest api to perform a search: [http://localhost:8081/service/rest/beta/search?repository=yum-hosted&format=yum&name=bakefile&version=0.2.8-3.el6.centos] will return something similar to the response shown below, i.e. one component containing two assets.   # Use one of the assets id value to issue a delete [http://localhost:8081/service/rest/beta/assets/eXVtLWhvc3RlZDoyZmZmNTA5YTdjMmE5ZWJlMjI1MmIxNTY5OWI3MzdjYg]     # Perform the same search as in step 1   # Observe that the component and all rpms have now been deleted.         *Acceptance Criteria*   # delete by asset should not delete the component when other assets are associated with the component.   # delete by component should delete itself and all associated assets."""
"NEXUS-16510","Bug","Maven|Scheduled Tasks",2,"Remove Releases from Repository task may fail with IllegalArgumentException Comparison method violates its general contract!","""Deploy into a hosted Maven releases repository ( id: releases ) versions of the same GA that have a mixture of versions such as:        See attached versions.txt for the complete list of versions used in the reproduce case.    Then run a Remove Releases from Repository scheduled task that tries to remove released versions against this repo.    When the task is run with these settings:    Repository: Releases  Number to Keep: 500  Repository Target: All (Maven 2)  *Use Index: True ( checked )*    It fails like this:        When the task is run with    *Use index: False ( unchecked )*    the task also fails with a slightly different stack:        h4. Explanation    The Maven repository format does not strictly enforce non-semantic type versions:    https://maven.apache.org/guides/mini/guide-naming-conventions.html    However, in order for the task to decide which 500 release versions to keep, it must sort them in order of highest to lowest. Since it is impossible to accurately determine if a hash style version number is sorted before or after a semantic style version, using the Remove Releases from Repository task against such a set of mixed versions is not reliable.    h4. Expected    The task should not fail since there is no explicit Maven repository format rule that does not allow mixing both types of version numbers. However since the task is trying to determine which versions to keep after sorting them, and it can't reliably do this in this case, the task should skip problem GAs with mixed versions it encounters and log all the found versions for the GA it found with the problem at WARN level in the log.    Cleaning up such mixed versions releases is left as an exercise to the Nexus Administrator.   """
"NEXUS-16527","Bug","HA",1,"NotSerializableException com.orientechnologies.orient.core.sql.OSoftQueryResultList","""In an 3.9.0 HA-C cluster the following log message was noticed:        h4. Cause    This is a problem in the OrientDB database version used by Nexus reported upstream as https://www.prjhub.com/#/issues/9975.    """
"NEXUS-16545","Bug","Documentation|Yum",8,"NXRM 3 Yum Metadata Generation takes forever","""We recently migrated completely from nexus2 to nexus3, now that it has full yum support (hosted). Recently, we noticed that when we were doing our rpm builds/uploads, that when we would deploy new instances, we were not seeing the expected artifacts on the machines. I dug into this a bit and observed the following:        ==========    =========  In both of these cases, regenerating the metadata is taking 15 minutes.  That is a major detriment and will cause inefficiencies for developers, slowing down a company's ability to handle CI/CD"""
"NEXUS-16539","Bug","Maven|Transport",2,"Nexus 3 does not auto-block on 401 responses from https://maven.oracle.com","""Nexus 3 does not auto-block on receiving 401 responses from the remote of a proxy repository.  Instead, it keeps checking the remote for every inbound request.     This is not consistent with Nexus 2.x, which does auto-block for 401 responses.  See NEXUS-6515.    Failure to auto-block on receiving 401 can cause major performance problems, and also has the disadvantage that a user is not alerted to the fact that a proxy repository is not working.  It is true that there are cases where you would not want to auto-block on 401, but those are rare, and can be handled by disabling auto-block in the repository's configuration.           Note: The reproduce case for the above is to create a proxy of https://maven.oracle.com and enter invalid credentials into the authentication settings for it."""
"NEXUS-16565","Bug","Docker|Scalability",3,"IllegalStateException Insufficient configured threads from a docker repository connector configuration","""h4. Summary    On saving a Docker repository configuration that has defined a port number, the nexus.log may contain a message such as this:        This indicates that the pool of threads that Eclipse Jetty draws from to preallocate threads for the new connector is too small to meet the required total. The default pool size is 200 threads.    When this error happens on saving Docker repository configuration, no other changes can be made to the Docker repository configuration.    This error can also happen on Nexus startup, where each Docker repository connector is started.    Nexus version 3.8.0+ is more likely to trigger this issue because in that version Eclipse Jetty was upgraded to 9.4.8 from 9.3.20, and the thread allocation strategy has changed slightly in that version.    [Nexus Repository Manager 3.8.0 Release Notes|https://help.sonatype.com/repomanager3/release-notes/2018-release-notes#id-2018ReleaseNotes-RepositoryManager3.8.0]  [Eclipse Jetty 9.4.8 thread allocation|https://support.sonatype.com/hc/en-us/articles/360000744687]    h4. Short Term Workaround    You may be able to avoid the error by increasing the Jetty thread pool. Edit the configuration file ({{<nexus-app-dir>/etc/jetty/jetty.xml}}) and add a new maxThreads setter:        Caution: We suggest being conservative increasing the thread pool maxThreads - each new thread in the pool has the potential to increase workload inside of Nexus for concurrent request threads.     Restart Nexus to pick up changes to the file. Startup will fail if this file contains invalid XML.    h4. Long Term Workaround    Consider using a strategy where a reverse proxy in front of Nexus dynamically maps docker requests to Nexus Docker repositories. In this scenario, you will not need to specify port numbers and therefore new connectors, on your docker repos. Increasing the Jetty thread pool should not be required in this case:    https://support.sonatype.com/hc/en-us/articles/360000761828  """
"NEXUS-16560","Bug","Maven|Scheduled Tasks",0,"PublishMavenIndexTask failures may leave bad blob refs in the component database causing MissingBlobException","""Due to NEXUS-16303, The publish Maven indexes task may fail - and under certain circumstances can leave bad blob refs in the component database.        The bad blob ref gets left behind:        h4. Expected    The PublishMavenIndexTask should never leave bad blob refs in the component DB on any error."""
"NEXUS-16559","Bug","Scheduled Tasks",2,"DeadBlobFinder does not find bad blob refs for Maven indexer files","""Due to NEXUS-16303, The publish Maven indexes task may fail - and under certain circumstances can leave bad blob refs in the component database.        When the following script is run, the DeadBlobFinder does not find the bad blob refs:        Yet the bad blob ref exists:        h4. Expected    DeadBlobFinder should find ALL dead blobs."""
"NEXUS-16573","Bug","Yum",1,"Content validation still blocks some items even when permissive","""I noticed if I had a yum hosted repository with a permissive deploy policy if I curled in a nuget package, then I still got blocked by invalid content type detection as shown below.  My understanding from the setting and documentation is that permissive means you can add anything.  I believe this is how it works for other formats with validation.    e.g.  """
"NEXUS-16577","Bug","Configuration|HA|Logging",1,"Hazelcast XSD warning logged at startup.","""When you start up Nexus 3 in clustered mode you'll see a Hazelcast log warning message:      This is occurring because the XSD we're shipping with in our default configuration files does not match the version of Hazelcast we're shipping with:        https://github.com/hazelcast/hazelcast/blob/v3.7.8/hazelcast/src/main/java/com/hazelcast/config/AbstractXmlConfigHelper.java#L158    We should update our XMLS references to use the correct XSD version.         """
"NEXUS-16648","Bug","HA",1,"hazelcast-network.xml is not included in support zip","""As of changes since 3.7.0 per -NEXUS-14377-, Nexus uses [the following logic to load hazelcast config files|https://github.com/sonatype/nexus-internal/blob/f8d90379daff553edf6bcb8e8f983a794ccde6f6/private/plugins/nexus-hazelcast-plugin/src/main/java/com/sonatype/nexus/hazelcast/internal/HazelcastInstanceProvider.java#L128]:   - if {{$karaf.data/etc/fabric/hazelcast-network.xml}} does not exist, copy {{$karaf.base/etc/fabric/hazelcast-network-default.xml}} to {{$karaf.data/etc/fabric/hazelcast-network.xml}}   - if {{$karaf.base/etc/fabric/hazelcast.xml}} exists, load it   - if {{$karaf.base/etc/fabric/hazelcast.xml}} does not exist, log a WARN and load default values from internal configuration    The problem is the support zip generator does not include {{$karaf.data/etc/fabric/hazelcast-network.xml}} and therefore by default it is impossible to learn what the user has configured hazelcast to do.    [https://github.com/sonatype/nexus-internal/blob/f8d90379daff553edf6bcb8e8f983a794ccde6f6/private/plugins/nexus-hazelcast-plugin/src/main/java/com/sonatype/nexus/hazelcast/internal/HazelcastInstanceProvider.java#L128]    The other """"fabric"""" install dir files [are included|https://github.com/sonatype/nexus-internal/blob/d36a15cb656e48a290256155cf701b0f3da3742d/components/nexus-core/src/main/java/org/sonatype/nexus/internal/atlas/customizers/InstallConfigurationCustomizer.groovy#L90] and [sanitized|https://github.com/sonatype/nexus-internal/blob/d36a15cb656e48a290256155cf701b0f3da3742d/components/nexus-core/src/main/java/org/sonatype/nexus/internal/atlas/customizers/InstallConfigurationCustomizer.groovy#L60].  h4. Expected   - bundle {{$karaf.data/etc/fabric/hazelcast-network.xml}} into the support zip at path {{work/etc/fabric/hazelcast-network.xml}}   - *obfuscate any potential passwords* that may be in that file"""
"NEXUS-16674","Story","Documentation|NuGet|Staging",5,"Staging: NuGet move/delete","""Support move/delete for components in nuget hosted repositories."""
"NEXUS-16673","Story","Documentation|Staging|Yum",5,"Staging: YUM move/delete","""Support move/delete for components in yum hosted repositories."""
"NEXUS-16703","Bug","Content Selectors",3,"Content selectors based on negative regular expressions do not handle leading slashes in a consistent manner","""     Trying to create a regular expression that disallows access to paths starting with """"/com/foo/bar/"""".    This doesn't work:    And this does:    However, for positive regular expressions...    This doesn't work:    But this does work:    So that's at least consistent, but doesn't line up with [our documentation|https://help.sonatype.com/repomanager3/configuration/repository-management#RepositoryManagement-ContentSelectors] at all, which states that:  {quote}When writing a content selector, remember that the asset’s path will always begin with a leading slash when the selector is evaluated. This is true even though the leading slash is not displayed when searching or browsing assets.  {quote}   The above is the way it was supposed to work based on fixes done in ---NEXUS-11632--- and ---NEXUS-15545---.    For positive regular expressions things are also quite odd, but in a slightly different way.    This works:    As does this:    This doesn't work:    But this does work:    h3. Expected    Content selectors that use regular expressions should require that paths start with """"/"""". And using a leading """"^"""" should be optional. That is the way Nexus 2.x works, so this is required for compatibility when upgrading.  h4. Update 4/2/18    For upload this is needed:    So to make both upload and download work you need:     """
"NEXUS-16718","Bug","Docker",3,"""scope"" authentication errors when connecting to registry.connect.redhat.com","""Steps to reproduce:   # Create Docker proxy repository   # Remote storage: [https://registry.connect.redhat.com|https://registry.connect.redhat.com/]   # HTTP Authentication: RH Credentials |0|   # Connect to registry via Docker client or via HTTPs \{registry}/v2/sonatype/nexus-repository-manager/manifests/latest    Error:    Logs show error due to Could not retrieve token due to missing parameter: scope  """
"NEXUS-16717","Bug","NPM|REST|Upload UI",2,"uploading npm component where package.json bugs key is string fails","""+*steps to reproduce:*+   * download the file [http://registry.npmjs.org/crc/-/crc-3.0.0.tgz]   * use the Components API - Upload file with npm ([https://help.sonatype.com/repomanager3/rest-and-integration-api/components-api#ComponentsAPI-NPM)]    The request will fail with a 500 Internal Server Error and the message _child 'bugs' not a Map_    this error is caused by the fact that in the original package.json file bugs property is a string not an object.    based on [http://json.schemastore.org/package] the bugs property could be an object or a plain string    [original package.json|https://github.com/alexgorbatchev/node-crc/blob/v3.0.0/package.json#L28]    [npm registry package.json|http://registry.npmjs.org/crc/3.0.0]    _uploading (using npm publish) the raw *tgz* file works but the shasum changes and breaks the package_"""
"NEXUS-16729","Bug","Blobstore|Logging|Repository Health Check",1,"FileBlobStore Attempt to access non-existent blob path$health-check WARN log spam","""When healthcheck is first enabled for a repository, warning messages are written to the log for missing files:  {quote}2018-02-17 11:55:13,683-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/jquery-ui-1.8.17.custom.css (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/jquery-ui-1.8.17.custom.css.properties)  {quote}  {quote}2018-02-17 11:55:13,731-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/jquery.ui.selectmenu.min.js (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/jquery.ui.selectmenu.min.js.properties)  {quote}  {quote}2018-02-17 11:55:13,781-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/ui-bg_flat_75_ffffff_40x100.png (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/ui-bg_flat_75_ffffff_40x100.png.properties)  {quote}  {quote}2018-02-17 11:55:13,831-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/bg-selectmenu.png (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/bg-selectmenu.png.properties)  {quote}  {quote}2018-02-17 11:55:13,880-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/ui-icons_888888_256x240.png (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/ui-icons_888888_256x240.png.properties)  {quote}  {quote}2018-02-17 11:55:13,929-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/jquery.ui.selectmenu.css (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/jquery.ui.selectmenu.css.properties)  {quote}  {quote}2018-02-17 11:55:13,980-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/licenses.json (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/licenses.json.properties)  {quote}  {quote}2018-02-17 11:55:14,034-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/security.json (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/security.json.properties)  {quote}  {quote}2018-02-17 11:55:14,084-0600 WARN [quartz-5-thread-3] NexusHDQ-One *SYSTEM org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access non-existent blob path$health-check/Jcenter.bintray-proxy/details/details.html (/opt/NexusSharedStorage/default/content/directpath/health-check/Jcenter.bintray-proxy/details/details.html.properties)  {quote}  These files appear to be created on the fly if missing and do not impact the healthcheck task itself, but the warnings can spam the log and suggest there is an issue.    *Expected:*    These warning should be suppressed in respect to health check files, in the situation when it first enabled on a repository (and files are created if missing).     """
"NEXUS-16753","Bug","Docker|Transport",1,"Connection pool leak when docker hub proxy repository receives 401 responses from auth.docker.io","""Configure a proxy repository against docker hub, and configure credentials under """"http/authentication"""" section of the proxy's settings.  Use login credentials that _will not work_ on docker hub.    Now fire 21 requests for content to the proxy, as in:  {code:java}   curl [http://localhost:8081/repository/docker-proxy/v2/maprtech/pacc/manifests/6.0.0_4.0.0_centos7]  {code}  The first 20 will fail unauthorized JSON responses, as expected.  But the 21st will fail with a connection pool timeout as Nexus attempts to retrieve the bearer token.    It is necessary to restart Nexus to clear this connection pool leak.     """
"NEXUS-16757","Bug","Scheduled Tasks",2,"Delete incomplete uploads DockerUploadPurgeTask task fails in 3.10.0","""We have a task, that runs daily, to delete incomplete docker uploads (Docker - Delete incomplete uploads). This has been functioning normally, until our upgrade to 3.10.0-04 yesterday.     Task schedule details:   * Age in hours: 24   * Task frequency: Daily   * Start date: 02/07/2016   * Time to run this task: 00:00    Run Summary, after manually clicking """"Run"""":   * Status: Waiting   * Next Run: Wed Apr 11 2017 00:00:00 GMT-0500   * Last result: Error[0s]    It has been failing since then, with the following error in the logs:  """
"NEXUS-16778","Bug","Security|UI",3,"Regression:  UI allows creation of roles with spaces in their ID's.  These can't be seen in the roles list, and can't be deleted","""Not sure what happened here, but at some point we lost the fix for NEXUS-2904.    It is now (once again) possible to create roles with spaces in their ID's.  As before, the resulting role cannot be deleted via the UI."""
"NEXUS-16777","Bug","HA|Migration",1,"upgrade from Nexus 2.x to Nexus 3.x is allowed when nexus.clustered=true","""Apparently you can start the upgrade process of 2.x into 3.x when 3.x is in HA-C clustered mode.    This type of upgrade is not tested or intended to be supported, therefore we should prevent a user from migrating in this mode or at the very least document that this is not supported."""
"NEXUS-16853","Improvement","Maven",3,"Enhance content validation for maven-metadata.xml files","""The current file content validation for maven-metadata.xml files does very little. Caching bad metadata.xml files from servers can lead to build failures.  Furthermore, it can often be difficult to clean up bad maven-metadata.xml files from a proxy's cache, and even if you do, they may just get downloaded again.    Content validation for maven-metadata.xml files can be improved by parsing the files and making sure that:   # The GAV coordinates metadata.xml file are present in the file   # The GAV coordinates match the path the file is stored in    Files which do not meet the above criteria should be rejected."""
"NEXUS-16881","Improvement","Database",1,"Disable OrientDB live query support","""We don't use the https://orientdb.com/docs/2.2.x/Live-Query.html feature and disabling it completely (ie. removing it totally as a DB hook) will give a small performance boost.    It will also guarantee to remove future log-spam issues caused by {{OLiveQueryHook}}'s erroneous assumption that there will be a valid DB thread-local when it's called - which isn't always the case depending on the scenario.    This has led in the past to a number of log-spam reports, mostly on HA but also on a couple of non-HA systems under stress. The previous reports should have been fixed now by upstream changes, but disabling the live-query feature would protect us from similar issues in the future."""
"NEXUS-16903","Bug","Logging",1,"include HTTP request Content-Length header value in request.log","""Sometimes problem diagnosis in Nexus Repository Manager may benefit from knowing the inbound http request anticipated content size.     For a large majority of requests to add content into Nexus,  HTTP PUT is used. In those cases, the request content size information is usually available from the [HTTP request Content-Length header value.|https://tools.ietf.org/html/rfc7230#section-3.3.2]. ( there are some exceptions, in particular """"When a message does not have a Transfer-Encoding header field, a Content-Length header field can provide the anticipated size, as a decimal number of octets, for a potential payload body."""" )    h4. Expected    The default request.log log pattern for Nexus should be adjusted to include the value if any of the Content-Length header of the request.    Current pattern as of nexus 3.10.0        Proposed new format:        This change should be announced in release notes in case users have configured external request log parsers which may need adjustment or prefer to not have this value printed.  """
"NEXUS-16915","Bug","HA|LDAP",3,"certificate errors using LDAP over SSL in HA","""In a cluster configuration, if you configure a connection to an LDAP server that utilizes LDAP over SSL (an """"ldaps:"""" connection) and attempt to store the certificate in the Nexus Truststore, only the node where the connection is created will be able to connect to the LDAP server.     If you attempt to click the 'Verify connection' button in the user interface of any other node, you will receive an error in the user interface and an error similar to the following in the nexus.log file:        You may also see the following errors in the nexus.log when an authentication attempt is made:         """
"NEXUS-16935","Improvement","Scheduled Tasks",5,"upgrade to a newer version of Quartz scheduler","""Heres the list of commits that went in between Quartz-scheduler 2.2.2 (our version) and latest as of this ticket (2.3.0):    https://github.com/quartz-scheduler/quartz/compare/7733bc97c7c8ca6cd927049d7d36ae25261a1a0c...quartz-scheduler:0bd9adeace3e5dc238e84397573c3e9c6be265a6    Looks like JobStore interface has a couple of new methods which our JobStoreImpl would need to implement...    https://github.com/quartz-scheduler/quartz/milestone/1?closed=1 shows the issues which went into 2.3.0"""
"NEXUS-16930","Bug","Logging|Scheduled Tasks",1,"no reason logged at default levels why QuartzTaskFuture healthcheck task is canceled","""Example log messages that might be visible when a task is Cancelled:        The healthcheck task has no task logs by design in this case, and no other log messages indicate why the healthcheck task was immediately cancelled.    h4. Expected    There should be a log message at INFO explaining the root cause of task cancellation ( server shutting down, user manually cancelled the task, specific programatic condition not met to proceed, etc.? ) every time any task is cancelled."""
"NEXUS-16944","Bug","Scheduled Tasks|UI",2,"Uncaught TypeError Cannot read property 'loading' of null editing task schedules","""When editing a scheduled task with a schedule, and trying to change that schedule, users may notice the UI report the following message:    *Uncaught TypeError Cannot read property 'loading' of null*    This has been noticed on various task types when trying to change the schedule from hourly to daily or weekly to monthly and then clicking the Save button.     After the error occurs, the UI may not refresh the task settings correctly and the Discard button may not appear to function properly.    Another side effect is the dialog to confirm discarding changes may not render in the correct position or not be visible entirely.    Despite the UI errors, the original changes that were attempted to be saved appear to be persisted correctly.    h4. Workaround    There is no option to prevent the UI error. However recovery is relatively simple.    Use the browser refresh button to refresh the entire Nexus UI. After that the current state of the task settings should render correctly.      """
"NEXUS-16950","Bug","Repository",1,"org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepositoriesMap permission bottleneck for large numbers of repositories in nested group repos","""org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepositoriesMap is a synchronized method allowing only one thread access at a time.    There are several triggers to get a repositories map per HTTP request into Nexus.    Example thread stacks which confluence around this method are:    {noformat:title=/service/local/data_index}  qtp162721995-13491 id=13491 state=BLOCKED      - waiting to lock <0x0e70f442> (a org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry)       owned by qtp162721995-5951 id=5951      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepositoriesMap(DefaultRepositoryRegistry.java:207)      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepository(DefaultRepositoryRegistry.java:118)      at org.sonatype.nexus.proxy.repository.AbstractGroupRepository.getMemberRepositories(AbstractGroupRepository.java:390)      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getGroupsOfRepository(DefaultRepositoryRegistry.java:175)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePathCascade(DefaultNexusItemAuthorizer.java:69)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePath(DefaultNexusItemAuthorizer.java:63)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePathCascade(DefaultNexusItemAuthorizer.java:71)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePath(DefaultNexusItemAuthorizer.java:63)      at org.sonatype.nexus.index.DefaultIndexArtifactFilter.filterArtifactInfo(DefaultIndexArtifactFilter.java:85)      at org.sonatype.nexus.index.DefaultIndexerManager$18.accepts(DefaultIndexerManager.java:1854)      at org.apache.maven.index.AndMultiArtifactInfoFilter.accepts(AndMultiArtifactInfoFilter.java:46)      at org.apache.maven.index.AbstractMultiArtifactInfoFilter.accepts(AbstractMultiArtifactInfoFilter.java:80)      at org.apache.maven.index.DefaultIteratorResultSet.createNextAi(DefaultIteratorResultSet.java:245)      at org.apache.maven.index.DefaultIteratorResultSet.next(DefaultIteratorResultSet.java:153)      at org.apache.maven.index.DefaultIteratorResultSet.next(DefaultIteratorResultSet.java:51)      at org.sonatype.nexus.rest.index.AbstractIndexerNexusPlexusResource.ai2NaColl(AbstractIndexerNexusPlexusResource.java:67)      at org.sonatype.nexus.rest.index.AbstractIndexPlexusResource.get(AbstractIndexPlexusResource.java:165)      at org.sonatype.nexus.rest.index.DefaultIndexPlexusResource.get(DefaultIndexPlexusResource.java:86)      at org.sonatype.plexus.rest.resource.RestletResource.represent(RestletResource.java:233)      {noformat:title=NexusTargetMappingAuthorizationFilter}  qtp162721995-408 id=408 state=BLOCKED      - waiting to lock <0x0e70f442> (a org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry)       owned by qtp162721995-5951 id=5951      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepositoriesMap(DefaultRepositoryRegistry.java:207)      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepository(DefaultRepositoryRegistry.java:118)      at org.sonatype.nexus.proxy.repository.AbstractGroupRepository.getMemberRepositories(AbstractGroupRepository.java:390)      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getGroupsOfRepository(DefaultRepositoryRegistry.java:175)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePathCascade(DefaultNexusItemAuthorizer.java:69)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePath(DefaultNexusItemAuthorizer.java:63)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePathCascade(DefaultNexusItemAuthorizer.java:71)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePath(DefaultNexusItemAuthorizer.java:63)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePathCascade(DefaultNexusItemAuthorizer.java:71)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePath(DefaultNexusItemAuthorizer.java:63)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePathCascade(DefaultNexusItemAuthorizer.java:71)      at org.sonatype.nexus.proxy.access.DefaultNexusItemAuthorizer.authorizePath(DefaultNexusItemAuthorizer.java:63)      at org.sonatype.nexus.proxy.router.DefaultRepositoryRouter.authorizePath(DefaultRepositoryRouter.java:619)      at org.sonatype.nexus.security.filter.authz.NexusTargetMappingAuthorizationFilter.isAccessAllowed(NexusTargetMappingAuthorizationFilter.java:160)      at org.apache.shiro.web.filter.AccessControlFilter.onPreHandle(AccessControlFilter.java:162)      at org.apache.shiro.web.filter.PathMatchingFilter.isFilterChainContinued(PathMatchingFilter.java:203)      at org.apache.shiro.web.filter.PathMatchingFilter.preHandle(PathMatchingFilter.java:178)      at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:131)      at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)      at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)      at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)      at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)      at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)      at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)      at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)      at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)      at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)      at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)      at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)      at org.sonatype.nexus.web.internal.SecurityFilter.executeChain(SecurityFilter.java:90)      {noformat:title=Smart Proxy event publishing}  qtp162721995-1015 id=1015 state=BLOCKED      - waiting to lock <0x0e70f442> (a org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry)       owned by qtp162721995-5951 id=5951      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepositoriesMap(DefaultRepositoryRegistry.java:207)      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getRepository(DefaultRepositoryRegistry.java:118)      at org.sonatype.nexus.proxy.repository.AbstractGroupRepository.getMemberRepositories(AbstractGroupRepository.java:390)      at org.sonatype.nexus.proxy.registry.DefaultRepositoryRegistry.getGroupsOfRepository(DefaultRepositoryRegistry.java:175)      at com.sonatype.nexus.plugins.smartproxy.event.internal.PublishEventInspector.includeRecipients(PublishEventInspector.java:268)      at com.sonatype.nexus.plugins.smartproxy.event.internal.PublishEventInspector.includeRecipients(PublishEventInspector.java:271)      at com.sonatype.nexus.plugins.smartproxy.event.internal.PublishEventInspector.includeRecipients(PublishEventInspector.java:271)      at com.sonatype.nexus.plugins.smartproxy.event.internal.PublishEventInspector.includeRecipients(PublishEventInspector.java:271)      at com.sonatype.nexus.plugins.smartproxy.event.internal.PublishEventInspector.recipients(PublishEventInspector.java:253)      at com.sonatype.nexus.plugins.smartproxy.event.internal.PublishEventInspector.maybeFireItemUpdated(PublishEventInspector.java:354)      at com.sonatype.nexus.plugins.smartproxy.event.internal.PublishEventInspector.on(PublishEventInspector.java:182)      at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)      at java.lang.reflect.Method.invoke(Method.java:498)      at org.sonatype.sisu.goodies.eventbus.internal.guava.EventHandler.handleEvent(EventHandler.java:80)      at org.sonatype.sisu.goodies.eventbus.internal.guava.EventBus.dispatch(EventBus.java:329)      at org.sonatype.sisu.goodies.eventbus.internal.DefaultGuavaEventBus.dispatch(DefaultGuavaEventBus.java:34)      at org.sonatype.sisu.goodies.eventbus.internal.ReentrantGuavaEventBus.dispatchQueuedEvents(ReentrantGuavaEventBus.java:57)      at org.sonatype.sisu.goodies.eventbus.internal.guava.EventBus.post(EventBus.java:281)      at org.sonatype.sisu.goodies.eventbus.internal.DefaultEventBus.post(DefaultEventBus.java:78)      at org.sonatype.nexus.proxy.repository.AbstractRepository.storeItem(AbstractRepository.java:1032)      at org.sonatype.nexus.proxy.maven.AbstractMavenRepository.storeItem(AbstractMavenRepository.java:466)      at org.sonatype.nexus.proxy.repository.AbstractRepository.storeItem(AbstractRepository.java:661)      at org.sonatype.nexus.proxy.router.DefaultRepositoryRouter.storeItem(DefaultRepositoryRouter.java:179)      at org.sonatype.nexus.content.internal.ContentServlet.doPut(ContentServlet.java:602)      at org.sonatype.nexus.content.internal.ContentServlet.service(ContentServlet.java:357)  {noformat}    Since the method is synchronized and this method copies the entire list of repository members which may be large, in a heavily loaded system receiving combinations of the mentioned code paths, threads may backup to the point where Nexus may appear non-responsive due to a high number of blocked threads on the subject method.    This performance problem is most likely to surface under two primary conditions:    1) nested group repo hierarchy  2) group repos containing thousands of repositories  """
"NEXUS-16985","Bug","Migration",3,"Nexus 2 to 3 migration fails if there are staging build promotion repositories.","""An upgrade from Nexus 2.x to 3.x will fail if Nexus 2.x has staging build promotion repositories.   The repositories migration list will not even load.  The logs show this error:       {quote}2018-05-02 14:11:03,506-0600 ERROR [pool-21-thread-3] admin org.sonatype.nexus.extdirect.internal.ExtDirectServlet - Failed to invoke action method: migration_Repository.read, java-method: com.sonatype.nexus.migration.ui.RepositoryComponent.read   java.lang.NullPointerException: Cannot get property 'memberNames' on null object   at org.codehaus.groovy.runtime.NullObject.getProperty(NullObject.java:60)   at org.codehaus.groovy.runtime.InvokerHelper.getProperty(InvokerHelper.java:174)   at org.codehaus.groovy.runtime.DefaultGroovyMethods.getAt(DefaultGroovyMethods.java:257)   at org.codehaus.groovy.runtime.dgm$241.invoke(Unknown Source)   at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoMetaMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:251)   at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.call(PogoMetaMethodSite.java:71)   at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48)   at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:35)   at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48)   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:58)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)   at com.sonatype.nexus.migration.ui.RepositoryComponent$_closure3.doCall(RepositoryComponent.groovy:144)  {quote}  *Expected*    There isn't currently an upgrade path from Nexus 2.x staging to Nexus 3.x staging, so it is expected that the the build promotion repositories should show up in the migration repository list as disabled."""
"NEXUS-16994","Bug","Scheduled Tasks|UI",2,"Task scheduling can give UI errors","""Example repro steps:  Enable a Repository Health Check on a maven, nuget, or npm repo.    Attempt to edit the settings for the resulting task, e.g.    System - Repository Health Check: maven-central (Check for new report availability)    Change the 'Task frequency', from """"Hourly"""" to """"Daily"""" and save the changes.     The UI shows errors like the one below, and the page will need to be reloaded at the browser level to function:    !UIErrorRHCTaskSchdule.png!"""
"NEXUS-17001","Bug","HTML View|Security",1,"NullPointerException when accessing the HTML view of a repository as anonymous user (when anonymous access is disabled)","""I've received this error under these circumstances:   * Nexus OSS 3.11.0-01   * anonymous access disabled   * used browser to access: [https://nexus/service/rest/repository/browse/maven-group/]   ** result: ERROR: (ID e58724ee-f009-47f8-9c17-2be6ee10e8e7) java.lang.NullPointerException   * if I log in to Nexus, the browsing succeeds    I am happy an anonymous user cannot browse the repository (as intended), but they should probably just get an Unauthorized message rather than a NullPointerException."""
"NEXUS-17008","Bug","Scheduled Tasks",3,"Task Does Not Run (Again) if it Overruns into Next Run Time","""If a scheduled task overruns into its 'Next Run' time, the 'Next Run' time is not updated and the task does not run again.    *To reproduce:*  1. Configure an 'Execute Script' task with the attached script. It is coded to run for ~5minutes.  2. Configure a cron expression for the task frequency so that it runs every 2 minutes i.e. 0 0/2 * 1/1 * ? *    As the task runs for 5 minutes, it goes past its next run time which does not get updated and the task does not run after its first run."""
"NEXUS-17097","Bug","IQ Integration",2,"Error communicating with IQ server persists indefinitely in repository manager if IQ server connection fails while displaying the repository list","""*Historical:* This issue moved from https://issues.sonatype.org/browse/NEXUS-16977    Start a new NXRM 3.10.0-04.    Start a new IQ server 1.46 install.    Click Analyze button in NXRM repo list for maven-central, choose Yes to All to enable healthcheck for all repos. ( by default this means maven-central and nuget.org-proxy )    Click away from repo list. Click back to repo list.    Eventually the Healthcheck column states """"insufficient trend data"""" for nuget.org-proxy and maven-central. The IQ Policy Violation column shows a circle with a line through it icon.    Configure IQ server connection to the running IQ server. Verify it is working.    Add a IQ: Audit and Quarantine capability for maven-central.    Click back to repositories list several times. Eventually the IQ Policy Violations column will show """"No Violations"""" and a link to the report.    Make a request for an artifact in maven-central.  curl -v -u admin:admin123 http://localhost:8081/repository/maven-central/abbot/abbot/0.13.0/abbot-0.13.0.jar -o /dev/null    Click away and back to the repo list. IQ Policy Violations still shows No Violations.    Login to IQ server and verify that maven-central is registered correctly.    In NXRM click the maven-central report link. The report should show 1 component identified.    Stop IQ server    Request a different component from maven-central repo. This one takes 15 seconds to fail because Nexus can't reach IQ server.    Click away and then back to the repo list in NXRM. IQ Policy Violation column shows Error communicating with IQ Server Connection refused.    Start IQ Server.    Click away from then back into the repositories list. The IQ policy Violations column now shows No Violations + link again as it is able to detect that IQ server is available.    Stop IQ Server.    Click away and then back to the repo list in NXRM. IQ Policy Violation column shows Error communicating with IQ Server.    Start IQ Server.    Click away and then back to the repo list in NXRM. IQ Policy Violation column still shows Error communicating with IQ Server.    **{color:red}From this point on there is no non-destructive way to clear the Error Communicating with IQ server message except to restart NXRM{color}**    Other non-destructive things tried that do not work:    1) request a component through maven-central while IQ server is offline  2) signin and signout of NXRM UI  3) Click Verify Connection again successfully in NXRM IQ Server configuration screen    h4. Workaround    To reset the UI connection state:    First wait until or at least verify the IQ server has become available again - you can test this with the Verify Connection button in the IQ Server configuration screen. Even though the verification of connection succeeds, this will not make the UI error message go away.    Make a request for a jar artifact that exists at the remote but is not yet cached in the proxy repository local cache OR has never been requested while IQ server was disconnected. This should trigger a new evaluation for that artifact. If you do not know of an artifact that is at the remote that is not yet cached, use the Nexus UI to find jar asset, delete it, then make a request for it again through your proxy repository. This re-requesting of a non-cached asset that needs auditing will reset the audit task error state, if the IQ server can be reached.     Then click away from and click back to the repositories list view. The IQ Policy Evaluation column should then remove the error state.    h4. Expected    Brief interruptions to IQ server communication ( like restarts) should not persist indefinitely. If they do persist, there should be a well travelled trivial method to reset the connection state.    The stated workaround is not adequate:    - it is confusing and a manual process - it should be automatic  - it may be impossible to know the path of an artifact that exists at the remote that has never been requested through the proxy repository  - the manual process may need repeating for 10s to 100s of repos  - the log messages in the application log are not informative to solve or diagnose the problem  - Verify Connection on the IQ Server configuration verifies that the connection works, yet the UI state never resolves itself despite this    """
"NEXUS-17101","Bug","Repository",1,"PartialFetchHandler produces invalid HTTP responses","""Any URL that has PartialFetchHandler involved produces invalid HTTP responses.    This was reported against the Nexus APT community plugin here: https://github.com/sonatype-nexus-community/nexus-repository-apt/issues/47    I looked into this, and it looks like {{PartialFetchHandler}} sends an invalid {{Content-Range}} header here: [https://github.com/sonatype/nexus-public/blob/82d58ec7e1464cf25efca423bbe5f8cbd6eb4f55/components/nexus-repository/src/main/java/org/sonatype/nexus/repository/http/PartialFetchHandler.java#L119]    According to the RFC ([https://tools.ietf.org/html/rfc7233#section-4.2]) the {{Content-Range}} header needs a unit specifier of """"bytes"""" in it.    Also, it seems like the base code doesn't correctly handle the {{If-Range}} header. I have a PR that fixes both issues if you want to merge to the base code: [mpoindexter/nexus-public#1|https://github.com/mpoindexter/nexus-public/pull/1]"""
"NEXUS-17156","Bug","Staging",1,"Staging REST endpoint returns 401 when a user is authenticated but lacks needed privileges","""The new staging REST API returns a 401 response when a user has been successfully authenticated, but does not have the needed privileges to perform the operation.  This is incorrect, and very confusing.  It should return a 403 error when a user has been authenticated, but is not permitted.                    """
"NEXUS-17199","Story","Staging",5,"Apply content selector permissions to staging operations","""*Acceptance*   * Allow staging operations on the basis of content selector permissions   * e.g. admins can allow staging operations to and from portions of repositories, rather than at the whole-repo level as specified in --NEXUS-15153--   * In situations where users don't have privileges to complete the operation*, the entire operation should return an error code.    e.g. components ABC are in Repo 1 (R1).   # A user can see ABC in R1   # User initiates move ABC to R2   # System checks permissions, finds user can't delete A from R1, and can't write C to R2."""
"NEXUS-17202","Bug","NPM",0.5,"""out of range"" error when installing some npm packages via proxy","""I use nexus 2.14.8 as a npm proxy, when I run    npm install rc-select --registry=[http://nexus.example.com/content/repositories/npm-proxy/]    I get a lot of 500 error from the server.     nexus log:    """
"NEXUS-17218","Bug","IQ Integration|Repository Health Check",1,"Soft deleted blob in health check prevents viewing of component information","""After upgrading from 3.10.0 to 3.12.0 a soft deleted blob prevented viewing of component information in an IQ integrated Nexus Repo instance.  The UI shows an NPE:  {quote}ERROR: (ID 4ceb7d0e-0bd4-49a1-9d71-1b72515fc38a) java.lang.NullPointerException  {quote}  The nexus.log shows:          """
"NEXUS-17222","Bug","Logging|Maven|Scheduled Tasks",2,"rebuild maven metadata task may fail against permissive layout coordinates missing a base version","""The following repository was defined in Nexus 3.6.0:        When the rebuild Maven metadata task processes this repository, the following exception can be triggered and the task will fail:        There are no seemingly no obvious log messages that can be enabled to identify the problem coordinates.    h4. Expected.    First priority is that the task should not fail. The repo policy was permissive and possibly could have been mixed or releases at some point. We allowed files to be stored in the repository that cause the task to fail. The task should proceed to fix the metadata as best as it can.    Second priority is to improve logging. The [next log line|https://github.com/sonatype/nexus-internal/blob/de1c5692c0ce54b20ea5c8eeab9c6b7a4a459320/plugins/nexus-repository-maven/src/main/java/org/sonatype/nexus/repository/maven/internal/hosted/metadata/MetadataBuilder.java#L263] below [where the exception is thrown|https://github.com/sonatype/nexus-internal/blob/de1c5692c0ce54b20ea5c8eeab9c6b7a4a459320/plugins/nexus-repository-maven/src/main/java/org/sonatype/nexus/repository/maven/internal/hosted/metadata/MetadataBuilder.java#L261]  is exactly what we could use to find the offending path and possibly delete the data from the repository to work around the problem.        Putting logging statements printing method arguments after method arguments are validated is an anti-pattern that should be avoided."""
"NEXUS-17225","Bug","UI",5,"Nexus Repository 3.12.0 render error -- removeAll","""After installation and login I receive the following error randomly when browsing.    Remove IP address.    ----------------    Unable to get property 'removeAll' of undefined or null reference (http://X.X.X.X:8081/static/rapture/nexus-rapture-prod.js?_v=3.12.0-01:1) """
"NEXUS-17233","Bug","Backup|Database",5,"Restarting while backup is in progress leaves NXRM as read-only","""If you restart a node while it's taking a backup then the node will preserve the read-only state, despite the backup task having been aborted. The following log entry during restart is where it restores the frozen state:      The first thing to determine is whether this is acceptable behaviour - while it leaves NXRM in a degraded state (only returning previously cached content, disallowing write operations) it could be considered the safe option given NXRM was shutdown during backup. It is also not difficult to return NXRM to a writable state, using the UI or REST. If we decide this is """"working-as-designed"""" then this ticket will verify that manual intervention can quickly return NXRM to normal service. We should also verify that the behaviour is the same whether NXRM is clustered or non-clustered.    However, if it's determined that this is not acceptable behaviour then this ticket will look at how to avoid leaving the freeze state if the backup task was aborted during shutdown. This may involve making sure we update the freeze state regardless of how the backup task ends. Note if the process is forcibly killed (eg. power-cut) then we won't get any chance to update it, but in that case it might be best to start as read-only as there may be data integrity issues. Another option would be to try and detect when backup was aborted and ignore restoring the freeze state - but that sounds more fragile. Other suggestions are welcome :)    While recreating these scenarios (restarting while non-clustered backup is in progress and restarting while clustered backup is in progress) also consider whether extra logging would be useful.    Summary:    * Confirm restarting while non-clustered backup is in progress leaves NXRM as read-only  * Confirm restarting while clustered backup is in progress leaves NXRM as read-only  * Get input from team / PO about desired behaviour  * Implement any change in behaviour (if necessary)  * Consider additional logging / recording reason for freeze"""
"NEXUS-17231","Bug","Security",2,"user role mappings do not match user ids case insensitively","""[User ids are intentionally matched case insensitively|https://issues.sonatype.org/browse/NEXUS-4115]. This issue demonstrates a case where they are not.    Setup Nexus to connect to an LDAP realm which contains a user record which has a userid of lowercase *testuser1*. Verify that you can authenticate as that user id.    Create a new role in nexus called *custom_role*.    Create a script to map roles to users like this and put in an Execute Script task:        Execute the script. The script creates a record in the Nexus security database similar to this:        One can authenticate the userid as testuser1 or TESTUSER1. However when the user record is found in the Users list in nexus, there will be no roles shown in the Active role list and after signin, the user will not have the permissions granted by the custom_role either.    Conversely if the user mapping is created using the Nexus UI, the userid is stored in Nexus with the lettercase matching that as stored in the LDAP server. In that case, when the custom_role is mapped to the user, there is no problem reading back its active roles.    h4. Expected    Since userids are to be authenticated case insensitively, a users roles should also be looked up by userid from the Nexus database case insensitively.  Since it is also possible that NXRM has allowed multiple user role mappings to the same userid ( but with different lettercase ), there should be a way for an admin to reconcile these duplicate userids either by way of running a script or from an upgrade step.    """
"NEXUS-17257","Bug","Docker|Logging",1,"Spurious warnings logged when pushing docker images","""Run the following commands:    {code}  docker pull hello-world  docker tag hello-world localhost:8444/hello-world  docker push localhost:8444/hellowworld  {code}    The upload succeeds, but 21 warnings are logged. They don't appear to hurt anything, the push succeeds.  But it is causing a lot of log noise.     """
"NEXUS-17262","Bug","Scheduled Tasks",1,"Removing repository does not remove tasks specific to the removed repository","""Removing repository does not remove tasks specific to the removed repository. This cause the task to fail with a NullPointerException.    *Steps to Reproduce*   * Create a rebuild index task on existing repo.   * Remove that repository   * Run the rebuild index task    *Expected Behaviour*    Either remove the task linked to the removed repository or provide an error when task is run that explains the repository has been deleted (instead of the NullPointerException) .     """
"NEXUS-17277","Bug","LDAP|UI",0.5,"LDAP ""Verify login"" validates for credentials before closing","""I noticed that on LDAP config, if you click """"Verify login"""" and then immediately cancel, it prompts for username validation before closing the window resulting in the need for an additional click to cancel."""
"NEXUS-17285","Bug","UI|Yum",0.5,"Yum component properties update does not reflect in UI summary pane","""As part of """"update"""" test (so redeploy was on/allowed), I pushed the below item:    Then I realized I had the wrong package pushing so before metadata rebuild, I pushed again:    On review of the component/asset properties via the UI summary pane, it shows the fuse-sshfs property not the electricfence property. The rpm downloaded is electricfence however and the xml shows the right one as well (electricfence). Also not really surprisingly considering that, the blob updated attribute shows a different date than created.    If you do the reverse of this (eletricfence then fuse-sshfs) it does the reverse as well so it's not just something with electricfence.    I then tried waiting til after the metadata rebuild and seeing if that helped. It didn't.   It seems only the first package being pushed not the update is updating the properties in the UI even though the package is updated.   I tried clearing browser cache and restarting my browser in between as well, no dice."""
"NEXUS-17296","Bug","Transport",1,"legacy URLs do not work with nexus.view.exhaustForAgents property","""In 3.10.0, set these in etc/nexus.properties:        This request will be exhausted by the ExhaustRequestFilter filter:    {{curl -v -X PUT [http://localhost:8081/repository/maven-releases/foo] -H """"user-agent: maven-artifact"""" -o /dev/null}}    This request will not and fail as per scenarios described in https://issues.sonatype.org/browse/NEXUS-10234 :    {{curl -v -X PUT [http://localhost:8081/content/repositories/maven-releases/foo] -H """"user-agent: maven-artifact"""" -o /dev/null}}    Debugging shows that the filter still has the default agent pattern of *Apache-Maven.** for legacy URLs instead of the one set by the property.    h4. Expected    Legacy URLs should be perform identically to the regular URLs in the described case.          """
"NEXUS-17450","Bug","Security|UI",2,"Uncaught TypeError: Cannot read property 'length' of null trying to save a role which cannot be found","""Configure LDAP server and LDAP realm.    Try to map an LDAP role that does not exist.    Click Save to save the role.    Saving the role will fail because Nexus tries to find the role in LDAP and can't. The UI should show a message that the role could could not be found.    The javascript console contains this error:    nexus-coreui-plugin-prod.js?_v=3.12.1-01:1 Uncaught TypeError: Cannot read property 'length' of null   at j.load (nexus-coreui-plugin-prod.js?_v=3.12.1-01:1)   at A.fire (baseapp-prod.js?_v=3.12.1-01:1)   at j.continueFireEvent (baseapp-prod.js?_v=3.12.1-01:1)   at j.fireEventArgs (baseapp-prod.js?_v=3.12.1-01:1)   at j.a.fireEventArgs (baseapp-prod.js?_v=3.12.1-01:1)   at j.fireEvent (baseapp-prod.js?_v=3.12.1-01:1)   at j.onProxyLoad (baseapp-prod.js?_v=3.12.1-01:1)   at A.processResponse (baseapp-prod.js?_v=3.12.1-01:1)   at A.<anonymous> (baseapp-prod.js?_v=3.12.1-01:1)   at baseapp-prod.js?_v=3.12.1-01:1         !image-2018-06-20-17-23-16-072.png!  h4. Expected    - there should be no javascript errors in this case."""
"NEXUS-17441","Bug","HA|UI",1,"IllegalStateException: ""Clustering is not enabled"" when refreshing nodes view in PRO when not clustered","""Unpack a fresh PRO instance and start it without clustering.    Log in as admin and navigate to the nodes view: http://127.0.0.1:8081/#admin/system/nodes    Click the refresh symbol in the top-right of the UI and you'll get the following warning in the UI:        The issue appears to be that refreshing the nodes view using the """"refresh"""" icon triggers reloading of the {{NX.coreui.store.Node}} store which then calls {{NX.direct.proui_Node.read}}. Note this store is not loaded when you navigate to the nodes view, only when you subsequently refresh the view using the """"refresh"""" icon (and the warning only appears when not clustered).    One fix would be to avoid reloading the store when non-clustered, even when the """"refresh"""" icon is clicked. The other option would be to make the PRO {{NodeComponent}} more robust and only use the {{hazelcastProvider}} when clustered - the other methods in that class have similar checks, it's only the """"read"""" method that attempts to use the {{hazelcastProvider}} without checking if clustering is enabled or not. (We should pass back an empty result if clustering is not enabled.)"""
"NEXUS-17460","Bug","Database|Migration|NuGet",3,"Nexus 2 to 3 upgrade fails with concurrency error in NuGet","""The exception below was observed in the migration logs of someone who was upgrading from Nexus 2 to Nexus 3. This looks like a concurrency bug to me:  """
"NEXUS-17455","Bug","Migration",2,"Last-Modified not returned in header for migrated RAW artifacts.","""Nexus 3.x does not return a Last-Modified header for migrated artifacts (migrated from Nexus 2.x)  {quote}$ curl --head localhost:8081/repository/jenkins-tools/jdk/jdk1.8.0_171.tar.gz    HTTP/1.1 200 OK    Date: Thu, 21 Jun 2018 11:22:30 GMT    Server: Nexus/3.12.0-01 (PRO)    X-Content-Type-Options: nosniff    Content-Security-Policy: sandbox allow-forms allow-modals allow-popups allow-presentation allow-scripts allow-top-navigation    Content-Type: application/x-gzip    Content-Length: 190882219  {quote}  If you upload an artifact to that Nexus 3.x instance, then the Last-Modified is returned.  {quote}$ curl --head localhost:8081/repository/jenkins-tools-test/jdk/jdk1.8.0_171.tar.gz    HTTP/1.1 200 OK    Date: Thu, 21 Jun 2018 11:22:24 GMT    Server: Nexus/3.12.0-01 (PRO)    X-Content-Type-Options: nosniff    Content-Security-Policy: sandbox allow-forms allow-modals allow-popups allow-presentation allow-scripts allow-top-navigation    Last-Modified: Thu, 21 Jun 2018 11:17:29 GMT    Content-Type: application/x-gzip    Content-Length: 190882219  {quote}  Reproduced this with Raw repo. Works fine with Maven2"""
"NEXUS-17470","Bug","Logging|Proxy Repository",1,"RemoteBlockedIOException WARN logged for every content request to manual or auto blocked repository should be TRACE","""# In Nexus 3.12.1, create a proxy repository.   # Set """"block outbound connections on the repository"""" to true.   # Fire a request to the repository.    You will see a warning like this in the log:  {quote}2018-06-26 14:30:18,863-0500 WARN [qtp335821775-220] admin org.sonatype.nexus.repository.maven.internal.proxy.MavenProxyFacet - Exception org.sonatype.nexus.repository.httpclient.RemoteBlockedIOException: Remote Manually Blocked checking remote for update, proxy repo maven-central failed to fetch foo/bar/maven-metadata.xml, content not in cache.  {quote}  This will be repeated for every single request received by the proxy, filling the logs with useless messages.    *Expected:*    -This should be logged at DEBUG, if at all.- Upon closer review moving the level to TRACE seems better since the blocked status of a repository can be easily determined from other INFO level log messages and the repository status is also visible when viewing the repositories list in the UI."""
"NEXUS-17477","Bug","RubyGems",3,"Unable to install hosted gem which has multiple version requirements","""h1. Bug description    If a ruby gem is hosted on Sonatype Nexus 3 Rubygems repository    and that ruby gem contains multiple version requirements for a dependency in its gemspec    and the project which uses the ruby gem does not have a Gemfile.lock    Then bundler will fail to install the project gems with the following message:    h1.      We noticed this bug on our production ruby gem repositories (hosted on Sonatype Nexus3 v3.6.2) then I reproduced with the latest version of SNRM : v3.12.1         This issue has been reported on bundler github (see [https://github.com/bundler/bundler/issues/6048] ). The original bug report mentions Sonatype Nexus but was closed without having been properly investigated.       h1. Reproducing the bug       h2. Nexus Setup         I installed an instance of Sonatype Nexus3 using docker like this:    Once the instance has booted, I open [http://localhost:8081|http://localhost:8081/] with browser, log to nexus using default credentials (admin/admin123), then I create a hosted rubygems repository which I name """"gems-test"""".    The URL of the gems-test repository on the local nexus is [http://localhost:8081/repository/gems-test/]       h2. Test ruby gem         I generated a sample ruby gem using the `bundle gem test-gem-requirement-bug` command on my workstation.         I edited the generate gem to supply the following gemspec:         Note the multiple version requirement on the activesupport dependency above.         Then I `gem build *gemspec` to produce the gem.         Then I upload the produced gem to Nexus using the upload interface at http://localhost:8081/#browse/upload:gems-test         If everything was successful the ruby gem should be listed in the """"browse"""" view of the gems-test repository (URL [http://localhost:8081/#browse/browse:gems-test] )  h2. Test ruby project         I create a sample ruby project on my workstation, which uses the """"test-gem-requirement-bug"""" gem.              Then, still in the testproject directory, I run:         To install the project dependencies.         The """"bundle install"""" command fails with the trace at the top of project.                   Note that running """"bundle install --path .bundle --full-index"""" succeeds to install the gem and its dependencies (even if it is inefficient).               """
"NEXUS-17492","Story","Content Selectors|Docker|Documentation",2,"Document Docker use of content selectors","""*Acceptance*    Amend the documentation so that users know how to subdivide Docker repositories based on tag names. Make sure they know it's leaky (since you can request content by hash, in theory.)"""
"NEXUS-17502","Bug","Logging|Proxy Repository",1,"Content validation message does not log which repository the invalid content is coming from","""When a proxy repository gets a file that fails content validation it logs a message like this:  {quote}2018-06-29 00:00:23,232-0600 WARN [qtp194194304-13484] *UNKNOWN org.sonatype.nexus.repository.view.handlers.ExceptionHandler - Invalid content: GET /net/jcip/annotations/1.0/annotations-1.0.pom: org.sonatype.nexus.repository.InvalidContentException: Detected content type [text/html], but expected [application/xml, text/xml, application/x-xml]: net/jcip/annotations/1.0/annotations-1.0.pom.xml  {quote}  Note that it does *not* say which repository has the problem. This makes it very difficult to track the problem down. A problem which could have been solved in seconds requires enabling http debug logging, and then laboriously scanning the output to determine where the bad content is coming from.  This can take a considerable amount of time.    *Expected:* The log message should clearly log which repository the invalid content was received in."""
"NEXUS-17501","Bug","Database|NuGet",3,"Caching of NuGet metadata causes thread serialization, query slowdowns under load","""Threads updating NuGet metadata are getting be serialized, as can be seen in the examples below.  This is causing a slowdown in processing of NuGet queries.      This was observed in Nexus Repo 3.8.0-02.    Full thread dump is attached.  In that thread dump there are 5 threads waiting on the one that is running.        Example blocked thread:      """
"NEXUS-17498","Improvement","PyPI|User Token",1,"option to generate URL-safe user tokens for URL based authentication","""Certain primitive repository formats only support URL based authentication of the form:    [http://user:password@localhost:8081|http://user:password@localhost:8081/artifactory/api/pypi/pypi-virtual/simple]    For example PyPI is known to have this problem:    [https://github.com/pypa/pip/issues/4315]    YUM auth has a similar problem.    Admins may want to enforce authenticated access to repo manager using our User Tokens feature.    Currently User Tokens may be generated with non-url safe characters which prevent using the above URL based authentication mechanism.  h4. Expected    To support primitive url based auth, there should be an optional option to generate user tokens which only contain url safe characters.    h4. Possible Workaround Some Formats    Take the user token name and value generated by Nexus and URL Encode them manually first, before adding them to the URL being used in your tool.    For example, Nexus generates these tokens:    Name Code: {{/pKsRLhn}}   Pass Code: {{/HAx3rBkdsQAmsUhc2RWt73CY8zrMJPS9e1HnYeseYbh}}    As expected, this URL will not parse correctly:    [URL Encode|https://www.urlencoder.org/] each value:    Name Code: {{%2FpKsRLhn}}   Pass Code: {{%2FHAx3rBkdsQAmsUhc2RWt73CY8zrMJPS9e1HnYeseYbh}}    All of these requests authenticate properly:      While no authentication for the same URL will fail:         h4. Windows Pip     This workaround does not solve this issue for python/pip when running on Windows, see my comment below for details."""
"NEXUS-17510","Bug","REST",1,"clarify intent when no components found for REST API","""The REST call to associate a tag with components may return a 200 status code indicating the operation was successful, but the response body may not contain any components the tag was associated with.    {    """"status"""": 200,    """"message"""": """"Association successful"""",    """"data"""": {      """"components associated"""": []    }  }    One user of the REST API was confused by this. Their usage pattern was:    1. create tag - 204  2. associate tag with components - 200  3. move components with tag  - 404    At step 2, the status code implies some components were associated with the tag, when in fact there were no components found matching the criteria submitted.    The script moved to step 3 and failed curiously because there were no components fou d with the tag applied in step 2.    So while the API seems to be acting as designed, there may be a slight disconnect between tagging and staging here.     h4. Expected    Clarify why the associate tag API returns 200 status code even though no components VS the staging move API which returns 404 for no components.    Add this type of caveat or clarity to our documentation.  """
"NEXUS-17548","Bug","Docker|Transport",3,"Docker proxy repositories auto-block for images that don't exist","""Create a proxy repository to docker hub.  Put this in a group repository.    Then request a manifest from through the group repository that doesn't exist on docker hub:    [http://localhost:8081/repository/docker-group/v2/fff/ggg/manifests/4.0.8r0]    The remote will return an authorization error, causing the repository to auto-block:       {quote}2018-07-06 14:40:20,433-0500 WARN  [qtp335821775-248] admin org.sonatype.nexus.repository.docker.internal.V2Handlers - Error: GET /v2/fff/ggg/manifests/4.0.8r0: 401 - org.sonatype.nexus.repository.docker.internal.V2Exception: authentication required   2018-07-06 14:40:41,441-0500 INFO  [elasticsearch[441A7C2A-D43F4D99-9C18AFA3-F4D24CC1-BA03CB4F][management][T#2|#2]] *SYSTEM org.elasticsearch.cluster.routing.allocation.decider - [441A7C2A-D43F4D99-9C18AFA3-F4D24CC1-BA03CB4F] low disk watermark [85%] exceeded on [ggbWczR0QgqEeYJBh3TOpg][441A7C2A-D43F4D99-9C18AFA3-F4D24CC1-BA03CB4F][/Users/rseddon/temp/foo/sonatype-work/nexus3/elasticsearch/nexus/nodes/0] free: 67.8gb[14.5%], replicas will not be assigned to this node   2018-07-06 14:41:00,609-0500 INFO  [Check Status https://registry-1.docker.io] admin org.sonatype.nexus.repository.httpclient.internal.HttpClientFacetImpl - Repository status for docker-io changed from AUTO_BLOCKED_UNAVAILABLE to AVAILABLE - reason n/a for n/a   2018-07-06 14:41:11,451-0500 INFO  [elasticsearch[441A7C2A-D43F4D99-9C18AFA3-F4D24CC1-BA03CB4F][management][T#3|#3]] *SYSTEM org.elasticsearch.cluster.routing.allocation.decider - [441A7C2A-D43F4D99-9C18AFA3-F4D24CC1-BA03CB4F] low disk watermark [85%] exceeded on [ggbWczR0QgqEeYJBh3TOpg][441A7C2A-D43F4D99-9C18AFA3-F4D24CC1-BA03CB4F][/Users/rseddon/temp/foo/sonatype-work/nexus3/elasticsearch/nexus/nodes/0] free: 67.8gb[14.5%], replicas will not be assigned to this node  {quote}       I'm not entirely sure why the remote returns unauthorized when it should return 404.  It seems to be some sort of misguided attempt at security?    But the current behavior of auto-blocking potentially makes them unavailable for subsequent requests.       This behavior might be OK for a direct request to to a docker proxy repository (maybe). But it causes a real problem for group repositories. They can contain many docker proxy repositories, so it is expected that requests will be coming into them frequently be for images that don't exist on at least some of the remotes of their contained proxy repositories.     """
"NEXUS-17616","Bug","Browse Storage|LDAP|Logging",1,"On browse w/ LDAP, if no perms, a bunch of warns are fired","""With an LDAP user with just analytics permission, I logged in and clicked Browse (as it was clickable) and while nothing showed (proper since I have no permission), I noticed a bunch of WARNs in the nexus.log (see below, for example, attached for full nexus.log snip).  It seems to be looking through various groups, not sure why.  I didn't think this was all of them but looking at the size, it might be.        While not a functional issue, I left minor given the unknown of how much log bloat this would cause.  It's a lot for the number it seemed to hit here."""
"NEXUS-17611","Bug","NuGet",2,"404s returned for packages containing build-metadata in version","""A 404 is returned from nuget.org when attempting to get a package containing build-metadata in it's version e.g. 1.0.0+githash (see [SemVer 2.0.0|https://docs.microsoft.com/en-us/nuget/reference/package-versioning#semantic-versioning-200])    For example [https://www.nuget.org/packages/NuGet.Frameworks/4.7.0]    To reproduce:   # Setup proxy repo, proxying to [https://www.nuget.org/api/v2] (i.e. nuget.org-proxy)   # Enable DEBUG logging on com.sonatype.nexus.repository.nuget.internal   # Via a nuget client attempt to install package e.g.     _nuget install NuGet.Frameworks -Version 4.7.0 -source [http://localhost:8081/repository/nuget.org-proxy]_         *Expected:*    Package metadata and package are located and installed.    *Actual:*    Package is not installed and a 404 is returned. Following observed in logs showing 404 returned from nuget:  {quote}2018-07-13 15:22:08,672+0100 DEBUG [qtp784640267-626]  *UNKNOWN com.sonatype.nexus.repository.nuget.internal.NugetFeedFetcher - Fetching: GET [https://www.nuget.org/api/v2/Packages(Id='NuGet.Build.Tasks.Pack',Version='4.8.0-preview3.5278+c3240b16fcf3276246fc8c610771d14ab94fdc02]') HTTP/1.1    2018-07-13 15:22:08,792+0100 DEBUG [qtp784640267-626]  *UNKNOWN com.sonatype.nexus.repository.nuget.internal.NugetFeedFetcher - Response: HttpResponseProxy\{HTTP/1.1 404 Not Found [Cache-Control: private, Transfer-Encoding: chunked, Content-Type: text/html; charset=utf-8, Content-Security-Policy: frame-ancestors 'none', X-Frame-Options: deny, X-XSS-Protection: 1; mode=block, X-Content-Type-Options: nosniff, Strict-Transport-Security: max-age=31536000, Date: Fri, 13 Jul 2018 14:22:03 GMT, Connection: close] ResponseEntityProxy\{[Content-Type: text/html; charset=utf-8,Chunked: true]}}    2018-07-13 15:22:08,792+0100 DEBUG [qtp784640267-626]  *UNKNOWN com.sonatype.nexus.repository.nuget.internal.NugetFeedFetcher - Status: HTTP/1.1 404 Not Found    2018-07-13 15:22:08,793+0100 WARN  [qtp784640267-626]  *UNKNOWN com.sonatype.nexus.repository.nuget.internal.NugetFeedFetcher - Status code 404 contacting [https://www.nuget.org/api/v2/Packages(Id='NuGet.Build.Tasks.Pack',Version='4.8.0-preview3.5278+c3240b16fcf3276246fc8c610771d14ab94fdc02]')    2018-07-13 15:22:08,901+0100 DEBUG [qtp784640267-626]  *UNKNOWN com.sonatype.nexus.repository.nuget.internal.proxy.NugetProxyFacet - Proxy repo nuget.org-proxy found no package entry for NuGet.Build.Tasks.Pack 4.8.0-preview3.5278+c3240b16fcf3276246fc8c610771d14ab94fdc02 at remote  {quote}       The request being made is [https://www.nuget.org/api/v2/Packages(Id='NuGet.Frameworks',Version='4.7.0+9245481f357ae542f92e6bc5e504fc898cfe5fc0')] which fails.     Request omitting the build-metadata is successful:  [https://www.nuget.org/api/v2/Packages(Id='NuGet.Frameworks',Version='4.7.0')] and [https://www.nuget.org/api/v2/package/NuGet.Frameworks/4.7.0]    Installing the package directly against nuget.org is successful and it is observed the build-metadata is being omitted i.e. _nuget install NuGet.Frameworks -Version 4.7.0 -source [https://nuget.org/api/v2] -Verbosity detailed_     """
"NEXUS-17621","Bug","RubyGems|UI",1,"Rubygems metadata shows ""{}"" when empty","""I noticed that when Rubygems has no metadata it shows """"{}"""".  This is in contrast to other fields around it (e.g. requirements and executables) as well as I believe browse wide that when empty show nothing.  See attached.    Seems the same for both hosted and proxy."""
"NEXUS-17756","Bug","Documentation|Jenkins Plugin",2,"Nexus Jenkins plugin fails requests when repository manager is not at base webapp context path","""Front Nexus repository manager 3 with a webapp context path that is not """"/"""". For example is """"/nexus"""" instead.    In this scenario, connections to list repositories from the Nexus Platform Plugin for Jenkins fail.        h4. Expected    Nexus may be hosted at any level of webapp context path ( ie. /nexus/server/1/ - connections to it must expect this and handle it normally."""
"NEXUS-17794","Bug","Browse Storage|NPM|Repository Health Check",1,"Log noise when clicking on an npm package root in the tree UI","""If you click on an npm package node in the tree UI a very verbose warning is printed in the log.  There is no need for this to be a warning, it should be logged at DEBUG.     {quote}  2018-08-14 15:21:38,728-0400 ERROR [qtp545619715-42]  nexusadmin org.sonatype.nexus.extdirect.internal.ExtDirectServlet - Failed to invoke action method: healthcheck_AssetDetail.identify, java-method: com.sonatype.nexus.plugins.healthcheck.pro.internal.ui.HealthCheckAssetDetailComponent.identify  java.lang.IllegalArgumentException: Malformed npm component with path ajv. If this is packageRoot, try selecting the tarball instead.   at com.google.common.base.Preconditions.checkArgument(Preconditions.java:210)   at com.sonatype.nexus.plugins.healthcheck.pro.internal.ui.AssetIdentificationService.getNpmComponentIdentifier(AssetIdentificationService.java:163)   at com.sonatype.nexus.plugins.healthcheck.pro.internal.ui.AssetIdentificationService.getComponentIdentifier(AssetIdentificationService.java:137)   at com.sonatype.nexus.plugins.healthcheck.pro.internal.ui.AssetIdentificationService.identify(AssetIdentificationService.java:100)   at com.sonatype.nexus.plugins.healthcheck.pro.internal.ui.AssetIdentificationService$identify.call(Unknown Source)   at com.sonatype.nexus.plugins.healthcheck.pro.internal.ui.HealthCheckAssetDetailComponent.identify(HealthCheckAssetDetailComponent.groovy:47)   at com.palominolabs.metrics.guice.ExceptionMeteredInterceptor.invoke(ExceptionMeteredInterceptor.java:49)   at com.palominolabs.metrics.guice.TimedInterceptor.invoke(TimedInterceptor.java:47)   at org.sonatype.nexus.validation.internal.ValidationInterceptor.invoke(ValidationInterceptor.java:53)   at org.apache.shiro.guice.aop.AopAllianceMethodInvocationAdapter.proceed(AopAllianceMethodInvocationAdapter.java:49)   at org.apache.shiro.authz.aop.AuthorizingAnnotationMethodInterceptor.invoke(AuthorizingAnnotationMethodInterceptor.java:68)   at org.apache.shiro.guice.aop.AopAllianceMethodInterceptorAdapter.invoke(AopAllianceMethodInterceptorAdapter.java:36)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:497)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeJavaMethod(DispatcherBase.java:142)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeMethod(DispatcherBase.java:133)   at org.sonatype.nexus.extdirect.internal.ExtDirectServlet$3.invokeMethod(ExtDirectServlet.java:236)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.dispatch(DispatcherBase.java:63)   at com.softwarementors.extjs.djn.router.processor.standard.StandardRequestProcessorBase.dispatchStandardMethod(StandardRequestProcessorBase.java:73)   at com.softwarementors.extjs.djn.router.processor.standard.json.JsonRequestProcessor.processIndividualRequest(JsonRequestProcessor.java:502)   at com.softwarementors.extjs.djn.router.processor.standard.json.JsonRequestProcessor.processIndividualRequestsInThisThread(JsonRequestProcessor.java:150)   at com.softwarementors.extjs.djn.router.processor.standard.json.JsonRequestProcessor.process(JsonRequestProcessor.java:133)   at com.softwarementors.extjs.djn.router.RequestRouter.processJsonRequest(RequestRouter.java:83)   at com.softwarementors.extjs.djn.servlet.DirectJNgineServlet.processRequest(DirectJNgineServlet.java:630)   at com.softwarementors.extjs.djn.servlet.DirectJNgineServlet.doPost(DirectJNgineServlet.java:593)   at org.sonatype.nexus.extdirect.internal.ExtDirectServlet.doPost(ExtDirectServlet.java:141)    {quote}     """
"NEXUS-17790","Bug","Blobstore|Scheduled Tasks",2,"Repair - Reconcile component database from blob store failure","""A user ran the """"Repair - Reconcile component database from blob store"""" task this failed with the exception seen below.  From the stack, it seems a maven-metadata.xml file was being restored and the file already existed in the target repository.   This is logged as a warning, which is fine, but there are two big problems with the way this was handled:   # The repository and path of the failed asset are not printed   # The task should not stop just because one file couldn't be restored       """
"NEXUS-17784","Bug","Maven",1,"Maven proxy repository download of exe artifact fails","""2018-08-14 10:15:45,262+0800 WARN [qtp1501796269-2439] *UNKNOWN org.sonatype.nexus.repository.view.handlers.ExceptionHandler - Invalid content: GET /com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe: org.sonatype.nexus.repository.InvalidContentException: Detected content type [application/x-executable], but expected [application/x-dosexec]: com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe  2018-08-14 10:15:45,479+0800 WARN [qtp1501796269-2439] *UNKNOWN org.sonatype.nexus.repository.view.handlers.ExceptionHandler - Invalid content: GET /com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe: org.sonatype.nexus.repository.InvalidContentException: Detected content type [application/x-executable], but expected [application/x-dosexec]: com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe  2018-08-14 10:16:57,386+0800 WARN [qtp1501796269-2437] *UNKNOWN org.sonatype.nexus.repository.view.handlers.ExceptionHandler - Invalid content: GET /com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe: org.sonatype.nexus.repository.InvalidContentException: Detected content type [application/x-executable], but expected [application/x-dosexec]: com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe  2018-08-14 10:16:57,712+0800 WARN [qtp1501796269-2437] *UNKNOWN org.sonatype.nexus.repository.view.handlers.ExceptionHandler - Invalid content: GET /com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe: org.sonatype.nexus.repository.InvalidContentException: Detected content type [application/x-executable], but expected [application/x-dosexec]: com/google/protobuf/protoc/3.6.0/protoc-3.6.0-linux-x86_64.exe"""
"NEXUS-17797","Story","Blobstore|S3",2,"No provision to provide server side encryption kms key details while creating s3 type blobstore","""We are testing Nexus-3 PRO 3.12.1-01 & S3 Integration and hit a road block w.r.t creating the S3 based Blob store.. We use the Bring your own key i.e aws-kms  and not the default aws provided kms key as part of Server side encryption on the S3 buckets for obvious security reasons in our organisation.         Current policy mandates that the request header should have the Server side encryption kms-keyId for the PUT’s to succeed and when I am trying to create a blobstore by specifying the existing bucket details it throws Access denied error because of the missing kms keyId in the request.         I don’t see any field exposed to provide this configuration details i.e listed below on the Nexus-3 PRO 3.12.1-01 or on 3.13 version and was suggested to raise a enhancement request by the support team as this is preventing us to use the s3 for HA configuration.         a.       SSE Alogirthm type (Example : aws:kms)    b.      SSE KMS key Id  (Example : 6fxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)    c.       Storage Class (Example : Standard)         *_Example arg params i.e sent from the aws Cli to upload any objects to s3:_*    --sse aws:kms --storage-class STANDARD --sse-kms-key-id : 6fxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx     """
"NEXUS-17808","Bug","HA|NPM",5,"NpmProxyFacetImpl.mergeNewRootWithExistingRoot should handle when an existing root disappears","""{{NpmProxyFacetImpl.mergeNewRootWithExistingRoot}} was added in 3.13.0 under NEXUS-15714 to handle situations where we have previously proxied NPM packages that no longer exist on the remote. It does this by iterating over versions that no longer appear in the new package root, checking to see if any records exist for them locally. To do this it needs to load any existing package root.    Unfortunately this introduces a window where the proxy facet could load an existing package root record, only to find that the referenced blob has since been replaced by another thread. This can happen in HA when the package root is being updated from multiple nodes. This will cause a retry whereas before the new package root would simply be replaced.    The reason we'd prefer to retry is so we don't lose any cached NPM package versions that no longer exist in the upstream package root. If we never retried and just saved the (un-merged) upstream package root then we could potentially lose that cached information. That said, removal of upstream package versions should be rare - and failing to proxy the package root just because the existing root went away between loading the record and fetching the blob is not very robust.    One option is to always ignore when an existing package root's blob is not available.    Another option would be to allow retries, but if we reach the retry limit (you can tell this by catching {{RetryDeniedException}}) then go ahead and cache the upstream package root without merging."""
"NEXUS-17832","Bug","Jenkins Plugin",3,"Jenkins Platform Plugin unable to determine Nexus Repository Manager version using Server URL with trailing slash","""Create a nexus repository manager 3.x global connection in Jenkins using a Server url of *http://localhost:8081/*. Tab or click away from the field. The field is validated and a request is sent to your Nexus 3 instance.    An error displays in the Jenkins UI:    {quote}Unable to determine Nexus Repository Manager version. Certain operations may not be compatible with your server which could result in failed builds.  {quote}    Your jenkins log will contain the cause:        The request is sent to http://localhost:8081//service/rest/wonderland/status instead of http://localhost:8081/service/rest/wonderland/status. Nexus 3 returns 404.    h4. Expected    Trailing slashes should be handled better by this field and ALL outbound requests ( see NEXUS-17756 for alternates )  to NXRM made using this value, either at the configuration screen or during builds. An end user can't be expected to readily know the semantic differences between http://localhost:8081 and http://localhost:8081/ ."""
"NEXUS-17830","Bug","Repository",2,"Can not upload binary to Nexus Repo Raw Repository - Detected content type","""When I try to upload a binary component to Nexus Repo Raw Repository (NRM3.11 pro), I get an error. I tried uploading a  tgz, dmg and a nupkg and received similar errors.    What I would expect is that the component would be uploaded and be made available as a http link for users to be able to download.    I would expect to be able to upload any binary as per [https://help.sonatype.com/repomanager3/user-interface/uploading-components#app]         Reproduced in 3.11 and 3.13    error messages    [https://files.slack.com/files-pri/T0LE5769K-FCDPBKVEW/image.png]    [https://files.slack.com/files-pri/T0LE5769K-FCEQF6AT1/image.png]    [https://files.slack.com/files-pri/T0LE5769K-FCD6QSND7/image.png]         Error message:     Detected content type [application/zlib, application/x-deflate], but expected [application/x-apple-diskimage]: intellij/ideaIC-2018.2.2.dmg               """
"NEXUS-17852","Bug","LDAP|Logging",2,"Problem connecting to LDAP server root cause is not logged at default log levels",""" The exception handling in DefaultLdapConnector and FailoverLdapConnector provides no information at all as to what actually went wrong:    [https://github.com/sonatype/nexus-internal/blob/release-3.13.0-01/private/plugins/nexus-ldap-plugin/src/main/java/org/sonatype/nexus/ldap/internal/connector/DefaultLdapConnector.java#L241]    [https://github.com/sonatype/nexus-internal/blob/release-3.13.0-01/private/plugins/nexus-ldap-plugin/src/main/java/org/sonatype/nexus/ldap/internal/connector/FailoverLdapConnector.java#L248]    This results in useless messages logged at INFO level:  {quote}2018-08-27 07:55:17,511-0600 WARN  [qtp389114725-397394] node3 SCH1844 org.sonatype.nexus.ldap.internal.connector.FailoverLdapConnector - Problem connecting to LDAP server: org.sonatype.nexus.ldap.internal.connector.dao.LdapDAOException: Failed to retrieve ldap information for users.   2018-08-27 07:55:23,582-0600 WARN  [qtp389114725-397122] node3 SCH1844 org.sonatype.nexus.ldap.internal.connector.FailoverLdapConnector - Problem connecting to LDAP server: org.sonatype.nexus.ldap.internal.connector.dao.LdapDAOException: Failed to retrieve ldap information for users.   2018-08-27 07:55:25,237-0600 WARN  [qtp389114725-397304] node3 *UNKNOWN org.sonatype.nexus.ldap.internal.connector.FailoverLdapConnector - Problem connecting to LDAP server: org.sonatype.nexus.ldap.internal.connector.dao.LdapDAOException: Failed to retrieve ldap information for users.   2018-08-27 07:55:29,548-0600 WARN  [qtp389114725-397405] node3 SCH1844 org.sonatype.nexus.ldap.internal.connector.FailoverLdapConnector - Problem connecting to LDAP server: org.sonatype.nexus.ldap.internal.connector.dao.LdapDAOException: Failed to retrieve ldap information for users.   2018-08-27 07:55:35,954-0600 WARN  [qtp389114725-397248] node3 SCH1844 org.sonatype.nexus.ldap.internal.connector.FailoverLdapConnector - Problem connecting to LDAP server: org.sonatype.nexus.ldap.internal.connector.dao.LdapDAOException: Failed to retrieve ldap information for users.  {quote}  Please pass the caused by messages up the chain at INFO.  Note that this applies to all the exceptions thrown in DefaultLdapConnector, not just the one I highlighted above."""
"NEXUS-17850","Bug","Content Selectors|Scripting",1,"API does not validate contents of content selectors. Invalid content selectors can lead to failed upgrade.","""The SelectorManager.create(SelectorConfiguration config) method does not validate the contents passed into it. This allows the creation of invalid content selectors that have no attributes.    These invalid content selectors do not show up in the UI, and therefore cannot be removed by the users.  Furthermore, when an upgrade to a new version of Nexus is performed these invalid records cause a failure, the upgraded instance cannot be started.    *Expected*: The API should protect against invalid content.    Example of invalid record that can be created via API:          Example of failure that occurs when this record is encountered during upgrade:  {quote}2018-08-26 08:15:32,634-0500 INFO [FelixStartLevel] NexusHDQ-One *SYSTEM org.sonatype.nexus.upgrade.internal.UpgradeServiceImpl - Upgrade rubygems from 1.0 to 1.1   2018-08-26 08:15:32,821-0500 WARN [FelixStartLevel <command>sql.update selector_selector set attributes.expression = attributes.expression.replace('//','/')</command>] NexusHDQ-One *SYSTEM com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinary - $ANSI\{green \{db=config}} Error deserializing record with id #39:5 send this data for debugging: ACJzZWxlY3Rvcl9zZWxlY3RvcgEAAAAoJQAAADZvAAAAO3EAAABMABpjb20uYWEuZG9nYWRzCENTRUwgVGVzdGluZyBieSBOaWhhcgIHFGV4cHJlc3Npb24AAABeFLoBY29tLm9yaWVudGVjaG5vbG9naWVzLm9yaWVudC5jb3JlLnNlcmlhbGl6YXRpb24uc2VyaWFsaXplci5yZWNvcmQuYmluYXJ5Lk9TZXJpYWxpemFibGVXcmFwcGVyzgSs7QAFc3IAJ29yZy5jb2RlaGF1cy5ncm9vdnkucnVudGltZS5HU3RyaW5nSW1wbDGzS2f7fekSAgABWwAHc3RyaW5nc3QAE1tMamF2YS9sYW5nL1N0cmluZzt4cgATZ3Jvb3Z5LmxhbmcuR1N0cmluZ9tj3naQywjNAgABWwAGdmFsdWVzdAATW0xqYXZhL2xhbmcvT2JqZWN0O3hwdXIAE1tMamF2YS5sYW5nLk9iamVjdDuQzlifEHMpbAIAAHhwAAAAAXQADS8vLy8vLy8vLy8vLy91cgATW0xqYXZhLmxhbmcuU3RyaW5nO63SVufpHXtHAgAAeHAAAAACdAAhZm9ybWF0ID09ICdtYXZlbjInIGFuZCBwYXRoID1+ICdedAADLion    2018-08-26 08:15:32,835-0500 ERROR [FelixStartLevel <command>sql.update selector_selector set attributes.expression = attributes.expression.replace('//','/')</command>] NexusHDQ-One *SYSTEM com.orientechnologies.orient.core.storage.impl.local.paginated.OLocalPaginatedStorage - Exception `2F976A01` in storage `plocal:/opt/sonatype-work/nexus3/db/config`: 2.2.36 (build d3beb772c02098ceaea89779a7afd4b7305d3788, branch 2.2.x)   com.orientechnologies.orient.core.exception.OCommandExecutionException: Error on execution of command: sql.select from selector_selector   DB name=""""config""""   at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:3421)   at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.command(OAbstractPaginatedStorage.java:3318)   at com.orientechnologies.orient.core.sql.query.OSQLQuery.run(OSQLQuery.java:78)   at com.orientechnologies.orient.core.sql.query.OSQLAsynchQuery.run(OSQLAsynchQuery.java:74)   at com.orientechnologies.orient.core.query.OQueryAbstract.execute(OQueryAbstract.java:33)   at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.query(ODatabaseDocumentTx.java:756)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLUpdate.execute(OCommandExecutorSQLUpdate.java:291)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelegate.execute(OCommandExecutorSQLDelegate.java:70)   at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:3400)   at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.command(OAbstractPaginatedStorage.java:3318)   at com.orientechnologies.orient.core.command.OCommandRequestTextAbstract.execute(OCommandRequestTextAbstract.java:69)   at org.sonatype.nexus.repository.rubygems.upgrade.RubygemsUpgrade_1_1.updateContentSelectorExpressions(RubygemsUpgrade_1_1.java:115)   at org.sonatype.nexus.repository.rubygems.upgrade.RubygemsUpgrade_1_1.apply(RubygemsUpgrade_1_1.java:79)   at org.sonatype.nexus.upgrade.internal.UpgradeServiceImpl.lambda$3(UpgradeServiceImpl.java:193)   at java.util.ArrayList.forEach(ArrayList.java:1257)   at org.sonatype.nexus.upgrade.internal.UpgradeServiceImpl.doUpgrade(UpgradeServiceImpl.java:135)   at org.sonatype.nexus.upgrade.internal.UpgradeServiceImpl.doStart(UpgradeServiceImpl.java:91)   at org.sonatype.nexus.common.stateguard.StateGuardLifecycleSupport.start(StateGuardLifecycleSupport.java:67)   at org.sonatype.nexus.upgrade.internal.UpgradeServiceImpl$$EnhancerByGuice$$dbe05174.CGLIB$start$4(<generated>)   at org.sonatype.nexus.upgrade.internal.UpgradeServiceImpl$$EnhancerByGuice$$dbe05174$$FastClassByGuice$$2ae549d6.invoke(<generated>)   at com.google.inject.internal.cglib.proxy.$MethodProxy.invokeSuper(MethodProxy.java:228)   at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:76)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$TransitionImpl.run(StateGuard.java:191)   at org.sonatype.nexus.common.stateguard.TransitionsInterceptor.invoke(TransitionsInterceptor.java:56)   at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)   at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)   at org.sonatype.nexus.upgrade.internal.UpgradeServiceImpl$$EnhancerByGuice$$dbe05174.start(<generated>)   at org.sonatype.nexus.extender.NexusLifecycleManager.startComponent(NexusLifecycleManager.java:155)   at org.sonatype.nexus.extender.NexusLifecycleManager.to(NexusLifecycleManager.java:95)   at org.sonatype.nexus.extender.NexusContextListener.frameworkEvent(NexusContextListener.java:191)   at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1429)   at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)   at java.lang.Thread.run(Thread.java:748)   Caused by: java.lang.RuntimeException: com.orientechnologies.orient.core.exception.ODatabaseException: Error on deserialization of Serializable   DB name=""""config""""   at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV0.deserializeValue(ORecordSerializerBinaryV0.java:497)   at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV0.readEmbeddedMap(ORecordSerializerBinaryV0.java:576)   at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV0.deserializeValue(ORecordSerializerBinaryV0.java:472)   at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV0.deserializePartial(ORecordSerializerBinaryV0.java:148)   at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinary.fromStream(ORecordSerializerBinary.java:78)   at com.orientechnologies.orient.core.record.impl.ODocument.deserializeFields(ODocument.java:1854)   at com.orientechnologies.orient.core.record.impl.ODocument.checkForFields(ODocument.java:2626)   at com.orientechnologies.orient.core.record.impl.ODocument.rawField(ODocument.java:773)   at com.orientechnologies.orient.core.sql.filter.OSQLFilterItemField.getValue(OSQLFilterItemField.java:129)   at com.orientechnologies.orient.core.sql.OSQLHelper.resolveFieldValue(OSQLHelper.java:310)   at com.orientechnologies.orient.core.sql.OSQLHelper.bindParameters(OSQLHelper.java:401)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLUpdate.handleSetEntries(OCommandExecutorSQLUpdate.java:614)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLUpdate.result(OCommandExecutorSQLUpdate.java:348)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLResultsetAbstract.pushResult(OCommandExecutorSQLResultsetAbstract.java:279)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.addResult(OCommandExecutorSQLSelect.java:759)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.handleResult(OCommandExecutorSQLSelect.java:670)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearchRecord(OCommandExecutorSQLSelect.java:627)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.serialIterator(OCommandExecutorSQLSelect.java:1638)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.fetchFromTarget(OCommandExecutorSQLSelect.java:1585)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearch(OCommandExecutorSQLSelect.java:522)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.execute(OCommandExecutorSQLSelect.java:485)   at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelegate.execute(OCommandExecutorSQLDelegate.java:70)   at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:3400)   ... 33 common frames omitted   Caused by: com.orientechnologies.orient.core.exception.ODatabaseException: Error on deserialization of Serializable   DB name=""""config""""   at com.orientechnologies.orient.core.serialization.serializer.record.binary.OSerializableWrapper.fromStream(OSerializableWrapper.java:48)   at com.orientechnologies.orient.core.serialization.serializer.record.binary.ORecordSerializerBinaryV0.deserializeValue(ORecordSerializerBinaryV0.java:491)   ... 55 common frames omitted   Caused by: java.lang.ClassNotFoundException: org.codehaus.groovy.runtime.GStringImpl   at java.net.URLClassLoader.findClass(URLClassLoader.java:381)   at java.lang.ClassLoader.loadClass(ClassLoader.java:424)   at java.lang.ClassLoader.loadClass(ClassLoader.java:357)   at org.apache.felix.framework.BundleWiringImpl.doImplicitBootDelegation(BundleWiringImpl.java:1764)   at org.apache.felix.framework.BundleWiringImpl.searchDynamicImports(BundleWiringImpl.java:1693)   at org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1528)   at org.apache.felix.framework.BundleWiringImpl.access$200(BundleWiringImpl.java:79)   at org.apache.felix.framework.BundleWiringImpl$BundleClassLoader.loadClass(BundleWiringImpl.java:1958)   at java.lang.ClassLoader.loadClass(ClassLoader.java:357)   at java.lang.Class.forName0(Native Method)   at java.lang.Class.forName(Class.java:348)   at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:683)   at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1863)   at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1746)   at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2037)   at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1568)   at java.io.ObjectInputStream.readObject(ObjectInputStream.java:428)  {quote}"""
"NEXUS-17877","Bug","Firewall|Logging|Migration",3,"Errors logged from firewall migration for new/updated maven-metadata.xml files","""Set up Nexus Repo 2.14.9 with firewall enabled on the maven-central proxy repository.   Run an upgrade to Nexus Repo 3.13.0, and  proceed with this until you get to the synchronize phase, where it's looking for updates in Nexus 2.x.  Now request a new (not previously cached) maven-metadata.xml file through Nexus Repo 2:     [http://localhost:8081/nexus/content/repositories/central/org/apache/httpcomponents/httpclient/maven-metadata.xml]    Observe that Nexus Repo 3 gets an error logged for this:       {quote}2018-08-29 12:45:09,592-0500 ERROR [plan-executor-10-thread-3] admin com.sonatype.nexus.migration.repository.ProcessChangesStep - Failed processing of CREATE central:/org/apache/httpcomponents/httpclient/maven-metadata.xml, will ignore and move on. null   java.lang.NullPointerException: null   at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:882)   at com.sonatype.nexus.migration.repository.migrators.Maven2RepositoryMigratorSupport.getIngestedContent(Maven2RepositoryMigratorSupport.java:206)   at com.sonatype.nexus.migration.repository.migrators.RepositoryMigratorSupport.lambda$0(RepositoryMigratorSupport.java:172)   at com.sonatype.nexus.migration.firewall.EnabledFirewallMigrationFacadeImpl.updateAssetAttributes(EnabledFirewallMigrationFacadeImpl.java:225)   at com.sonatype.nexus.migration.firewall.EnabledFirewallMigrationFacadeImpl.lambda$2(EnabledFirewallMigrationFacadeImpl.java:209)   at org.sonatype.nexus.transaction.OperationPoint.lambda$0(OperationPoint.java:53)   at org.sonatype.nexus.transaction.OperationPoint.proceed(OperationPoint.java:64)   at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)   at org.sonatype.nexus.transaction.Operations.transactional(Operations.java:200)   at org.sonatype.nexus.transaction.Operations.run(Operations.java:155)   at com.sonatype.nexus.migration.firewall.EnabledFirewallMigrationFacadeImpl.maybeRecordAssetAttributes(EnabledFirewallMigrationFacadeImpl.java:208)   at com.sonatype.nexus.migration.repository.migrators.RepositoryMigratorSupport.createOrUpdate(RepositoryMigratorSupport.java:170)   at com.sonatype.nexus.migration.repository.migrators.RepositoryMigratorSupport.processChange(RepositoryMigratorSupport.java:146)   at com.sonatype.nexus.migration.repository.migrators.Maven2ProxyRepositoryMigrator.processChange(Maven2ProxyRepositoryMigrator.java:118)   at com.sonatype.nexus.migration.repository.RepositoryMigrator$processChange$3.call(Unknown Source)   at com.sonatype.nexus.migration.repository.ProcessChangesStep$_submit_closure2.doCall(ProcessChangesStep.groovy:333)   at com.sonatype.nexus.migration.repository.ProcessChangesStep$_submit_closure2.doCall(ProcessChangesStep.groovy)   at sun.reflect.GeneratedMethodAccessor350.invoke(Unknown Source)  {quote}       This is because the firewall migration is attempting to find the GAV coordinates of the file:    [https://github.com/sonatype/nexus-internal/blob/release-3.13.0-01/private/plugins/nexus-migration-plugin/src/main/java/com/sonatype/nexus/migration/repository/migrators/Maven2RepositoryMigratorSupport.java#L206]    The good news is that the affected file is migrated.  And since they are maven-metadata.xml files there isn't anything the firewall migration needed to do with them anyhow.    So this seem to only cause log noise. """
"NEXUS-17884","Bug","Yum",2,"upload of source rpm fails in yum hosted","""I uploaded a src rpm to a up to date nexus3 (3.13.0-01 OSS). And it worked fine:   {quote}  > PUT /repository/Test/SRPMS/7/ngcurl-7.61.0-2.src.rpm HTTP/1.1   ...   < HTTP/1.1 200 OK   < Date: Fri, 31 Aug 2018 12:52:48 GMT   < Server: Nexus/3.13.0-01 (OSS)   < X-Content-Type-Options: nosniff   < Content-Security-Policy: sandbox allow-forms allow-modals allow-popups allow-presentation allow-scripts allow-top-navigation   < Content-Length: 0   < Keep-Alive: timeout=5   < Connection: Keep-Alive   < Content-Type: application/x-rpm  {quote}  But the the content of <hash>-primary.xml.gz is wrong:  {quote}  curl -k [https://nexus.XXX.com/repository/Test/SRPMS/7/repodata/338c845819a87d70703b3ffbbfe763b15bf325c7c017229997621899f7c04b89-primary.xml.gz|https://nexus.xxx.com/repository/Test/SRPMS/7/repodata/338c845819a87d70703b3ffbbfe763b15bf325c7c017229997621899f7c04b89-primary.xml.gz] | gunzip | xmllint --format /dev/fd/0   {quote}    returns:  {quote}  <metadata xmlns=""""[http://linux.duke.edu/metadata/common]"""" xmlns:rpm=""""[http://linux.duke.edu/metadata/rpm]"""" packages=""""1"""">   <package type=""""rpm"""">   <name>ngcurl</name>   <arch>x86_64</arch>   ...  {quote}    Look at the arch attribute, it says x86_64.    I created a local yum repo with createro (0.9.9) on Centos7 (7.5.1804)   With exactly the same rpm, running:   {quote}  $ createrepo -o $PWD -p $PWD -s sha --database --workers 4  {quote}    The equivalent file says:   {quote}  <metadata xmlns=""""[http://linux.duke.edu/metadata/common]"""" xmlns:rpm=""""[http://linux.duke.edu/metadata/rpm]"""" packages=""""1"""">   <package type=""""rpm"""">   <name>ngcurl</name>   <arch>src</arch>  {quote}    The arch is now src    A diff after formatting both with xmllint --format returns:  {quote}  5c5   < <arch>x86_64</arch>   ---   > <arch>src</arch>   7c7   < <checksum type=""""sha256"""" pkgid=""""YES"""">b2a7e8d1a5ff56c9028493ba7103488323578187d4c282e4711199b04509319d</checksum>   ---   > <checksum type=""""sha"""" pkgid=""""YES"""">49d3b8b28b256d65f7cafc94d2a6ed16456bd024</checksum>   17c17   < <time file=""""1535719990576"""" build=""""1535623528""""/>   ---   > <time file=""""1535623528"""" build=""""1535623528""""/>  {quote}    The only real problem is indeed the arch."""
"NEXUS-17886","Bug","Yum",2,"Yum metadata from unrelated folder incorrectly removed when regenerating a folder","""I have two nexus repository (oss) servers, trying to host RPMs.  There seems to be an issue in generating the repodata, as most of the time, i only end up with a repomd.xml file, and none of the files referenced from within repomd.xml.  I've got this happening in two different servers, with similar, but unique RPMs.    The only unique thing we are doing is having multiple yum repositories (repodata's) in a single nexus repository.  So we would have centos/6/ and centos/7/ inside the same nexus repository.  Which should create (with repo depth of 2) centos/6/repodata/ and centos/7/repodata/.      In the log files, there are no errors, just lines like this:    {{2018-08-31 21:00:17,412-0400 INFO [event-9-thread-474] anonymous org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository tempyum}}  {{2018-08-31 21:00:18,515-0400 INFO [event-9-thread-483] anonymous org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Scheduling rebuild of yum metadata to start in 60 seconds}}  {{2018-08-31 21:00:20,507-0400 INFO [event-9-thread-474] anonymous org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository tempyum}}  {{2018-08-31 21:01:20,507-0400 INFO [event-9-thread-486] anonymous org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository tempyum}}  {{2018-08-31 21:01:26,006-0400 INFO [event-9-thread-486] anonymous org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository tempyum}}  {{2018-08-31 21:01:26,221-0400 INFO [event-9-thread-512] anonymous org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Scheduling rebuild of yum metadata to start in 60 seconds}}"""
"NEXUS-17898","Technical Debt","Analytics",1,"Prevent Analytics collection from being enabled by default in HA mode","""Prevent Analytics collection from being enabled by default in HA mode. The user can still opt to turn the capabilities on."""
"NEXUS-17896","Bug","NPM",13,"concurrent requests for large npm metadata can lead to OutOfMemory during serialization","""1. Install nexus 3.13.0   2. Create npm-proxy for [https://registry.npmjs.org|https://registry.npmjs.org/]   3. Create npm-group group repo containing npm-proxy as member.   4. Make concurrent requests to GET [http://localhost:8081/repository/npm-group/npm] ( example of 20 concurrent requests was tested )   At the time of this report, the anticipated response size was 8MB of JSON:     5. Within a short time frame, Nexus will start throwing OutOfMemory errors - either:   OutOfMemoryError: Java heap space   OutOfmemoryError: GC overhead limit exceeded    The stack trace in the application log may take the following forms:      Other symptoms may include gradually slowing requests to the same package metadata."""
"NEXUS-17908","Bug","Tags",2,"Tag association may intermittently fail for new artifact","""Attempting to associate a tag with a newly-uploaded artifact can sometimes result in a 404, """"No components found"""" error.  The core issue appears to be a race condition involving the Elasticsearch subsystem which stores the tag associations."""
"NEXUS-17920","Bug","Yum",1,"Deleting an rpm via DELETE to /repository does not update metadata","""Reproduce case:    Upload two rpm's which have the same artifact ID but different versions into a hosted yum repository in Nexus Repo 3.13.0. I've attached the two i used in testing to make this easy to reproduce.      Wait for yum metadata to be generated, then verify both versions are in the primary.xml.gz file.    Delete the newer version via rest:    Request.log will show it has been successfully deleted:  {quote}127.0.0.1 - admin [07/Sep/2018:16:52:21 -0500] """"DELETE /repository/platform-centos7/my-webapp2-3.1.5-1.noarch.rpm HTTP/1.1"""" 200 - 0 6 """"curl/7.54.0""""  {quote}  The nexus.log shows the yum metadata rebuilds 60 seconds later:  {quote}2018-09-07 16:53:21,255-0500 INFO  [event-9-thread-32] admin org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Rebuilding yum metadata for repository platform-centos7    2018-09-07 16:53:21,330-0500 INFO  [event-9-thread-32] admin org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl - Finished rebuilding yum metadata for repository platform-centos7  {quote}  Download the primary.xml.gz file, and observe both versions are still listed:  """
"NEXUS-17919","Bug","REST",1,"Swagger generated curl command for POST /v1/components can be incorrect","""1. Using the API Swagger interface for POST /v1/components, Try it Out by entering the following information:    repository: nuget-hosted    nuget.asset: Choose a valid nuget package off your local file system.    2. Execute the call    3. The call should be successful. The problem is the example curl command generated is wrong:         Executing this results in an error, even if you correct the file location and add the user/password:              The correct command would be:       """
"NEXUS-17921","Story","REST|Upload UI",1,"REST API upload that fails due to lack of permissions returns 404 (not found)","""*Background*    1. Start up Nexus 3.13.0 with default configuration   2. Create a user """"test"""" that does not have read or browse permissions to the nuget-hosted repository   3. Attempt to upload a nuget component to nuget-hosted using the REST API   4. Observe that this fails with 404 (not found)    Bizarrely, if I grant the test user read access to the nuget-hosted repository the request fails with a 403, but if they have both read and browse privileges it fails with a 400!    *Acceptance*    If the REST API upload fails due to permissions Nexus should always return 403 (forbidden).      """
"NEXUS-18058","Bug","Firewall|HA",2,"Firewall Audit: re-evaluate when no transaction exists for pending asset","""When a pending asset does not have a corresponding firewall transaction, create one and continue with evaluation."""
"NEXUS-18078","Bug","Blobstore",1,"Dynamic Storage - Blob store incorrectly displays Enable Soft Quota as checked","""Blob stores show that Soft Quota is enabled when they are not. This happens when a periodic quota check job runs and tires to access the config from the blob stores attributes. Calling the attributes method on the BlobStoreConfiguration object has a side effect of changing the backing map. Our view objects determine if it is enabled by simply evaluating if the config is null.    Possible solutions:  * Calling attributes should not have side effects. (This seems like the best route)  * Checking to see if the option is enabled could check for more than null  * Save the enabled state separate from the config"""
"NEXUS-18083","Bug","NuGet",0,"potential slow processing of concurrent nuget requests using FindPackagesById()","""Concurrent processing of Nuget gallery types of requests using """"FindPackagesById()"""" queries may become increasingly slow, potentially failing builds.    These types of queries have at least two impacts.    # They lease all of the available HTTP outbound connections per route ( proxy repository remote host, currently 20 per route as of 3.13.0), impacting any other inbound requests that will also require an outbound HTTP request for that same proxy repository route in order to complete work.  # They potentially steal all of the available component database connections available for the entire repository instance ( currently 25 as of 3.13.0 ), impacting other non-related requests that also need a database connection to perform work.    The problem is exasperated when    * the parallel processing feature of a NuGet or MSBuild client is used so that concurrent requests are sent to the same Nexus instance from the same build  * a Nuget build is configured with more than one source gallery instead of just a single Nexus group repository or single proxy repository  * the nexus host is a virtual machine or one with limited I/O resources such as one being run in a cloud service - the network I/O capacity could have considerable impact on triggering this issue    {code:title=Example Request Format}  """"GET /repository/nuget-group/FindPackagesById()?id='Microsoft.AspNetCore.Hosting' HTTP/1.0""""  {code}    Examining Nexus thread dumps taken at the time of concurrent requests may reveal two types of bottlenecks.    h4. Bottleneck 1: HTTPClient Connection Pool Leases    A high number of waiting threads trying to lease an outbound HTTP connection from the Nuget proxy repository outbound HTTP connection pool. The 20 available connections are already leased by running threads.    {noformat:title=Example waiting thread on outbound Nuget HTTP connections}  qtp993807585-20058 id=20058 state=TIMED_WAITING      - waiting on <0x106a98ec> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)      - locked <0x106a98ec> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)      at sun.misc.Unsafe.park(Native Method)      at java.util.concurrent.locks.LockSupport.parkUntil(Unknown Source)      at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitUntil(Unknown Source)      at org.apache.http.pool.AbstractConnPool.getPoolEntryBlocking(AbstractConnPool.java:378)      at org.apache.http.pool.AbstractConnPool.access$200(AbstractConnPool.java:69)      at org.apache.http.pool.AbstractConnPool$2.get(AbstractConnPool.java:246)      at org.apache.http.pool.AbstractConnPool$2.get(AbstractConnPool.java:193)      at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:303)      at org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:279)      at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:191)      at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)      at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)      at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111)      at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)      at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:72)      at org.sonatype.nexus.repository.httpclient.FilteredHttpClientSupport.lambda$0(FilteredHttpClientSupport.java:56)      at org.sonatype.nexus.repository.httpclient.FilteredHttpClientSupport$$Lambda$406/1505474857.call(Unknown Source)      at org.sonatype.nexus.repository.httpclient.internal.BlockingHttpClient.filter(BlockingHttpClient.java:116)      at org.sonatype.nexus.repository.httpclient.FilteredHttpClientSupport.doExecute(FilteredHttpClientSupport.java:56)      at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)      at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108)      at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedFetcher.getPayload(NugetFeedFetcher.java:160)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedFetcher.cachePackageFeed(NugetFeedFetcher.java:77)      at com.sonatype.nexus.repository.nuget.internal.NugetRemoteGalleryFacetSupport$FeedLoader.call(NugetRemoteGalleryFacetSupport.java:392)      at com.sonatype.nexus.repository.nuget.internal.NugetRemoteGalleryFacetSupport.passQueryToRemoteRepos(NugetRemoteGalleryFacetSupport.java:327)      at com.sonatype.nexus.repository.nuget.internal.NugetRemoteGalleryFacetSupport.feed(NugetRemoteGalleryFacetSupport.java:206)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedHandler.feed(NugetFeedHandler.java:69)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedHandler.doHandle(NugetFeedHandler.java:49)      at com.sonatype.nexus.repository.nuget.internal.AbstractNugetHandler.handle(AbstractNugetHandler.java:46)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedHandler.handle(NugetFeedHandler.java:1)        {noformat:title=Nuget Gallery Database WAITING Orderby Thread}  qtp993807585-20116 <command>sql.select from asset where (component.ci_name = :p0) and (bucket=#13:0 or bucket=#13:25) ORDER BY attributes.nuget.download_count DESC, component.ci_name ASC, component.version ASC LIMIT 40</command> id=20116 state=RUNNABLE      at java.util.WeakHashMap.hash(Unknown Source)      at java.util.WeakHashMap.get(Unknown Source)      at com.orientechnologies.orient.core.cache.ORecordCacheWeakRefs.get(ORecordCacheWeakRefs.java:44)      at com.orientechnologies.orient.core.cache.OLocalRecordCache.updateRecord(OLocalRecordCache.java:66)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.executeReadRecord(ODatabaseDocumentTx.java:2034)      at com.orientechnologies.orient.core.tx.OTransactionOptimistic.loadRecord(OTransactionOptimistic.java:187)      at com.orientechnologies.orient.core.tx.OTransactionOptimistic.loadRecord(OTransactionOptimistic.java:162)      at com.orientechnologies.orient.core.tx.OTransactionOptimistic.loadRecord(OTransactionOptimistic.java:291)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.load(ODatabaseDocumentTx.java:1725)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.load(ODatabaseDocumentTx.java:103)      at com.orientechnologies.orient.core.id.ORecordId.getRecord(ORecordId.java:329)      at com.orientechnologies.orient.core.sql.method.misc.OSQLMethodField.execute(OSQLMethodField.java:62)      at com.orientechnologies.orient.core.sql.method.OSQLMethodRuntime.execute(OSQLMethodRuntime.java:139)      at com.orientechnologies.orient.core.sql.filter.OSQLFilterItemAbstract.transformValue(OSQLFilterItemAbstract.java:140)      at com.orientechnologies.orient.core.sql.filter.OSQLFilterItemField.getValue(OSQLFilterItemField.java:138)      at com.orientechnologies.orient.core.sql.filter.OSQLFilterCondition.evaluate(OSQLFilterCondition.java:379)      at com.orientechnologies.orient.core.sql.filter.OSQLFilterCondition.evaluate(OSQLFilterCondition.java:88)      at com.orientechnologies.orient.core.sql.filter.OSQLFilterCondition.evaluate(OSQLFilterCondition.java:384)      at com.orientechnologies.orient.core.sql.filter.OSQLFilterCondition.evaluate(OSQLFilterCondition.java:88)      at com.orientechnologies.orient.core.sql.filter.OSQLFilter.evaluate(OSQLFilter.java:105)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLResultsetAbstract.evaluateRecord(OCommandExecutorSQLResultsetAbstract.java:414)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLResultsetAbstract.filter(OCommandExecutorSQLResultsetAbstract.java:404)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearchRecord(OCommandExecutorSQLSelect.java:609)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.serialIterator(OCommandExecutorSQLSelect.java:1638)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.fetchFromTarget(OCommandExecutorSQLSelect.java:1585)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearch(OCommandExecutorSQLSelect.java:522)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.execute(OCommandExecutorSQLSelect.java:485)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelegate.execute(OCommandExecutorSQLDelegate.java:70)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:3400)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.command(OAbstractPaginatedStorage.java:3318)      at com.orientechnologies.orient.core.command.OCommandRequestTextAbstract.execute(OCommandRequestTextAbstract.java:69)      at org.sonatype.nexus.repository.storage.MetadataNodeEntityAdapter.browseByQuery(MetadataNodeEntityAdapter.java:202)      at org.sonatype.nexus.repository.storage.MetadataNodeEntityAdapter.browseByQuery(MetadataNodeEntityAdapter.java:170)      at org.sonatype.nexus.repository.storage.StorageTxImpl.findAssets(StorageTxImpl.java:371)      at org.sonatype.nexus.repository.storage.StorageTxImpl.findAssets(StorageTxImpl.java:377)      at sun.reflect.GeneratedMethodAccessor268.invoke(Unknown Source)      at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)      at java.lang.reflect.Method.invoke(Unknown Source)      at org.sonatype.nexus.common.stateguard.SimpleMethodInvocation.proceed(SimpleMethodInvocation.java:53)      at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)      at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:270)      at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)      at org.sonatype.nexus.common.stateguard.StateGuardAspect$1.invoke(StateGuardAspect.java:63)      at com.sun.proxy.$Proxy225.findAssets(Unknown Source)      at com.sonatype.nexus.repository.nuget.internal.NugetLocalGalleryFacetSupport.findAssetsForQuery(NugetLocalGalleryFacetSupport.java:244)      at com.sonatype.nexus.repository.nuget.internal.NugetLocalGalleryFacetSupport.queryFeed(NugetLocalGalleryFacetSupport.java:211)      at com.sonatype.nexus.repository.nuget.internal.NugetGroupGalleryFacet$$EnhancerByGuice$$e42c13d9.CGLIB$queryFeed$27(<generated>)      at com.sonatype.nexus.repository.nuget.internal.NugetGroupGalleryFacet$$EnhancerByGuice$$e42c13d9$$FastClassByGuice$$8717b62f.invoke(<generated>)      at com.google.inject.internal.cglib.proxy.$MethodProxy.invokeSuper(MethodProxy.java:228)      at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:76)      at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)      at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:54)      at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)      at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)      at com.sonatype.nexus.repository.nuget.internal.NugetGroupGalleryFacet$$EnhancerByGuice$$e42c13d9.queryFeed(<generated>)      at com.sonatype.nexus.repository.nuget.internal.NugetRemoteGalleryFacetSupport.feed(NugetRemoteGalleryFacetSupport.java:211)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedHandler.feed(NugetFeedHandler.java:69)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedHandler.doHandle(NugetFeedHandler.java:49)      at com.sonatype.nexus.repository.nuget.internal.AbstractNugetHandler.handle(AbstractNugetHandler.java:46)      at com.sonatype.nexus.repository.nuget.internal.NugetFeedHandler.handle(NugetFeedHandler.java:1)      {noformat:title=Acquiring exclusive lock}  qtp993807585-20180 <command>sql.select from component where (ci_name = :p0 AND version = :p1) and (bucket=#13:25)</command> id=20180 state=WAITING      - waiting on <0x7aac32e3> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)      - locked <0x7aac32e3> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)      at sun.misc.Unsafe.park(Native Method)      at java.util.concurrent.locks.LockSupport.park(Unknown Source)      at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(Unknown Source)      at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(Unknown Source)      at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(Unknown Source)      at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(Unknown Source)      at com.orientechnologies.common.concur.lock.OPartitionedLockManager.acquireExclusiveLock(OPartitionedLockManager.java:213)      at com.orientechnologies.common.concur.lock.OPartitionedLockManager.acquireExclusiveLocksInBatch(OPartitionedLockManager.java:266)      at com.orientechnologies.orient.core.storage.cache.local.twoq.O2QCache.doLoad(O2QCache.java:355)      at com.orientechnologies.orient.core.storage.cache.local.twoq.O2QCache.load(O2QCache.java:294)      at com.orientechnologies.orient.core.storage.impl.local.paginated.base.ODurableComponent.loadPage(ODurableComponent.java:145)      at com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.readRecordBuffer(OPaginatedCluster.java:766)      at com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.readRecord(OPaginatedCluster.java:742)      at com.orientechnologies.orient.core.storage.impl.local.paginated.OPaginatedCluster.readRecord(OPaginatedCluster.java:721)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.doReadRecord(OAbstractPaginatedStorage.java:4220)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.readRecord(OAbstractPaginatedStorage.java:3807)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.readRecord(OAbstractPaginatedStorage.java:1410)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx$SimpleRecordReader.readRecord(ODatabaseDocumentTx.java:3395)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.executeReadRecord(ODatabaseDocumentTx.java:2008)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.load(ODatabaseDocumentTx.java:656)      at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.load(ODatabaseDocumentTx.java:103)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearchRecord(OCommandExecutorSQLSelect.java:585)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.serialIterator(OCommandExecutorSQLSelect.java:1638)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.fetchFromTarget(OCommandExecutorSQLSelect.java:1585)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.fetchValuesFromIndexCursor(OCommandExecutorSQLSelect.java:2466)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.searchForIndexes(OCommandExecutorSQLSelect.java:2280)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.searchInClasses(OCommandExecutorSQLSelect.java:1017)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLResultsetAbstract.assignTarget(OCommandExecutorSQLResultsetAbstract.java:203)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.assignTarget(OCommandExecutorSQLSelect.java:527)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.executeSearch(OCommandExecutorSQLSelect.java:509)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLSelect.execute(OCommandExecutorSQLSelect.java:485)      at com.orientechnologies.orient.core.sql.OCommandExecutorSQLDelegate.execute(OCommandExecutorSQLDelegate.java:70)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.executeCommand(OAbstractPaginatedStorage.java:3400)      at com.orientechnologies.orient.core.storage.impl.local.OAbstractPaginatedStorage.command(OAbstractPaginatedStorage.java:3318)      at com.orientechnologies.orient.core.command.OCommandRequestTextAbstract.execute(OCommandRequestTextAbstract.java:69)      at org.sonatype.nexus.repository.storage.MetadataNodeEntityAdapter.browseByQuery(MetadataNodeEntityAdapter.java:202)      at org.sonatype.nexus.repository.storage.MetadataNodeEntityAdapter.browseByQuery(MetadataNodeEntityAdapter.java:170)      at org.sonatype.nexus.repository.storage.StorageTxImpl.findComponents(StorageTxImpl.java:449)      at org.sonatype.nexus.repository.storage.StorageTxImpl.findComponents(StorageTxImpl.java:455)    --disable-parallel    -DisableParallelProcessing  {noformat}    Reference: https://docs.microsoft.com/en-us/nuget/tools/cli-ref-restore    Related: https://github.com/NuGet/Home/issues/1556  https://docs.microsoft.com/en-us/nuget/tools/cli-ref-restore#options  https://docs.microsoft.com/en-us/nuget/tools/cli-ref-install#options    Also only configure your builds to access a single nuget group per Nexus instance used.  """
"NEXUS-18100","Bug","LDAP|UI",2,"LDAP UI does not reflect configuration","""The option 'Use certificates stored in the Nexus truststore to connect to external systems' does not get reflected correctly in the UI.    Steps to reproduce:  1. Create a new LDAP connection and check the box to 'Use certificates stored in the Nexus truststore to connect to external systems'  2. Save the connection  3. Refresh the page  4. View that the box is no longer checked but the data in orientdb is correct.    The side effect of this is if you try to verify the connection it uses the state from the UI to test the connection and not the state on the backend so it will fail for SSL connections."""
"NEXUS-18117","Bug","PyPI",0,"PyPI ignoring python_requires metadata","""The full details are discussed in this github issue: [https://github.com/Azure/azure-sdk-for-python/issues/3447].    PyPI uses `python_requires` metadata to communicate that a python package is only required for a particular version of python. It appears that nexus is not acting on this metadata so when I do a pip install packages are being pulled which shouldn't be which prevents the install from completing.    As detailed in the github issue there's a relatively simple workaround for my current problem. However I'm unsure these workarounds will work with more complex dependencies."""
"NEXUS-18134","Bug","Blobstore|Database|Scripting",1," The blobStore.getBlobStoreManager().delete() API allows for deleting blob stores that are in use","""Reproduce steps:   # Fire up Nexus 3.13.0 with default configuration.   # Go to system/tasks, and create a new execute script task.   # For the script text use: blobStore.getBlobStoreManager().delete('default')   # Run the script    Observe that the default blob store is deleted, even though it is in use by many repositories.    *Expected*:  The API should make the same validation checks the UI does, and prevent deleting blob stores that are in use by repositories."""
"NEXUS-18132","Bug","LDAP",1,"specially crafted ldap queries can blacklist LDAP servers on javax.naming.directory.InvalidSearchFilterException","""It is possible that a specially crafted LDAP user lookup can invoke the built in blacklisting of configured LDAP servers in Nexus 3, triggering an outage period equivalent to the retry delay configured for the LDAP server. When the server is in this temporary blacklisted state, all LDAP queries are bypassed for that server configuration, thereby affecting LDAP authentication and authorization requests for that server.    h4.     h4. Expected    A configured LDAP server should not be blacklisted due to a scenario where no LDAP connection attempt was made, as is the case when client side LDAP query validation determines that the attempted query is syntactically invalid and an *javax.naming.directory.InvalidSearchFilterException* is thrown.           """
"NEXUS-18187","Bug","PyPI",5,"Pypi proxy of https://bloomberg.bintray.com/pip does not work","""    Create a pypi proxy of [https://bloomberg.bintray.com/pip] in Nexus Repo 3.13.0, and try to download the """"blpapi"""" package from it using pip.    This will fail. The nexus.log shows:  {quote}2018-10-11 14:02:55,123-0300 WARN [qtp370829994-301] *UNKNOWN org.sonatype.nexus.repository.pypi.internal.PyPiIndexUtils - Found unexpected PyPI link format, skipping: blpapi-3.9.2-cp27-cp27m-win32.whl   2018-10-11 14:02:55,124-0300 WARN [qtp370829994-301] *UNKNOWN org.sonatype.nexus.repository.pypi.internal.PyPiIndexUtils - Found unexpected PyPI link format, skipping: blpapi-3.9.2-cp27-cp27m-win_amd64.whl   2018-10-11 14:02:55,124-0300 WARN [qtp370829994-301] *UNKNOWN org.sonatype.nexus.repository.pypi.internal.PyPiIndexUtils - Found unexpected PyPI link format, skipping: blpapi-3.9.2-cp35-cp35m-win32.whl  {quote}       The issue can be seen in the HTML source of [https://bloomberg.bintray.com/pip/simple/blpapi:]    Note that the packages are not under a """"/packages"""" directory. Our code is explicitly looking for that, and will not download anything that is not in a """"/packages"""" location:    [https://github.com/sonatype/nexus-public/blob/release-3.13.0-01/plugins/nexus-repository-pypi/src/main/java/org/sonatype/nexus/repository/pypi/internal/PyPiIndexUtils.java#L130]    *Expected*: A Nexus Repo 3 pypi proxy should be able to proxy anything pip can download."""
"NEXUS-18196","Bug","Maven",3,"ArrayIndexOutOfBoundsException when uploading large POM","""Deploy fails when parsing (large) pom due to ArrayIndexOutOfBoundsException:  {quote}org.sonatype.nexus.repository.httpbridge.internal.ViewServlet - Failure servicing: PUT /repository/maven-releases/x123/x123/0.0.1/x123-0.0.1.pom    java.lang.ArrayIndexOutOfBoundsException: 8213    at org.codehaus.plexus.util.xml.pull.MXParser.parsePI(MXParser.java:2502)    at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl(MXParser.java:1283)    at org.codehaus.plexus.util.xml.pull.MXParser.next(MXParser.java:1131)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.nextTag(MavenXpp3Reader.java:528)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.parsePluginExecution(MavenXpp3Reader.java:2888)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.parsePlugin(MavenXpp3Reader.java:2701)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.parseBuildBase(MavenXpp3Reader.java:1069)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.parseProfile(MavenXpp3Reader.java:3069)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.parseModel(MavenXpp3Reader.java:2257)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read(MavenXpp3Reader.java:3807)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read(MavenXpp3Reader.java:557)    at org.apache.maven.model.io.xpp3.MavenXpp3Reader.read(MavenXpp3Reader.java:586)    at org.sonatype.nexus.repository.maven.internal.MavenModels.readModel(MavenModels.java:133)    at org.sonatype.nexus.repository.maven.internal.MavenFacetImpl.fillInFromModel(MavenFacetImpl.java:376)    at org.sonatype.nexus.repository.maven.internal.MavenFacetImpl.putArtifact(MavenFacetImpl.java:327)    at org.sonatype.nexus.repository.maven.internal.MavenFacetImpl.doPutAssetBlob(MavenFacetImpl.java:296)    at org.sonatype.nexus.repository.maven.internal.MavenFacetImpl.doPut(MavenFacetImpl.java:249)    at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)    at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:54)    at org.sonatype.nexus.repository.maven.internal.MavenFacetImpl.put(MavenFacetImpl.java:201)  {quote}  To reproduce deploy the attached pom file to a maven repo on a 3.13 instance:    {{mvn deploy:deploy-file -DgeneratePom=false -DrepositoryId=nexus -Durl=[http://localhost:8081/repository/maven-releases] -DpomFile=pom.xml -Dfile=pom.xml}}    +Acceptance+     Looks to be related/caused by [https://issues.apache.org/jira/browse/MNG-6216] and [https://github.com/codehaus-plexus/plexus-utils/issues/22] and an update is needed to plexus-util 3.1.0"""
"NEXUS-18227","Bug","Yum",2,"RPMs with non-zero epoch are incorrectly recorded in primary.xml in hosted repositories","""RPMs with non-zero epoch have missing epoch attribute in <rpm:provides>  element of the primary.xml database. This causes yum to fail during depsolve when such packages are being used.     Specific case: I have a subset of packages from official centos7 repositories that I keep in a hosted yum repository. The failing package is dhclient (which uses epoch value """"12""""), resolved as a dependency of NetworkManager.    Comparsion of primary.xml from nexus with the one created by createrepo shows the differences. Both files are attached."""
"NEXUS-18234","Bug","HA",3,"provide a health endpoint for an individual node in an HA-C cluster","""HA-C consists of nodes, each node being an Nexus instance.    It is common practice to put a load balancer in front of an HA-C cluster. It is anticipated one node may be deliberately brought offline while the others remain functioning.    A load balancer needs to reliably determine the health of an individual node in order to reroute requests to available nodes.    Our [documentation|https://help.sonatype.com/repomanager3/high-availability/operating-your-cluster#Operatingyourcluster-MonitoringNodeHealth] currently suggests using:    http://<serveripaddress>:<port>/service/metrics/data    but that endpoint    - requires authentication, unless the anonymous user is granted access  - returns a lot of data irrelevant to server health and not appropriate for an anonymous user    h4. Expected    Each node in an HA-C cluster should expose an endpoint that can be used by a load balancer to determine the """"health"""" of the node with regards to its ability to participate in spreading the work of incoming cluster load.    As a guideline, the endpoint should meet the requirements for """"health checks"""" as defined by common load balancers such as ELB and nginx:    https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-healthchecks.html  https://docs.nginx.com/nginx/admin-guide/load-balancer/http-health-check/  https://httpd.apache.org/docs/2.4/mod/mod_proxy_hcheck.html    Example  - anything other than a HTTP 200 status code indicates the node is not ready to do work  - require no authentication by default, but MAY have a privilege specific to the endpoint  - the endpoint MAY provide a response body that provides information about health, but in order to determine health, it MUST NOT be required that the client actually parse this response body  - HTTP should be the primary protocol, on the main HTTP(S) connector of the Nexus instance, used to check node health, since this greatly simplifies navigating firewalls  - if the cluster/node is read-only, document if this affects the status code as far as a load balancer is concerned"""
"NEXUS-18262","Bug","Jenkins Plugin",3,"Nexus Platform Plugin repository manager compatibility check does not use Jenkins configured HTTP proxy server","""Configure an HTTP proxy server in Jenkins  Try to add a new Nexus 3 connection inside Jenkins. After you enter the Server URL and tab or click away, an HTTP request is sent to /service/rest/wonderland/status  However the request is NOT sent through the configured Jenkins HTTP proxy server.    h4. Expected    The Nexus Platform Plugin should respect the HTTP proxy settings of the Jenkins server in all outbound HTTP requests it makes.    This could be caused indirectly by it using URLConnection instead of the built in HTTPClient library.    Also relevant is the value in the check itself: NEXUS-18260"""
"NEXUS-18261","Bug","Yum",3,"Hosted yum metadata not rebuilding due to parsing issues in the path being queried","""The metadata for a hosted yum repo is not rebuilding. Errors are recorded in the Nexus log which indicate a problem parsing a Windows directory name within the SQL query:               """
"NEXUS-18260","Bug","Jenkins Plugin",3,"Nexus Platform Plugin cannot determine Nexus 3 status when Anonymous access is disabled","""# Start 3.13.0   # *Disable Anonymous access by unchecking the Enabled checkbox*   # Using Jenkins 2.121.1 + Nexus Platform Plugin 3.3.2, try to configure the nexus 3 connection.   ## Enter the Server name   ## Enter the Server ID   ## Enter the server URL ( [http://localhost:8081|http://localhost:8081/] ) and tab away from or click away from the Server URL field.    The platform plugin sends a request to Nexus without credentials and gets a 403 response from Nexus.    As a result, The jenkins plugin displays a yellow warning in the UI.    Now choose valid credentials in order to correct the problem:    1. Choose valid credentials in the credential dropdown or create these on the fly.   2. Choose Test Connection. The Connection should work. *Meanwhile the yellow warning text remains.*   3. Click inside the Server URL field. Change the value to [http://localhost:123|http://localhost:123/] ( invalid) . Tab or click away. Yellow message remains.   4. Correct the server URL back to [http://localhost:8081|http://localhost:8081/]. Tab or click away. Nexus receives another request like this which does not use you already verified valid credentials.    h4. Expected    One of the """"Notes"""" in -NEXUS-17303- was """"version info should be available without auth"""". When Nexus has anonymous disabled ( reproduce step 2 ) this requirement is not satisfied.    Also see NEXUS-18274 for interaction redesign proposals."""
"NEXUS-18254","Story","Blobstore",2,"Allow blobstore configuration updates when changes are necessary","""Currently, blobstore configuration is immutable. However, there are situations where the blobstore configuration needs to be changed. For instance, if the NXRM admin configured an S3 blobstore with a set of access keys that have been compromised. In this case, the admin would need to update the access keys in the blobstore configuration.     As an NXRM admin, I need to be able to make changes to the blobstore configuration so that I can correct potential configuration errors so that I can bring a blobstore back online.     *Acceptance Criteria*  * if a blobstore is offline, the configuration of the blobstore can be updated  * Quota config should only be editable when the user selects editing mode per NEXUS-18317"""
"NEXUS-18252","Bug","Blobstore",2,"repository manager will not start on blobstore problems or errors","""As an admin, I need for NXRM to launch even in the event that a blobstore is unavailable.     Currently, if you are using an S3 blobstore and AWS is unavailable or your access keys have expired, then NXRM will not launch. S3 is not the only potential problem, if a blobstore is configured to point to an NFS mount point that is not mounted, then NXRM may fail to launch.     *Acceptance Criteria*  * NXRM launches even if blobstores are not available"""
"NEXUS-18263","Bug","Docker",5,"docker proxy repositories configured with a remote URL including extra path info will not proxy correctly","""h4. Problem    Docker proxy repositories in Nexus 3.14.0 do not successfully proxy content in a docker hosted repository in Nexus 3.14.0 when the remote URL of the proxy repository includes extra path info.    Example:    Docker Proxy Remote URL: http://192.168.2.73:8081/repository/docker-hosted    {noformat:title=Logging from Nexus 3.13.0 which shows a successful proxy of docker content}  2018-10-24 13:14:00,822-0300 INFO  [qtp2079105816-225] admin org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl - Invalidating proxy caches of docker-proxy  2018-10-24 13:14:07,622-0300 DEBUG [qtp2079105816-186] admin org.sonatype.nexus.httpclient.outbound - http://192.168.2.73:8081/repository/docker-hosted/v2/hello-world/manifests/latest > GET /repository/docker-hosted/v2/hello-world/manifests/latest HTTP/1.1  2018-10-24 13:14:07,630-0300 DEBUG [qtp2079105816-186] admin org.sonatype.nexus.httpclient.outbound - http://192.168.2.73:8081/repository/docker-hosted/v2/hello-world/manifests/latest < HTTP/1.1 200 OK @ 7.711 ms  2018-10-24 13:14:07,630-0300 INFO  [qtp2079105816-186] admin org.sonatype.nexus.repository.httpclient.internal.HttpClientFacetImpl - Repository status for docker-proxy changed from READY to AVAILABLE - reason n/a for n/a  2018-10-24 13:14:07,682-0300 INFO  [elasticsearch[3A375783-7FDD90D8-4B372B08-E9E9D13D-0E5F7F54][clusterService#updateTask][T#1]] *SYSTEM org.elasticsearch.cluster.metadata - [3A375783-7FDD90D8-4B372B08-E9E9D13D-0E5F7F54] [2e558aa1029f36cea9ed93e744bf70bea008fe46] update_mapping [component]  2018-10-24 13:14:17,489-0300 DEBUG [qtp2079105816-61] admin org.sonatype.nexus.httpclient.outbound - http://192.168.2.73:8081/repository/docker-hosted/v2/hello-world/manifests/8.11.2-slim > GET /repository/docker-hosted/v2/hello-world/manifests/8.11.2-slim HTTP/1.1  2018-10-24 13:14:17,497-0300 DEBUG [qtp2079105816-61] admin org.sonatype.nexus.httpclient.outbound - http://192.168.2.73:8081/repository/docker-hosted/v2/hello-world/manifests/8.11.2-slim < HTTP/1.1 200 OK @ 8.334 ms      h4. Diagnosis    The issue seems to be the extra path info of {{/repository/docker-hosted/}} in the docker proxy repository URL is stripped off before the outbound request for the layer manifest is retrieved. This means it receives HTML content from a 400 error response instead of the actual docker content.    h4. Workaround    If possible, change the Proxy repository Remote URL to not use extra path info. ie.    1. Setup the remote repository to listen on a specific connector port so that it can be accessed at a root path context  2. Change the proxy repository URL to that remote specific port and remove the extra path info.    When implementing the workaround, be cautious of the potential performance penalty of extra HTTP connectors in the remote Nexus 3 as described in NEXUS-16565."""
"NEXUS-18285","Bug","Jenkins Plugin",2,"Nexus Platform Plugin for Jenkins fails to send any authorization headers over HTTPS to Nexus Repository Manager","""Configure Nexus Platform Plugin for Jenkins failing to connect and perform operations against a Nexus Repository manager version 3.x server at a URL using HTTPS instead of plain HTTP. Operations will *fail to send* the configured credentials, even though the credentials are valid.    This affects all versions of the Jenkins Platform Plugin from 3.2.20180724-142843.2f5144d to 3.3.20181025-134249.614c5f4 ( see https://wiki.jenkins.io/display/JENKINS/Nexus+Platform+Plugin ).  """
"NEXUS-18304","Bug","Docker",3,"Docker content validation against certain Etag header value formats may fail","""The content validation added via ----NEXUS-16242---- uses the ETag header.    Dockerhub always returns ETag in the """"algorithm:checksum"""" format.  But in general the format of the Etag header cannot be relied on, it's just a generic identifier.  This causes our validation to break when the ETag is in a format which is not compatible with Docker digests:    Example received header:  {quote}2018-11-01 18:24:37,850+0000 DEBUG [qtp639366847-55651]  example org.apache.http.headers - http-outgoing-42 << _ETag: b558e108382f75d3cb440cec3bdaca8fa4bbfe30_  {quote}  The exception caused by this in a Docker proxy repository:  {quote}2018-11-01 18:24:37,861+0000 WARN [qtp639366847-55651] example org.sonatype.nexus.repository.docker.internal.V2Handlers - Error: GET /v2/build/asap/manifests/latest   java.lang.IllegalArgumentException: Digest must be formed as 'alg:hex': b558e108382f75d3cb440cec3bdaca8fa4bbfe30   at com.google.common.base.Preconditions.checkArgument(Preconditions.java:210)   at org.sonatype.nexus.repository.docker.internal.DockerDigest.parse(DockerDigest.java:71)   at org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl.fetchTagDigestByContentDigest(DockerProxyFacetImpl.java:916)  {quote}  FWIW, it seems the Docker-Content-Digest's format can be relied on:    [https://docs.docker.com/registry/spec/api/#content-digests]  {quote}2018-11-01 18:24:37,851+0000 DEBUG [qtp639366847-55651] example org.apache.http.headers - http-outgoing-42 << Docker-Content-Digest: sha256:9030d11b2bf8181a39b010c27359a7557c753c7f5272211ca2c12ec02656a25b  {quote}  *Note that this problem causes a Docker proxy which has Artifactory as its remote to fail*, since Artifactory does not use the """"algorithm:checksum"""" format in its ETag headers."""
"NEXUS-18337","Bug","Logging|Support Tools",2,"Support zip generation fails if nexus.log file is not called ""nexus.log""","""If the name of the nexus.log file is changed in the """"logback.xml"""" file:         Then this will prevent generation of a support zip file:  {quote}2018-11-06 17:08:59,672+0000 WARN [qtp1032748576-49] admin org.sonatype.nexus.internal.atlas.customizers.InstallConfigurationCustomizer - Skipping: /var/lib/nexus3/etc/fabric/hazelcast-network.xml   2018-11-06 17:09:00,926+0000 WARN [qtp1032748576-49] admin org.sonatype.nexus.internal.log.LogbackLogManager - Log file does not exist: nexus.log   2018-11-06 17:09:00,936+0000 ERROR [qtp1032748576-49] admin org.sonatype.nexus.internal.atlas.SupportZipGeneratorImpl - Failed to create support ZIP   java.lang.NullPointerException: Cannot invoke method withStream() on null object   at org.codehaus.groovy.runtime.NullObject.invokeMethod(NullObject.java:91)   at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:47)   at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)   at org.codehaus.groovy.runtime.callsite.NullCallSite.call(NullCallSite.java:34)   at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:128)   at org.sonatype.nexus.internal.atlas.customizers.LogCustomizer$1.generate(LogCustomizer.groovy:63)   at org.sonatype.nexus.supportzip.GeneratedContentSourceSupport.prepare(GeneratedContentSourceSupport.java:51)   at org.sonatype.nexus.supportzip.SupportBundle$ContentSource$prepare.call(Unknown Source)   at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)   at org.sonatype.nexus.supportzip.SupportBundle$ContentSource$prepare.call(Unknown Source)   at org.sonatype.nexus.internal.atlas.SupportZipGeneratorImpl$_generate_closure3.doCall(SupportZipGeneratorImpl.groovy:181)   at sun.reflect.GeneratedMethodAccessor167.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:98)  {quote}   *Expected*:  The support zip code should take the name of the log file from the logback configuration, and use that when attempting to open the file."""
"NEXUS-18344","Improvement","REST",1,"REST API list repositories to provide the remote url.","""Improvement request for REST API of listing repositories to show the remote URL."""
"NEXUS-18493","Bug","Logging|Support Tools",1,"Support zip generation fails without a blobstore","""Nexus admins cannot create a support zip if there is no blobstore present. Per the nexus.log:    2018-11-12 16:10:10,285+0000 INFO [qtp1301159914-189] admin com.sonatype.nexus.hazelcast.internal.BlobStoreDownloadServiceImpl - Move: support-20181112-161009-3.zip  2018-11-12 16:10:10,286+0000 ERROR [qtp1301159914-189] admin org.sonatype.nexus.extdirect.internal.ExtDirectServlet - Failed to invoke action method: atlas_SupportZip.create, java-method: org.sonatype.nexus.coreui.internal.atlas.SupportZipComponent.create  java.util.NoSuchElementException: null   at com.google.common.collect.AbstractIndexedListIterator.next(AbstractIndexedListIterator.java:75)   at com.sonatype.nexus.hazelcast.internal.BlobStoreDownloadServiceImpl.move(BlobStoreDownloadServiceImpl.java:108)*   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoCachedMethodSiteNoUnwrap.invoke(PojoMetaMethodSite.java:213)   at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:56)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:136)   at org.sonatype.nexus.internal.atlas.SupportZipGeneratorImpl.generate(SupportZipGeneratorImpl.groovy:151)   at org.sonatype.nexus.supportzip.SupportZipGenerator$generate.call(Unknown Source)   at org.sonatype.nexus.coreui.internal.atlas.SupportZipComponent.create(SupportZipComponent.groovy:57)    New Nexus HA-C environments don't have blobstores, which means we can't get support zips to troubleshoot issues encountered early on."""
"NEXUS-18487","Bug","Scripting|Security",1,"Scripting API allows invalid Roles","""The addRole() method in security scripting API allows for creation of a role that contains itself.  This is not valid, and causes stack overflows in Nexus when the role is traversed.         The API should validate that no cycles are created in roles, and ensure that any other required validation is applied regardless of whether the input comes from REST/UI/etc.     """
"NEXUS-18520","Bug","Search|Tags",2,"repository.rebuild-index task TagsComponentEntityAdapterExtension.readFields NullPointerException","""A maven2-hosted repository with the following config:        Encountered a problem when the RebuildIndexTask was run against it:        h4. Expected    NullPointerException should not happen. Code should be defensive."""
"NEXUS-18529","Bug","HA|Support Tools",2,"support zips are deleted from the downloads directory in HA environment","""NEXUS-15163 implemented in 3.13.0 moves support zips into the default blobstore and then DELETES them from the $data-dir/downloads directory after they are moved.    This is not desired. Support zips are to be treated contextually like log files, specific to a node, and should not need to be moved into a shared blobstore to be accessible by all nodes.    h4. Expected    Do not delete a support zip from the original generated downloads directory on the node it was created.    There is also no customer use case to move support zips into the shared blob store. It appears the original reason for reporting NEXUS-15163 was because HTTP sessions were not sticky."""
"NEXUS-18540","Bug","HA|Repository Health Check",1,"assetdownloadcount.internal.CacheRemovalListener.accept NullPointerException","""A 3 node HA cluster started recording running 3.14.0 started capturing 2366 NullPointerException across nodes with the following log message.    very oddly, one node logs these as WARN messages, while another logs these as ERROR - even though both are on the same version of Nexus?        h4. Expected    NullPointerExceptions should not be triggered during normal operation. Determine cause and mitigate.  Explain why one is WARN and one is ERROR."""
"NEXUS-18562","Technical Debt","REST",1,"Drop beta rest endpoints.","""Drop deprecated beta enpoints."""
"NEXUS-18631","Bug","Blobstore|S3",2,"allow multipart copy for AWS S3 blob storage","""Hey there,    Can multipart upload for aws s3 blob storage be enabled through some setting?    I get the following in the nexus logs, when trying to upload artifacts larger than 5GB:  {quote}com.amazonaws.services.s3.model.AmazonS3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: Amazon S3; Status Code: 400; Error Code: InvalidRequest;  {quote}  Thanks in advance"""
"NEXUS-18638","Bug","Docker",1,"GET requests against Docker group repository returning duplicate headers","""While testing NEXUS-12684, it was discovered that issuing a GET request for a manifest in a docker group repository returned duplicate headers for Docker-Distribution-Api-Version, ETag, and Last-Modified.    Example request:     Example Response Headers:  """
"NEXUS-18675","Bug","Logging|S3",1,"log blob deleted reason for s3 based deleted blobs","""Note the file-based blobstore explicitly logs the deleted reason when it unexpectedly encounters a soft-deleted blob:    [https://github.com/sonatype/nexus-public/blob/release-3.14.0-04/components/nexus-blobstore-file/src/main/java/org/sonatype/nexus/blobstore/file/FileBlobStore.java#L437]    Whereas the S3 blobstore logs the {{S3BlobAttributes}}:    [https://github.com/sonatype/nexus-public/blob/release-3.14.0-04/plugins/nexus-blobstore-s3/src/main/java/org/sonatype/nexus/blobstore/s3/internal/S3BlobStore.java#L321]    Unfortunately that delegates to {{S3PropertiesFile.toString()}} which just dumps the bucket and key, not the actual attributes.    *Acceptance*    The path or bucket-key pair and the attributes for an accessed soft-deleted blob shall be logged in the warning msg.           """
"NEXUS-18712","Bug","Configuration",2,"jetty-http-redirect-to-https.xml overrides the default Jetty security filter allowing TRACE requests","""By default Nexus does not allow HTTP TRACE requests.    However, if you add the """"jetty-http-redirect-to-https.xml"""" file to the nexus-args in nexus.properties TRACE requests will succeed. I think this is because that configuration file is setting a new securityHandler without calling the existing one."""
"NEXUS-18711","Story","HA|Support Tools",5,"Collect all three support zips from a single node","""*Background*    Currently, when HA-C customers generate a support zip, they almost always fetch a single support zip. Also, in some cases the admin trying to obtain the support zip doesn't have the ability to reach specific nodes to fetch all three manually.    *Acceptance*   * There a method in the UI for users to generate and fetch support zips for their entire cluster.   * This should not fail if some nodes are down! Best effort is necessary. (Inability to get a support zip from the one working node is worse than not getting all three.)"""
"NEXUS-18715","Bug","Security",1,"/atlas/security-diagnostic api returns 500 halfway through response.","""Hello,    We've been using the /atlas/security diagnostics API in Nexus OSS 3.3.1-01. However, after an upgrade to 3.11.0-01 (and we tried 3.14.0-04 as well) we faced issues with the API. It had been moved, and it wasn't working correctly after the move.    We have created a fake user to test if our ldap configuration is working correctly. When trying to request the details of this user, the API crashes when trying to expand the users permissions.    Like this:    [root@someserver /]# curl -k --silent --write-out HTTPSTATUS:%\{http_code} -u admin:admin123 https://nexus.url/nexus/service/rest/atlas/security-diagnostic/user/fakeusr    """"user"""" : \{      """"userId"""" : """"fakeusr"""",      """"status"""" : """"active"""",      """"firstName"""" : """"fake user"""",      """"version"""" : """"1"""",      """"lastName"""" : """"User"""",      """"emailAddress"""" : """"dummy@nomail.com"""",      """"readOnly"""" : false,      """"source"""" : """"default"""",      """"name"""" : """"fake user"""",      """"roles"""" : \{        """"nx-admin"""" : \{          """"version"""" : null,          """"description"""" : """"Administrator Role"""",          """"readOnly"""" : true,          """"source"""" : """"default"""",          """"name"""" : null,          """"privileges"""" : \{            """"nx-all"""" : \{              """"type"""" : """"wildcard"""",              """"properties"""" : \{                """"pattern"""" : """"nexus:*""""              },              """"version"""" : null,              """"description"""" : """"All permissions"""",              """"permission""""            }          }        }      }    }  }ERROR: (ID 1d7ee5f4-3c65-4dad-8a3a-61d812aab041) com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class org.sonatype.nexus.security.authz.WildcardPermission2 and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: java.util.LinkedHashMap[""""user""""]->java.util.LinkedHashMap[""""roles""""]->java.util.LinkedHashMap[""""nx-admin""""]->java.util.LinkedHashMap[""""privileges""""]->java.util.LinkedHashMap[""""nx-all""""]->java.util.LinkedHashMap[""""permission""""])  HTTPSTATUS:500"""
"NEXUS-18751","Bug","Cleanup",2,"need cleanup policy permissions to view UI repository list","""I am using OSS nexus-3.14.0-04 version where I have created a user and gave admin privileges, as in the screen shot, to create repositories alone. When I logged in as that user and whenever I navigate to admin repositories page, Danger alert box with 'User is not permitted: nexus:repository-admin' message is getting displayed. If I give full admin privileges like nx-repository-admin-*-*-*, then the message is not appearing but for my use I cannot give full admin privileges.    What should I do to get rid of the alert message or is it a bug in nexus-3.14?    Thanks in advance.    h4. Cause    Related to cleanup policies.        h4. Expected     Should not need cleanup policy permissions to view the repository list."""
"NEXUS-18758","Story","Metrics|UI",3,"System Status check failures are prominent in the UI","""*Background*    We have a small-but-growing number of automated system status checks, but they are not prominent in the UI. Some reflect critical problems (e.g. NEXUS-18706), but there's no warning whatsoever unless an admin generates a support zip or digs into the UI.    *Acceptance*   * When a health check is failing, raise a visible alarm in the admin UI   * We don't want failing system status check alerts visible to non-admin users.    *Note*    This will trigger the need to schedule system status checks being run on a periodic basis, since they are currently run on demand."""
"NEXUS-18776","Story","HA|Metrics",1,"System check for node count","""*Acceptance*   * Add a health check that fails if a cluster doesn't have three nodes"""
"NEXUS-18774","Bug","NPM",2,"allow scoped NPM package name parts that start with '.' or '_'","""Currently, Nexus will not proxy NPM packages which begin with a leading '.' or '_'; attempting to pull these results in:    {{2019-01-03 11:22:25,450-0800 WARN  [qtp799270832-391]  admin org.sonatype.nexus.repository.npm.internal.NpmHandlers - Error: GET /@angular-toolkit/_utils_: Status\{successful=false, code=400, message='null'} - Name starts with '.' or '_': _}}   {{utils}}   {{java.lang.IllegalArgumentException: Name starts with '.' or '_': _utils}}   {{        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:210)}}   {{        at org.sonatype.nexus.repository.npm.internal.NpmPackageId.<init>(NpmPackageId.java:63)}}   {{        at org.sonatype.nexus.repository.npm.internal.NpmHandlers.packageId(NpmHandlers.java:83)}}   {{        at org.sonatype.nexus.repository.npm.internal.NpmProxyFacetImpl.getCachedContent(NpmProxyFacetImpl.java:100)}}   {{        at org.sonatype.nexus.repository.proxy.ProxyFacetSupport.maybeGetCachedContent(ProxyFacetSupport.java:342)}}   {{        at org.sonatype.nexus.repository.proxy.ProxyFacetSupport.get(ProxyFacetSupport.java:218)}}   {{        at org.sonatype.nexus.repository.proxy.ProxyHandler.handle(ProxyHandler.java:50)}}   {{        ...}}    This aligns with the advice provided by NPM:  [https://docs.npmjs.com/files/package.json#name]    {{""""Some rules:}}   * {{The name must be less than or equal to 214 characters. This includes the scope for scoped packages.}}   * {{_*The name can’t start with a dot or an underscore.""""*_}}    Unfortunately, packages with such names exist in the NPM registry ([http://registry.npmjs.org/@angular-toolkit/_utils|http://registry.npmjs.org/@angular-toolkit/_utils).] ).  Affected customers are therefore prevented from bringing them down through Nexus.  Since the de facto naming restrictions at the NPM registry apparently allow these characters, Nexus should follow suit.    h4. Expected    - _unscoped package names are not allowed to start with underscore or period_  - a scope is part of the complete package name, so _the non-scoped name part of a scoped package name is allowed to start with a period or a dot_  - current package name validation logic should reference [the official npm validation logic|https://github.com/npm/validate-npm-package-name/blob/9ee8d54e28204b762f11451cf01207a3dc6be679/index.js#L37-L39]"""
"NEXUS-18816","Bug","Database|REST",3,"Slow delete performance when using REST API","""Deletions of yum components are taking 60 seconds and sometimes failing with 500.    We can see database query reaching 60 second timeout.  {quote}2019-01-10 04:51:29,684+0000 WARN  [qtp1990821672-1160244]  xxxxx org.sonatype.nexus.siesta.internal.UnexpectedExceptionMapper - (ID f3abb01b-7dda-4c74-8f03-4eeb03d78911) Unexpected exception: java.lang.IllegalStateException: Timed out reading query result from queue 11e742fa after 60 seconds    java.lang.IllegalStateException: Timed out reading query result from queue 11e742fa after 60 seconds     at org.sonatype.nexus.repository.storage.OrientAsyncHelper$QueueConsumingIterable.hasNext(OrientAsyncHelper.java:201)     at java.util.Spliterators$IteratorSpliterator.tryAdvance(Spliterators.java:1811)     at java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:126)     at java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:498)     at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:485)     at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)     at java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:152)     at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)     at java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:464)     at org.sonatype.nexus.repository.browse.internal.BrowseServiceImpl.getById(BrowseServiceImpl.java:309)     at org.sonatype.nexus.repository.browse.internal.BrowseServiceImpl.getComponentById(BrowseServiceImpl.java:282)     at org.sonatype.nexus.repository.rest.internal.resources.ComponentsResource.getComponent(ComponentsResource.java:210)     at org.sonatype.nexus.repository.rest.internal.resources.ComponentsResource.deleteComponent(ComponentsResource.java:228)  {quote}       We can also see the the following thread is taking time:  {quote}*event-6-thread-2281* - priority:5 - threadId:0x00007fe298055800 - nativeId:0x7e23 - nativeId (decimal):32291 - state:*RUNNABLE*  stackTrace:  java.lang.Thread.State: RUNNABLE  at java.util.zip.Inflater.inflateBytes(Native Method)  at java.util.zip.Inflater.inflate(Inflater.java:259)  - locked *<0x0000000707ae4788>* (a java.util.zip.ZStreamRef)  at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.read(GzipCompressorInputStream.java:311)  at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)  at java.io.BufferedInputStream.read(BufferedInputStream.java:345)  - locked *<0x0000000707aec6a8>* (a java.io.BufferedInputStream)  at com.google.common.io.CountingInputStream.read(CountingInputStream.java:63)  at org.sonatype.nexus.common.hash.MultiHashingInputStream.read(MultiHashingInputStream.java:66)  at java.io.FilterInputStream.read(FilterInputStream.java:107)  at com.google.common.io.ByteStreams.copy(ByteStreams.java:109)  at org.sonatype.nexus.repository.yum.internal.utils.YumMetadataUtils.readCompressedMetadata(YumMetadataUtils.java:79)  at org.sonatype.nexus.repository.yum.internal.utils.YumMetadataUtils.readCompressedMetadata(YumMetadataUtils.java:59)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.readMetadataAndAppendToRepomd(CreateRepoServiceImpl.java:371)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.writeRepomd(CreateRepoServiceImpl.java:355)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.convertDirectoriesToMetadata(CreateRepoServiceImpl.java:184)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:150)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadata(CreateRepoServiceImpl.java:134)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl.buildMetadataUsingCaching(CreateRepoServiceImpl.java:120)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl$$EnhancerByGuice$$7fce96dd.CGLIB$buildMetadataUsingCaching$0(*<generated>*)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl$$EnhancerByGuice$$7fce96dd$$FastClassByGuice$$6d83d52f.invoke(*<generated>*)  at com.google.inject.internal.cglib.proxy.$MethodProxy.invokeSuper(MethodProxy.java:228)  at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:76)  at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)  at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:54)  at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)  at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoServiceImpl$$EnhancerByGuice$$7fce96dd.buildMetadataUsingCaching(*<generated>*)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.buildMetadata(CreateRepoFacetImpl.java:190)  at org.sonatype.nexus.repository.yum.internal.createrepo.CreateRepoFacetImpl.on(CreateRepoFacetImpl.java:175)  at sun.reflect.GeneratedMethodAccessor274.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:498)  at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)  at com.google.common.eventbus.Subscriber$SynchronizedSubscriber.invokeSubscriberMethod(Subscriber.java:144)  - locked *<0x00000006cec7b2a8>* (a com.google.common.eventbus.Subscriber$SynchronizedSubscriber)  at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)  at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)  at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)  at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)  at java.lang.Thread.run(Thread.java:748)  Locked ownable synchronizers:  - *<0x00000006ec0c4ee8>* (a java.util.concurrent.ThreadPoolExecutor$Worker)  {quote}"""
"NEXUS-18855","Bug","REST|Tags",2,"/tags/associate does not return JSON if repository cannot be found","""Try to associate a tag to a set of components in *a repository that does not exist*. The response body is not JSON, but the response status code is 404.    Try to associate a tag to a set of components in *a repository that does exist*, but the other search criteria do not match a valid component. The response body is JSON:    {noformat:title=Valid repo, but other search criteria do not find components}  {    """"status"""" : 404,    """"message"""" : """"No components found""""  }      """
"NEXUS-18905","Bug","Cleanup|Search",3,"Cleanup tasks fail with ""No search context found for id"" error","""On three occasions now we've seen the repository cleanup tasks fail with an error like the one below. This seems to occur when the elasticsearch query takes too long to complete.    We've had users work around this by reducing the amount of components selected by the cleanup policy.  But I think this points to a flaw in our code.  If the machine that Nexus Repo is running on is slow, or the cleanup policy is selecting a large number of components it should cause the cleanup to run slowly, not fail entirely.       {quote}2019-01-18 12:20:26,371-0500 WARN [quartz-2-thread-20] *SYSTEM org.sonatype.nexus.quartz.internal.task.QuartzTaskJob - Task 080a3b05-ada1-45a0-91e1-8b715fbc3dc4 : 'Cleanup service' [repository.cleanup] execution failure   org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed   at org.elasticsearch.action.search.SearchScrollQueryAndFetchAsyncAction.onPhaseFailure(SearchScrollQueryAndFetchAsyncAction.java:155)   at org.elasticsearch.action.search.SearchScrollQueryAndFetchAsyncAction.access$300(SearchScrollQueryAndFetchAsyncAction.java:41)   at org.elasticsearch.action.search.SearchScrollQueryAndFetchAsyncAction$1.onFailure(SearchScrollQueryAndFetchAsyncAction.java:142)   at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)   at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:874)   at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:852)   at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:389)   at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)   at java.lang.Thread.run(Thread.java:748)   Caused by: org.elasticsearch.transport.RemoteTransportException: [03BE0352-264E5695-7C25F189-A53EC373-372DEEDA][local[1]][indices:data/read/search[phase/query+fetch/scroll]]   Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [7]   at org.elasticsearch.search.SearchService.findContext(SearchService.java:626)   at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:553)   at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:416)   at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:413)   at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)   at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)   at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:378)   at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)   at java.lang.Thread.run(Thread.java:748)  {quote}   """
"NEXUS-18952","Bug","Transport",1,"High CPU usage observed caused by a bug in older Jetty version","""Customer complained of high CPU usage in Nexus Repo 3.14.0.  A thread dump from their instance showed the following active Jetty threads:         This is due to a bug in Jetty 9.4.11:    [https://github.com/eclipse/jetty.project/issues/2233#issuecomment-404082685]    This has been fixed in 9.4.12:    [https://github.com/eclipse/jetty.project/commit/17b6eee5aca00460913a2b7847325b6e3df39fd2]    So bumping our Jetty version should prevent future occurrences of this."""
"NEXUS-18949","Bug","Documentation|REST",3,"/service/rest/v1/status returns 200 status code when node is read-only","""http://<hostname>:<port>/service/rest/v1/status currently returns 200 status code if the specific node being accessed is in a read-only state.    The main use case of adding this endpoint was to determine the availability of a node that is in an HA-C cluster.    Given an HA-C node is not fully functional unless it is writable, there needs to be an endpoint that returns 503 service unavailable if:    - *the node is read-only*  - all other criteria of a valid HA-C node participant are satisfied    By meeting these requirements, a load balancer can more accurately represent if a node is ready to server ALL types of requests.    Note: _A side benefit_ of this endpoint may have been to see if a node is """"alive"""" at all ie. is up, but is read-only. We need to answer what endpoint should be used for this use case - should it be this endpoint or something else?            """
"NEXUS-19049","Bug","Repository",2,"Request for artifacts sometimes returns invalid Last-Modified header","""Sometimes randomly we get the wrong timezone (CET) in the `Last-Modified` header for artifacts    The header should always be in GMT according to discussion here: [https://stackoverflow.com/a/1639028/2122701] (inc links to RFCs)    Here is a bash loop on the vm hosting nexus for the same artifact. Depending on the frequency it has taken hours to show itself.         You can see generally this is the correct timezone, but sometimes oddly not. See request made at 15:19:00 GMT    I've seen this affecting raw and maven repositories    This runs into issues specifically with Akka HTTP library in our case which expects the header to be correctly formed          """
"NEXUS-19075","Bug","Analytics|Documentation",1,"Analytics documentation unchanged after feature removal","""In 3.15.0 we removed analytics from NXRM3.  However, the h.s.c documentation has no indication of this whatsoever.  See https://help.sonatype.com/display/NXRM3/Support+Features#SupportFeatures-Analytics."""
"NEXUS-19085","Bug","Staging",3,"staging promotion move of more than 500 components may fail with IllegalStateException Unable to find component by id","""A staging move operation may return a 500 status code:            Reporter that experienced the issue claims this happens when trying to move 5000 components at once. When the move attempts to move less than 500 components at one time, it succeeded.    """
"NEXUS-19084","Bug","Database|Search",3,"Many threads blocked in Elasticsearch while updating LastDownloaded attribute","""The attached thread dump shows 291 threads blocked with the stack trace below.    The method """"org.sonatype.nexus.repository.view.handlers.LastDownloadedHandler.maybeUpdateLastDownloaded"""" sometimes initiates a database transaction. When this transaction completes, the post-commit hook triggers an event that normally triggers an asynchronous elasticsearch update. However if the event thread pool is totally depleted then it will fall back to dispatching the update on the calling thread, in other words the update will become synchronous (due to a lack of threads).    The database connection will only be released back to the pool once the post-commit hook is complete, so if elasticsearch is also running slow then this can result in the database connection being held open for a long time.    In the case of the user this thread dump came from there is evidence that the disk elasticsearch is as running on is slow, and so this caused a massive backup of database connections.       {quote}""""qtp540936684-234"""" #234 prio=5 os_prio=0 tid=0x00007f3480013000 nid=0x170 waiting for monitor entry [0x00007f31f8f49000]   java.lang.Thread.State: BLOCKED (on object monitor)   at org.elasticsearch.action.bulk.BulkProcessor.internalAdd(BulkProcessor.java:283)   - waiting to lock <0x00000006cbbeb6c0> (a org.elasticsearch.action.bulk.BulkProcessor)   at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:268)   at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:264)   at org.elasticsearch.action.bulk.BulkProcessor.add(BulkProcessor.java:250)   at org.sonatype.nexus.repository.search.SearchServiceImpl.lambda$0(SearchServiceImpl.java:321)   at org.sonatype.nexus.repository.search.SearchServiceImpl$$Lambda$289/275393005.accept(Unknown Source)   at java.lang.Iterable.forEach(Iterable.java:75)   at org.sonatype.nexus.repository.search.SearchServiceImpl.bulkPut(SearchServiceImpl.java:315)   at org.sonatype.nexus.repository.search.SearchFacetImpl.bulkPut(SearchFacetImpl.java:133)   at org.sonatype.nexus.repository.search.SearchFacetImpl$$EnhancerByGuice$$7cd025a2.CGLIB$bulkPut$3(<generated>)   at org.sonatype.nexus.repository.search.SearchFacetImpl$$EnhancerByGuice$$7cd025a2$$FastClassByGuice$$753cd40b.invoke(<generated>)   at com.google.inject.internal.cglib.proxy.$MethodProxy.invokeSuper(MethodProxy.java:228)   at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:76)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:272)   at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)   at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)   at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)   at org.sonatype.nexus.repository.search.SearchFacetImpl$$EnhancerByGuice$$7cd025a2.bulkPut(<generated>)   at org.sonatype.nexus.repository.search.IndexRequest.bulkApply(IndexRequest.java:92)   at org.sonatype.nexus.repository.search.IndexRequestProcessor.lambda$0(IndexRequestProcessor.java:139)   at org.sonatype.nexus.repository.search.IndexRequestProcessor$$Lambda$406/297358118.accept(Unknown Source)   at java.util.Optional.ifPresent(Optional.java:159)   at org.sonatype.nexus.repository.search.IndexRequestProcessor.doUpdateSearchIndex(IndexRequestProcessor.java:135)   at org.sonatype.nexus.repository.search.IndexRequestProcessor.maybeUpdateSearchIndex(IndexRequestProcessor.java:115)   at org.sonatype.nexus.repository.search.IndexRequestProcessor$$Lambda$385/1669743823.accept(Unknown Source)   at java.util.HashMap.forEach(HashMap.java:1289)   at org.sonatype.nexus.repository.search.IndexBatchRequest.apply(IndexBatchRequest.java:93)   at org.sonatype.nexus.repository.search.IndexRequestProcessor.process(IndexRequestProcessor.java:99)   at org.sonatype.nexus.repository.search.IndexRequestProcessor.on(IndexRequestProcessor.java:88)   at sun.reflect.GeneratedMethodAccessor235.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at com.google.common.eventbus.Subscriber.invokeSubscriberMethod(Subscriber.java:87)   at com.google.common.eventbus.Subscriber$1.run(Subscriber.java:72)   at org.sonatype.nexus.internal.event.AffinityBarrier.lambda$1(AffinityBarrier.java:91)   at org.sonatype.nexus.internal.event.AffinityBarrier$$Lambda$384/1417277946.run(Unknown Source)   at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)   at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)   at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)   at org.sonatype.nexus.internal.event.EventExecutor.lambda$0(EventExecutor.java:72)   at org.sonatype.nexus.internal.event.EventExecutor$$Lambda$24/155969703.rejectedExecution(Unknown Source)   at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)   at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)   at org.apache.shiro.concurrent.SubjectAwareExecutor.execute(SubjectAwareExecutor.java:129)   at org.sonatype.nexus.internal.event.AffinityBarrier.execute(AffinityBarrier.java:89)   at org.sonatype.nexus.internal.event.EventExecutor.execute(EventExecutor.java:215)   at com.google.common.eventbus.Subscriber.dispatchEvent(Subscriber.java:67)   at com.google.common.eventbus.Dispatcher$ImmediateDispatcher.dispatch(Dispatcher.java:186)   at com.google.common.eventbus.EventBus.post(EventBus.java:212)   at org.sonatype.nexus.internal.event.EventManagerImpl.lambda$0(EventManagerImpl.java:132)   at org.sonatype.nexus.internal.event.EventManagerImpl$$Lambda$213/1347710971.run(Unknown Source)   at org.sonatype.nexus.internal.event.AffinityBarrier.lambda$0(AffinityBarrier.java:75)   at org.sonatype.nexus.internal.event.AffinityBarrier$$Lambda$252/1880833496.run(Unknown Source)   at com.google.common.util.concurrent.SequentialExecutor$1.run(SequentialExecutor.java:120)   at com.google.common.util.concurrent.SequentialExecutor$QueueWorker.workOnQueue(SequentialExecutor.java:227)   at com.google.common.util.concurrent.SequentialExecutor$QueueWorker.run(SequentialExecutor.java:171)   at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)   at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)   at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)   at org.sonatype.nexus.internal.event.EventExecutor.lambda$0(EventExecutor.java:72)   at org.sonatype.nexus.internal.event.EventExecutor$$Lambda$24/155969703.rejectedExecution(Unknown Source)   at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)   at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)   at org.apache.shiro.concurrent.SubjectAwareExecutor.execute(SubjectAwareExecutor.java:129)   at com.google.common.util.concurrent.SequentialExecutor.execute(SequentialExecutor.java:128)   at org.sonatype.nexus.internal.event.AffinityBarrier.coordinate(AffinityBarrier.java:71)   at org.sonatype.nexus.internal.event.EventExecutor.executeWithAffinity(EventExecutor.java:199)   at org.sonatype.nexus.internal.event.EventManagerImpl.post(EventManagerImpl.java:132)   at org.sonatype.nexus.orient.entity.EntityHook.postEvents(EntityHook.java:325)   at org.sonatype.nexus.orient.entity.EntityHook.flushEvents(EntityHook.java:289)   at org.sonatype.nexus.orient.entity.EntityHook.onAfterTxCommit(EntityHook.java:174)   at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.commit(ODatabaseDocumentTx.java:2949)   at com.orientechnologies.orient.core.db.document.ODatabaseDocumentTx.commit(ODatabaseDocumentTx.java:2870)   at org.sonatype.nexus.repository.storage.StorageTxImpl.commit(StorageTxImpl.java:183)   at sun.reflect.GeneratedMethodAccessor181.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.sonatype.nexus.common.stateguard.SimpleMethodInvocation.proceed(SimpleMethodInvocation.java:53)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$TransitionImpl.run(StateGuard.java:193)   at org.sonatype.nexus.common.stateguard.TransitionsInterceptor.invoke(TransitionsInterceptor.java:56)   at org.sonatype.nexus.common.stateguard.StateGuardAspect$1.invoke(StateGuardAspect.java:66)   at com.sun.proxy.$Proxy218.commit(Unknown Source)   at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:67)   at org.sonatype.nexus.transaction.Operations.transactional(Operations.java:200)   at org.sonatype.nexus.transaction.Operations.call(Operations.java:146)   at org.sonatype.nexus.repository.view.handlers.LastDownloadedHand.maybeUpdateLastDownloaded(LastDownloadedHandler.java:107){quote}"""
"NEXUS-19097","Bug","Docker|Logging",1,"Log spam when remote docker repository returns 400 or 401 and no credentials are configured or remote requires bearer token","""Configure a docker proxy repository with a remote URL of """"https://gcr.io"""". Then request a manifest that does not exist on the remote through the proxy repository:    [https://localhost:8081/repository/docker-gcr-proxy/v2/abcd/efgh/manifests/1472|https://localhost:8081/repository/repository/docker-gcr/-proxy/v2/abcd/efgh/manifests/1472]    This will result in a warning in the log along with a stack trace because the bearer token can't be retrieved.  The """"https://gcr.io"""" allows anonymous access, but it will return 401 rather than 404 for images that don't exist on it.  I believe the official docker registry also behaves this way.  The result of this is that you get extremely noisy logs if you are requesting docker images through a group repository, since inevitably some of them won't exist on the remote.    *Expected*:  A 401 should be logged at as it currently does, with no stack trace.    {quote}2019-02-11 00:04:50,847-0700 WARN [qtp-952548213-606] admin org.sonatype.nexus.repository.docker.internal.V2Handlers - Error: GET /v2/abcd/efgh/manifests/1472: 401 - org.sonatype.nexus.repository.docker.internal.V2Exception: authentication required   2019-02-11 00:04:51,117-0700 WARN [qtp-952548213-513] admin org.sonatype.nexus.repository.docker.internal.auth.BearerScheme - Failed to retrieve docker bearer token   org.apache.http.auth.AuthenticationException: Could not retrieve token from [https://gcr.io/v2/token]. Status code: 401   at org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl.executeOK(DockerProxyFacetImpl.java:514)   at org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl.retrieveBearerToken(DockerProxyFacetImpl.java:457)   at org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl.access$3(DockerProxyFacetImpl.java:442)   at org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl$2.retrieveBearerToken(DockerProxyFacetImpl.java:841)   at org.sonatype.nexus.repository.docker.internal.auth.DockerAuthHttpClientContext$2.getToken(DockerAuthHttpClientContext.java:76)   at org.sonatype.nexus.repository.docker.internal.auth.BearerScheme.authenticate(BearerScheme.java:105)   at org.apache.http.impl.auth.HttpAuthenticator.doAuth(HttpAuthenticator.java:239)   at org.apache.http.impl.auth.HttpAuthenticator.generateAuthResponse(HttpAuthenticator.java:202)   at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:263)   at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)   at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)   at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111)   at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)   at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:72)   at org.sonatype.nexus.repository.httpclient.FilteredHttpClientSupport.lambda$0(FilteredHttpClientSupport.java:56)   at org.sonatype.nexus.repository.httpclient.FilteredHttpClientSupport$$Lambda$377.00000000042A7760.call(Unknown Source)   at org.sonatype.nexus.repository.httpclient.internal.BlockingHttpClient.filter(BlockingHttpClient.java:123)   at org.sonatype.nexus.repository.httpclient.FilteredHttpClientSupport.doExecute(FilteredHttpClientSupport.java:56)   at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)   at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)   at org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl.execute(DockerProxyFacetImpl.java:325)   at org.sonatype.nexus.repository.proxy.ProxyFacetSupport.fetch(ProxyFacetSupport.java:405)   at org.sonatype.nexus.repository.proxy.ProxyFacetSupport.fetch(ProxyFacetSupport.java:375)   at org.sonatype.nexus.repository.proxy.ProxyFacetSupport.doGet(ProxyFacetSupport.java:245)   at org.sonatype.nexus.repository.docker.internal.DockerProxyFacetImpl.doGet(DockerProxyFacetImpl.java:859)   at org.sonatype.nexus.repository.proxy.ProxyFacetSupport.lambda$1(ProxyFacetSupport.java:234)   at org.sonatype.nexus.repository.proxy.ProxyFacetSupport$$Lambda$376.00000000042A6A20.call(Unknown Source)   at org.sonatype.nexus.common.io.CooperatingFuture.performCall(CooperatingFuture.java:122)   at com.sonatype.nexus.hazelcast.internal.io.DistributedCooperatingFuture.performCall(DistributedCooperatingFuture.java:50)   at org.sonatype.nexus.common.io.CooperatingFuture.call(CooperatingFuture.java:64)   at org.sonatype.nexus.common.io.ScopedCooperationFactorySupport$ScopedCooperation.cooperate(ScopedCooperationFactorySupport.java:99)   at org.sonatype.nexus.repository.proxy.ProxyFacetSupport.get(ProxyFacetSupport.java:225)   at org.sonatype.nexus.repository.proxy.ProxyHandler.handle(ProxyHandler.java:50)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.view.handlers.LastDownloadedHandler.handle(LastDownloadedHandler.java:54)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.storage.UnitOfWorkHandler.handle(UnitOfWorkHandler.java:39)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.view.Context$proceed.call(Unknown Source)   at org.sonatype.nexus.repository.docker.internal.V2Handlers$_closure16.doCall(V2Handlers.groovy:269)   at sun.reflect.GeneratedMethodAccessor245.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)   at java.lang.reflect.Method.invoke(Method.java:508)   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:98)   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:264)   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1034)   at groovy.lang.Closure.call(Closure.java:418)  {quote}  A 400 response may also be logged with a similar large stack trace at WARN level:  """
"NEXUS-19092","Bug","RubyGems",1,"NPE When Installing Gem with no Date Attribute","""When attempting to install a gem that does not contain a date attribute in its spec file a NullPointerException is thrown e.g.    To reproduce with example linux-kstat v0.2.1 gem:   # Configure proxy repo for [https://rubygems.org|https://rubygems.org/]   # On a linux env, run gem install e.g.   #       Expected:    Nexus should handle cases where gem spec does not contain expected values."""
"NEXUS-19091","Bug","S3",3,"S3 Blobstore expiration is at Bucket level - ignores path prefix","""When creating an S3 Blobstore, the expiration days can be set.    This is tied to a lifecycle policy on the S3 bucket.    When multiple S3 Blobstores are created in the same bucket - using different path prefixes - they each allow a separate setting for expiration days, however, they simply overwrite the common lifecycle policy.    Either:    1) It should be clear from the UI that they share a single lifecycle policy, and cannot be separately set.    or    2) The lifecycle policy should be maintained with appropriate filters when path prefixes are in place.         Steps to reproduce:   * Create an S3 Blobstore with path prefix of test1 and expiration days set to 10   * Run aws s3api get-bucket-lifecycle-configuration --bucket <bucketName>   * Create a second S3 Blobstore in the same bucket with path prefix of test2 and expiration days set to 20   * Run aws s3api get-bucket-lifecycle-configuration --bucket <bucketName>   * Note that the single expiration rule now has changed to 20 days."""
"NEXUS-19103","Bug","Cleanup|Logging",0.5,"Components and assets deleted by cleanup policies should be logged only to ""Cleanup service"" task log","""Post NEXUS-18731, Cleanup Policies log details about what they delete to the nexus.log and Cleanup task log.  h4. Expected    Specific components and assets deleted by cleanup policies should be logged at default levels _only_ to the """"Cleanup service"""" task log."""
"NEXUS-19102","Bug","Docker",3,"Unable to proxy private Azure (ACR) registry","""Hello,    We are trying to use Nexus 3 as Proxy for a Private ACR registry (for caching), we set the URL and authentication on configuration but I seem that Nexus is not able to correctly authenticate on ACR. See bellow the logs    Using version 3.15-2 of Nexus        Seems a bug to me         A docker login work directly with ACR url and provider username/password    Regards,"""
"NEXUS-19121","Bug","PyPI",2,"Delete of component or asset from PyPi proxy repository fails","""Select a component or an asset in a pypi proxy repository in Nexus Repo 3.15.2, and try to delete it in the UI.  This will fail.    Expected: It should be possible to delete cached content from pypi proxy repositories.    {quote}  2019-02-14 12:46:20,165-0600 ERROR [qtp125386036-232] admin org.sonatype.nexus.extdirect.internal.ExtDirectExceptionHandler - Failed to invoke action method: coreui_Component.deleteAsset, java-method: org.sonatype.nexus.coreui.ComponentComponent.deleteAsset  org.sonatype.nexus.repository.MissingFacetException: No facet of type PyPiHostedFacet attached to repository pypi-proxy   at org.sonatype.nexus.repository.manager.internal.RepositoryImpl.facet(RepositoryImpl.java:322)   at org.sonatype.nexus.repository.pypi.internal.PyPiHostedComponentMaintenance.deleteRootIndex(PyPiHostedComponentMaintenance.java:94)   at org.sonatype.nexus.repository.pypi.internal.PyPiHostedComponentMaintenance.deleteAssetTx(PyPiHostedComponentMaintenance.java:59)   at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)   at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:54)   at org.sonatype.nexus.repository.storage.DefaultComponentMaintenanceImpl.deleteAsset(DefaultComponentMaintenanceImpl.java:93)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:272)   at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)   at org.sonatype.nexus.repository.storage.DefaultComponentMaintenanceImpl.deleteAsset(DefaultComponentMaintenanceImpl.java:84)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:272)   at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)   at org.sonatype.nexus.repository.maintenance.internal.MaintenanceServiceImpl.deleteAsset(MaintenanceServiceImpl.java:85)   at org.sonatype.nexus.repository.maintenance.MaintenanceService$deleteAsset$0.call(Unknown Source)   at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:136)   at org.sonatype.nexus.coreui.ComponentComponent.deleteAsset(ComponentComponent.groovy:318)   at com.palominolabs.metrics.guice.ExceptionMeteredInterceptor.invoke(ExceptionMeteredInterceptor.java:49)   at com.palominolabs.metrics.guice.TimedInterceptor.invoke(TimedInterceptor.java:47)   at org.sonatype.nexus.validation.internal.ValidationInterceptor.invoke(ValidationInterceptor.java:53)   at org.apache.shiro.guice.aop.AopAllianceMethodInvocationAdapter.proceed(AopAllianceMethodInvocationAdapter.java:49)   at org.apache.shiro.authz.aop.AuthorizingAnnotationMethodInterceptor.invoke(AuthorizingAnnotationMethodInterceptor.java:68)   at org.apache.shiro.guice.aop.AopAllianceMethodInterceptorAdapter.invoke(AopAllianceMethodInterceptorAdapter.java:36)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeJavaMethod(DispatcherBase.java:142)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeMethod(DispatcherBase.java:133)   at org.sonatype.nexus.extdirect.internal.ExtDirectDispatcher.invokeMethod(ExtDirectDispatcher.java:82)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.dispatch(DispatcherBase.java:63)   at com.softwarementors.extjs.djn.router.processor.standard.StandardRequestProcessorBase.dispatchStandardMethod(StandardRequestProcessorBase.java:73)   at com.softwarementors.extjs.djn.router.processor.standard.json.JsonRequestProcessor.processIndividualRequest(JsonRequestProcessor.java:502)   at com.softwarementors.extjs.djn.router.processor.standard.json.JsonRequestProcessor.processIndividualRequestsInThisThread(JsonRequestProcessor.java:150)   at com.softwarementors.extjs.djn.router.processor.standard.json.JsonRequestProcessor.process(JsonRequestProcessor.java:133)   at com.softwarementors.extjs.djn.router.RequestRouter.processJsonRequest(RequestRouter.java:83)   at com.softwarementors.extjs.djn.servlet.DirectJNgineServlet.processRequest(DirectJNgineServlet.java:632)   at com.softwarementors.extjs.djn.servlet.DirectJNgineServlet.doPost(DirectJNgineServlet.java:595)   at org.sonatype.nexus.extdirect.internal.ExtDirectServlet.doPost(ExtDirectServlet.java:130)   at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)   at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)   at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286)   at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276)   at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181)   at com.google.inject.servlet.DynamicServletPipeline.service(DynamicServletPipeline.java:71)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:112)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)   at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)   at org.sonatype.nexus.security.SecurityFilter.executeChain(SecurityFilter.java:85)   at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)   at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)   at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)   at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)   at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)   at org.sonatype.nexus.security.SecurityFilter.doFilterInternal(SecurityFilter.java:101)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at com.sonatype.nexus.licensing.internal.LicensingRedirectFilter.doFilter(LicensingRedirectFilter.java:108)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at com.codahale.metrics.servlet.AbstractInstrumentedFilter.doFilter(AbstractInstrumentedFilter.java:97)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.sonatype.nexus.internal.web.ErrorPageFilter.doFilter(ErrorPageFilter.java:68)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.sonatype.nexus.internal.web.EnvironmentFilter.doFilter(EnvironmentFilter.java:101)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.sonatype.nexus.internal.web.HeaderPatternFilter.doFilter(HeaderPatternFilter.java:98)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at com.google.inject.servlet.DynamicFilterPipeline.dispatch(DynamicFilterPipeline.java:104)   at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135)   at org.sonatype.nexus.bootstrap.osgi.DelegatingFilter.doFilter(DelegatingFilter.java:73)   at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1634)   at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)   at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)   at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)   at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)   at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)   at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)   at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)   at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)   at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)   at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)   at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)   at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)   at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)   at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:175)   at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)   at org.eclipse.jetty.server.Server.handle(Server.java:531)   at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)   at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)   at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)   at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)   at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)   at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)   at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)   at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)   at java.lang.Thread.run(Thread.java:748)  2019-02-14 12:46:33,683-0600 ERROR [qtp125386036-233] admin org.sonatype.nexus.extdirect.internal.ExtDirectExceptionHandler - Failed to invoke action method: coreui_Component.deleteComponent, java-method: org.sonatype.nexus.coreui.ComponentComponent.deleteComponent  org.sonatype.nexus.repository.MissingFacetException: No facet of type PyPiHostedFacet attached to repository pypi-proxy   at org.sonatype.nexus.repository.manager.internal.RepositoryImpl.facet(RepositoryImpl.java:322)   at org.sonatype.nexus.repository.pypi.internal.PyPiHostedComponentMaintenance.deleteRootIndex(PyPiHostedComponentMaintenance.java:94)   at org.sonatype.nexus.repository.pypi.internal.PyPiHostedComponentMaintenance.deleteComponentTx(PyPiHostedComponentMaintenance.java:85)   at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)   at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:54)   at org.sonatype.nexus.repository.storage.DefaultComponentMaintenanceImpl.deleteComponent(DefaultComponentMaintenanceImpl.java:60)   at org.sonatype.nexus.repository.storage.DefaultComponentMaintenanceImpl.deleteComponent(DefaultComponentMaintenanceImpl.java:49)   at org.sonatype.nexus.repository.maintenance.internal.MaintenanceServiceImpl.deleteComponent(MaintenanceServiceImpl.java:97)   at org.sonatype.nexus.repository.maintenance.MaintenanceService$deleteComponent$2.call(Unknown Source)   at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:47)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)   at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:136)   at org.sonatype.nexus.coreui.ComponentComponent.deleteComponent(ComponentComponent.groovy:276)   at com.palominolabs.metrics.guice.ExceptionMeteredInterceptor.invoke(ExceptionMeteredInterceptor.java:49)   at com.palominolabs.metrics.guice.TimedInterceptor.invoke(TimedInterceptor.java:47)   at org.sonatype.nexus.validation.internal.ValidationInterceptor.invoke(ValidationInterceptor.java:53)   at org.apache.shiro.guice.aop.AopAllianceMethodInvocationAdapter.proceed(AopAllianceMethodInvocationAdapter.java:49)   at org.apache.shiro.authz.aop.AuthorizingAnnotationMethodInterceptor.invoke(AuthorizingAnnotationMethodInterceptor.java:68)   at org.apache.shiro.guice.aop.AopAllianceMethodInterceptorAdapter.invoke(AopAllianceMethodInterceptorAdapter.java:36)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeJavaMethod(DispatcherBase.java:142)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.invokeMethod(DispatcherBase.java:133)   at org.sonatype.nexus.extdirect.internal.ExtDirectDispatcher.invokeMethod(ExtDirectDispatcher.java:82)   at com.softwarementors.extjs.djn.router.dispatcher.DispatcherBase.dispatch(DispatcherBase.java:63)  {quote}"""
"NEXUS-19125","Bug","Docker",3,"Docker pull from mcr.microsoft.com results in 403 due to normalization of 307 redirect URLs removing double slash","""Pulling from 'mcr.microsoft.com' via a proxy repository results in a 403, whilst pulling directly from 'mcr.microsoft.com' does not:    pull directly:         pulling via proxy repo *(remote URL set to: [https://mcr.microsoft.com|https://mcr.microsoft.com/]):*    *Expected*:    Pulling via proxy repo should behave in the same way as pulling directly from mcr.microsoft.com."""
"NEXUS-19146","Story","REST",3,"Add REST API Endpoint for Content Selector CRUD","""In order to support managing teams we need to be able to support CRUD operations for Content Selectors."""
"NEXUS-19145","Story","REST",2,"Add REST API to Configure IQ Server","""As part of initial server configuration some administrators may need to configure the IQ server used by NXRM. We may want a separate endpoint to enable/disable the server."""
"NEXUS-19144","Story","REST",2,"Add REST API to Set NXRM License","""As part of initial server configuration some administrators may need to upload a license file."""
"NEXUS-19143","Story","REST",5,"Add REST API Endpoint for LDAP CRUD Operations","""As part of initial server configuration some administrators may need to configure the LDAP server(s) used by NXRM. They would also need to be able to change the order of the LDAP servers which would be a separate endpoint."""
"NEXUS-19142","Story","REST",2,"Add REST API to Set Email Server & Enable/Disable","""As part of initial server configuration some administrators may need to configure the email server used by NXRM. They would also need to be able to enable & disable the server which may be separate endpoint(s)."""
"NEXUS-19141","Story","REST",3,"Add REST API Endpoints for User CRUD Operations","""Add REST endpoints to perform CRUD operations on users."""
"NEXUS-19170","Story","REST",1,"Add REST API to Reset User Token","""In order to facilitate administrating NXRM we need a REST endpoint which can be used to reset the user token for a given user."""
"NEXUS-19189","Bug","NPM",0,"Invalid JSON input error when loading some NPM packages","""I try to install protoduck@5.0.1 ([https://www.npmjs.com/package/protoduck)] from my Nexus OSS 3.14.0-04 configured as NPM proxy.    NPM responds with error """"npm ERR! 404 Not Found: protoduck@latest"""" and Nexus logs """"Invalid JSON input"""" (complete stack-trace in attachment).    This package is successfully installed when directly calling registry.npmjs.org         Related issue [https://github.com/zkat/protoduck/issues/6]"""
"NEXUS-19187","Bug","Blobstore",1,"Blob store promotion leads to persistent class cast exception in UI","""When trying to promote a blob store the follow exception occurs and leads to a persistent error in the UI that doesn't allow blob stores to be viewed or repos created since the blob store drop down is empty.    Reproduce steps:   # Create a blob store, set a soft quota of 50   # Attempt to promote the blob store   # Observe that the blob store UI no longer works due to class cast exception   # Reboot Nexus Repo.... observe that the blob store UI is still broken"""
"NEXUS-19235","Bug","Firewall|Proxy Repository",3,"Proxy to a Nexus firewall proxy repos returns 404 for quarantined artifacts","""A proxy in Nexus A, which points to a proxy repo on Nexus B. The proxy on Nexus B has firewall enabled with quarantine.    When a request comes in, for a component that has been quarantined, Nexus B will return a 403, but the proxy in Nexus A will then return a 404.    Expectation is that Nexus A proxy should also return a 403, so that it is clear why the build failed to the end developer.    Many customers will use a layered approach with Nexus instances, where Nexus instance used by builds do not have direct access to outside internet and firewall is enabled on the Nexus instance that does have outside access."""
"NEXUS-19303","Bug","PyPI",0,"PyPi hosted repository doesn't update index file after uploading new version of existing component","""In the latest 3.15.2 version some changes were made to PyPiHostedFacetImpl to store the simple index html file in the blobstore instead of creating the file on the fly.    This introduced a bug.  When uploading a new version of an existing component the deleteIndex function does not delete the file.  This causes the file to remain in the initial state when it was created and fetching the latest version of a component does not work correctly.    *3.15.2-01 Patch*:    A patch is attached to this issue which fixes this problem for 3.15.2-01.  Note that this patch is only applicable to that version, do not try to install it in any other version.    To apply this patch, shut down Nexus, and replace the following jar file with the one attached on this issue which has the same file name.    {{nexus-3.15.2-01/system/org/sonatype/nexus/plugins/nexus-repository-pypi/3.15.2-01/nexus-repository-pypi-3.15.2-01.jar}}    *3.16.0-01 Patch*:    A patch is attached to this issue which fixes this problem for 3.16.0-01.  Note that this patch is only applicable to that version, do not try to install it in any other version.    To apply this patch, shut down Nexus, and replace the following jar file with the one attached on this issue which has the same file name.    {{nexus-3.16.0-01/system/org/sonatype/nexus/plugins/nexus-repository-pypi/3.16.0-01/nexus-repository-pypi-3.16.0-01.jar}}    *3.16.1-02 Patch*    A patch is attached to this issue which fixes this problem for 3.16.1-02.  Note that this patch is only applicable to that version, do not try to install it in any other version.    To apply this patch, shut down Nexus, and replace the following jar file with the one attached on this issue which has the same file name.    {{nexus-3.16.1-02/system/org/sonatype/nexus/plugins/nexus-repository-pypi/3.16.1-02/nexus-repository-pypi-3.16.1-02.jar}}    *3.16.2-01 Patch*    A patch is attached to this issue which fixes this problem for 3.16.2-01.  Note that this patch is only applicable to that version, do not try to install it in any other version.    To apply this patch, shut down Nexus, and replace the following jar file with the one attached on this issue which has the same file name.    {{nexus-3.16.1-02/system/org/sonatype/nexus/plugins/nexus-repository-pypi/3.16.1-02/nexus-repository-pypi-3.16.1-02.jar}}    h4. Diagnosis    We found that this was caused by characters in the package names that needed to be normalized (https://www.python.org/dev/peps/pep-0503/#normalized-names). The caching mechanism works be generating and saving the index when it is fetched and then deleting the index file whenever the contents of the repository change. In some cases the delete failed because it was looking for an index using a non-normalized name.    Once this fix is released changes to the repository will correctly remove the index and the updated index will be available on the next fetch. After upgrade there may be a small number of cases where new packages (that contain . or _ characters in there name) have been uploaded and the index hasn't been invalidated, this will be obvious because the latest version of that package won't be available. Both uploading a new version of the package or deleting the index file in these cases will fix the problem.    To delete an index use the browse function in the UI and find the corresponding index file under the /simple/ tree node and then delete the asset."""
"NEXUS-19384","Bug","NPM",3,"NPM Group fails to forward requests for updating to members","""When proxying NPM packages, the metadata max-age does not work, resulting in new versions of a package not to be downloaded.    To reproduce:    1. Create an NPM proxy repo, proxying to a hosted NPM repo on another NXRM 3 instance _(this can be any NPM registry, a remote Nexus 3 is only for the sake of convenience)._   2. Create an NPM group repo and the add the NPM proxy repo to it.    3. Upload v1.0.0 of NPM package to the remote hosted NPM repo.   4. run _npm i <package>@1.0.0_ against group repo - install works (expected).   5. run _npm i <package>@2.0.0_ against group repo - install fails (expected).    6. Upload v2.0.0 of NPM package to the remote hosted NPM repo.    7. Invalidate the proxy repo cache or wait for 'metadata max-age' time to pass.   8 run _npm i <package>@2.0.0_ against group repo - install fails (unexpected).    9. Invalidate the group repo cache.   10. run _npm i <package>@2.0.0_ against group repo - this time install works."""
"NEXUS-19404","Bug","Transport|Yum",2,"Conditional GET requests for repodata/repomd.xml files always return 304 unmodified","""Conditional GET requests made to """"repodata/repomd.xml"""" files in a Nexus Repo 2.x yum enabled hosted repository always return 304 unmodified.     This breaks proxying of these repositories from Nexus Repo 3.x.    Reproduce case:    # Enable yum metadata creation on a snapshot maven repository in 2.14.12  # Deploy an rpm into it  # Configure a yum proxy repository in Nexus Repo 3, set the remote to the snapshot repository in Nexus Repo 2  # Retreive 'repodata/repomd.xml"""" through the proxy  # Deploy a new snapshot rpm into Nexus Repo 2  # Verify that yum metadata was regenerated  # Invalidate cache in the Repo 3 proxy  # Retreive 'repodata/repomd.xml"""" through the proxy    Observe that in the Nexus Repo 2 log a """"304"""" unmodified response is sent for the repodata/repomd.xml file.    {quote}  192.168.1.78 - - [13/Mar/2019:13:26:25 -0500] """"GET /nexus/content/repositories/snapshots/repodata/repomd.xml HTTP/1.1"""" 304 0 3  {quote}    Replaying the same request (with the same headers) through curl shows the same result.            But that file has been modified.  I'm in GMT-5, btw.         """
"NEXUS-19424","Improvement","Cleanup|Documentation",5,"Add ability to cleanup by ""Never downloaded""","""We have a repositories (Docker and Maven) with assigned cleanup policy with only """"Last Downloaded Before"""" field specified in Criteria    And scheduled task of type """"Admin - Cleanup repositories using their associated policies"""" to apply those policies    But looks like this criteria is not working properly, because I see dozens of maven artifacts and docker tags that haven't been downloaded at all, and created months ago    In repository browser I can see something like this  {code}  Blob created   Thu Jan 17 2019 14:35:20 GMT+0300 (Moscow Standard Time)  Blob updated   Thu Jan 17 2019 14:35:20 GMT+0300 (Moscow Standard Time)  Last downloaded   has not been downloaded  {code}    When I examine task logs, I see something like this  {code:java}  2019-03-14 20:01:37,871+0000 INFO  [quartz-3-thread-19]  *SYSTEM org.sonatype.nexus.cleanup.internal.service.CleanupServiceImpl - Deleting components in repository docker-snapshots using policy Clean_not_in_use_for_2_weeks  2019-03-14 20:01:37,879+0000 INFO  [quartz-3-thread-19]  *SYSTEM org.sonatype.nexus.cleanup.internal.service.CleanupServiceImpl - 2 components cleaned up for repository docker-snapshots  {code}  """
"NEXUS-19462","Story","Security",2,"On first admin login ask user to choose anonymous access configuration","""As part of the onboarding process, possibly as part of NEXUS-19461 ask the admin user to choose whether anonymous access should be enabled.    Acceptance   * Don't bug existing users   * Tests continue to pass, ensure there's a development-friendly mode   * Anonymous access should be the default   * Don't impair REST-provisioned instances    Notes   * This feature should explain the two choices clearly so users understand which one they should choose.     """
"NEXUS-19494","Improvement","S3",5,"Provide mechanism to test S3 permissions on Blobstore","""When using an S3 blobstore, the user is required to define a policy in AWS, following our documentation, and ensure they have the correct permissions.    When things go wrong, we see errors in the log, but don't have an easy way to see any AWS information.    There would be value in having a 'Verify connection' or similar button on an S3 Blobstore that would ensure Nexus has the correct permissions for the various actions, and in case of error, detailing what is missing (if possible).    *Acceptance Criteria*  When provisioning or connecting with S3 fails then an appropriate and more exact error message should be reported to the user. Exception cases should consider invalid access id/access token, insufficient permissions or other policy issues such as kms."""
"NEXUS-19566","Improvement","S3",2,"Multithread S3 blob store upload","""The current MultipartUploader is single threaded ([https://github.com/sonatype/nexus-public/blob/master/plugins/nexus-blobstore-s3/src/main/java/org/sonatype/nexus/blobstore/s3/internal/MultipartUploader.java]) and uploads chunks sequentially.    For files larger than 5mb, this introduces a considerable slowdown in upload times. In a local test with a 1gb file, comparing against the S3 command line: uploading via S3 CLI took 21 seconds, NXRM took 680 seconds. Much of this time difference appears to be due the the sequential 5mb chunks.    If the uploader utilizes multiple threads, it should be considerably faster."""
"NEXUS-19607","Bug","HA",1,"NullPointerException in NugetLocalGalleryFacetSupport.maintainAggregateInfo","""The following exception was noticed in an HA 3.15.2 environment - unclear if this can happen in a non HA-C environment.        h4. Expected    NPE should be avoided."""
"NEXUS-19618","Bug","Maven|Repository",0,"Expensive, error prone check done for content validation of checksums","""Currently, the validation of checksum file content is going through the tika mime type evaluation layer.  This is an expensive check, and is error prone for checksums, since they can potentially start with the same magic byte patterns as many different file formats.  See -NEXUS-19018- for a specific example of this.    We should simply do a check that the file contains only hex digits, and the right number of them.  A simple regex match would work for this.    Example: When a Remove Maven snapshots task runs ( or any administrative task like Cleanup Policies) , rebuilds the checksum for a rebuilt maven-metadata.xml file, the generated md5, can fail the entire task:    h4. Expected   - A failure processing a single asset inside the RemoveSnapshotsTask should not stop/fail the entire task. It should keep going and process as many assets as it can ( possible separate issue ).   - We certainly should *not be performing any validation* of our own generated hash files.   - do not do mime type of any hash files, even proxied remotes or uploaded into NXRM - instead use a simpler hash file format detection validation algorithm using a regex or similar - NXRM 2 did that, NXRM 3 should adopt something similar"""
"NEXUS-19702","Bug","Yum",1,"Regression: Yum metadata leaks between trees in a hosted yum repository.","""     Set up a hosted yum repository in Nexus Repo 3.16.0 with """"repodata depth"""" of """"1"""".    Deploy rpm files to it under two separate directories, such as:    Now download the primary.xml.gz file from the """"tools/repodata"""" directory.  You'll find that it contains all of the rpm files.    *Expected*: Only rpm files in the same tree as the """"repodata"""" directory should be included in its metadata.    I also tested on 3.11.0, and this bug does not occur in that version."""
"NEXUS-19733","Bug","Logging|Support Tools",2,"UnsupportedOperationException: Maintenance method 'getDatabaseStatus' or 'getDatabaseRole' is only supported in clustered mode when generating support zip","""Generating a support zip in 3.16.0 puts 2 giant stack traces in logs. Don't do that.    """
"NEXUS-19801","Bug","Maven|Scheduled Tasks",3,"Exception thrown in metadata rebuild stops snapshot removal task completely.","""     I can't tell what the exact cause of this was, but this exception caused the maven metadata rebuild at the end of a snapshot removal task to stop entirely.      The task should not stop because it wasn't able to rebuild one maven-metadata.xml file.    The task should properly log the file(s) involved in the failure if an exception occurs so we can find out the cause.     {quote}  2019-04-20 09:02:22,325-0400 ERROR [quartz-2-thread-11]  *SYSTEM org.sonatype.nexus.repository.maven.tasks.RemoveSnapshotsTask - Failed to run task 'Remove Maven snapshots from maven-snapshots' on repository 'maven-snapshots'  java.lang.RuntimeException: java.io.EOFException: input contained no data   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataUpdater.update(MetadataUpdater.java:111)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataUpdater.processMetadata(MetadataUpdater.java:72)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataRebuilder$Worker.lambda$2(MetadataRebuilder.java:461)   at org.sonatype.nexus.transaction.OperationPoint.proceed(OperationPoint.java:64)   at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)   at org.sonatype.nexus.transaction.Operations.transactional(Operations.java:200)   at org.sonatype.nexus.transaction.Operations.call(Operations.java:146)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataRebuilder$Worker.rebuildMetadataInner(MetadataRebuilder.java:415)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataRebuilder$Worker.rebuildMetadata(MetadataRebuilder.java:382)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataRebuilder.rebuild(MetadataRebuilder.java:120)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataRebuilder.deleteAndRebuild(MetadataRebuilder.java:240)   at org.sonatype.nexus.repository.maven.internal.hosted.MavenHostedFacetImpl.deleteMetadata(MavenHostedFacetImpl.java:129)   at org.sonatype.nexus.repository.maven.internal.RemoveSnapshotsFacetImpl.removeSnapshots(RemoveSnapshotsFacetImpl.java:141)   at org.sonatype.nexus.common.stateguard.MethodInvocationAction.run(MethodInvocationAction.java:39)   at org.sonatype.nexus.common.stateguard.StateGuard$GuardImpl.run(StateGuard.java:272)   at org.sonatype.nexus.common.stateguard.GuardedInterceptor.invoke(GuardedInterceptor.java:53)   at org.sonatype.nexus.repository.maven.tasks.RemoveSnapshotsTask.execute(RemoveSnapshotsTask.java:72)   at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)   at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)   at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)   at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)   at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)   at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)   at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)   at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)   at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)   at org.sonatype.nexus.repository.maven.tasks.RemoveSnapshotsTask.execute(RemoveSnapshotsTask.java:61)   at org.sonatype.nexus.repository.RepositoryTaskSupport.execute(RepositoryTaskSupport.java:73)   at org.sonatype.nexus.scheduling.TaskSupport.call(TaskSupport.java:93)   at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.doExecute(QuartzTaskJob.java:145)   at org.sonatype.nexus.quartz.internal.task.QuartzTaskJob.execute(QuartzTaskJob.java:108)   at org.quartz.core.JobRunShell.run(JobRunShell.java:202)   at org.sonatype.nexus.thread.internal.MDCAwareRunnable.run(MDCAwareRunnable.java:40)   at org.apache.shiro.subject.support.SubjectRunnable.doRun(SubjectRunnable.java:120)   at org.apache.shiro.subject.support.SubjectRunnable.run(SubjectRunnable.java:108)   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)   at java.util.concurrent.FutureTask.run(FutureTask.java:266)   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)   at java.lang.Thread.run(Thread.java:748)  Caused by: java.io.EOFException: input contained no data   at org.codehaus.plexus.util.xml.pull.MXParser.fillBuf(MXParser.java:3037)   at org.codehaus.plexus.util.xml.pull.MXParser.more(MXParser.java:3080)   at org.codehaus.plexus.util.xml.pull.MXParser.parseProlog(MXParser.java:1451)   at org.codehaus.plexus.util.xml.pull.MXParser.nextImpl(MXParser.java:1436)   at org.codehaus.plexus.util.xml.pull.MXParser.next(MXParser.java:1131)   at org.apache.maven.artifact.repository.metadata.io.xpp3.MetadataXpp3Reader.read(MetadataXpp3Reader.java:913)   at org.apache.maven.artifact.repository.metadata.io.xpp3.MetadataXpp3Reader.read(MetadataXpp3Reader.java:519)   at org.apache.maven.artifact.repository.metadata.io.xpp3.MetadataXpp3Reader.read(MetadataXpp3Reader.java:548)   at org.sonatype.nexus.repository.maven.internal.MavenModels.readMetadata(MavenModels.java:83)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataUtils.read(MetadataUtils.java:84)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataUpdater.lambda$0(MetadataUpdater.java:93)   at org.sonatype.nexus.transaction.OperationPoint.proceed(OperationPoint.java:64)   at org.sonatype.nexus.transaction.Operations.transactional(Operations.java:196)   at org.sonatype.nexus.transaction.Operations.call(Operations.java:146)   at org.sonatype.nexus.repository.maven.internal.hosted.metadata.MetadataUpdater.update(MetadataUpdater.java:89)   ... 39 common frames omitted  {quote}     """
"NEXUS-19811","Improvement","Blobstore|Documentation|UI",1,"Offline/Misconfigured blob store should be flagged in UI","""Follow on to NEXUS-18103.  When a blob store can't start correctly NXRM now starts correctly and allows reconfiguration.  However, it's not apparent from the UI that the blob store has a problem.  NXRM should flag the blob store in the UI as offline."""
"NEXUS-19858","Bug","Blobstore",2,"No confirmation when promoting a blob store.","""Fire up Nexus 3.16.1 with default configuration, go to """"repository --> blob stores"""".    Click on the default blob store, and then hit """"promote to group"""".    No warning is given, the blob store is immediately promoted.  This operation cannot be undone because the new group blobstore is in use by 7 repositories, and the promoted blobstore is a member of the group.      It is entirely too easy to make this irreversible configuration change by mistake.    *Expected*:  A confirmation should be given before performing an operation which cannot be undone."""
"NEXUS-19891","Bug","Scripting|Yum",0,"Yum repository configured through script API cache not working","""when creating a yum proxy; according to [https://support.sonatype.com/hc/en-us/articles/115010182627-Understanding-Caching-Configuration] setting *maximum metadata age to 0* should make sure that new versions are always found.    I have ran the following tests scenario:   * create a small repository with *my.1.rpm*   * create proxy in nexus   * install using this proxy: works fine   * now add *my.2.rpm* in the repository   * try to update; yum/zypper says no new versions are to be found (not even after forcing refresh,...)    -Note that when I set *maximum metadata age to 1* and I wait one minute, then yum/zypper does find the new package. However I would like to set this to 0 in order to never have to wait a minute. Yet that configuration does not seem to work; the nexus cache is never invalidated somehow.-    *EDIT*    I have been able to pin down the problem more specifically: I create the proxy in nexus using the script API (a major hassle by the way). The repository seems correctly created but doesn't seem to work perfectly well unless I change something manually. The exact same configuration entered manually in the UI works fine.   * API configured repo:   ** I use python to do something similar to this:    *** curl -X POST -u admin:admin123 --header 'Content-Type: application/json' [http://localhost:8081/service/rest/v1/script] -d '\{""""name"""":""""test"""",""""type"""":""""groovy"""",""""content"""":""""repository.createYumProxy('\''test'\'', '\''http://repository:8080/'\'')""""}'   *** curl -X POST -u admin:admin123 --header """"Content-Type: text/plain"""" '[http://127.0.0.1:8081/service/rest/v1/script/test/run']   ** the exact script that I post (more readable here than with all those escaped quotes):   *** repository.createYumProxy('\{name}', '\{url}');   *** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'proxy'.'contentMaxAge' = 0;   *** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'proxy'.'metadataMaxAge' = 0;   *** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'negativeCache'.'timeToLive' = 0;   *** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'cleanup' = ['policyName': null];   * manually configured repo:   ** *create yum (proxy)*   ** name: test   ** url: [http://repository:8080|http://repository:8080/]   ** maximum component age: 0   ** maximum metadata age: 0   ** not found cache TTL: 0   ** *create repository*    I don't see a single difference between the manually configured repository and the one I configured using the script API. Yet the manually configured repository is correctly functional.    *EDIT2*    using the script API again to fetch the repository attributes I found that a manually configured repository was slightly different from the one configured through the API:   * API configured repo:   ** httpclient:[connection:[blocked:false, autoBlock:true]],   proxy:[remoteUrl:http://repository:8080/, contentMaxAge:0, metadataMaxAge:0],    negativeCache:[enabled:true, timeToLive:0],    storage:[blobStoreName:default, strictContentTypeValidation:true]   * manually configured repo:   ** httpclient:[blocked:false, autoBlock:true],   proxy:[remoteUrl:http://repository:8080/, contentMaxAge:0.0, metadataMaxAge:0.0],    routingRules:[routingRuleId:null],    negativeCache:[enabled:true, timeToLive:0.0],    storage:[blobStoreName:default, strictContentTypeValidation:true],    cleanup:[policyName:None]   * so I enhanced my script to create a proxy repository through the API like this:   ** repository.createYumProxy('\{name}', '\{url}')   ** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'httpclient' = repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'httpclient'.'connection'   ** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'routingRules' = ['routingRuleId':'null']   ** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'proxy'.'contentMaxAge' = 0.0   ** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'proxy'.'metadataMaxAge' = 0.0   ** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'negativeCache'.'timeToLive' = 0.0   ** repository.getRepositoryManager().get('\{name}').getConfiguration().getAttributes().'cleanup' = ['policyName':'None']   * now both a manually configured repository give exactly the same response (except for the order of the attributes). Yet still the cache of the repository configured through the API seems never to expire...    *EDIT3:*    **I have attached a complete test scenario to this ticket; running *docker-compose up* ** will run the tests. The tests will fail because the newest version of the rpm cannot be installed.  If you want to inspect and/or run the tests manually; then change the COMMAND in the Dockerfile to sleep; and you can then run the tests manually and inspect the test container."""
"NEXUS-19900","Bug","Cleanup",0,"Cleanup service task failing with NPE","""Cleanup service task fails consistently on our nexus repository. I have tried deleting it and restarting the service to recreate it. It always end up in the same state         Log from the general log    Log from the specific task execution log file    I realize this is not a support channel, but I hope this is something that can help getting the bug we are seeing fixed."""
"NEXUS-19955","Bug","Repository",0,"GroupFacetImpl#isStale should not fail request if CacheInfo is missing","""GroupFacetImpl#isStale should be more lenient in its checking of whether CacheInfo is missing. Instead, it should assume in both group/proxy code that the asset is stale if CacheInfo is missing, and log a warning.    [https://github.com/sonatype/nexus-public/blob/f4316cbb68f776d4af2dafe1aca146c138783e50/components/nexus-repository/src/main/java/org/sonatype/nexus/repository/group/GroupFacetImpl.java#L242]    Example of failed request:  {quote}org.sonatype.nexus.repository.httpbridge.internal.ViewServlet - Failure servicing: GET /repository/xxx/yyy    java.lang.IllegalStateException: Missing: org.sonatype.nexus.repository.cache.CacheInfojava.lang.IllegalStateException: Missing: org.sonatype.nexus.repository.cache.CacheInfo at com.google.common.base.Preconditions.checkState(Preconditions.java:504) at org.sonatype.nexus.common.collect.AttributesMap.require(AttributesMap.java:207) at org.sonatype.nexus.repository.group.GroupFacetImpl.isStale(GroupFacetImpl.java:243) at org.sonatype.nexus.repository.npm.internal.NpmGroupFacet.getFromCache(NpmGroupFacet.java:222) at org.sonatype.nexus.repository.npm.internal.NpmGroupFacet$getFromCache.call(Unknown Source) at org.sonatype.nexus.repository.npm.internal.NpmGroupPackageHandler.buildMergedPackageRoot(NpmGroupPackageHandler.groovy:68)  {quote}  *Update*:  I've attached a patch which fixes this issue for Nexus Repo 3.18.1-01.  To apply this patch, download the attached jar and replace  this file in the installation with it:  {quote}   nexus-3.18.1-01/system/org/sonatype/nexus/nexus-repository/3.18.1-01/nexus-repository-3.18.1-01.jar  {quote}  Then restart Nexus Repo.    The sha1 checksum of the attached jar is:     fc6d69a387e60370aed07d15fb8c6318602351ea"""
"NEXUS-20029","Bug","nxrm3-maven-plugin",5,"nxrm3-maven-plugin does not deploy pom","""When using the plugin with {{mvn install nxrm3:staging-deploy -Dtag=1.2.3}} I see that the actual artifact (jar file) of my project is uploaded to my Nexus repository. However, the {{pom.xml}} of my project is missing in the Nexus repository and it looks like it was not uploaded.    Is this a bug or am I missing something in my configuration (I haven't specified a {{packaging}} type in my pom)?    For reference, I'm using the plugin with the following configuration:  """
"NEXUS-20246","Bug","Backup|Database|Documentation",1,"clarify which databases are exported per NXRM version by task ""Admin - Export databases for backup""","""After updating from Nexus 3.15.2 to version 3.16.1 I discovered that only 4 databases are exported by the task _""""Admin - Export databases for backup""""_:   * security   * acccesslog   * config   * component    But the [help.sonatype|https://help.sonatype.com/repomanager3/backup-and-restore/configure-and-run-the-backup-task] page says that the task should also export databases:   * audit   * analytics         All of the 6 databases were exported by the same task before the update to Nexus 3.16.1.         *For the Nexus 3.16.1 is exporting only 4 databases is an issue or a feautre?*         Task log:     """
"NEXUS-20281","Bug","Blobstore|NPM",5,"""java.io.IOException: Write end dead"" occurs when group members do not use the same blobstore","""When requesting components from a group repository that contains member repos configured with different blob stores, an """"org.sonatype.nexus.blobstore.api.BlobStoreException: java.io.IOException: Write end dead, Cause: Write end dead"""" error can randomly occur causing build/install failure.    *Reproduce (using NPM):*   1. Create 2 (file-based) blob stores e.g. store_A and store_B   2. Create the following repo setup:   NPM_Group (store_A): [NPM_Hosted (store_A), NPM_Proxy (store_B)]    3. Upload some components to the hosted repo.   4. Perform an npm install against the group repo that will request components from both member repos.    *Expected:*   Install completes with all required components served.    *Actual:*   Install fails with """"org.sonatype.nexus.blobstore.api.BlobStoreException: java.io.IOException: Write end dead, Cause: Write end dead"""" e.g.        Issue not observed when repos are using the same blob store."""
"NEXUS-20552","Bug","Scheduled Tasks|Yum",2,"Repair - Reconcile component database from blobstore task dry run option can block download of hosted assets which do not have an owning component","""h4. Problem Summary    Some repository formats allow storing asset records without an owning component. When the Dry Run option of the Repair - Reconcile component database from blobstore task is used, logic is triggered that incorrectly deletes such asset records from the component database. The related blobs are seemingly not deleted from the blobstore ( ie. they are recoverable ) but the assets are no longer downloadable after being deleted from the database.    Recover the incorrectly deleted asset records by running a Repair - Reconcile component database from blobstore task, without the Dry Run option enabled.    h4. Sample Reproduce for Yum Hosted repo    1. Create a yum-hosted repo at zero depth   2. Deploy an rpm file into it. I used [http://mirror.centos.org/centos/7/os/x86_64/Packages/389-ds-base-1.3.8.4-15.el7.x86_64.rpm] and this command:    3. Attempt to download the RPM file at the same path you uploaded it. This should work with 200 response.   4. After 60 seconds)expected), the Browse and Search views show the repodata/* files are generated. Attempt to download these metadata files - it works.   5. Create a Repair - Reconcile component database from blobstore task. *Select the Dry Run option.* Run the task.   6. *Bug 1*: Logging reports that an RPM file would have been restored by the task:    Why would it restore that file? The file downloads fine, is a valid RPM and there are no inconsistencies between blobstore and database.   7. 60 seconds after the reconcile task finishes, the YUM metadata is rebuilt automatically - supposedly from the manipulation which did not happen? ( ie dry run ):    8. Now you are in the following state:   * the RPM blob file is still in the blob store, not marked soft-deleted or changed in any way from when it was first uploaded   * the Browse view only shows os/x86_64/Packages empty node ( normal according to docs after running reconcile )   * the Search view for yum-hosted is empty ( normal according to docs after running reconcile )   * *Bug 2:* attempts to download yum-hosted metadata files or the rpm file all fail with 404    9. Run the Reconcile component database from blobstore task again, this time without the Dry Run option selected. After 60 seconds yum metadata is rebuilt and repodata files and the rpm is downloadable.    The dry run option is essentially acting like it is manipulating the database in the inverse of reconciling.    h4. Expected   - dry run should not trigger any other tasks, or act like it actually did anything other than log what it would do   - reconcile task had no business manipulating a perfectly good and working RPM, even if this was not dry run   - rpms that are not soft-deleted or manipulated visibly should still be downloadable from the yum-hosted repo, regardless of the presence of metadata"""
"NEXUS-20640","Bug","Docker",0,"docker push may fail with blob upload unknown due to race condition","""We have several reports, that in rare cases, docker pushes to NXRM hosted repos may fail with messages similar to:    {quote}  48983fe960c3: Preparing   128dd347f60c: Preparing   e1a204bf4760: Preparing   28f6517e2463: Preparing   d9ff549177a9: Preparing   28f6517e2463: Layer already exists   128dd347f60c: Layer already exists   d9ff549177a9: Layer already exists   e1a204bf4760: Layer already exists   48983fe960c3: Pushed   *blob upload unknown*  ##[error]blob upload unknown   ##[error]/usr/bin/docker failed with return code: 1  {quote}    {noformat:title=3.16.1 nexus.log messages}  2019-07-25 16:47:13,420-0700 WARN  [qtp884065834-20671]  username org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access soft-deleted blob path$nexus-repository-docker/b7654fa4-5b64-4db2-adab-7f5d1159b658/0 (F:\nexus3\blobs\primary blob store\content\directpath\nexus-repository-docker\b7654fa4-5b64-4db2-adab-7f5d1159b658\0.properties), reason: Docker upload cleaned up.  2019-07-25 16:47:13,420-0700 WARN  [qtp884065834-20671]  username org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access soft-deleted blob path$nexus-repository-docker/b7654fa4-5b64-4db2-adab-7f5d1159b658/1564098432951 (F:\nexus3\blobs\primary blob store\content\directpath\nexus-repository-docker\b7654fa4-5b64-4db2-adab-7f5d1159b658\1564098432951.properties), reason: Docker upload cleaned up.  2019-07-25 16:47:13,420-0700 WARN  [qtp884065834-20671]  username org.sonatype.nexus.blobstore.file.FileBlobStore - Attempt to access soft-deleted blob path$nexus-repository-docker/b7654fa4-5b64-4db2-adab-7f5d1159b658/1564098433295 (F:\nexus3\blobs\primary blob store\content\directpath\nexus-repository-docker\b7654fa4-5b64-4db2-adab-7f5d1159b658\1564098433295.properties), reason: Docker upload cleaned up.  2019-07-25 16:47:13,420-0700 WARN  [qtp884065834-20671]  username org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl - Failed to complete upload  java.lang.IllegalStateException: Missing upload with uuid: b7654fa4-5b64-4db2-adab-7f5d1159b658   at org.sonatype.nexus.repository.docker.internal.UploadManagerImpl.ensureGetUpload(UploadManagerImpl.java:126)   at org.sonatype.nexus.repository.docker.internal.UploadManagerImpl.complete(UploadManagerImpl.java:85)   at org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl.completeBlobUpload(DockerHostedFacetImpl.java:585)   at org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl$$EnhancerByGuice$$43e6bc99.CGLIB$completeBlobUpload$15(<generated>)   at org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl$$EnhancerByGuice$$43e6bc99$$FastClassByGuice$$b3a8c7a0.invoke(<generated>)   at com.google.inject.internal.cglib.proxy.$MethodProxy.invokeSuper(MethodProxy.java:228)   at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:76)   at org.sonatype.nexus.transaction.TransactionalWrapper.proceedWithTransaction(TransactionalWrapper.java:56)   at org.sonatype.nexus.transaction.TransactionInterceptor.invoke(TransactionInterceptor.java:54)   at com.google.inject.internal.InterceptorStackCallback$InterceptedMethodInvocation.proceed(InterceptorStackCallback.java:77)   at com.google.inject.internal.InterceptorStackCallback.intercept(InterceptorStackCallback.java:55)   at org.sonatype.nexus.repository.docker.internal.DockerHostedFacetImpl$$EnhancerByGuice$$43e6bc99.completeBlobUpload(<generated>)   at org.sonatype.nexus.repository.docker.internal.DockerHostedFacet$completeBlobUpload$8.call(Unknown Source)   at org.sonatype.nexus.repository.docker.internal.V2Handlers$_closure5.doCall(V2Handlers.groovy:150)   at sun.reflect.GeneratedMethodAccessor423.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:98)   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:264)   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1034)   at groovy.lang.Closure.call(Closure.java:418)   at org.codehaus.groovy.runtime.ConvertedClosure.invokeCustom(ConvertedClosure.java:54)   at org.codehaus.groovy.runtime.ConversionHandler.invoke(ConversionHandler.java:124)   at com.sun.proxy.$Proxy181.handle(Unknown Source)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.storage.UnitOfWorkHandler.handle(UnitOfWorkHandler.java:39)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.security.SecurityHandler.handle(SecurityHandler.java:52)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.view.Context$proceed.call(Unknown Source)   at org.sonatype.nexus.repository.docker.internal.V2Handlers$_closure18.doCall(V2Handlers.groovy:298)   at sun.reflect.GeneratedMethodAccessor128.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:98)   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:264)   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1034)   at groovy.lang.Closure.call(Closure.java:418)   at org.codehaus.groovy.runtime.ConvertedClosure.invokeCustom(ConvertedClosure.java:54)   at org.codehaus.groovy.runtime.ConversionHandler.invoke(ConversionHandler.java:124)   at com.sun.proxy.$Proxy181.handle(Unknown Source)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.view.Context$proceed.call(Unknown Source)   at org.sonatype.nexus.repository.docker.internal.V2Handlers$_closure1.doCall(V2Handlers.groovy:90)   at sun.reflect.GeneratedMethodAccessor127.invoke(Unknown Source)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:498)   at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:98)   at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)   at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:264)   at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1034)   at groovy.lang.Closure.call(Closure.java:418)   at org.codehaus.groovy.runtime.ConvertedClosure.invokeCustom(ConvertedClosure.java:54)   at org.codehaus.groovy.runtime.ConversionHandler.invoke(ConversionHandler.java:124)   at com.sun.proxy.$Proxy181.handle(Unknown Source)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.view.handlers.TimingHandler.handle(TimingHandler.java:46)   at org.sonatype.nexus.repository.view.Context.proceed(Context.java:80)   at org.sonatype.nexus.repository.view.Context.start(Context.java:114)   at org.sonatype.nexus.repository.view.Router.dispatch(Router.java:64)   at org.sonatype.nexus.repository.view.ConfigurableViewFacet.dispatch(ConfigurableViewFacet.java:52)   at org.sonatype.nexus.repository.view.ConfigurableViewFacet.dispatch(ConfigurableViewFacet.java:43)   at org.sonatype.nexus.repository.httpbridge.internal.ViewServlet.dispatchAndSend(ViewServlet.java:212)   at org.sonatype.nexus.repository.httpbridge.internal.ViewServlet.doService(ViewServlet.java:174)   at org.sonatype.nexus.repository.httpbridge.internal.ViewServlet.service(ViewServlet.java:126)   at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)   at com.google.inject.servlet.ServletDefinition.doServiceImpl(ServletDefinition.java:286)   at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:276)   at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:181)   at com.google.inject.servlet.DynamicServletPipeline.service(DynamicServletPipeline.java:71)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:85)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:112)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:61)   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)   at org.apache.shiro.web.servlet.AdviceFilter.executeChain(AdviceFilter.java:108)   at org.apache.shiro.web.servlet.AdviceFilter.doFilterInternal(AdviceFilter.java:137)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at org.apache.shiro.web.servlet.ProxiedFilterChain.doFilter(ProxiedFilterChain.java:66)   at org.apache.shiro.web.servlet.AbstractShiroFilter.executeChain(AbstractShiroFilter.java:449)   at org.sonatype.nexus.security.SecurityFilter.executeChain(SecurityFilter.java:85)   at org.apache.shiro.web.servlet.AbstractShiroFilter$1.call(AbstractShiroFilter.java:365)   at org.apache.shiro.subject.support.SubjectCallable.doCall(SubjectCallable.java:90)   at org.apache.shiro.subject.support.SubjectCallable.call(SubjectCallable.java:83)   at org.apache.shiro.subject.support.DelegatingSubject.execute(DelegatingSubject.java:383)   at org.apache.shiro.web.servlet.AbstractShiroFilter.doFilterInternal(AbstractShiroFilter.java:362)   at org.sonatype.nexus.security.SecurityFilter.doFilterInternal(SecurityFilter.java:101)   at org.apache.shiro.web.servlet.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:125)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.sonatype.nexus.repository.httpbridge.internal.ExhaustRequestFilter.doFilter(ExhaustRequestFilter.java:80)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at com.sonatype.nexus.licensing.internal.LicensingRedirectFilter.doFilter(LicensingRedirectFilter.java:108)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at com.codahale.metrics.servlet.AbstractInstrumentedFilter.doFilter(AbstractInstrumentedFilter.java:112)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.sonatype.nexus.internal.web.ErrorPageFilter.doFilter(ErrorPageFilter.java:68)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.sonatype.nexus.internal.web.EnvironmentFilter.doFilter(EnvironmentFilter.java:101)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at org.sonatype.nexus.internal.web.HeaderPatternFilter.doFilter(HeaderPatternFilter.java:98)   at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:82)   at com.google.inject.servlet.DynamicFilterPipeline.dispatch(DynamicFilterPipeline.java:104)   at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:135)   at org.sonatype.nexus.bootstrap.osgi.DelegatingFilter.doFilter(DelegatingFilter.java:73)   at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1602)   at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)   at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:146)   at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)   at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:257)   at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1588)   at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)   at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)   at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)   at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)   at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1557)   at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)   at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)   at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)   at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:239)   at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)   at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)   at org.eclipse.jetty.server.Server.handle(Server.java:502)   at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)   at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)   at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)   at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)   at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)   at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)   at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)   at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)   at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)   at java.lang.Thread.run(Thread.java:748)  {noformat}      {code:title=3.161 Audit Log entries}  {""""timestamp"""":""""2019-07-25 16:47:13,388-0700"""",""""nodeId"""":""""A15B4527-0B17ADD5-D28C7AC1-EF2D5D16-ADAE52AE"""",""""initiator"""":""""username/172.20.8.6"""",""""domain"""":""""repository.asset"""",""""type"""":""""created"""",""""context"""":""""v2/-/blobs/sha256:a4ff2e7a414c9476f7edcdb69a93ba0f81f975912a7e51db8173784c50b2a7b4"""",""""attributes"""":{""""repository.name"""":""""docker-internal"""",""""format"""":""""docker"""",""""name"""":""""v2/-/blobs/sha256:a4ff2e7a414c9476f7edcdb69a93ba0f81f975912a7e51db8173784c50b2a7b4""""}}    {""""timestamp"""":""""2019-07-25 16:47:13,748-0700"""",""""nodeId"""":""""A15B4527-0B17ADD5-D28C7AC1-EF2D5D16-ADAE52AE"""",""""initiator"""":""""username/172.20.8.6"""",""""domain"""":""""repository.asset"""",""""type"""":""""updated"""",""""context"""":""""v2/-/blobs/sha256:a4ff2e7a414c9476f7edcdb69a93ba0f81f975912a7e51db8173784c50b2a7b4"""",""""attributes"""":{""""repository.name"""":""""docker-internal"""",""""format"""":""""docker"""",""""name"""":""""v2/-/blobs/sha256:a4ff2e7a414c9476f7edcdb69a93ba0f81f975912a7e51db8173784c50b2a7b4""""}}  {code}    h4. Initial Analysis    This appears to occur when two zero byte chunks for two different layers are uploaded to the same image namespace at the exact same time. Predicting when this could happen is near impossible.    h4. Workaround    The only workaround at this time is to repeat the same docker push - in the case of CI builds this is especially troublesome, but has been reported to workaround the initial failure."""
"NEXUS-20708","Bug","Upgrade",0,"Upgrade wizard always prompts to disable anonymous access","""When upgrading to Nexus 3.18.0 from a previously configured version, the upgrade wizard always prompts about enabling anonymous access.  Why is this done? If the instance was previously configured to allow anonymous, we should respect that setting.  Someone who isn't looking closely or doesn't understand could accidentally disable anonymous access. In most organizations, this would result in a ton of builds breaking.    *Expected:* In the wizard, if anonymous access had been enabled prior to the upgrade, then the wizard should default to having anonymous access enabled (and should be disabled if anonymous access had been disabled). For new installs, anonymous access can be defaulted to disabled.         """
"NEXUS-20705","Bug","PyPI",1,"pypi proxy remote simple indexes with absolute URLs are not rewritten correctly causing the pip client to bypass nxrm","""1. Create a PyPi proxy repository to https://pypi.rasa.com  2. Execute an install:      The problem is the pip client tries to access other hosts ( """"Downloading https://pypi.rasa.com/api/package/rasa-x/rasa-x-0.19.5.tar.gz"""" ) to install packages.    Loading this page: http://localhost:8081/repository/rasa/simple/rasa-x    Will return HTML with absolute URLs to other hosts:        h4. Expected    A pip client properly configured to access NXRM PyPi repos should always send requests for packages to the configured index/indexUrl repository.     The simple index that NXRM returns to the client should contain URLs mapped to repository from which it is served.        """
"NEXUS-20861","Bug","NuGet",1,"Deleting a nuget package from a proxy repository removes its version from group level metadata","""Create a proxy repository to [https://www.powershellgallery.com/api/v2/]   Create a nuget group repository """"nuget-group"""" and add the proxy repository into it.    Download:    [http://localhost:8081/repository/nuget-group/FindPackagesById()?id=%27PowerShellGet%27&$skip=0&$top=40]    Observe that version 2.0.0 is in the metadata.    Download:    [http://localhost:8081/repository/nuget-group/PowerShellGet/2.0.0]    Now delete the PowerShell-2.0.0.nupkg repository from the proxy, and download the group level metadata again:    [http://localhost:8081/repository/nuget-group/FindPackagesById()?id=%27PowerShellGet%27&$skip=0&$top=40]    Notice that version 2.0.0 is no longer present in the metadata.  This will make subequent download attempts of that version through the group using NuGet client fail.    Expiring the group level cache fixes this problem.    *Expected*:  Removing a nuget package from a proxy repository should not remove its version from the metadata     """
"NEXUS-20871","Bug","Docker",3,"zero-byte layers uploaded using docker push fail strict content type validation","""Docker push may upload zero-byte layers - *this is normal.* This is allowed according to Docker API and a variation of which was allowed in NEXUS-9847.    When NXRM has Strict Content Type Validation enabled for the hosted Docker repository, then the content type of the layer cannot be interpreted and therefore evaluated by NXRM. The docker push will fail on a zero byte layer and docker push will report an error.    regular log levels reveal:        Failures at TRACE level logging reveals:        The examined file in the tmp directory is zero-bytes and the Content-Length header for the docker push is also zero."""
"NEXUS-20964","Story","helm",2,"[Helm Features] Cleanup policy facet should be added for hosted and proxy Helm repositories","""Accepted criteria:  * User should be able to set up a cleanup policy for hosted/proxy Helm repository"""
"NEXUS-21579","Task","R",3,"[R format] regression test full format","""Regression test full R format when all tickets are finished.    *Technical notes*  * Merged into master of nexus-repository-r  * Merged into master of nexus-internal"""
"NEXUS-22268","Bug","Cleanup|REST",1,"NullPointer appears if run cleanup for hosted created via REST","""*Steps to reproduce:*   1. Create repository using REST e.g. for helm    2. Run cleanup policy task    *Expected:* No errors appear   *Actual results:* NullPointer appears in the log.   """
"NEXUS-22714","Story","Documentation",1,"Update h.s.c docs to make it clear which formats do/don't have HA-C support","""*Acceptance criteria*  * The following section needs updating to include the latest formats [https://help.sonatype.com/repomanager3/high-availability#HighAvailability-KnownIssuesandLimitations |https://help.sonatype.com/repomanager3/high-availability#HighAvailability-KnownIssuesandLimitations ] (Missing formats: Cocoapods, Conda, R, Helm, P2, Conan)  * Add a new column to https://help.sonatype.com/repomanager3/formats that says whether the format has HA-C support or not. (Formats without HA-C: Apt, Go, Cocoapods, Conda, R, Helm, P2, Conan)  """
"NEXUS-23048","Bug","NuGet",1,"Problem proxying NuGet packages hosted by GitHub Packages","""* ,create a new NuGet (proxy) repository   ** name it """"agents-net""""   ** use [https://nuget.pkg.github.com/agents-net/index.json] as Remote storage URL   ** configure authentication with GitHub username / personal access token    => repo is available at [http://SERVERURL/repository/agents-net/|https://serverurl/repository/agents-net/]   * add NuGet package source in Visual Studio   ** [http://SERVERURL/repository/agents-net/index.json|https://serverurl/repository/agents-net/index.json]   * create new project in Visual Studio or use an existing one   ** Manage NuGet Packages for the project   ** select new package source   ** select browse tab   *** check """"include prerelease""""     => VS shows error   * in Output / Package Manager the following error is shown   """"[Agents.Net] Failed to retrieve metadata from source 'http://.../repository/agents-net/v3/query/1?q=&skip=0&take=26&prerelease=true&semVerLevel=2.0.0'.   Response status code does not indicate success: 500 (javax.servlet.ServletException: java.lang.NullPointerException).""""   * Log View shows stacktrace to NullPointerException (see also attached log file)          """
"NEXUS-23352","Bug","conan",0,"Conan integration in 3.22.0 does not handle Header Only packages","""I am unable to upload header only conan pacakges, I get the following error:         Those files are not generaed with header only packages. If I touch them in the package local cache (to create empty ones), the upload works.    I was using the community supported plugin previsously and I never had the problem."""
