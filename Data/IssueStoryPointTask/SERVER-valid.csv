"issuekey","type","components","storypoint","title","description_text"
"SERVER-40758","Task","Testing Infrastructure",1,"Increase the amount of memory available for logical_session_cache_replication* tasks","""There have been several instances where the OOM killer has killed a mongod process when running one of {{logical_session_cache_replication*}} tasks on Enterprise RHEL 6.2. With the {{num_jobs_available}} expansion equal to the number of CPUs, we end up running 4 tests concurrently (each using a 3-node replica set) on the {{rhel62\-small}} distro which is a c4.xlarge (4 CPU, 7.5GiB memory). We should change to use the {{rhel62\-large}} distro and limit the maximum number of resmoke.py jobs to 12. The {{rhel62\-large}} distro is a c4.4xlarge (16 CPU, 30GiB memory), so we'll end up running 12 tests concurrently, but have a larger ratio of available memory to number of concurrent tests."""
"SERVER-40801","Bug","Testing Infrastructure",1,"resmoke.py logs invalid --excludeWithAnyTags command line argument for local execution","""[The {{to_local_args()}} function serializes {{option_value}} as a string|https://github.com/mongodb/mongo/blob/18181d9825ddc62351a6ba94325a38353086248c/buildscripts/resmokelib/parser.py#L450] even when it may actually be a list. The {{\-\-excludeWithAnyTags}} and {{\-\-includeWithAnyTags}} command line options are currently the only two which use {{action=""""append""""}}.        https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_jsCore_compatibility_f202c4c1ba24b9f561e8b11dac5b04fa0eeb4919_19_04_24_14_47_53"""
"SERVER-41170","Improvement","Performance",2,"Run Genny on Microbenchmarks (CBI; etc/perf.yml)","""Compile genny and run and run any workload and send to evergreen the """"legacy"""" perf.json file produced by the genny metrics post-processing (not expanded metrics yet)."""
"SERVER-40868","Improvement","Replication",1,"Log when copying source coll to temporary coll during a renameCollection across dbs","""Currently we don't log that the contents of the source coll have been copied to the temporary coll. Logging that this happened would make it easier to see that the temporary coll corresponds to the source coll."""
"SERVER-40862","Improvement","Replication",1,"Log collection options for createCollection commands","""This involves changing this line https://github.com/mongodb/mongo/blob/8cbbba49935f632e876037f9f2d9eecc779eb96a/src/mongo/db/catalog/database_impl.cpp#L687.     This can be useful to know things like whether the collection was created as a capped collection or not. """
"SERVER-40895","Improvement","Testing Infrastructure",3,"Dynamically generate burn_in_tests for tag validation","""There are certain build variants that are only around to ensure newly added or modified tests are tagged correctly. These variants copy the flags used from other variants and just run 'compile' and 'burn_in_tests'. We could use generate.tasks to dynamically build these variants, which would make them easier to manage and remove some configuration from evergreen.  ----  As a mongo engineer,  I want a script to generate burn_in_tests for testing tagged variants  So there doesn't have to be an explicit evergreen configuration for each one.  ----  AC  * The following build variants are built dynamically and removed from etc/evergreen.yml.  ** ! Enterprise RHEL 6.2 (majority read concern off)  ** ! Linux (No Journal)"""
"SERVER-40924","Improvement","Testing Infrastructure",3,"Add Evergreen task to sanity check fuzzer can parse JavaScript tests","""The mutational (jstestfuzz) fuzzer uses [acorn|https://github.com/acornjs/acorn] to parse the JavaScript tests into an abstract syntax tree. We've had cases where a Server engineer attempts to use newer JavaScript features supported by the version of SpiderMonkey integrated into the mongo shell than the ECMAScript version we've configured acorn to parse the JavaScript as. This is because special handling for these features (e.g. {{class}}) may need to be done to rewrite the generated JavaScript to avoid uncatchable {{SyntaxErrors}} or strict-mode violations.    We should add a {{lint_fuzzer_sanity_patch}} Evergreen task to the Enterprise RHEL 6.2 required builder which takes the contents of [the {{patch_files.txt}} file generated by the """"get modified patch files"""" function|https://github.com/mongodb/mongo/blob/f7a4c4a9632f75996ed607ffc77e2a3cab15ea88/etc/evergreen.yml#L1038-L1057] and calls {{npm run parse\-jstest}} on them. The {{lint_fuzzer_sanity_patch}} task should be declared in the {{requires}} section for the {{compile}} task such that scheduling the {{compile}} task (either implicitly or explicitly) implicitly schedules the {{lint_fuzzer_sanity_patch}} task.    We should also add {{lint_fuzzer_sanity_all}} Evergreen task to the (since removed) TIG Daily Cron build variant which calls {{npm run parse\-jstest}} on the contents of the {{jstests/}} and {{src/mongo/db/modules/enterprise/jstests}} directories. This is to handle how we cannot guarantee all commits to mongodb/mongo repository have a corresponding patch build, nor can we guarantee Evergreen schedules every commit to the mongodb/mongo repository. Having a periodic task (once a day) which checks all JavaScript tests means we don't need complicated logic like the {{burn_in_tests}} task to diff against the files changed since the commit the task last ran against in the mainline."""
"SERVER-40923","Task","Testing Infrastructure",1,"Remove npm test command from ""run jstestfuzz"" Evergreen function","""The """"run jstestfuzz"""" function lives [here|https://github.com/mongodb/mongo/blob/e3796fef68ec17ef475e669cd04193aac506bf58/etc/evergreen.yml#L1803] in the {{etc/evergreen.yml}} project configuration file. With the introduction of the {{jstestfuzz\-self\-tests}} Evergreen project, we shouldn't need to run {{npm test}} before every execution of the fuzzer."""
"SERVER-40922","Task","Testing Infrastructure",1,"Add npm install command to ""run jstestfuzz"" Evergreen function","""The """"run jstestfuzz"""" function lives [here|https://github.com/mongodb/mongo/blob/e3796fef68ec17ef475e669cd04193aac506bf58/etc/evergreen.yml#L1803] in the {{etc/evergreen.yml}} project configuration file. We'd like to remove the vendored copy of the fuzzer's dependencies from the 10gen/jstestfuzz repository. Note that removing the {{node_modules/}} directory is a """"break the world"""" kind of change where older mongodb/mongo commits will fail because the dependencies are no longer present. Until we remove the {{node_modules/}} directory, running {{npm install}} should have no effect because the versions of all the fuzzer's transitive dependencies are pinned by the {{package\-lock.json}} file. We can therefore add the {{npm install}} command now in preparation for removing the {{node_modules/}} directory and break fewer older commits.    This should happen in a {{type=system}} task so that if Artifactory is down, then the Evergreen task turns {color:purple}*purple*{color}. Note that this means it should happen before the {{shell.exec}} command to call {{npm test}} and {{npm run}}."""
"SERVER-41003","Improvement","Testing Infrastructure",2,"When generating suites, don't set repeat-suites if repeat is in options","""In generate_resmoke_suites, there is an option to set repeat_suites to send to resmoke. By default, this is set to 1. If, however, a user tries to add repeat in the resmoke_args, the default will overwrite it. This will not give the user what they want. We could just not set the repeat_suite option if repeat is set [see here|https://github.com/mongodb/mongo/blob/b1a9c9adea89b475fb05660e2a1cad00971e6899/buildscripts/evergreen_generate_resmoke_tasks.py#L294-L295]  ----  As a server engineer,   when I set repeat in the resmoke_args of a task,   I do not want it to be overwritten by the default 1.  ----  AC   * A _gen task in a patch build can have the repeat_suite value set in the resmoke_args and resmoke will use that value."""
"SERVER-41096","Bug","Testing Infrastructure",3,"ContinuousStepdown thread and resmoke runner do not synchronize properly on the ""stepdown permitted file"" and ""stepping down file""","""Before running workload teardowns, the fsm runner's main thread    * [removes the """"stepdown permitted file""""|https://github.com/mongodb/mongo/blob/54cc4d76250719b247080c1195d4b672322d989e/jstests/concurrency/fsm_libs/resmoke_runner.js#L166]   * [waits for the """"stepping down file"""" to not be present.|https://github.com/mongodb/mongo/blob/54cc4d76250719b247080c1195d4b672322d989e/jstests/concurrency/fsm_libs/resmoke_runner.js#L167-L172]      But the continuous stepdown thread does the following:    * [checks for the """"stepdown permitted file""""|https://github.com/mongodb/mongo/blob/54cc4d76250719b247080c1195d4b672322d989e/buildscripts/resmokelib/testing/hooks/stepdown.py#L183]   * on [starting a stepdown round|https://github.com/mongodb/mongo/blob/54cc4d76250719b247080c1195d4b672322d989e/buildscripts/resmokelib/testing/hooks/stepdown.py#L187], [writes the """"stepping down file""""|https://github.com/mongodb/mongo/blob/54cc4d76250719b247080c1195d4b672322d989e/buildscripts/resmokelib/testing/hooks/stepdown.py#L440]   * on completing the stepdown round, [removes the """"stepping down file.""""|https://github.com/mongodb/mongo/blob/54cc4d76250719b247080c1195d4b672322d989e/buildscripts/resmokelib/testing/hooks/stepdown.py#L444]      This allows the following interleaving:    * continuous stepdown thread checks for """"stepdown permitted file"""" and sees it    * fsm runner thread removes """"stepdown permitted file""""    * fsm runner thread checks for """"stepping down file"""" and doesn't see it    * fsm runner thread starts executing a workload's teardown    * continuous stepdown thread starts a stepdown round, which can cause the workload's teardown thread to get a network error|"""
"SERVER-41169","Bug","Testing Infrastructure",1,"Most powercycle testing for Linux was removed from Evergreen","""The {{powercycle\*}} Evergreen tasks were mostly being run on the SSL Ubuntu 14.04 build variant which was removed as part of SERVER-37765."""
"SERVER-41231","Task","Testing Infrastructure",1,"Fix verify_versions_test.js for 4.2","""Change from 4.1 to 4.2 after branching.    See SERVER-35198 as an example."""
"SERVER-41227","Task","Testing Infrastructure",3,"Update multiversion tests following 4.2 branch","""See SERVER-35152 for work done following 4.0 branch."""
"SERVER-41295","New Feature","Testing Infrastructure",2,"Add timeouts to burn_in generated tasks","""Since we do not explicitly set a timeout for burn_in generated tasks, a hung test would not fail until the default timeout is hit. This has lead to some timeouts causing large log files to be written and makes it difficult to access the log files. If a given test has test history, we should be able to get an approximation of how long the test should run for and set timeouts dynamically. The """"generate_resmoke_tasks"""" already does this.   ----  As a server engineer,  I want burn_in generated tasks to timeout if they run for too long,  so that the log file for the test stays a manageable size.  ----  AC:  * Every task generated by burn_in that has a test history, sets a timeout based on that test history."""
"SERVER-41309","New Feature","Testing Infrastructure",3,"Create a commit_queue task in evergreen.yml","""Create a task in evergreen.yml that will run the commit queue. This should use generate.tasks to generate the tasks the commit queue should run.  ----  As a server engineer  I was a commit queue task that runs as part of the commit queue  So that changes I want to merge are validated.    As a DAG engineer  I want to commit queue to be a generating task  So that I can update what tasks are included in the commit queue without disruption.  ----  AC:  * A new task, """"commit_queue"""", is added to evergreen which will generate other tasks that the commit queue should require.  * The commit_queue task should generate a """"lint"""" task on rhel-62 and a """"compile"""" task on all required (!) builders.  * The new tasks should no-op on mainline builds."""
"SERVER-41304","Task","Testing Infrastructure",1,"Update EXIT_CODE_MAP in resmoke.py for Python 3 changes on Windows","""The changes from [python/cpython@f2244ea|https://github.com/python/cpython/commit/f2244eaf9e3148d6270839e9aa7c2ad9752c17ed] as part of https://bugs.python.org/issue20172 changed the {{DWORD}} return value from [the {{GetExitCodeProcess()}} Win32 API|https://docs.microsoft.com/en-us/windows/desktop/api/processthreadsapi/nf-processthreadsapi-getexitcodeprocess] to be correctly interpreted as an {{unsigned long}}. That is to say, the code in Python 2.7 previously did    {code:c}  PyInt_FromLong(exit_code)      ([where """"k"""" means {{unsigned long}}|https://docs.python.org/3/c-api/arg.html#c.Py_BuildValue])."""
"SERVER-41322","Bug","Testing Infrastructure",1,"Cygwin rsync errors with ""No medium found"" during powercycle testing","""    https://evergreen.mongodb.com/task/mongodb_mongo_master_windows_64_2k8_ssl_powercycle_1397d1398b3b9b1723cd9b93de6b345f940a17e8_19_05_21_15_20_22/0"""
"SERVER-41321","Bug","Testing Infrastructure",1,"Stopping 'mongod-powertest' service returns an error on Windows","""    https://evergreen.mongodb.com/task/mongodb_mongo_master_windows_64_2k8_ssl_powercycle_kill_mongod_39413ef58dd1f667728b67c86e1bf09146952242_19_05_16_20_32_34/0"""
"SERVER-41351","Improvement","Storage",1,"Improve error message from failure to obtain lock for storage stats collection","""SERVER-41327 reported a query failure with {{CursorKilled}} error along with the following error in acquiring the lock for storage stats collection:    """"Timed out obtaining lock while trying to gather storage statistics for a slow operation.""""    Though the above message is just a warning, it sounds like this is the cause of the query being killed. Improve the error to remove this confusion and to notify that at worst this would result in the absence of storage statistics from slow operation logs."""
"SERVER-41401","Bug","Testing Infrastructure",1,"patch_files.txt doesn't distinguish between enterprise and community files","""patch_files.txt does not distinguish between enterprise and community files, so if it has a line that looks like: {{src/module/my_feature.cpp}}, it's impossible to tell if it is a file in the enterprise or community repo."""
"SERVER-41393","Bug","Testing Infrastructure",2,"Drives don't come back up on Enterprise Amazon Linux during powercycle testing","""Through experimentation with [~brian.mccarthy], we've found that EBS volumes only come back up with the {{amazon1\-2018\-test}} Evergreen distro if the entry in {{/etc/fstab}} uses the UUID of the drive. The {{ubuntu1604\-powercycle}} Evergreen distro doesn't appear to be impacted. Note that this is also the recommendation in Amazon's documentation as well:    {quote}  To mount an attached EBS volume on every system reboot, add an entry for the device to the /etc/fstab file.    You can use the device name, such as /dev/xvdf, in /etc/fstab, but we recommend using the device's 128-bit universally unique identifier (UUID) instead. Device names can change, but the UUID persists throughout the life of the partition. By using the UUID, you reduce the chances that the system becomes unbootable after a hardware reconfiguration. For more information, see Identifying the EBS Device.    https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html#ebs-mount-after-reboot  {quote}    Based on https://unix.stackexchange.com/a/270216, we can use either the {{blkid -o value -s UUID /dev/xvd...}} or {{lsblk -no UUID /dev/xvd...}} command to get the UUID and write that [in place of {{$device_name}} in the {{mount_drives.sh}} script|https://github.com/mongodb/mongo/blob/da754e6c0490a3ccacd04339f34fafbd878331b4/buildscripts/mount_drives.sh#L155]."""
"SERVER-41390","Improvement","Testing Infrastructure",1,"validate_mongocryptd should not check variants without push","""The validate_mongocryptd script checks that buildvariants that push the mongocryptd binary also add the variant to a list. However, the script currently does this for all buildvariants, including ones that are only for testing and do not contain a push task. This is causing problems with some dynamic variants we are trying to create. The script could also check for a push task.  ----  As a Server Engineer,  I want validate_mongocryptd to check for a push task,  So that variants that do not push mongocryptd do not need to be included in the variant list.  ----  AC:  * Variants the do not contain a push task will not fail to validate_mongocryptd if they are not included in the list of variants."""
"SERVER-41439","Improvement","Testing Infrastructure",2,"Dynamically choose distro for burn_in_tags.py","""We're currently hardcoding the distro for compile (""""rhe162-large"""") but we need this to be dynamic: [https://github.com/mongodb/mongo/commit/ff945d4698dfcc61236537d7a5912ddd1abd9695#diff-2b1323d0ae241e7dacc3d4c913d481d8R99]         As a server engineer,     I want burn_in_tags compile distro to be dynamically selected based on which base variants are provided to the script    So that adding a new variant would pick up the right distro         AC:   * burn_in_tags picks distro based on buildvariant (i.e. a Windows buildvariant will not run compile on rhe162-large)"""
"SERVER-41423","Bug","Testing Infrastructure",1,"""shell: bash"" being set incorrectly for shell.exec commands in etc/evergreen.yml","""The {{shell}} option should be nested under the {{params}} option for the command definition. [The """"setup jstestfuzz"""" function has it as a top-level key for the command definition|https://github.com/mongodb/mongo/blob/182f15f37344118b33419c05820c2753d06191ed/etc/evergreen.yml#L1790-L1795]. We should audit for other command definitions which have {{shell: bash}} in the incorrect location."""
"SERVER-41488","Bug","Testing Infrastructure",1,"Lint drivers_nightly.yml","""YAML linting was added to 4.2, which failed on the new {{drivers_nightly.yml}} file. """
"SERVER-41562","Task","Testing Infrastructure",1,"Add new Evergreen task for the query fuzzer","""The following YAML blob is modeled off of what we do for [the {{aggregation_wildcard_fuzzer_gen}} task|https://github.com/mongodb/mongo/blob/f3d9452220039ba74c68fe58b382a237d4e07ad1/etc/evergreen.yml#L5097-L5110]. The """"generate fuzzer tasks"""" function uses [the {{generate.tasks}} command|https://github.com/evergreen-ci/evergreen/wiki/Project-Commands#generate-tasks] to create separate Evergreen tasks for actually running the fuzzer so that we can take advantage of using multiple Evergreen hosts. The main difference compared to the aggregation and rollback fuzzers is that we'll use {{npm run query\-fuzzer}} as the entry point.    {code:yaml}  ## Standalone fuzzer for checking find and aggregate equivalence ##  - <<: *jstestfuzz_template    name: query_fuzzer_gen    tags: [""""query_fuzzer""""]    commands:    - func: """"generate fuzzer tasks""""      vars:        <<: *jstestfuzz_config_vars        num_files: 5        num_tasks: 10        npm_command: query-fuzzer        resmoke_args: --suites=generational_fuzzer        name: query_fuzzer  {code}    The """".query_fuzzer"""" task selector (aka tag) should be added to the following build variants:  * enterprise-rhel-62-64-bit  * enterprise-rhel-62-64-bit-coverage  * macos  * ubuntu1804-debug-asan  * ubuntu1804-debug-ubsan  * windows-64-2k8-ssl"""
"SERVER-41680","Task","Testing Infrastructure",1,"Propagate ${branch_name} Evergreen expansion to fuzzer invocation","""[The {{npm run}} command for invoking the fuzzer|https://github.com/mongodb/mongo/blob/ce740566543792bfa4402d278a23e5cb4b1a80fe/etc/evergreen.yml#L1913] should include        This way the fuzzer can be aware of how it should generate the files based on the version of the server it is running against."""
"SERVER-41677","Improvement","Performance",2,"perf.json should not call 'json.get_history' directly","""The 'json.get_history' in evergreen will fail if a new task is being added since that task has no history. The """"etc/system_perf.yml"""" file added a workaround for this in SERVER-35207, but """"etc/perf.yml"""" was not updated at that time. We should also do this in """"etc/perf.yml"""" so that newly added tasks do not fail as well.  ----  As a Server Engineer,  I want etc/perf.yml to not use 'json.get_history'  So that I can add new tasks and have them succeed.  ----  AC:   * New tasks added to etc/perf.yml should be able to succeed despite not having any history."""
"SERVER-41708","Bug","Testing Infrastructure",1,"""set up virtualenv"" failure should be a system failure.","""Currently it is marked as a test failure."""
"SERVER-41762","Improvement","Testing Infrastructure",2,"burn_in_tags should not need to generate a compile task","""The burn_in_tags scripts generates build_variants to variants that would not normally run, but we only want to run burn_in_tests on. It currently also generates a compile task for each build_variant it creates. It should not need to do this. It should be able to depend and use the artifacts from an existing build_variant.  ----  As a server engineer,  I want burn_in_tags generated tasks to depend on existing compiles,  So that the compile work is not duplicated.  ----  AC:  * No build_variants generated by burn_in_tags have their own compile task."""
"SERVER-41802","Bug","Testing Infrastructure",2,"generate_resmoke_tasks doesn't apply max_sub_suites option","""When max_sub_suites is set for a given suite, generate_resmoke_tasks does not actual apply it to split up the suites. See [here|https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_display_multi_shard_local_read_write_multi_stmt_txn_jscore_passthrough_patch_259bd089d0265ac510acbe4512eb706cd553562b_5d0173e39ccd4e3b3d8a4ef8_19_06_12_21_52_04##%257B%2522compare%2522%253A%255B%257B%2522hash%2522%253A%2522259bd089d0265ac510acbe4512eb706cd553562b%2522%257D%255D%257D]  ----  As a Server engineer,  I want max_sub_suites to control how much resmoke suites are split up  so that I have a way to prevent suites being split up too much.  ----  AC:  * A generated suite with max_sub_suites set should only be split into the specified number of sub-suites."""
"SERVER-41836","Improvement","Testing Infrastructure",1,"Log thread Id as part of error in FSM tests","""It looks like we don't log the thread id when there is a single unique [stacktrace|https://github.com/mongodb/mongo/blob/2b34b45c83f03354cc88c295cf24aca7fb9418cc/jstests/concurrency/fsm_libs/runner.js#L337-L341].  """
"SERVER-41940","Improvement","Testing Infrastructure",2,"Remove use of evergreen_client library in favor of evergreen.py in burn_in_tests","""The [burn_in_tests script calls out to evergreen|#L32] to get test history and other information about the task running. We recently build a python evergreen [client|https://github.com/evergreen-ci/evergreen.py] and [added it to the burn_in_tests buildscript|https://mongodbcr.appspot.com/461660007]. We should be consistent and use evergreen.py to access the evergreen api everywhere in the burn_in_tests script.  ----  As a mongo engineer   I want burn_in_tests to use a common evergreen client library   so that I don't have to maintain code to connect to evergreen.  ----  AC   * The burn_in_tests script does not use the evergreen client in buildscripts.          Related ticket: https://jira.mongodb.org/browse/SERVER-40893"""
"SERVER-41926","Task","Storage|Upgrade/Downgrade",5,"Enumerate and remove Storage-related FeatureCompatibilityVersion 4.0-dependent code and tests","""The following tasks need to be completed:    1. Create a list of tickets with code and tests to remove, add them to the 4.4 Upgrade/Downgrade Epic, and mark them as """"is depended on by"""" this ticket. This will assist the Upgrade/Downgrade team in tracking progress. If there is an insufficient amount of work to warrant multiple tickets, then the work can be done under this ticket directly.    2. Complete all necessary tickets promptly.    3. Create a ticket identifying Storage-related generic upgrade/downgrade references that the Upgrade/Downgrade team should update now that the 4.0-dependent references have been removed."""
"SERVER-42032","Bug","Testing Infrastructure",1,"mongodb-javascript-stack always fails when running in hang_analyzer.py","""    https://evergreen.mongodb.com/task_log_raw/mongodb_mongo_master_ubuntu1604_replica_sets_auth_1_ubuntu1604_c939010fe98ba0a8affe7d0d30d4e8d57e68242b_19_06_05_00_44_58/0?type=T#L2539    ----    h6. Original description    In gdb, if we're in a frame that does not know about mongo::mozjs:kCurrentScope, then we will not print a javascript stack trace.     This can be especially useful when debugging our integration tests and gdb optimizes variables out of the core dump.     [~max.hirschhorn] figured out that switching the frame in gdb (frame 1) and then running mongodb-javascript-stack will work fine.     Perhaps we can arbitrarily switch frames [here|https://github.com/mongodb/mongo/blob/a351f48ad122ca59ed45e5df877ef398c099c938/buildscripts/gdb/mongo.py#L530-L532] before trying to print the stack trace. """
"SERVER-50085","Task","Testing Infrastructure",2,"Make it easier to correlate mongo process names, ports, PIDs in logs of fixtures started by resmoke","""For test suites whose underlying mongod/s processes are managed by resmoke.py, it can be hard to figure out the mapping between mongo process ports, replica states, and PIDs in the log messages. For example, in the {{replica_sets_jscore_passthrough}} suite, a log line only shows the state of that replica set node:        Ideally, we would have an easy way to know, when looking at any log line, the replica set state (primary, secondary, etc.), the PID, and the port that the mongo process started up on. In sharding suites we also want to know whether the node is a config server, mongoS, etc. Currently, figuring out this information requires one to trace back to the beginning of the logs (which may be in an entirely separate file if the fixture was not restarted recently) and look for startup messages with this information. One thought would be to print out a complete mapping of ports, PIDs, and current replica set states at the beginning of every new test execution. We could also include this info directly in the log message prefix.  """
"SERVER-42075","Improvement","Performance",2,"Add DSI module to perf.yml","""We explicitly git clone mongo-perf and DSI in perf.yml. We should clean it up to use modules for all those things, and review all the module calls."""
"SERVER-42071","Task","Build",2,"notary client errors should not be system-failures","""in cases like this: https://evergreen.mongodb.com/task/mongodb_mongo_v4.0_enterprise_rhel_70_64_bit_push_5f93fc9db3a3475dd2c7543b9f1e1179e6f9065f_19_06_14_13_51_46 notary client errors obscured a different issue in evergreen.    this is a one line change (adding {{type: """"test""""}} on line 2437, the hard part is figuring what kind of error (test=red or setup=lavender) we want this to be. I think it shouldn't be a system failure, as this will probably make it harder to diagnose other issues. """
"SERVER-42094","Improvement","Performance",1,"perf.yml should check out the enterprise module revision from the manifest, not master","""This is a problem if you have changes to performance test with an old merge base. When running a patch build against the {{performance}} project, the system will apply the changes under test against the correct version of the mongodb/mongo repo, but will attempt to compile them against HEAD of the master branch of enterprise modules. This can cause a spurious compile failure if in the interim changes have been merged to enterprise modules which required paired changes in mongodb/mongo."""
"SERVER-42144","Improvement","Testing Infrastructure",3,"Remove use of evergreen /rest/v1 API in favor of evergreen.py in bypass_compile_and_fetch_binaries.py","""The bypass_compile_and_fetch_binaries and burn_in_tags_bypass_compile_and_fetch_binaries scripts call out to the evergreen API to get build ids for a given revision. We recently built a python evergreen client and added it to the burn_in_tests buildscript. We should be consistent and use evergreen.py to access the evergreen api everywhere.    Currently, the scripts call this endpoint directly:  https://evergreen.mongodb.com/rest/v1/projects/<project>/revisions/<revision>    Instead, they can use evergreen.py to call this v2 endpoint:  https://evergreen.mongodb.com/rest/v2/versions/<version>    ------------------------------------------------------    As a mongo engineer  I want bypass_compile_and_fetch_binaries and burn_in_tags_bypass_compile_and_fetch_binaries to use a common evergreen client library  so that I don't have to maintain code to connect to evergreen.    AC  * The bypass_compile_and_fetch_binaries and burn_in_tags_bypass_compile_and_fetch_binaries scripts do not directly call the Evergreen API     ------------------------------------------------------    Related tickets:  * https://jira.mongodb.org/browse/SERVER-40893  * https://jira.mongodb.org/browse/SERVER-41940"""
"SERVER-42136","Task","Testing Infrastructure",1,"Add new Evergreen task for sharded cluster version of the query fuzzer","""* [The existing {{query_fuzzer_gen}} task|https://github.com/mongodb/mongo/blob/1433d75e416e1078bb490ecda04c9e12b1a0ab3d/etc/evergreen.yml#L5203-L5215] should be updated to have {{jstestfuzz_vars: \-\-diffTestingMode standalone}} specified.  * A new {{query_fuzzer_sharded_gen}} task should be added that runs {{npm run query\-fuzzer \-\- \-\-diffTestingMode sharded}}. Note that because we'll tag it with """"query_fuzzer"""" there shouldn't be a need to update the task lists for any build variants explicitly.    {code:yaml}  ## jstestfuzz sharded cluster fuzzer for checking find and aggregate equivalence ##  - <<: *jstestfuzz_template    name: query_fuzzer_sharded_gen    tags: [""""query_fuzzer""""]    commands:    - func: """"generate fuzzer tasks""""      vars:        <<: *jstestfuzz_config_vars        num_files: 5        num_tasks: 10        jstestfuzz_vars: --diffTestingMode sharded        npm_command: query-fuzzer        resmoke_args: --suites=generational_fuzzer        name: query_fuzzer_sharded  {code}"""
"SERVER-42156","Bug","Packaging",3,"Install of mongodb-org-tools 3.2.22 not possible on RHEL 7","""It's not possible to install mongodb-org-tools.x86_64 0:3.2.22-1.el7 on RHEL.           A yum clean all, rm -rf /var/cache/yum/* does not fix the issue.    The exact same problem at SERVER-39005 and SERVER-26564.         It does not matter if I use the 7server or 7 repository."""
"SERVER-42377","Bug","Testing Infrastructure",2,"burn_in_tests looks at incorrect commit to compare against","""I pushed a change that I expected to have a one-off {{burn_in_tests}} failure, but it failed in a few commits (see BF-13954) because the same tests were still being run. Here is the [task|https://evergreen.mongodb.com/task/mongodb_mongo_v4.2_enterprise_rhel_62_64_bit_required_inmem_display_burn_in_tests_9723ffc820396ca6ccf542cd5d1c3518b5d2db12_19_07_11_20_36_15] for the commit. It looks like {{burn_in_tests}} is looking at the wrong commit to compare against to find changed tests."""
"SERVER-42195","Bug","Testing Infrastructure",1,"Stepdown suites fail with Python exception when run with --repeat >1","""[We're attempting to use the same {{FlagBasedStepdownLifecycle}} instance across executions of the test suite|https://github.com/mongodb/mongo/blob/9ae337bd27f7a513df548256400596a6eba4d7a3/buildscripts/resmokelib/testing/hooks/stepdown.py#L70-L84]. This would mean {{FlagBasedStepdownLifecycle.__should_stop == True}} the moment the second execution of the test suite begins. We should instead construct a new {{FlagBasedStepdownLifecycle}} instance when constructing a new {{_StepdownThread}} instance.    """
"SERVER-42228","Bug","Testing Infrastructure",2,"LoggerRuntimeConfigError exceptions can lead to background dbhash thread running until Evergreen task times out","""[If {{TestReport.startTest()}} raises an exception, then {{Job._run_hooks_after_tests()}} won't be called|https://github.com/mongodb/mongo/blob/e6644474d876eb99579101e81d38c363feef07cd/buildscripts/resmokelib/testing/job.py#L198-L222]. For test suites which use the {{CheckReplDBHashInBackground}} hook, this leads to the background thread continuing to spawn mongo shell processes and running the {{run_check_repl_dbhash_background.js}} hook. If logkeeper is overwhelmed, then an {{errors.LoggerRuntimeConfigError}} exception can also occur when attempting to tear the fixture down. This leads the Evergreen task to time out instead of failing with code 75 because the background dbhash check will continue to run while the fixture is still running and resmoke.py's flush thread will therefore never exit.    We don't want to always run the {{after_test()}} method for a hook though. For example, if running a test crashes the server, then we shouldn't attempt to run any data consistency checks because they'll just fail to connect to the downed server."""
"SERVER-42227","Improvement","Testing Infrastructure",1,"Cap how many tasks burn_in_tests will generate","""If burn_in_tests generates too many tests, it can push evergreen to the limit and cause bad slowdowns. We should put a cap on how many tasks burn_in_tests will generate and fail it we want to generate more tasks than that.  ----  As a server engineer,  I want burn_in_test to limit how many tasks are generated  So that it doesn't cause evergreen to slow down.  ----  AC:  * burn_in_tests should not generate more than 1000 tasks."""
"SERVER-42240","Bug","Testing Infrastructure",1,"burn_in_tags_gen tasks should use the binaries from the patch build","""The tasks created from burn_in_tags gen are using binaries from the base commit, not the patch commit.    See comments for more detail."""
"SERVER-42309","Bug","Testing Infrastructure",1,"test_generator should clean up files it creates","""The test_generator tests create several files as they run, but do not clean them up. These files cause problems when running lint locally and should just be cleaned up after the test is run.  ----  As a server engineer,  I want test_generator to clean up the files it creates,  So they don't cause problems when I'm trying to do other things.  ----  AC:   * After running `buildscripts_test`, no extra files are left around."""
"SERVER-42356","Bug","Testing Infrastructure",1,"teardown(finished=True) isn't ever called for the NoOpFixture","""The [flush thread will block forever for the next event |https://github.com/mongodb/mongo/blob/de38a35403c64e2dfe7e9ffc38fb95f9674773b3/buildscripts/resmokelib/logging/flush.py#L106] if there isn't one lined up.    We should make it not wait. One way could be to use the non-blocking version of [scheduler.run()|https://docs.python.org/3/library/sched.html#sched.scheduler.run]"""
"SERVER-42440","Improvement","Testing Infrastructure",2,"burn_in_test should run tasks on the distro they are normally run","""When burn_in_tests runs the tests it discovered, it should run those tests on the distro they are normally run on. Otherwise, tests could fail due to resource constraints that are not normally there.  ----  As a Server Engineer,  I want burn_in_tests to run on their normal distro,  so that I don't spend time investigating failure due to resource constraints.  ----  AC:  * Tasks that normally run on non-default distros run on the same distros during burn_in_tests."""
"SERVER-42452","Bug","Concurrency|Testing Infrastructure",1,"failNonIntentLocksIfWaitNeeded failpoint interrupts lock requests in UninterruptibleLockGuard","""[Interrupting the lock request leads to a {{LockTimeout}} exception|https://github.com/mongodb/mongo/blob/25d5f6a0b01f261e633587013e4ab8116ea2930a/src/mongo/db/concurrency/lock_state.cpp#L905-L912] which is known not to be handled by the C++ code due to the presence of the {{UninterruptibleLockGuard}} and causes the server to abort. This issue was found during the rollback fuzzer where we suspect background thread (e.g. the TTL monitor) was holding an intent lock on the collection and prevented [the collection lock acquisition in {{MultiIndexBlock::cleanUpAfterBuild()}} for a background index build from being acquired immediately|https://github.com/mongodb/mongo/blob/25d5f6a0b01f261e633587013e4ab8116ea2930a/src/mongo/db/catalog/multi_index_block.cpp#L98]."""
"SERVER-42482","Bug","Testing Infrastructure",1,"burn_in_tests needs to take minimum test runs into account for timeouts","""burn_in_tests failed due to timeouts in this [patch|https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_display_burn_in_tests_patch_dae371c478e1a828ac911096d85f94be8e936ef9_5d3f0da056234359d94af31c_19_07_29_15_15_53/0#/%23%257B%2522compare%2522%253A%255B%257B%2522hash%2522%253A%2522dae371c478e1a828ac911096d85f94be8e936ef9%2522%257D%255D%257D#%257B%2522compare%2522%253A%255B%257B%2522hash%2522%253A%2522dae371c478e1a828ac911096d85f94be8e936ef9%2522%257D%255D%257D]. This is because when we calculated the timeout value to use, we did not take into account that a minimum number of executions could be specified (which is 2 by default). The timeout is set based on how much over the 10 minute repeat time we expect the test to be, but in this case, a whole other execution of the test will occur cause it to hit the timeout.  ----  As a server engineer,  I want burn_in_tests not to timeout on tests that have a runtime greater than 10 minutes  so that burn_in_tests can properly validate those tests.  ----  AC:  * burn_in_tests is able to run successfully on tests with runtimes > 10 minutes."""
"SERVER-42575","Improvement","Testing Infrastructure",2,"compiling and running unittests should be a single task","""As part of SERVER-33963, the unittest tests were split up into 2 tasks, one to compile the unittests and one to run the unittests. They were also put in a task group with max hosts of 1 since in order to run the unittests, you need the artifacts generated by compiling them. However, task groups do not have a hard guarantee that later tasks will run on the same host as earlier task. For tasks that can share a setup, task groups work well for saving some time by sharing setup execution, but they provide inconsistent results when sharing artifacts between tasks.     We should switch the compile and run tasks back to be a single task, so that the unittest  task can be more reliable.  ----  As a Server Engineer,  I want compile unittest and run unittest to be in a single task,  So that it will not fail due to the tasks being run on different hosts.  ----  AC:  * compile unittests and run unittests run as a single task. """
"SERVER-42571","New Feature","Testing Infrastructure",2,"Collect Windows event logs on remote machine during powercycle","""We've had many failures since upgrading to Windows Server 2016 where the mongod service fails to start or the process abruptly terminates after having started. The Windows event logs revealed that after {{notmyfault.exe}} is used to crash the virtual machine, the log and data files, or in some cases the mongod.exe executable itself, cannot be opened.    We should collect the recent messages from the Application, Security, and System event logs on Windows using [the {{wevtutil}} utility|https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/wevtutil] in order to have more diagnostics around this issue and for other mysterious ones that will surely come up in the future."""
"SERVER-42615","New Feature","Testing Infrastructure",2,"Run chkdsk command on Windows after each powercycle loop","""We've seen a variety of errors during powercycle testing on Windows after upgrading to Windows Server 2016, none of which are indicative of a MongoDB issue:    * StartService fails with """"The service did not respond to the start or control request in a timely fashion""""  * StartService fails with """"The device is not ready""""  * StartService fails with """"Access is denied""""  * StartService fails with """"Error performing inpage operation""""  * The mongod-powertest service terminates unexpectedly due to not being able to access some file (unnamed by the Application event logs)    We should run [the {{chkdsk}} command|https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/chkdsk] in read-only mode (i.e. without any extra parameters) to see if we can collect diagnostics indicating the NTFS volume is corrupt after using {{notmyfault.exe}} to crash the machine."""
"SERVER-42607","Bug","Testing Infrastructure",2,"add quoting to resmoke's invocation for local usage line","""Currently, there are situations where resmoke's local invocation line cannot be used verbatim; one such example is:    [https://evergreen.mongodb.com/task_log_raw/mongodb_mongo_master_enterprise_rhel_62_64_bit_alt_create_indexes_replica_sets_multi_stmt_txn_stepdown_jscore_passthrough_0_enterprise_rhel_62_64_bit_alt_create_indexes_cc1a75e4a6d8de8478e7253da7bd6376052d57a6_19_07_15_15_35_13/0?type=T#L392]         Could resmoke quote the line so that it would work with standard Bash?  I'm not sure how hard it would be to figure out where quotes would need to go.  (One simple way to do this would be to simply add double quotes around every parameter.)"""
"SERVER-42623","Bug","Testing Infrastructure",2,"sched module in Python 3 causes close() event to mistakenly be canceled, leading to resmoke.py hang","""Discovered this issue while investigating SERVER-42356. It is yet another way for {{close()}} to never be called on a {{FixtureLogger}} or {{TestLogger}}. The changes from https://hg.python.org/cpython/rev/d8802b055474 made it so [{{sched.Event}} instances returned by {{sched.scheduler.enter()}} and {{sched.scheduler.enterabs()}} are treated as equal if they have the same (time, priority)|https://github.com/python/cpython/blob/v3.7.0/Lib/sched.py#L36]. [It is therefore possible to remove the wrong event from the list when {{sched.scheduler.cancel()}} is called|https://github.com/python/cpython/blob/v3.7.0/Lib/sched.py#L96]."""
"SERVER-42622","Bug","Testing Infrastructure",1,"resmoke.py doesn't attempt to tear the fixture down if setting it up raises an exception","""Discovered this issue while investigating SERVER-42356. It is yet another way for {{close()}} to never be called on the {{FixtureLogger}}. [{{Job.teardown_fixture()}} won't be called if {{Job.setup_fixture()}} raises an exception|https://github.com/mongodb/mongo/blob/ba434d76511a28336d23c0bb2985f5cf8164670a/buildscripts/resmokelib/testing/job.py#L103-L105]."""
"SERVER-42671","Improvement","Testing Infrastructure",1,"_gen task failure due to missing tests should be marked test failures","""When generating tasks for sub-suite execution, a common problem is that a test that is referenced in a resmoke configuration has been moved or deleted. This will cause the generate script to fail, since the referenced file cannot be found. However, the _gen task fails as a system failure in evergreen and those errors often get ignored. We should switch the failure to show up as a test failure to let the developer know that action is required on their part to fix the issue.  ----  As a Server engineer,  I was _gen task failures to show up as test failures  So that I can know to investigate them  ----  AC:  * _gen task failures caused by a missing test being referenced show up as test failures in evergreen."""
"SERVER-42664","New Feature","Shell",2,"Add function to mongo shell for converting BSONObj to Array","""The purpose of this function is to make it possible to convert the object        for sort keys returned by compound sort specifications into the array        which can be meaningfully interacted with via JavaScript. The duplicate empty string field names in the object form are otherwise hidden by the first one. It should be possible to use this function on Objects which have non-duplicate and non-empty field names as well. In those cases, it can be thought of as a function similar to [{{Object.values()}}|https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_objects/Object/values] but one that actually works for {{BSONInfo}} instances.    We can model this function off [the {{bsonWoCompare()}} and {{bsonBinaryEqual()}} functions|https://github.com/mongodb/mongo/blob/b8602c086ff469967bedc82b14d63d4a236d092c/src/mongo/scripting/mozjs/bson.cpp#L271-L299] where the mongo shell will define a {{bsonObjToArray()}} global function that uses [{{ObjectWrapper::toBSON()}}|https://github.com/mongodb/mongo/blob/b8602c086ff469967bedc82b14d63d4a236d092c/src/mongo/scripting/mozjs/objectwrapper.cpp#L529] to convert/extract the argument as a {{BSONObj}}. It should then use [{{ValueReader::fromBSONArray()}}|https://github.com/mongodb/mongo/blob/b8602c086ff469967bedc82b14d63d4a236d092c/src/mongo/scripting/mozjs/valuereader.cpp#L230-L246] to convert the {{BSONObj}} into a JavaScript Array."""
"SERVER-42704","Improvement","Testing Infrastructure",1,"Add placeholder task for evergreen commit queue","""Add a placeholder task that currently no-ops for the commit queue. This task should not be included in one of the required build variants.  ----  As a Server engineer  I want to be able to run the commit queue without any real tasks  So that it I can use the commit queue while minimizing the change of colliding with other merges.  ----  AC:  * A single no-op task is available to be part of the commit queue.  * The task is not part of the required builders."""
"SERVER-42913","Improvement","Testing Infrastructure",2,"Use pre_error_fails_task in etc/evergreen.yml","""In Evergreen, 'pre' tasks are tasks that run at the start of all task execution. However, failures in these tasks are silently ignored by default. This can lead to tasks being in different states when they run if any of the 'pre' tasks fail.     Evergreen has an option, [pre_error_fails_task|https://github.com/evergreen-ci/evergreen/wiki/Project-Files#pre-post-and-timeout], that will cause failure in the 'pre' tasks to fail the task execution. Enabling this will allow us to avoid running tasks in a different state than they normally would be.  ----  As a server engineer,  I want 'pre_error_fails_task' to be enabled in the evergreen configuration,  So that I can know my tasks are running in a consistent state.  ----  AC:  * pre_error_fails_task is enabled in etc/evergreen.yml."""
"SERVER-43022","Improvement","Testing Infrastructure",1,"Allow compile to be run independently on rhel 62","""For commit queue, we want to be able to run just """"compile"""" with no other tasks. However, right now, several other tasks are pulled in with compile including burn_in_tests. We should remove this link in order to ensure the commit queue is stable."""
"SERVER-43055","Bug","Testing Infrastructure",2,"Prevent an exception from being thrown when gdb prints a BSONObj with datetimes beyond datetime.MAXYEAR","""The BSON Python package can throw exceptions; the GDB pretty printer allows these exceptions to escape up into gdb/lldb, which can cause them to crash (I'm not sure why).     Here is an example where I managed to get gdb to print a python stack trace and not crash (this is difficult to achieve):      """
"SERVER-43067","Improvement","Testing Infrastructure",2,"Add end to end tests for generating sub-tasks","""Add end to end tests for buildscripts/evergreen_generate_resmoke_suites.py.    ----  As a server engineer,  I want end to end tests for evergreen_generate_resmoke_suites  so that I can makes changes to the scripts without worrying about breaking things.  ----  AC  * At least 1 test executes the main body of evergreen_genreate_resmoke_suites."""
"SERVER-43143","Improvement","Testing Infrastructure",1,"Add timeouts to evergreen lint tasks.","""The lint task in evergreen normally takes around 15 - 20 minutes to complete. Over the last 6 months, the highest runtimes we have seen have been around 35 minutes. About a week ago, however, we saw the lint task get hung and didn't exit until the task timed out. The lint task just uses the default timeouts, so it took over 3 hours before the task actually ended.     Since the lint task is included as part of the commit queue, hangs like this are problematic. They would block the entire queue for a number of hours. To avoid this issue, we should add a more aggressive timeout to the lint task. Something around 40 minutes should be acceptable.    ----  As a server engineer,  I want to lint task to timeout if it runs for too long,  So that I am not waiting on a hung task.  ----  AC  * Lint tasks in evergreen time out if running for more than 40 minutes."""
"SERVER-43153","Improvement","Shell|Testing Infrastructure",3,"Expose pids of spawned processes in the shell","""Expose the existing {{getRunningMongoChildProcessIds}} function as a shell-native so we can run the hang-analyzer on all sub-processes in error scenarios."""
"SERVER-43150","Improvement","Testing Infrastructure",1,"Reduce duration of jstestfuzz_interrupt_replication_flow_control_gen and jstestfuzz_replication_continuous_stepdown_flow_control_gen","""This ticket is about the jstestfuzz_interrupt_replication_flow_control_gen and jstestfuzz_replication_continuous_stepdown_flow_control_gen tasks. I recently ran a patch (https://evergreen.mongodb.com/version/5d6456d461837d02851d7ac8) and noticed that these two tasks took ~1hr (since they take 30-40 mins each and compile takes ~20 mins).    The Targeted Test Selection project hopes to bring down patch build times, and it will run all fuzzer tests as part of it, so it would be great if these tasks could each run in < 20 mins (not including compile time).    [~robert.guo] recommends we reduce the number of generated files for these tasks like we do for some existing fuzzer tasks (https://github.com/mongodb/mongo/blob/ff685d2d6e370594261eccbef8e60b2f7cc61e28/etc/evergreen.yml#L5508-L5520).    ----  As a server engineer, I should be able to run both jstestfuzz_interrupt_replication_flow_control_gen and jstestfuzz_replication_continuous_stepdown_flow_control_gen tasks in under 20 mins, so that the patch build time of all tasks run as part of Targeted Test Selection is less than an hour.  ----    AC  * Running jstestfuzz_interrupt_replication_flow_control_gen and jstestfuzz_replication_continuous_stepdown_flow_control_gen in a patch build takes under 20 mins (not including compile time)."""
"SERVER-43186","Improvement","Testing Infrastructure",2,"Limit the number of tests added to a generated suite","""The """"CleanEveryN"""" test hook gets run every """"N"""" tests. Due to the way tests are run, this could be run against a different test every execution. This means that when we use test runtime to calculate timeouts, we might not properly account for the """"CleanEveryN"""" runtime and set a timeout too short. This is most problematic on suites made up of lots of short running tests. If we had a maximum number of tests per suite we used when dividing the tests up, this would no longer be a problem.  ----  As a server engineer,  I want there to be a maximum number of tests per generated sub-suite,  So that the """"CleanEveryN"""" hook does not cause timeouts.  ----  AC:  * All suites that run the """"CleanEveryN"""" hook set a maximum number of tests per suite."""
"SERVER-43256","New Feature","Internal Code",3,"Fix incorrect uses of assert.soon and make hang-analysis call exit","""A number of places use assert.soon as a retry mechanism (e.g. ssl_test.js), and at least one test (index_delete.js) runs assert.soon in a try/finally block to capture better error messages.    ↑ looks like this:        In the cases where assert.soon fails due to timeout, we want to instead run hang-analysis and exit rather than returning control to the caller.    (Comment from Robert: I think assert.soon almost always fails due to a timeout. What's the reason for exiting in this case?)    Uses of assert.soon are pervasive in {{jstests}} so do a best-effort fix here. Could modify assert.soon in a patch-build to throw immediately and any test that *doesn't* fail is likely using it incorrectly.    If there are more than (say) 3-4 cases of using assert.soon as a retry mechanism, create a helper {{assert.retry}} method (name/location tbd) that has a similar signature to assert.soon but doesn't call hang-analyzer or barf if the callback is never truthy and instead returns a {{[success, error]}} array where {{success}} is the last result of the callback and error is any errors thrown by the callback. (Exact signature tbd depending on how it's used by callers of extant assert.soon.)    Finally, once this is done, modify existing callers of the hang-analyzer (probably just assert.soon) to call exit after running hang-analysis."""
"SERVER-43255","New Feature","Internal Code",2,"Automatically call whatthefailure from assert.soon and friends","""Add logic to {{assert.soon}} to automatically call hang-analysis prior to throwing.    Add an additional optional parameter to {{assert.soon}} which is additional params to pass to hang-analysis js function.    For now do this *in addition to* throwing such that hang-analysis is just a fancy new bonus-feature. Need to do this while some users of assert.soon are using assert.soon as a retry mechanism. A separate ticket will fix all callers of assert.soon"""
"SERVER-43254","New Feature","Internal Code",3,"Hang Analyzer shell integration uses child and peer mongo processes","""This can be done in parallel with any other PM-1546 work.    Create a new {{whatthefailure.js}} javascript file (class) in the shell. For now this will just have a single static method called {{WTF.areOtherMongosDoing(opts)}} (name subject to change). This will shell out to the existing {{hang_analyzer.py}} script via the {{runProgram}} shell built-in.    The {{opts}} parameter has the following fields:    # {{pids}} optional array of pids to pass into hang-analyzer. If not specified will use the pids of child and/or peer mongo processes obtained via TestData or shell built-ins (both added in other tickets)  # {{args}} optional array of strings - additional set of args to pass to hang-analyzer. If not specified will use reasonable defaults (probably just empty). This is a """"private"""" parameter (not documented or required to be backward-compatible) because most users should never need to use/see/set it."""
"SERVER-43253","New Feature","Internal Code|Testing Infrastructure",3,"Resmoke passes pids of peer mongo* process in TestData","""A """"peer"""" {{mongo*}} process is one started by resmoke rather than by the shell itself (via {{_startMongoProgram}}) for a particular test.    Modify resmoke to pass peer {{mongo*}} process PIDs into spawned {{mongo}} (shell) processes via the existing {{TestData}} mechanism."""
"SERVER-43288","Improvement","Testing Infrastructure",2,"Update fallback values for generated tasks","""The cached historic test results have been turned off in evergreen for the past few weeks. This has lead to generated tasks not being able to use runtime to split up the tasks. All tasks have a fallback value to use to split tasks if there isn't historic data, but some of those value are set to 1 and are causing timeouts. For [example|https://evergreen.mongodb.com/task/mongodb_mongo_master_ubuntu1804_debug_asan_display_logical_session_cache_sharding_100ms_refresh_jscore_passthrough_0031fa41177db46789e411895a5bcd33b2847ed5_19_09_04_12_24_50], we should do a pass through all the suites and make sure the fallback values are appropriate.  ----  As a server engineer,  I want the generated task fallback value to be set appropriately  so that tasks are still split even if we can't get historic test results.  ----  AC:  * All """"generate resmoke tasks"""" in etc/evergreen.yml have a fallback_num_sub_suites value set that is not 1."""
"SERVER-43406","Improvement","Testing Infrastructure",2,"Reduce pip logging in tasks","""In most evergreen tasks, we setup a python virtualenv for python scripts to run in. As part of that, we do a `pip install` for the requirements. This writes a lot of information to the logs that is rarely needed. We could pip this output to a file and upload it, that would clean up the logs, but still provide traceability if what python packages have been installed needs to be investigated.  ----  As a Server engineer,  I want pip install of requirements not to write to the evergreen logs,  So that it is easier to find what I'm looking for in the logs.  ----  AC:  * pip install does not write all the installed packages to the log.  * The packages and version installed by pip are still available if needed."""
"SERVER-43608","Improvement","Testing Infrastructure",3,"End to end tests for burn_in_tests","""There was an bug introduced to resmoke that caused burn_in_tests to start failing. We should add some end to end tests for burn_in_tests so that we can catch these type of errors in buildscripts_test.  ----  As a server engineer,  I want to catch errors in burn_in_tests before I commit,  So that I can trust burn_in_tests is running correctly.  ----  AC:  * At least 1 end to end tests for burn_in_tests exists and is run as part of """"buildscripts_test""""."""
"SERVER-43732","Bug","Testing Infrastructure",2,"burn_in_tests did not detect changes in core","""I recently caused a failure due to a new test in the core suite not working in the sharded_collections_jscore_passthrough suite, even though I ran burn_in_tests in a patch build. I felt like burn_in_tests should have caught this."""
"SERVER-43866","Task","Performance",0,"Remove parallel insert task from M60 like sys-perf variant","""This should be a one-line removal of the above task."""
"SERVER-43900","Improvement","Testing Infrastructure",2,"Set max_hosts to 1 for stitch_support_lib_build_and_test and embedded_sdk_build_and_test  task groups","""In BF-11716, the second task in the stitch_support_lib_build_and_test task group (stitch_support_run_tests) is failing because it gets run on a different build variant (and at the same time as, rather than after) it's dependency task (stitch_support_install_tests). We should set max_hosts to 1 on the stitch_support_lib_build_and_test task group so these tasks get run in consecutive order on the same host.    Similarly, in BF-14342, embedded_sdk_install_dev is getting run before it's dependency task (embedded_sdk_build_cdriver). These tasks are run as part of the embedded_sdk_build_and_test task group.    User story:  As a server engineer,  When I run the stitch_support_lib_build_and_test and embedded_sdk_build_and_test task groups, I should know that each task group's tasks will get run in consecutive order on the same host, so that I do not have failures in my build.    AC:  * Tasks within the stitch_support_lib_build_and_test task and the embedded_sdk_build_and_test  task groups should run in consecutive order    BF: https://jira.mongodb.org/browse/BF-11716  """
"SERVER-43956","Bug","Testing Infrastructure",1,"Fix burn_in_tests file path on Windows","""The file paths of generated tasks were recently changed to always use the """"/"""" separator, as generate.task calls always run in bash, even on Windows. This was achieved by auditing and replacing usages of os.path.join.     burn_in_tests.py also uses {{os.path.normpath}}, which needs to be supplemented to output the unix path as well."""
"SERVER-44017","New Feature","Testing Infrastructure",2,"Hang Analyzer Unzips Debug Symbols","""Modify hang_analyzer.py to automatically unzip debug symbols if necessary and if not already unzipped into cwd.    Take the logic from [here|https://github.com/mongodb/mongo/blob/master/etc/evergreen.yml#L818-L840] and port it to python. Once this is ported to python, consider calling the python instead of the shell on these lines.    This is necessary in cases where hang_analyzer.py is called from the shell as a part of failures from assert.soon and friends in SERVER-43254 etc.  """
"SERVER-44009","New Feature","Testing Infrastructure",1,"Upload pip freeze output for sys-perf and microbenchmarks","""In the dsi supported, _run-dsi_ sets up a python virtualenv for python scripts to run in. As part of that, we do a `pip install` for the requirements. We should pip this output to a file and upload it, to  provide traceability about which python packages have been installed if anything needs to be investigated.  ----  As a Server engineer,  I want pip to list / persist the requirements,  So that it is easier to find what I'm looking for in the logs.  ----  AC:  * The packages and version installed by pip are still available if needed."""
"SERVER-44072","Task","Build",5,"Platform Support: Add Enterprise RHEL 8 PPC","""Platform Support: Add Enterprise RHEL 8 PPC"""
"SERVER-44070","Task","Build",5,"Platform Support: Add Community & Enterprise Ubuntu 20.04 x64","""Platform Support: Add Community & Enterprise Ubuntu 20.04 x64"""
"SERVER-44140","Improvement","Testing Infrastructure",2,"Use signal processing without DSI","""As a DAG engineer,  I would like signal processing to be run outside of DSI.    AC:  * performance and sys-perf projects in mongo repo use signal processing directly for detect-changes and detect-outliers"""
"SERVER-44144","Improvement","Testing Infrastructure",2,"Allow commit queue patches to publish to scons cache","""The shared scons cache is only written to on non-patch builds. Since commit queue builds have a high likelihood of being merged into master, it would be valuable to have them write to the cache as well. In particular, the next item in the commit queue could reuse a lot of the artifacts.  ----  As a server engineer,  I want commit queue builds to write to the shared scons cache,  So that future commit queue builds can reuse the artifacts.  ----  AC:  * Build done as part of the commit queue are able to write to the shared scons cache.  ----  The logic for whether the commit queue is read-only or read/write can be found [here|https://github.com/mongodb/mongo/blob/563dc7451690efa475db5feda913098e777471da/buildscripts/generate_compile_expansions_shared_cache.py#L102-L109].    Additionally an expansion was added to tell if a given build is a commit queue build [here|https://jira.mongodb.org/browse/EVG-5877]."""
"SERVER-44254","Improvement","Build",2,"Don't run package tests on 'Enterprise RHEL 7.0 (libunwind)' variant","""We don't create packages for this build variant and therefore don't need to run package tests."""
"SERVER-44294","Improvement","Testing Infrastructure",2,"Cap runtime of generated tasks","""When an engineer tries to repro a test failure, they sometimes add a large {{resmoke_repeat_suites}} number to evergreen.yml. This causes generated tasks to compute a large Evergreen timeout and potentially leaving a host running for a long time.    We should cap the runtime of generated tasks and either error out and inform the user of the max repeat number they can use, or interally reduce the repeat count to a smaller number.    Almost always, if an issue fails to repro after 48 hours, it's unlikely for the repro to happen at all. This can indicate a bug with the way the repro is set up, or something wrong with the machine the original failure occurred on.    AC:  * Fails tasks that we expect to run over the specified time limit.  * Provide a message to the user explaining why that task was failed and what they can do if they want to work around it."""
"SERVER-44312","Improvement","Performance",1,"Specify evergreen auth in performance tests for signal processing","""As a performance engineer  I want signal processing commands to have proper evergreen auth,  so that they can access data from the evergreen api.  ----  AC:  * detect_outliers can access evergreen API data.  * detect_changes can access evergreen API data."""
"SERVER-44338","Improvement","Testing Infrastructure",2,"Validate commit message as part of commit queue process","""As part of the migration to commit queue, pre-commit git hooks are no longer run. One of the hooks that was run validated that the commit message conformed to certain rules. With EVG-6445, we should be able to create a task that runs as part of the commit queue to validate the commit message.  ----  As a server engineer,  I want a commit queue check to validate the commit message  So that I don't know accidentally commit with a bad message.  ----  AC:  * A commit queue task is run that fails if the commit has an invalid message."""
"SERVER-44400","Bug","Testing Infrastructure",1,"evergreen_task_tags uses the wrong option for tasks","""It uses tasks_for_tag_filter, but should use remove_tasks_for_tag_filter."""
"SERVER-44421","Bug","Testing Infrastructure",1,"Populate config values in burn_in_multiversion_gen","""Before generating burn in multiversion tasks, we assert that the number of tests defined in {{etc/evergreen.yml}} that have the [MULTIVERSION_TAG|https://github.com/mongodb/mongo/blob/1bbcedbd0c744c6ad880cbde2f46eb711c5acf20/buildscripts/burn_in_tests.py#L76] equals the number of yaml suite files (living in {{buildscripts/resmokeconfig/suites}}) with the [BURN_IN_CONFIG_KEY|https://github.com/mongodb/mongo/blob/1bbcedbd0c744c6ad880cbde2f46eb711c5acf20/buildscripts/evergreen_gen_multiversion_tests.py#L43].    [get_named_suites_with_root_level_and_key|https://github.com/mongodb/mongo/blob/1bbcedbd0c744c6ad880cbde2f46eb711c5acf20/buildscripts/resmokelib/suitesconfig.py#L22] is a helper function that requires the config values to be populated before being called. We should make sure we have called {{buildscripts.resmokelib.parser.set_options()}} before we ever make any calls to this helper."""
"SERVER-44537","Task","Testing Infrastructure",1,"Update multiversion platform for windows in 4.4","""In SERVER-33049, we've renamed the windows platform. After cutting MongoDB 4.4, we should update the multiversion platform so the tests can still find it."""
"SERVER-44604","Bug","Testing Infrastructure",1,"Move benchmarks off of Enterprise RHEL 6.2","""The benchmarks are taking up most of the available CBI machines and they don't provide much value in patch builds, we should move them to a different build variant."""
"SERVER-44641","Task","Build",2,"Platform Support: Remove Enterprise RHEL 7 zSeries and SLES 12 zSeries from 3.6","""Remove the following build variants from MongoDB 3.6:    - Enterprise RHEL 7.2 s390x  - Enterprise SLES 12 s390x       """
"SERVER-44632","Task","Build",2,"Platform Support: Remove Community zSeries from 4.2","""Following MongoDB 4.4 GA, we will be removing community zseries support from MongoDB 4.2.   * SSL RHEL 6.7 s390x   * SSL RHEL 7.2 s390x   * SSL SLES 12 s390x   * SSL SLES 15 s390x   * SSL Ubuntu 18.04 s390x"""
"SERVER-44651","Improvement","Performance",1,"Update signal processing version","""Update performance and sys-perf to use latest version of signal processing 1.0.14"""
"SERVER-44727","Bug","Performance",1,"detect-changes should not be called via run-dsi","""The detect-changes script should not be called via run-dsi. It should be called via its own setup script."""
"SERVER-44790","Bug","Replication|Shell",0,"Should not run hang analyzer on shouldFail test in mixed_mode_repl_nossl","""mixed_mode_repl_nossl expects the replsettest to fail.  It does, due to an assert.soon, which runs the hang analyzer.    It seems impractical to plumb the """"shouldRunHangAnalyzer"""" all the way down to the particular assert.soon which fails.  It might make more sense to put the default in TestData.  """
"SERVER-44832","New Feature","Testing Infrastructure",2,"Modify HookTestArchival to reset fixtures","""When archiving files, HookTestArchival should call a sig kill test case to terminate the fixture before archiving. After archiving, it should execute the FixtureSetupTestCase to restart the fixture. Failure on either of these steps should raise a StopExecution exception."""
"SERVER-44831","New Feature","Testing Infrastructure",3,"Create a fixture sigkill test case","""This test case is intended to be executed before archiving begins in resmoke. It should be a subclass of FixtureTestCase and should send a SIGKILL terminating the fixture's processes."""
"SERVER-44874","Bug","Testing Infrastructure",0,"Windows Mongo shell not being included by hang analyzer","""For a particular BF I'm investigating, which has failed on all branches 3.6, 4.0, 4.2, and master, the hang analyzer finds interesting processes for python.exe, mongobridge.exe, and mongod.exe, but not mongo.exe.   For this particular BF, we believe that the hanging process *is* actually the shell, so for this case it would have been very helpful to have the shell process stack traces.  See linked BF for examples.  Note that the first BF does actually have a mongo.exe in the Hang Analysis output, but we believe that it's incorrectly linked as a dup; the other BFG's are the ones to look at here."""
