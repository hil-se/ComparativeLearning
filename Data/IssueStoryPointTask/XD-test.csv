"issuekey","type","components","storypoint","title","description_text"
"XD-2583","Bug","UI",5,"Spring XD Admin UI does not show all the streams","""From Spring XD Shell, running this command """"stream list"""", we counted 30 streams, however Spring XD Admin UI shows only 20. When destroying some streams from Admin UI, the others that was not in the list start appearing. We have not reviewed if there is a configuration parameter that tells how many streams to show in the Admin UI."""
"XD-2593","Bug","UI",3,"Web UI is not displayed","""Web UI management interface does not display the stream  list data or deploy status  js ERROR： definitions.js:28 Uncaught TypeError: undefined is not a function  eg：http://120.27.44.69:9393/admin-ui/#/streams/definitions"""
"XD-2592","Story","YARN Runtime",3,"XD Yarn deployment requires the ability to set permsize","""When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default.   The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source & sink more than 2 times, stream deployment begins to fail.    The only exception that was captured was the following:   Logs are not available at this time.  """
"XD-2597","Story","YARN Runtime",5,"Add an ""xd-yarn info"" command to list admin servers and ports","""As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment.  Best way, for now, would be to add an info command to the xd-yarn script.  With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient."""
"XD-2595","Story","Hadoop|Testing",8,"Test recent Hadoop distro changes","""Test basic functionality (hdfs sink, jdbchdfs job) on hadoop26, hdp22, cdh5, phd21  Test XD on YARN on hadoop26, hdp22, cdh5 and phd21 """
"XD-2594","Story","Hadoop|Packaging",3,"Update spring-data-hadoop version to 2.1.0.RC1","""Update spring-data-hadoop version to 2.1.0.RC1. This also includes updating the following:  - adding hadoop26 (Apache Hadoop 2.6.0) as distro - adding hdp22 (Hortonworks HDP 2.2) as distro - set default distro to hadoop26 - update cdh5 to version 5.3.0 - remove older distros - hadoop24, hdp21 """
"XD-2598","Improvement","Packaging",2,"Update PostgreSQL JDBC Driver Version","""Update the supplied PostgreSQL JDBC Driver to the latest version (9.3-1102 - 2014-07-10), the current supplied version is from 2012.   In our particular use case the latest driver allows use of the JDBC connection.unwrap feature which gives access to the underlying connection from a pooled connection which in turn enables use of the postgres copyManager.copyIn functionality which can speed up batch inserts in a batch process.   See http://jdbc.postgresql.org/documentation/changelog.html  """
"XD-2605","Bug","Stream Module|YARN Runtime",3,"TwitterStream/TwitterSearch sources fail when deploying on Yarn","""We're getting a CNF on org.apache.http.impl.client.HttpClients """
"XD-2601","Bug","Stream Module",1,"Mismatch between configuration class and script XML for location/script","""The *org.springframework.xd.module.options.mixins.ScriptMixin* options class shipped with XD 1.0.3 refers to *script* rather than *location* however the XML configuration still references *$\{location\}* in the service activator:    Creating a stream using the old *location* argument no longer works obviously:    Creating the same stream using *--script* reports success at the shell prompt but results in an error in the container/admin logs:    Working around this by overriding the XML setting in our deployment:  """
"XD-2608","Bug","YARN Runtime",3,"XD Gemfire modules fail to deploy in  Yarn","""1 admin on slave1 1 container on slave2  Gemfire modules fail to deploy.  with the following exception: Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy This is because the modules require a XD_HOME environment variable and this is not set by the yarn deployment.   """
"XD-2610","Bug","Runtime",2,"Job definition is deleted after restart the srping xd service in single node mode","""Job definition is deleted after restart the srping xd service in single node mode  repro step: 1.start service as single node 2.create a batch module 3.create a job based on batch module 4.restart service  expect result: job definition is displayed on the job list  actual result: job list is empty, all job definitions are missed"""
"XD-2616","Bug","Runtime",3,"Ensure that metadata for Kafka message bus is propagated before producing/consuming","""Currently, `ensureTopicCreated` will invoke the creation of the topic on the brokers, however, the calls is not blocking. So, before proceeding, we should make sure that the metadata is readable (therefore propagated)"""
"XD-2638","Bug","Packaging",1,"The shell distribution zip is missing hadoop26 libraries","""The spring-xd-[version]-shell.zip distribution zip doesn't include the lib/hadoop26 directory and libraries, so we get the following exception when starting the shell:  Exception in thread """"main"""" java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration """
"XD-2646","Bug","Hadoop",1,"XD should use same hadoop security keys as Spring for Apache Hadoop","""For kerberos and other security related settings we use keys like 'spring.hadoop.userPrincipal' mentioned in https://github.com/spring-projects/spring-xd/wiki/Hadoop-Kerberos. However when we added boot config props to shdp, we used a sub keys like 'spring.hadoop.security.userPrincipal'.  It'd be good if we'd fix these to be same in both XD and SHDP not to cause confusion."""
"XD-2677","Technical task","Packaging",1,"Remove jline from xd-dirt classpath","""Currently jline 2.11 gets added via zookeeper dependency, we need to remove this so we can have jline 1.0 fir Pig jobs in the hadoop depndencies  This jline version should remain for xd-shell classpath though"""
"XD-2676","Story","Packaging",8,"Resolve classloading issues for custom Hadoop based batch jobs","""There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath."""
"XD-2674","Improvement","Configuration",1,"Provide more options for the MongoDB Sink","""See the SO question on the matter: http://stackoverflow.com/questions/28280206/how-can-i-use-authentication-in-mongo-sink"""
"XD-2689","Technical task","Hadoop",3,"Fix Sqoop job to allow for setting yarn.application.classpath","""Running Sqoop job against non Apache Hadoop installation  - YARN app fails       Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster  - Need to be able to set yarn.application.classpath for any distro that doesn't use the Hadoop defult classpath (Cloudera, Hortonworks, Pivotal HD)"""
"XD-2688","Technical task","Hadoop",1,"Fix mapreduce job submission on Cloudera CDH5","""Submitting jobs that submit YARN MR tasks on Cloudera 5.3.0  - job fails when submitting the YARN app       java.lang.NoClassDefFoundError: com/google/common/io/LimitInputStream  - this is from Guava and that class was removed starting with v. 15.0  - I can get around this by including guava-14.0.1.jar in lib/cdh5 (not sure if this breaks something else) """
"XD-2697","Technical task","Hadoop",5,"Make Sqoop job and MapReduce samples work with Hortonworks HDP 2.2 single-node cluster ","""Having problems testing against the Sandbox 2.2. We need to set the following properties:  yarn.application.classpath yarn.app.mapreduce.am.command-opts mapreduce.application.classpath mapreduce.application.framework.path  """
"XD-2698","Story","Testing",2,"Kafka Tests should use an external broker","""As a developer, I want to have to run Kafka tests on an external broker, so that I reduce the footprint of the build process. """
"XD-2717","Story","Stream Module",1,"Add nameExpression Property to File Sink","""As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file.  Add an alternative {{--nameExpression}} option, allowing complete control over the {{finename-generator-expression}} attribute.  See: http://stackoverflow.com/questions/28466477/issue-with-file-sink-and-filename-expression/28467069#28467069"""
"XD-2723","Story","Batch",1,"Increase the partitionResultsTimeout","""The partitionResultsTimeout is set to 300000 as default (5min). This is way to short for long running steps. We should increase this default."""
"XD-2722","Bug","Batch",5,"Partitioned job throws: java.lang.RuntimeException: Could not serialize lambda","""Running a partitioned jdbchdfs job with 12 partitions and 3 xd-containers. Some steps fail with the jdbc connection pool exception XD-2720. I also sometimes see a serialization exception. This results in the partitioner never getting the status for some of the steps, so it keeps running until it times out even though all steps are either complete of failed.   """
"XD-2721","Improvement","Batch",2,"Remove requirement for executionId to display step execution in shell","""When viewing a job's step execution via the shell, the user is required to provide both the job execution id and the step execution id.  Since the job repository is backed by a database and the step execution id is unique across jobs, the step execution id should be enough."""
"XD-2720","Bug","Batch",5,"Frequent connection pool errors with multi-partitioned jdbchdfs jobs","""I'm running a jdbchdfs job with 8 partitions and 2 containers. Some steps complete ok while some (3-4 on average) fail with a connection pool error (see below). This happens with a decent size table (1.8M rows).  I tried two different databases - Oracle 11g on a separate server and MySQL running locally where the XD containers where running. Same pattern with both databases.  """
"XD-2733","Bug","Stream Module",3,"Custom Modules can't be found wen using xd.customModule.home on windows ","""XD can not find the custom modules directory after Setting the xd.customModule.home in the windows environment   Deployment * xd-singlenode (embedded zookeeper) * Java 8 * Windows 8 or Windows Server 2012 r2  Steps to reproduce:  1) Start xd-singlenode 2) Start Shell 3) Build either the payload-conversion or rss-feed-source from the spring-xd-samples 4) use the shell to execute a module upload for the custom module (rss-feed-source, payload-conversion) 5) verify it uploaded xd:>module info processor:myTupleProcessor 6) stop xd single node 7) From the command line execute set xd.customModule.home=[path to your custom modules] i.e. C:\project\spring-xd-1.1.0.RELEASE\xd\custom-modules 8) restart xd-singlenode 9) execute module info processor:myTupleProcessor 10) you will get the following error """
"XD-2731","Bug","Stream Module",3,"Temp files for stream create not being cleaned","""During testing for Spring XD for PivotalCF we create, deploy, use, undeploy and destroy many streams. Each stream generates {{tmp}} directories (I think 2, one for source, one for sink) in the xd-admin VM's {{/tmp}} directory, e.g.    These {{tmp}} directories are not being cleared up, so our system has hit the inode limit of 32768 files for a volume:    This causes a Java {{IOException}}, the immediately relevant part of which appears to be:    This causes the test system to fail entirely."""
"XD-2751","Bug","Stream Module",1,"JDBC | FILE throws ConverterNotFoundException when split=0","""I am trying to create a simple JDBC|FILE stream with Split=0 at the jdbc source. following is the DSL  stream create --name test --definition """"jdbc --fixedDelay=5 --split=0 --query='select * from top_movie_companies'|file --dir=/tmp --suffix=xd --name=test"""" --deploy  It throws  org.springframework.core.convert.ConverterNotFoundException: No converter found capable of converting from type java.util.ArrayList<?> to type java.lang.String  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:138)  It works fine when I use LOG sink instead of FILE.   I am assuming that if LOG sink works with JDBC then file should be similar. The converter should be registered out of the box.  It could be something basic I am missing as I'm relatively new to XD."""
"XD-2755","Bug","Runtime",5,"Scala processor module executor trims messages","""How to reproduce:  1. Run xd-singlenode (for which setting the Spark master URL to 'local' is a requirement). Use more than 1 worker thread. e.g. {{local[4]}}  2. Deploy the word-count example  3. Create a stream {{stream create spark-streaming-word-count --definition """"http | word-count | log"""" --deploy}}  4. Send data {{xd:>http post --data """"a b c d e f g""""}}  {{xd:>http post --data """"a b c""""}}  5.Observe the result  2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (e,1) 2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (d,1) 2015-02-24 15:12:46,019 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1) 2015-02-24 15:12:46,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (g,1) 2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (a,1) 2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1) 2015-02-24 15:13:40,021 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (c,1)  (the last three results are coming from the second invocation))  Note: there seems to be a correlation between the number of values emitted and the number of workers, as, in all the attempts, there aren't more values emitted than the number of workers."""
"XD-2752","Bug","Configuration|Hadoop|Ingest",8,"SqoopTasklet not using hadoop configuration","""Hey Guys,  I'm trying to use a SqoopTasklet but for some reason it is not getting the hadoop configuration. In the attached sqoop job configuration using the sqooprunner class directly works without problems but the SqoopTasklet is not getting the correct configuration throwing kerberos authentication problems (see singlenode.log).  Could please you guys help me to solve this problem?  Thanks in advance. Regards,"""
"XD-2762","Story","Documentation",2,"Update RHEL/CentOS yum/rpm installation instructions","""As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA.   *Location for 1.1.0 RELEASE:* http://repo.spring.io/libs-release-local/org/springframework/xd/spring-xd/1.1.0.RELEASE/"""
"XD-2761","Story","Runtime",5,"Register only known classes with Kryo in PojoCodec","""Currently PojoCodec calls kryo.register(Class<?> type) on every ser/deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize.  See https://github.com/EsotericSoftware/kryo#registration.  The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus (passed between modules)"""
"XD-2765","Story","Batch",8,"Spike: Research Zookeeper-based mechanism for partitioned job management","""The current implementation of partitioned job management is entirely based on message exchange over the message bus, in a request reply scenario. This creates challenges when it comes to using certain types of transports, as well as acknowledging crashes.  To that effect, the option of using a different partitioned job coordination strategy, that relies on a distributed computing coordination mechanism such as ZooKeeper should be investigated. """
"XD-2769","Bug","Testing",1,"Inconsistent API in AbstractSingleNodeNamedChannelSink ","""In [AbstractSingleNodeNamedChannelSink|https://github.com/spring-projects/spring-xd/blob/6bd17162c8a6da0f09f6f8809f694a060c71ecc0/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/test/sink/AbstractSingleNodeNamedChannelSink.java] the receive() and receivePayload() methods  are non-blocking.  Methods without timeout parameter are usually blocking and return the first message delivered to the channel (e.g. org.springframework.integration.channel.AbstractPollableChannel#receive()).   Integration tests based on spring-xd-test dependency and embedded xd-singlenode are asynchronous. This makes AbstractSingleNodeNamedChannelSink receive method return null in all invocations because test thread is progressing faster than container can process the message in the background.  Would it be possible to make receive methods behave like in AbstractPollableChannel?"""
"XD-2768","Story","Stream Module",3,"Inconsistent Handling of Inherited servers.yml Properties","""Some modules inherit {{application.yml}} / {{servers.yml}} via a properties file in {{/config/modules}} ; others have the values defined in the {{...OptionsMetadata}} classes.  Switch all modules to use the latter technique for consistency."""
"XD-2767","Bug","Stream Module",1,"JMS Source Does Not Expose `acknowledge`","""Since the message-driven adapter uses a {{DMLC}}, the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked).  In order to provide recovery of such situations, the source needs to expose {{acknowledge}} so it can be set to {{transacted}}.  Or, perhaps, given that we don't expose complex configuration, the source should use a {{SimpleMessageListenerContainer}} instead (where the ack is sent after the listener is successfully invoked). """
"XD-2774","Bug","Hadoop",5,"Update to SHDP 2.1.1 for fixing hdfs store writer to recover after error writing to hdfs","""The hdfs sink doesn't recover after error writing to hdfs.  Steps to reproduce -  create a stream using hdfs sink with a small rollover:    stop the datanode(s) and wait for an exception like:    start the datanode(s) again, the sink never recovers and has to be undeployed and redeployed. """
"XD-2779","Bug","Batch|Hadoop",3,"Fix error handling in jdbchdfs job ","""The jdbchdfs job keeps the output stream open in case of error writing to HDFS. We should improve this and close it plus throw an exception.  We should also make sure the step is marked as failed instead of complete when an exception is thrown in the writer."""
"XD-2789","Bug","Stream Module",3,"module delete on windows throws exception","""used module upload for processor:payload-conversion (from XD samples) All worked well until I tried to delete the module. customModule in servers.yml was set to: xd:   customModule:     home: file://c:/project/mymodulehome  StackTrace: """
"XD-2790","Story","Stream Module",1,"Rabbit source and sink mappedRequestHeaders should include all headers by default","""Currently it is necessary to specify mappedRequestHeaders=*  on the rabbit sink, otherwise no headers are mapped to AMQP.  This should be the default behavior."""
"XD-2794","Improvement","Stream Module",5,"Add a MongoDB source","""As a developer, I'd like to add a mongodb source using an xml and a property file supporting mixing in of parameters so that I can use this module to ingest data from Mongo."""
"XD-2806","Bug","Configuration",3,"Module count not respected when label is used","""  ************************************* Works fine without the label: ************************************* """
"XD-2804","Bug","Runtime",1,"Module options are not trimmed","""Spring XD 1.1 container will throw following exception:    when module properties have a trailing whitespace character in type property (in example below there is a trailing space in options.myField.type value):    Can the property values be trimmed before comparing to DefaultModuleOptionsMetadataResolver#SHORT_CLASSNAMES map  to avoid this problem?"""
"XD-2817","Bug","Stream Module",3,"Classpath issues with gemfire-json-server sink","""The GemFire client for SpringXD is throwing java.lang.NoClassDefFoundError for the class com/gemstone/gemfire/cache/client/internal/PingOp after a Stream sinking to gemfire-json-server is destroyed.  Issue starts after destroying a stream, which makes me think we might be unloading the jar files from the classpath while still keeping a connection to the gemfire server.  Steps to reproduce:  1) Create a region in Gemfire to test  e.g.: gfsh>create region --name=Stocks --type=REPLICATE Member  | Status ------- | ------------------------------------- server1 | Region """"/Stocks"""" created on """"server1""""   2) Create a simple stream in Spring XD that writes to that region in gemfire-json-server. Deploy it for single node and let it run for a few seconds.   e.g.:  XD$ stream create streamx --definition """"trigger --fixedDelay=3 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\""""MSFT\"""")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=Stocks --keyExpression=payload.getField('Symbol')"""" --deploy   3)  Destroy the stream  e.g.: XD$  stream destroy streamx  3)  Wait a few seconds and check the xd-singlenode output.. you'll see the exception as following:  [error 2015/03/13 11:04:52.437 PDT  <poolTimer-client-pool-14> tid=0x15a] Unexpected error in pool task <com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask@635c9341> java.lang.NoClassDefFoundError: com/gemstone/gemfire/cache/client/internal/PingOp  at com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask.run2(LiveServerPinger.java:83)  at com.gemstone.gemfire.cache.client.internal.PoolImpl$PoolTask.run(PoolImpl.java:1197)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)  at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)  at com.gemstone.gemfire.internal.ScheduledThreadPoolExecutorWithKeepAlive$DelegatingScheduledFuture.run(ScheduledThreadPoolExecutorWithKeepAlive.java:252)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745)"""
"XD-2816","Story","Documentation",5,"Add 'about section' to module description.","""It should be possible to configure a (short) description for a module that is display above the module options  via {{module info --name ....}}.  The description could contain a few lines describing the core functionality and potentially hyperlinks  to additional information for a module.  This information should be exposed via the REST interface as well.  Currently only the module options are printed."""
"XD-2824","Bug","Hadoop|Stream Module",5,"hdfs sink loses messages/data when container killed","""Scenario running a """"rabbit | hdfs"""" stream and killing the xd-container while stream is running.  Looks like the messages get's acked before the data is flushed to hdfs.  This results in some data lost due to data either in tmp file or cached in the dfs client.  Reference: VESC-387"""
"XD-2823","Story","Runtime",2,"Composite Modules should inherit ""xd.*"" properties","""Currently when modules are composed to a single application context, properties are not inherited.  https://github.com/spring-projects/spring-xd/wiki/Modules#placeholders-available-to-all-modules  """
"XD-2820","Technical task","Runtime",2,"Composing transformer and gemfire-json-server leads to FileNotFoundException during deployment","""Composing """"transform"""" and """"gemfire-json-server"""" modules leads to FileNotFoundException during stream deployment when: - xd-admin and xd-container are started as system services (after installing from RPM).  - xd-singelonde is started outside of $XD_HOME/bin directory e.g.   but it's fully working and exception is *not* thrown when: - xd-singlenode script is started from within """"$XD_HOME/bin directory   Then using the XD Shell:    Stream deployment will result in following exception    Exporting XD_HOME as a global variable seems to have no effect on this behavior."""
"XD-2819","Technical task","Documentation",1,"Broken ""Deployment"" link in docs","""Please see """"Deployment"""" link on http://docs.spring.io/autorepo/docs/spring-xd/1.1.0.RELEASE/reference/html/#_module_deployment page.   !broken-link-deployment.png!  The link is broken and redirects to http://docs.spring.io/autorepo/docs/spring-xd/1.1.0.RELEASE/reference/html/Deployment which is a 404."""
"XD-2827","Story","Configuration|Stream Module",3,"Enable @Value, etc in Module Options Metadata","""A placeholder to investigate what can be done with Spring configuration in Module Options Metadata classes to simplify/enhance property configuration.  With @Configuration modules, these may now be beans in the module context. """
"XD-2825","Story","Runtime",1,"SCS - Verify/Fix AbstractKryoMultitypeCodec implementation","""This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http://stackoverflow.com/questions/3403909/get-generic-type-of-class-at-runtime.  We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException...   """
"XD-2829","Story","Documentation",1,"Add the Dependencies Required to Use #xpath in Streams","""Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring-xml.jar to the classpath (otherwise it is missing XPathException class).   http://stackoverflow.com/questions/29110757/spring-xd-work-with-xml-payload"""
"XD-2837","Bug","Runtime",3,"XD-Admin fails to start","""When starting xd-admin getting the following exception:   Reproduced Locally (mac) and on EC2. xd-singlenode works fine. Commit: 4673b5ab97"""
"XD-2848","Technical task","Performance Testing",3,"Design and budget Perf Env for XD on RackSpace","""Provide design for how we are going to run XD and Kafka on Rackspace.  This includes the base design for the Kafka Perf tests environment. This will be used to provide a budget for the cloud resources  for the performance environment.  """
"XD-2859","Story","UI",1,"UI: Deploy Stream - Return key does not submit form","""*http://localhost:9393/admin-ui/#/streams/definitions/test/deploy*"""
"XD-2855","Improvement","Documentation|REST",2,"Basic security makes xd-shell throw 403 Forbidden error","""After enabling admin endpoint security in servers.yml using basic authentication and single user   Spring XD UI is secured however xd-shell commands are resulting in a 403 error:    This can be fixed by adding configuration explained in """"File based authentication"""" docs section:    Following is the problem: # Configuration explained in """"Single user authentication"""" chapter should work out of the box without additional role setup # Docs should be more clear on authorization"""
"XD-2865","Bug","Runtime",2,"Message Bus: Shut down Kafka Consumers completely before unbinding","""This causes the following exception to be thrown in the log (without functional adverse effects)  org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'unknown.channel.name'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)  at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:43)  at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$AutoAcknowledgingChannelForwardingMessageListener.doOnMessage(KafkaMessageDrivenChannelAdapter.java:172)  at org.springframework.integration.kafka.listener.AbstractDecodingMessageListener.onMessage(AbstractDecodingMessageListener.java:50)  at org.springframework.integration.kafka.listener.QueueingMessageListenerInvoker.run(QueueingMessageListenerInvoker.java:121)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)  at java.lang.Thread.run(Thread.java:745) Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  ... 13 more  """
"XD-2864","Story","Stream Module",2,"JavaConfiguredModule should throw an exception when no @Configuration class is present ","""I had a custom module with a typo: base_packages=base_packages=com.acme.config  The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. """
"XD-2872","Bug","REST",3,"Able to bypass authorization checks by appending "".json"" or "".xml""","""How to reproduce:  1) Enable security 2) Use a user that has the following role only: """"ROLE_CREATE"""" 3) Make a normal REST call:    yields the *desired response*:    Now try:    This produces:  """
"XD-2868","Story","Runtime",5,"Support Partitioned Batch Jobs with a LocalMessageBus","""Initial support for partitioned batch jobs (initially tested with a local bus) had an {{ExecutorChannel}} in the job context to enable multiple partitions to run. Otherwise, with a local bus, only one partition would run at a time.  When further work was done to support other buses, this was removed and the bus was used to control partition concurrency.  The {{LocalMessageBus}} was changed to use an unbounded task executor; this was wrong because now all partitions ran at once.  Further changes to the local bus changed the task executor to be pooled, but with default properties that mean only one thread is used.  Further, the pool configuration is bus-wide so you can't use that configuration to select the concurrency for an individual job.  The bottom line is that the local bus is not suitable for partitioned batch jobs; it was not anticipated that it would be used for this scenario. With 1.0.x too many partitions run (all); with 1.1.x only one thread runs (by default).  In the local bus, we need to use a configurable, dedicated, bounded task executor for each batch job. """
"XD-2877","Improvement","Runtime",8,"Refactor deployment interfaces/class hierarchy","""As a pre-requisite for XD-2835 and a continuation of XD-2671, split apart the concepts of repository and deployment. This will affect the {{ResourceDeployer}} interface and the classes that implement it."""
"XD-2876","Improvement","Runtime",1,"Update Documentation Link","""  This should probably be changed to:  Documentation: http://docs.spring.io/spring-xd/docs/current/reference/html/  """
"XD-2882","Story","Hadoop|Stream Module",3,"Provide an option for hdfs sink to use ""Syncable"" writes","""As a user, I'd like to have an option to have the hdfs sink use """"Syncable"""" writes to provide better resiliency in the case of sink/container failures. I'm willing to accept the performance penalty if I choose this option. """
"XD-2908","Story","Acceptance Testing",5,"Acceptance Tests needs to wait for JobDefinitionResources to be populated ","""After the Introduction to XD-2861 the acquisition of JobResources takes more time.  We have to introduce a pause to wait for getJobDefinitionResource to be populated. """
"XD-2920","Improvement","Runtime",2,"Dynamic router should allow to discard messages","""Currently dynamic router sink has to return a valid queue name. This is problematic when the message should be discarded as part of the routing process. In this case one have to define a stream with {{filter | router}} steps where part of the SpEL is duplicated between {{filter}} and {{router}} modules.  Instead the dynamic router should allow to return null to discard the message and stop further processing. Spring Integration is already providing {{resolution-required}} attribute on {{<router/>}}. """
"XD-2928","Story","Batch",0,"Sqoop module SQL generation issue","""The Sqoop module is generating a SQL statement for --table argument that is not correct for Oracle source.  The Job definition is:  job create sqoop_lookup --definition """"sqoop --command=import --args='--connect=jdbc:oracle:thin:@XXXXXXXXX --driver=oracle.jdbc.OracleDriver --direct --username=********* --password=********* --table=W_LOOKUP_D  --target-dir=/user/zeybeb/ingest/gdw/masterdata/lookup_d --num-mappers=1'"""" --deploy   the --table=W_LOOKUP_D results in Sqoop Object generation:  13:22:59,798 INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM W_LOOKUP_D AS t WHERE 1=0 13:22:59,861 ERROR main manager.SqlManager - Error executing statement: java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended  the SQL shoudl be generate with '<table_name> t' instead of '<table_name> AS t'  The --table argument does not except a schema name. User should be able to provide schema.table_name syntax.  """
"XD-2938","Technical task","Batch",5,"Sqoop - Unable to create job using MERGE command","""As a user, I need to use XD Sqoop module to support the merge command. Currently, the SqoopRunner createFinalArguments method forces the requirement for connect, username and password options which are not valid for the merge option. A check of the module type to not force these options being assigned to sqoop arg list would be preferred"""
"XD-2942","Improvement","Stream Module",2,"Add ftp source to default source modules","""It would be nice to have a simple ftp source. I have to do it for one of my projects. Same as XD-2139 but for source modules."""
"XD-2941","Bug","REST",3,"Failure to get message rates for modules with labels.","""Start XD distributed XD with specified management port and xd:   messageRateMonitoring:     enabled: true in servers.yml to gather stats.  Create stream {{file | log}}, deploy it, navigate to Containers tab in Admin UI. Rates are shown correctly. Create stream {{MYFILE: file | log}}, deploy it, navigate to Containers tab in Admin UI - none of the message rates are shown. Open browser dev tools console and note 500 error response.  spring-xd-dirt -> ContainersController lines 109-112 creates request to get message rates for modules.  Typical request: {{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=log.*,component=*,name=input/MeanSendRate}}  Typical response: {code:json} {""""request"""":{""""mbean"""":""""xd.str4:component=*,module=log.1,name=input"""",""""attribute"""":""""MeanSendRate"""",""""type"""":""""read""""},""""value"""":{""""xd.str4:component=MessageChannel,module=log.1,name=input"""":{""""MeanSendRate"""":0.0}},""""timestamp"""":1428675070,""""status"""":200}   This reponse results in JSONException in the ContainersController because it's missing 'value' property.  The module id is somewhat problematic in the request: {{xd.str4:module=log.*}} index is {{\*}} but should be index within the stream, also node type (source/sink/processor) is missing. Therefore, stream {{mail | mail}} is suffering from the same problem.  Would be nice to have some sort of a bulk request to query more than one module for input/output message rates, such that I could get all message rates for modules in the stream."""
"XD-2939","Bug","Runtime",0,"All Modules are undeployed on Zookeeper Connection Loss / GC Pause","""We are currently running single node mode and experiencing the same problem as described here: http://stackoverflow.com/questions/28170864/spring-xd-jobs-automatic-undeployment-on-zookeeper-time-out-in-xd-singlenode-mo  I've turned on GC logs and can see that there is a 29.7 second GC pause around the time when this happens. We've already set the Zookeeper timeouts (as suggested in the stackoverflow question) - without effect - we can just see, that after the configured timeout the ConnectionLoss errors start to appear.  Sorry for the priorization - for us this currently is a major issue since we are running in singlenode mode (as a starter) and our system goes down once a day. Would this behavior change if we switch to distributed mode ?  I know that a GC pause of 29 secs is really long, however, I've already seen such pauses for batch systems pretty often. Long running jobs tend to move objects to older generations and sometimes there isn't much of a chance to do something against it. So I guess it's worth considering this in the behavior of XD ?"""
"XD-2948","Improvement","Documentation",1,"Document how to specify custom-modules location via Environment variable.","""It is possible to specify the location of custom modules via the environment variable {{XD_CUSTOMMODULE_HOME}} which is provided by Spring Boot property key derivation mechanism (in this case derived from {{xd.customModule.home}}).  This allows a user to specify a custom modules location that survives a complete wipe of spring-xd installations."""
"XD-2949","Story","REST",2,"Error Message for ""Missing Job Description"" needs to be updated","""When using the rest interface to create a Job with an empty description, used to generate the following exception, """"Definition can not be empty"""".   Now generates """"XD112E:(pos 0): Unexpectedly ran out of input^"""".  The correct error should be, """"definition cannot be blank or null""""  """
"XD-2984","Bug","Configuration|Runtime",3,"xd-admin script fails when providing --hadoopDistro option","""XD-2837 added back the --hadoopDistro option for xd-admin scripts. However, if I try to use it I get an error message saying: """"--hadoopDistro"""" is not a valid option """
"XD-3000","Story","Runtime",5,"Enhance TupleCodec performance","""Profile TupleCodec and implement performance optimizations"""
"XD-3022","Bug","Runtime",3,"Kafka Message Bus ignores consumer concurrency when computing partition count","""This is a combination of two issues: - the internal property `next.module.concurrency` is computed from `concurrency` when it should be computed from `consumer.concurrency` - even if `next.module.concurrency` is set, the KafkaMessageBus rejects it, since it's not set in SUPPORTED_CONSUMER_PROPERTIES  As a result, the value used in partition calculation is always 1.  A workaround exists, by setting the `module.[moduleName].producer.minPartitionCount` property to the expected total value. """
"XD-3018","Story","Hadoop|Packaging",2,"Update to spring-data-hadoop 2.2.0.M1","""We should update to use spring-data-hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there (syncable writes, timeout).  A few things to keep in mind: - this updates Cloudera CDH to 5.3.3 - Kite version is now 1.0 - need to test the hdfs-dataset sink """
"XD-3015","Bug","Batch|Hadoop",2,"RemoteFileToHadoopTests fails on 1.1.x","""This error surfaced recently as a result of a fix to a bug in HostNotWindowsRule which disabled this test in all environments. Now the test has been reactivated it is failing on the 1.1.x branch.  The test runs OK on master. """
"XD-3029","Bug","Packaging",2,"SqoopRunner class not found errror ","""We have installed the SpringXD 1.2 M1 release via the rpm and it seems that the sqoop-1.4.5-hadoop200.jar file are not part of the rpm. The sqoop jar file are not in the xd/lib directory.  This is causing a problem during customer module development if we include the sqoop-1.4.5-hadoop200 dependency as part of the pom file and forces us to redeploy the our jar as separate deployment.  Should we be referencing different dependencies or have or should the sqoop-1.4.5-hadoop200.jar be part of the rpm definition so it part of the xd/lib?  I have currently the following dependency in the pom file:    It would be great be great if the sqoop jar are part of rpm so we don't have to do any additional jar deployment.  Thanks, """
"XD-3036","Story","Documentation",1,"Fix section headers in reference TOC","""See: http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#_introduction_26  There should be chapter/section title before this."""
"XD-3048","Bug","REST",1,"RabbitMQ queue cleanup uses wildcard unexpectedly","""Calling the API to delete queues uses a wildcard-like behaviour unexpectedly. If I request to delete:  {{test-1}}  I expect it to delete streams named with the pattern:  {{test-1.*}}  For example, it would delete:  {{test-1.0, test-1.1, etc}}  In fact I believe it wildcards before and after the period, e.g.:  {{test-1*.*}}  And hence would delete:  {{test-1.0, test-11.0, test-123.0, etc}}  That way of working is potentially helpful, but it's also dangerous because it removes the ability to know that you're only deleting the exact queue you want to in all cases.  For the record the commit (https://github.com/spring-projects/spring-xd/commit/2d5f3f706330a6ead8e91c9a7a23d4372715614d) implies that it should work in the more restricted way above, not the less restricted way.  (Note: I've marked this as an improvement because, absent documentation, I don't know what the correct functionality is and hence can't say this is a bug)"""
"XD-3047","Story","Documentation",5,"Complete Camera Ready DEBS submission","""Complete and submit DEBS 2015 paper as described here:  http://www.debs2015.org/camera-ready-instructions.html"""
"XD-3051","Bug","Testing",1,"Gradle launch task is broken","""Spring XD has a gradle task available in the build called launch that starts a single node instance.  This is currently broken.  The command I was using for this command was: """
"XD-3056","Story","Stream Module",8,"Add a new source module to capture video frame from camera or video files","""This is a source module for video ingestion: the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image (encoded with JPEG) as the payload.   """
"XD-3066","Story","CLI|Configuration",3,"Make Enum Conversions for ModuleOptions more lenient","""If you have a an option *--mode=textLine*, presently the enum MUST be named *textLine*.  I think it would improve the user-experience if we allowed users to pass in values such as:  * --mode=textLine * --mode=text_line * --mode=TEXT_LINE  """
"XD-3064","Bug","Batch",3,"HdfsMongoDB Job failing due because of missing ID in Default Tuple","""Looks to have been introduced by https://github.com/spring-projects/spring-xd/pull/1577 Deployment: single admin, 2 container deployment using +RabbitMQ+ as the transport. Below is a partial stacktrace (please check log for full stacktrace). Log is attached. {noformat) 2015-05-15 10:50:15,843 1.2.0.SNAP ERROR xdbus.job:ec2Job3-1 step.AbstractStep - Encountered an error executing step readResourcesStep in job ec2Job3 org.springframework.dao.InvalidDataAccessApiUsageException: Cannot autogenerate id of type java.util.UUID for entity of type org.springframework.xd.tuple.DefaultTuple!         at org.springframework.data.mongodb.core.MongoTemplate.assertUpdateableIdIfNotSet(MongoTemplate.java:1153)         at org.springframework.data.mongodb.core.MongoTemplate.doSave(MongoTemplate.java:882)         at org.springframework.data.mongodb.core.MongoTemplate.save(MongoTemplate.java:837)         at org.springframework.batch.item.data.MongoItemWriter.doWrite(MongoItemWriter.java:128)         at org.springframework.batch.item.data.MongoItemWriter$1.beforeCommit(MongoItemWriter.java:156)         at org.springframework.transaction.support.TransactionSynchronizationUtils.triggerBeforeCommit(TransactionSynchronizationUtils.java:95)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.triggerBeforeCommit(AbstractPlatformTransactionManager.java:928)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:740)         at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:726)         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)         at java.lang.reflect.Method.invoke(Method.java:606) {noformat)"""
"XD-3063","Story","Stream Module",3,"Add Property maxMessagesPerPoll to All Polled Sources","""Polled message sources return only one message per poll by default.  When polling, say, a file directory with many files, files will be emitted once per {{fixedDelay}}.  As a user I need to configure a limit for the number of messages that will be emitted per poll."""
"XD-3070","Story","Runtime",5,"Spike: introduce xolpoc-admin to XD Admin","""The POC for XD on Lattice uses the following interface for module deployment:  https://github.com/markfisher/xolpoc-admin/blob/master/src/main/java/xolpoc/spi/ModuleDeployer.java    This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to: * Demo a POC showing simple stream deployment with the existing shell/admin to Lattice * Learn from the experience to help guide the re-architecture/splitting of stream/job repositories (especially in regard to {{AbstractDeployer}} and related classes).  Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR."""
"XD-3081","Bug","Stream Module",3,"When using file as a source and sink user can not use file sink --mode","""Cluster Type: SingleNode Machine: Mac PR: https://github.com/spring-projects/spring-xd/pull/1624,https://github.com/spring-projects/spring-xd/pull/1626 Stream that reproduces the problem:  Error Message:  Stacktrace: """
"XD-3079","Bug","Hadoop|Ingest|Runtime",5,"Create a new Kerberos ticket instead of renew the current one","""Running Spring-XD singlenode with a kerberized hadoop cluster on CDH 5.3.2. with JDK 1.7 and JCE 1.7. The kerberos ticket policies are: * expiration: 24 hours * renew: 7 days  I need to keep the Spring XD server running constantly because my flows are always waiting for incoming files to be ingested into the HDFS, but the kerberos session expires if there aren't jobs to run before the expiration date. The expiration policies can't be changed due internal company policies.  Is there a way which Spring XD can generate a new ticket instead of renew the current one when a job or stream start executing?  The Spring XD server has configured the hadoop.properties like:  # Use servers.yml to change URI for namenode # You can add additional properties in this file dfs.namenode.kerberos.principal=hdfs/_HOST@EDA.COMPANY.COM yarn.resourcemanager.principal=yarn/_HOST@EDA.COMPANY.COM  yarn.application.classpath=/opt/cloudera/parcels/CDH/lib/hadoop/*,/opt/cloudera/parcels/CDH/lib/hadoop/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/*  hadoop.security.authorization=true hadoop.security.authentication=kerberos  spring.hadoop.userKeytab=file:///export/home/user/user.keytab spring.hadoop.userPrincipal=user@ERS.COMPANY.COM  #Connecting to Kerberized Hadoop (Spring XD doc configuration Appendix D) spring.hadoop.security.authMethod=kerberos spring.hadoop.security.userKeytab=/export/home/user/user.keytab spring.hadoop.security.userPrincipal=user@ERS.COMPANY.COM spring.hadoop.security.namenodePrincipal=hdfs/_HOST@EDA.COMPANY.COM spring.hadoop.security.rmManagerPrincipal=yarn/_HOST@EDA.COMPANY.COM"""
"XD-3078","Bug","Runtime",8,"Spring XD admin fails to redeploy modules after Spring XD container successfully reconnectes to Zookeeper","""We are running Spring XD 1.1.1 in our production environment and Zookeeper 3.4.5.  Zookeeper is running in failover mode and consists of three independent nodes set up on three separate VMs. From time to time we get """"Connection to Zookeeper Suspended"""" event which causes one of the containers in the cluster to be removed from the SpringXD cluster. Modules being deployed on this removed node fail to be re-deployed to other containers in the cluster.  Affected versions: - SpringXD 1.1.1 - Zookeeper 3.4.5 and 3.4.6  Cluster set up in PROD environment where error occurs: - 4 Spring-XD dedicated servers - 4 spring-xd containers (each running on designated server ) - 2 spring-xd admins ( each running alongside one spring-xd container) - 3 Zookeeper nodes ( 3 designated servers on PAITO environment )  Cluster set up in TEST environment where error also occurred: - 2 Spring-XD dedicated servers running one spring-xd container and one spring-xd admin each - 3 Zookeeper nodes running on 3 dedicated servers (PAITO Test environment)  Cluster set up to reproduce error found in PROD environment: - 1 spring-xd admin - 3 spring xd-containers (each running on a designated VM ) - 3 zookeeper servers running on one VM  Steps to reproduce:  1) Set up three node Zookeeper cluster. Attached is example zoo.cfg, we are using default configuration values. In this particular test case we run all Zookeeper nodes on a single VM as we were not testing network layer interruptions. 2) Set up one Spring XD admin node. Please note that we have also observed this on two node Spring XD admin cluster.  3) Set up three Spring XD container nodes. All of them belong to one group (SA) and two of them also belong to second group (HA1). This is configured in $XD_HOME/config/servers.yml however so far group configuration never influenced test outcome. 4) Create and deploy a test stream using following XD Shell commands: stream create --name test-zookeeper-failover --definition """"syslog-udp --port=5140 | transform | file --dir='/opt/pivotal/spring-xd/xd/output'"""" stream deploy --name test-zookeeper-failover --properties """"module.syslog-udp.criteria=groups.contains('HA1'),module.syslog-udp.count=2,module.file.criteria=groups.contains('SA'),module.file.count=3,module.transform.criteria=groups.contains('SA')"""" 5) Ensure that test stream works and handles traffic on UDP port 5140 6) Shutdown one of the Zookeeper nodes by issuing a stop command. 7) Two Spring XD containers were not affected and remained in Spring XD cluster. 8) One Spring XD container was kicked out of Spring XD cluster and was no longer visible on Spring XD admin Web UI. Modules previously deployed to this container were not redeployed to other cluster members. 9) On the failed Spring XD container we have observed CONNECTION_SUSPEND, CONNECTION_RECONECTED and CHILD_REMOVE Zookeeper events (attached is container-log.txt). Please note that Java process is still running and we see “ConnectionStateManager-0 server.ContainerRegistrar - Waiting for supervisor to clean up prior deployments” messages. 10) Spring XD admin failed with exception in DepartingContainerModuleRedeployer (attached is admin-log.txt).  11) We have observed that departing container node in Zookeeper (/sa/deployments/modules/allocated/1d3fd4cc-5a70-47ed-b4f3-22deef1f4d4f/) had no children. We did this after few minutes so we are not sure at which point it was cleared.  12) Restarting failed Spring XD container fixed the problem, modules were correctly redeployed. Exception from point 10 is very similar to XD-1983 and this code was rewritten in XD-2004. """
"XD-3093","Bug","Batch",1,"Sqoop list-tables doesn't work oob","""Commands from docs:  xd:>job create sqoopListTables --definition """"sqoop --command=list-tables"""" --deploy xd:>job launch --name sqoopListTables  2015-05-21 19:12:36,211 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop job for 'list-tables' finished with exit code: 1 2015-05-21 19:12:36,212 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop err: Error: Required argument --connect is missing.  Adding --connect results  xd:>job create sqoopListTables --definition """"sqoop --command=list-tables --connect=jdbc:hsqldb:hsql://localhost:9101/xdjob"""" --deploy Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module sqoop of type job:     connect: option named 'connect' is not supported   This is with singlenode."""
"XD-3092","Story","Runtime",2,"Synchronous deployment/undeployments","""There are a range of issues (such as XD-3083, XD-2671) that are caused by asynchronous deployments issued by the REST API. The flow of events is: * deploy/undeploy request received by REST API * controller queues up request to be processed by supervisor * controller returns HTTP 2xx  This proposal is to have the thread executing the deploy/undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment/undeployment will have been fulfilled. """
"XD-3090","Bug","Acceptance Testing",2,"JdbcHdfsTests sporadically fail","""Acceptance tests sporadically fail after https://github.com/spring-projects/spring-xd/pull/1623 was merged XD-2309.  Additional tests were added but used fixed timeouts.  Will replace them with waitForJob.   """
"XD-3109","Bug","Stream Module",2,"SFTP socket closed error. Infinite loop","""Having the follow messages poping up on xd log. It seems they are being generated indefinitely.   Log files getting huge.   [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: ssh-rsa,ssh-dss [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: aes256-ctr,aes192-ctr,aes128-ctr,arcfour256 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: aes256-ctr,aes192-ctr,aes128-ctr,arcfour256 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: hmac-sha2-512,hmac-sha2-256,hmac-sha1,hmac-ripemd160 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: hmac-sha2-512,hmac-sha2-256,hmac-sha1,hmac-ripemd160 [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: none,zlib@openssh.com [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: none,zlib@openssh.com [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: [2015-05-27 15:57:51.039] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: diffie-hellman-group1-sha1,diffie-hellman-group-exchange-sha1 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: ssh-rsa,ssh-dss [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: aes128-ctr,aes128-cbc,3des-ctr,3des-cbc,blowfish-cbc [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: aes128-ctr,aes128-cbc,3des-ctr,3des-cbc,blowfish-cbc [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: hmac-md5,hmac-sha1,hmac-sha2-256,hmac-sha1-96,hmac-md5-96 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: hmac-md5,hmac-sha1,hmac-sha2-256,hmac-sha1-96,hmac-md5-96 [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client: [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: server->client aes128-ctr hmac-sha1 none [2015-05-27 15:57:51.040] boot - 2774  INFO [task-scheduler-1] --- jsch: kex: client->server aes128-ctr hmac-sha1 none [2015-05-27 15:57:51.044] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_KEXDH_INIT sent [2015-05-27 15:57:51.044] boot - 2774  INFO [task-scheduler-1] --- jsch: expecting SSH_MSG_KEXDH_REPLY [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: ssh_rsa_verify: signature true [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: Host 'XX.XXX.XX.X' is known and mathces the RSA host key [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_NEWKEYS sent [2015-05-27 15:57:51.049] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_NEWKEYS received [2015-05-27 15:57:51.050] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_SERVICE_REQUEST sent [2015-05-27 15:57:51.050] boot - 2774  INFO [task-scheduler-1] --- jsch: SSH_MSG_SERVICE_ACCEPT received [2015-05-27 15:57:51.052] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password [2015-05-27 15:57:51.052] boot - 2774  INFO [task-scheduler-1] --- jsch: Next authentication method: gssapi-with-mic [2015-05-27 15:57:51.054] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentications that can continue: publickey,keyboard-interactive,password [2015-05-27 15:57:51.054] boot - 2774  INFO [task-scheduler-1] --- jsch: Next authentication method: publickey [2015-05-27 15:57:51.086] boot - 2774  INFO [task-scheduler-1] --- jsch: Authentication succeeded (publickey). [2015-05-27 15:57:51.113] boot - 2774  INFO [task-scheduler-1] --- jsch: Disconnecting from 10.100.103.5 port 22 [2015-05-27 15:57:51.113] boot - 2774  INFO [Connect thread XX.XXX.XXX.X session] --- jsch: Caught an exception, leaving main loop due to Socket closed"""
"XD-3102","Story","Performance Testing",8,"Benchmark XD RC1 using Kafka 0.8.2 as transport","""As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases.   Sinks to be included in test: In-Memory Transport > Hdfs sink Direct Binding Transport > Hdfs Sink Kafka > Hdfs Sink"""
"XD-3100","Bug","Runtime",5,"module.*.count > 1 duplicates messages on taps","""Using module.name.count > 1 when deploying taps causes duplication of messages in those modules. This impacts balancing of the containers and modules in a cluster as messages should not be duplicated across modules if the same module is deployed twice to two containers in order to spread the load.  We use taps quite heavily in our project mainly for analytics of the life feed in real time but due to issue we have discovered and described in this bug we are currently facing a limitation where heavily processing modules can not be load balanced across the cluster as they are causing duplication of the messages and therefore the same module deployed to two  containers would still process the same message twice.  To demonstrate the problem please see test case scenario set up below:  h4. 1. Environment  - Spring-XD version 1.1.1-RELEASE - Running two spring-xd containers and one spring-xd admin  h4. 2. Set up  Stream definition is as follows:   {quote}stream create --name test-module-count --definition """"syslog-udp --port=5140 | transform | log"""" stream deploy --name test-module-count --properties """"module.*.count=2"""" stream create --name tap-test-module-count --definition """"tap:stream:test-module-count.syslog-udp > transform --expression='payload.toString() + \""""TAPPED\""""' | log"""" stream deploy --name tap-test-module-count --properties """"module.*.count=2""""{quote}   Please refer to the screen shots attached to see that after deploying those two streams we have:  - streams successfully deployed ( module-count-spring-xd-streams.png ) - streams successfully deployed with count=2 to both containers ( module-count-spring-xd-containers.png )  - 5 queues created in Rabbit ( module-count-rabbit.png ) where two were created for the syslog-udp collector as a result of using module.syslog-udp.count=2 - this is causing messages to be duplicated. Normally the expectation would be to have only one queue for the tap  h4. 3. Test input data  I have sent a very simple UDP message to the listening udp collector running on second container:   {quote}echo test-module-count >> /dev/udp/host02/5140{quote}  h4. 4. Test output data in the logs ( module-count-container01.log and module-count-container02.log )  h5. Expected result:  Below messages logged only on 1 container (it does not matter which one) {quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count}{quote} Below message logged only on one container (it does not matter which one)  {quote}2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}  h5. Actual result:  Stream that has been create as a tap has duplicated the same message and as a result the same message was proccessed twice on both containers by the same module ( transformer ) and logged twice to the console on both containers  Container01: {quote}2015-05-26 14:52:21,143 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}  Container02: {quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count } 2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count }TAPPED{quote}   """
"XD-3133","Bug","Packaging",1,"Update YARN deployment classpath settings for HDP 2.2 and PHD 3.0","""Need to update classpath settings for PHD 3.0 and HDP 2.2 """
"XD-3136","Bug","Hadoop|Runtime",5,"Example hashtag-count MR job fails when running XD on YARN with PHD 3.0","""Running XD on YARN on PHD 3.0 Ambari install.  Uploading and submitting a custom job fails with the following:    Same example jar works fine when submitted from XD cluster."""
"XD-3147","Story","DSL",8,"Composing Jobs via the DSL","""h2. Narrative As a developer, I want to be able to construct jobs using a DSL similar to the current syntax for streams.  h2.  Back story Streams currently provide a DSL for assembling modules into flows (streams) that consist of a source, n processors, and a sink.  While constructing the steps of jobs themselves would be difficult in this manor, creating flows of jobs (essentially a job that consists only of job steps) would be very useful.  It would allow a developer to create something like the following:    This approach also allows the existing packaging/module registry/etc to work out of the box.  This gets us closer to what Oozie provides out of the box without the need to create custom jobs to do the orchestration."""
"XD-3150","Bug","Batch",3,"the 'filepollhdfs' job fails on second submission","""Definitions:  >job create pollHdfs --definition """"filepollhdfs --names=name,age"""" --deploy true  >stream create csvStream --definition """"file --mode=ref --dir=/Users/trisberg/Test/files --pattern=*.csv > queue:job:pollHdfs"""" --deploy  Here is the exception:  """
"XD-3164","Story","Runtime",3,"Kafka bus defaults configurable at producer/consumer level","""As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour.   Such properties should include - autoCommitEnabled,queueSize,maxWait,fetchSize for consumers - batchSize,batchTimeout for producers"""
"XD-3161","Story","Acceptance Testing",3,"Add CI Acceptance Test for 1.2.x","""Need acceptance tests to run on the 1.2.X branch.  Needs to be setup as a child of the Publish 1.2.x"""
"XD-3176","Bug","Configuration|Runtime",3,"Using HDFS for custom module home doesn't work with Kerberized Hadoop cluster","""I tried setting the xd.customModule.home property to point to a Kerberized Hadoop cluster with all usual security config settings provided. It failed with the following exception:  """
"XD-3184","Story","YARN Runtime",1,"Update spring-xd-yarn servers.yml with settings for HDP 2.2.6.0","""We need to add the settings needed to run XD on YARN when using Hortonworks HDP 2.2.6.0 which is the version you now get when installing with Ambari."""
"XD-3189","Story","Acceptance Testing",3,"Testers need ability to wait for a file to be created in XD directory","""User's need ability to wait for user specified time in millis for a file to be created in the XD directory.  If file is not created in allotted time then return false else return true.  Also check to see if a file exists in the XD directory.  """
"XD-3188","Bug","Batch",1,"FileDeletionListener resolves resources once","""In the {{filejdbc}} job, there is the option to delete the imported files.  This functionality is created using a listener called the {{FileDeletionStepExecutionListener}}.  When you run the job the first time with the {{--deleteFiles=true}}, everything works as expected.  The second time you run the job, the files are not deleted.  I believe the issue here is that since the {{FileDeletionStepExecutionListener}} is a singleton, the resources are resolved only once (the first time the job runs) and so it works the first time, but if the job is run again later and new files match the expression, they are not picked up.  I believe the fix is to make the {{FileDeletionStepExecutionListener}} used in this job step scoped."""
"XD-3208","Bug","Stream Module",1,"Change in file source breaks backward compatibility ","""With version 1.2.0 the option ref of the file source was removed and a new option mode was introduced.  see XD-2850 and PR  https://github.com/spring-projects/spring-xd/pull/1624.  This means you have to destroy all streams using the ref option before you do an upgrade.  It would have been much better to leave the ref option in the code and emit a deprecation warning if it is still used. This way an upgrade would be possible without interruption.     """
"XD-3206","Bug","Stream Module",1,"An error message occurs about the shortDescription (header-enricher)","""Here is an error I got using the header-enricher from spring-xd-modules :     And if I look the config properties, indeed, short description doesn't end with a dot. """
"XD-3216","Bug","Runtime",2,"On specific shutdown scenarios, the stream resumes from the start of the bus topic","""https://github.com/spring-projects/spring-xd/issues/1727"""
"XD-3214","Bug","UI",2,"Enabling security breaks Jobs page in Admin UI","""After enabling Spring XD security in {{XD_HOME/config/servers.yml}}:    after logging in as {{user}} with only {{ROLE_VIEW}} privilege, Jobs admin page is broken and is not displaying data. 403 error code is returned for following URLs:    Looks like {{/jobs/configurations.\*}} and {{/jobs/definitions.\*}} URLs are not covered in security section of applications.yml file."""
"XD-3222","Story","Batch|Ingest",3,"Find a way to connect Sqoop job to Teradata","""As a user I would like to connect the Sqoop batch job to Teradata for import jobs.   I have tried the Teradata JDBC driver directly using:    but that results in an NPE.  The only way so far is to use the Hortonworks Connector for Teradata - http://public-repo-1.hortonworks.com/HDP/tools/2.2.4.2/hdp-connector-for-teradata-1.3.4.2.2.4.2-2-distro.tar.gz  That one allows me to use the following:  """
"XD-3234","Story","REST",3,"Remove XML REST Endpoints","""The XML REST endpoints:  * are not working correctly * interfere with security * are not used   """
"XD-3241","Story","Stream Module",1,"Add support for update in gpfdist sink","""Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts."""
"XD-3240","Story","Stream Module",2,"Add better support for using control file with gpfdist","""Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works."""
"XD-3263","Bug","UI",2,"Pagination for containers, it is limited to only 20","""Hi ,  Customer has 48 containers, but it only shows 20 containers. We need pagination to browse all containers."""
"XD-3262","Story","UI",2,"UI: Add Pagination to Containers Page ","""Add Pagination to Containers Page"""
"XD-3261","Story","Runtime",1,"Update Groovy to 2.4.4","""There is a vulnerability in Groovy that is fixed in 2.4.4:  CVE-2015-3253: Remote execution of untrusted code  See:  http://groovy-lang.org/security.html  http://mail-archives.apache.org/mod_mbox/incubator-groovy-users/201507.mbox/%3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt=u=pjH8+g@mail.gmail.com%3E   """
"XD-3266","Bug","UI",2,"No pagination for Jobs / Deployments page in Admin UI","""After successfully deploying 12 jobs the Jobs / Deployments page still shows only 10 results.  It looks like {{http://localhost:9393/jobs/configurations.json?page=0&size=10}} always returns {{content.page.totalPages}} of 1 regardless of the {{size}} parameter."""
"XD-3300","Story","Batch",5,"Spike: Determine best way to centrally configure the job repository for batch jobs.","""h2. Narrative As a developer, I need to be able to run batch jobs that use the centrally configured job repository to store job state.  h2. Back story The XD containers each used a {{BatchConfigurer}} implementation ({{RuntimeBatchConfigurer}}) to add a consistent configuration for the job repository.  This functionality needs to be replicated in some way in just a regular Spring Boot application."""
"XD-3298","Story","Batch",5,"Create basic TaskLauncher","""h2. Narrative As Spring XD, I will be able to launch Spring Boot jar files as Diego Tasks.  h2. Back story The {{TaskLauncher}} will be responsible for listening for launch requests, looking up the definition in the {{TaskDescriptorRepository}}, and launching it.  The first implementation of this would be a Receptor based implementation. The scope here is to produce a _basic_ version of {{TaskLauncher}} and incrementally evolve into comprehensive launch capabilities.  *See:* https://docs.google.com/document/d/1q964adRCA-kJke_i0GBToJHLXJJTV_7TaTpQT0ymsbc/edit"""
"XD-3296","Story","Batch",8,"Spike: Design a tasks repository","""h2. Narrative As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.  h2. Back story Currently Lattice/Diego's tasks implementation provides the ability to run things as short lived tasks.  However, obtaining the result of said task can be an issue.  There are two ways to do so:  # Poll for the result. # Register a callback URL to be called once the task completes.  Since a task is only available for a short time after its completion before it is deleted, polling can run the risk of missing the result completely.  When you consider the fact that the provided GUIDs that identify tasks can be re-used polling becomes a precarious option.  Registering a callback URL would be a better option, however there are no good guarantees that the message will be delivered.  The service will try to execute the callback until it's successful or the task is cleaned up.  """"Successful"""" is defined in this case as anything other than a 502 or a 503 return code.  In order for Spring XD to be able to support Diego tasks, a more durable option for maintaining the result of tasks will need to be developed.  *Note:* The outcome of this spike may be feature requests for the CF/Diego team."""
"XD-3295","Story","Packaging",8,"Spike: Determine options for configuring shared module dependencies","""h2. Narrative As a developer, I'd like to be able to configure common dependencies for the entire environment.  An example could be that I use MySql for my databases.  I want to be able to configure the MySql driver once and have all modules use it.  h2. Back story Spring Batch uses a database to store job state (the job repository).  This is a shared resource across all jobs (both custom developed and OOTB).  In order to support OOTB jobs, we'll need to have a way for users to provide the db driver to each module.  Ideally this would be possible without requiring that each of our OOTB modules be repackaged. """
"XD-3308","Bug","CLI|REST",2,"With Security - Unable to upload module","""Once security is enabled, one cannot upload modules using the shell any longer."""
"XD-3307","Improvement","Stream Module",5,"Add support for offline module resolution","""h2.  Narrarive As a developer, I need to be able to test modules without pushing them to a remote maven repository.  I should be able to do {{$ mvn install}} in my module project locally (which will install the artifact into my local repository) and have it resolvable by spring-cloud-streams."""
"XD-3306","Bug","UI",0,"[Flo] Some streams can't be created using FLO","""Trying to create streams from the flo UI may end up in weird exceptions, whereas doing the same thing (copying/pasting the stream) directly from XD shell works smoothly.  This simple stream is an example, but this situation happens in multiple scenarios (for example using the same module several times with labels).    """
"XD-3335","Bug","Stream Module",3,"Kafka Source must set autoStartup=false on KafkaMessageDrivenChannelAdapter","""If the value is not set, the source may start before being bound to the bus, throwing a """"Dispatcher has no subscribers"""" error"""
"XD-3358","Bug","UI",2,"Admin UI deploys job with wrong module count","""When deploying a job through admin UI with a count of 0 the module is actually deployed with count 1.  More info here: [http://stackoverflow.com/questions/31858631/how-to-define-named-channel-consumer-module-deployment-properties]"""
"XD-3377","Story","DSL",8,"Refactor Task parsing ","""Currently the DSL parsing for tasks is a copy and paste of what it is for streams (minus the ability to parse multiple modules).  This results in a lot of duplication.  This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code."""
"XD-3373","Bug","Batch",5,"First deploy/launch of Pig job that includes yarn-site.xml file fails","""Deploying and launching a Pig job that contains a yarn-site.xml config file fails on the first deploy after XD starts up. This happens consistently.  The error is:    Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster  which indicates that the yarn-site.xml file never made it to the classpath.  Un-deploying and re-deploying the job seems to fix the problem."""
"XD-3385","Bug","Runtime",3,"Can't build and run singlenode spring-cloud-data-rest app on Ubuntu","""Building and then running spring-cloud-data-rest app on Ubuntu fails when trying to create the first stream. The configuration ends up with a CloudFoundryConfig instead of LocalConfig for the moduleDeployer.  Env: Ubuntu 15.04 java version """"1.8.0_51"""" Java(TM) SE Runtime Environment (build 1.8.0_51-b16) Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)  Error: """
"XD-3387","Story","Runtime",2,"Hide the passwords in custom modules from being displayed.","""Hi, Passwords are visibly when using custom modules.  Attached is example custom module code and xd-shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE.   Compile with Maven (mvn clean install) and run xd-shell script (xd-shell --cmdfile ./runme.cmd). """
"XD-3423","Story","CLI",5,"Update Shell to support tasks","""h2. Narrative As a user, I need to be able to deploy a task (boot jar) via the CLI.  h2.  Back story Since the concept of jobs as an explicit primitive within Spring XD is going away in spring-cloud-data, the shell needs to be updated to reflect that."""
"XD-3456","Story","Batch",3,"Create infrastructure for Spring cloud task modules","""Create Parent pom file for build Create .settings file Migrate Timestamp task from SCSM to SCTM. """
"XD-3469","Improvement","Stream Module",3,"The new SCSM twitterstream module should produce same json as old XD source","""The new SCSM twitterstream module uses a different format than XD 1.x source module. It should match what Twitter uses so existing processors etc. will continue to work."""
"XD-3503","Story","REST",2,"Document the setting of the CORS allow_origin property","""We do set a default value in: xd/lib/spring-xd-dirt-1.2.1.RELEASE.jar/application.yml    We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using **servers.yml**"""
"XD-3509","Bug","Configuration",3,"CORS issue when trying to use HTTP in singlenode","""When I'm trying to send a json object to spring-xd I get the following error even though I opened up requests to allow all.   XMLHttpRequest cannot load http://localhost:9000/. No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://localhost:3000' is therefore not allowed access.  Config:  spring:   profiles: singlenode xd:   transport: local   ui:      allow_origin: """"*"""""""
"XD-3567","Bug","Hadoop|Runtime",3,"Fix classpath and servlet container issues","""Several issues with 1.3.0.M1 staged version  - we now use Tomcat instead of Jetty which prevent s xd-admin from starting on YARN  - we now have Guava 18.0 on classpath instead of 16.0.1  - xd-yarn push doesn't work, hadoop client for 2.7.1 needs Servlet API   - updating Hadoop to 2.7.1 instead of 2.6.0   -- this causes Curator to also update to 2.7.1 which throws exception on startup """
"XD-3566","Story","Acceptance Testing",3,"TwitterStream test must use unique name to prevent test collision","""XD Developer does not want the the twitter stream acceptance tests to interfere with other tests."""
"XD-3569","Story","Configuration|Hadoop|Packaging",3,"ResourceModuleRegistry doesn't support HA namenode for hdfs custom module location","""As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA.   We had an issue filed in the `spring-xd-ambari` project:  """"It seems like custom module doesn't pickup namenode HA? and still use NameNodeProxies.createNonHAProxy?""""  see: https://github.com/spring-projects/spring-xd-ambari/issues/14"""
"XD-3568","Bug","YARN Runtime",3,"AdminServer fails on HDP 2.3","""Submitting XD on YARN for HDP 2.3 fails due to some Solr issue in Boot - https://github.com/spring-projects/spring-boot/issues/2795  The xd-admin sysout is:   """
"XD-3589","Story","Batch",8,"Create Composed Job Module ","""h2. Narrative As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository.  While the user uses the composed job as if it is a normal job including seeing only the DSL.  In the background the JobFactory will deploy the composed job module.   * When the user destroys the job the module will be deleted from the file module repository. * When the user creates the job a module will be created in the file Module repository. h2. Back story For the composed job story, we need to create a """"real"""" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job."""
"XD-3613","Bug","Runtime",5,"Multiple module instances consuming from taps or topics get duplicate messages on redis Message Bus","""If I deploy more than one instance of a module (eg using module.name.count > 1 or module.name.count =0) that consumes from a tap or topic then I get duplicate messages if I’m using Redis as the message bus. It looks like this is the same issue as XD-3100 but the fix for that only fixed Rabbit as the message bus.  This is easy to reproduce on a 2 container cluster using a Redis Message Bus:  Create and deploy streams as follows:    On container 1 send a message:    Container 1 logs are then:    and container 2:    Ie the tapped message is duplicated (picked up by both tap module instances)  Similarly for topics create and deploy these streams:    On container 1 send a message:    Container 1 logs are then:    and container 2:    Ie the topic message is picked up by each instance of the module in each stream. In this case I would expect each stream to pick up the message once  ie I would get a single output for each stream   test message 002 TOPIC CONSUMER 2  once (on either container) test message 002 TOPIC CONSUMER 1  once (on either container)"""
"XD-3610","Bug","Stream Module",1,"Kafka source and sink headers shouldn't interfere with bus functionality","""The Kafka sink should not make use of the message headers sent by the Kafka receivers in the Kafka bus.   Similarly, the headers received from the Kafka source should not be propagated when sending to the Kakfa bus.   https://github.com/spring-projects/spring-xd/issues/1804"""
"XD-3632","Bug","Stream Module",0,"Reactor message handlers log completions at error level","""(copied from https://github.com/spring-projects/spring-xd/issues/1810):  While testing a reactive processor that I was building, I saw the following in my test environment's logs:    Completions don't really seem like error events. Perhaps this could be changed to INFO?  (will open a PR shortly)"""
"XD-3645","Bug","Runtime",2,"Tuple unable to serialize objects with nested arrays of objects","""Serializing a tuple object with that have a nested array which contains objects (as a tuple) fails to serialize. The error is:   when the input string (read from a Kafka topic in my case) looks something like:   If the inner array (the Pages array) is just an object, it works, when it is an array, it fails.   The stream used: kafka --topic=agent_mixed --outputType=application/x-xd-tuple | splitter --expression=payload.body | log"""
"XD-3652","Bug","Stream Module",5,"The shell processor module cannot be stopped while blocked in receive()","""Both lifecycle and send/receive methods are synchronized, so if the shell command processor is blocked reading from the script's input - e.g. when no proper terminator is sent by the script, the stop() method can't acquire the object lock and proceed stopping the instance, and therefore the module. """
"XD-3685","Bug","Batch",3,"Job Definitions page fails to display definitions if page ","""In this scenario we created 30 jobs that can be used for a composed job.   if the composed job uses jobs in its composition that are not present on the first page of the of the result set the following exception is thrown.    """
"XD-3687","Story","Batch",1,"Update Docs to add configs changes for Composed jobs","""Need to add the following instructions to setup the configurations for the Batch Repo to Composed Job Docs to support parallel jobs: 1) uncomment and change the following from  : ```spring:   batch: # Configure other Spring Batch repository values.  Most are typically not needed     isolationLevel: ISOLATION_SERIALIZATION ``` to ```spring:   batch: # Configure other Spring Batch repository values.  Most are typically not needed     isolationLevel: ISOLATION_READ_COMMITTED ```   And update the hsqldb datasource to: spring:   datasource:     url: jdbc:hsqldb:hsql://${hsql.server.host:localhost}:${hsql.server.port:9101}/${hsql.server.dbname:xdjob};sql.enforce_strict_size=true;hsqldb.tx=mvcc"""
"XD-3690","Story","Documentation",1,"Improve ""Server Configuration - Database Configuration"" section","""Make it more clear what drivers need to be copied where. See - https://github.com/spring-projects/spring-xd/issues/1653"""
"XD-3691","Bug","Batch",2,"Ensure Job definitions are escaped in UI","""If using the definition <aaa || bbb> where the definition starts with a """"<"""" and ends with a """">"""" the definition for the composed job does not appear on the definition page."""
"XD-3709","Bug","Runtime",1,"Duplicate MBean Names With router Sink","""For some reason, the Integration {{MBeanExporterHelper}} is not preventing the standard context {{MBeanExporter}} from exporting the {{AbstractMessageRouter}}. This should be suppressed (when an IMBE is present) because it's annotated {{@IntegrationManagedResource}}.  Causes {{InstanceAlreadyExistsException}}.  Workaround in the stack overflow answer.  http://stackoverflow.com/questions/33838502/error-deploying-more-than-one-stream-with-a-router-1-3-0  Could be an SI issue, but investigation needed. However, we should probably include the stream/job name in all MBeans for the stream (as is done for the integration exporter)."""
"XD-3716","Improvement","Runtime",2,"Support Configuring the RabbitMessageBus MessagePropertiesConverter LongString Limit","""http://stackoverflow.com/questions/34053997/passing-headerinformation-as-jsonobject-in-header-in-spring-xd"""
"XD-3719","Bug","UI",2,"Spring flo issue with unexpected char","""In Flo when creating a stream if you use asterisk you get an error. See the image attached."""
"XD-3721","Bug","UI",1,"XD Admin UI log out does not function properly","""I am using XD 1.2.1.RELEASE. I have following environment variables   XD_CONFIG_NAME = mycompany And  SPRING_PROFILE_ACTIVE= prod, admin  i have XD configuration file (mycompany-prod.yml) with following security configuration  # Config to enable security on administration endpoints (consider adding ssl) spring:   profiles: prod security:   basic:     enabled: true # false to disable security settings (default)     realm: SpringXD xd:   security:     authentication:       file:         enabled: true          users:           xdadmin: pwd, ROLE_ADMIN,ROLE_VIEW,ROLE_CREATE  I get a login screen, login works alright. When i logout - i still see all the tabs and contents in all the tabs. See the attached screenshot. """
"XD-3725","Bug","Runtime",1,"EmbeddedHeadersMessageConverter Buffer Overflow","""See https://github.com/spring-projects/spring-xd/issues/1871"""
"XD-3730","Bug","Stream Module",3,"NPE in spring-integration when using kafka as message bus when using aggrzgation module","""as stated in https://jira.spring.io/browse/INT-3908 sprint-integration in springxd can't use kafka as message bus in most case. Could it spring-xd integrat this fix for us to use it?"""
"XD-3733","Improvement","Configuration",1,"Document redis pool properties in servers.yml","""Add spring.redis.pool.*  properties to server.yml, commented out to show default values., e.g.,      maxIdle: 8,    minIdle: 0,     maxActive: 8,    maxWait: -1 """
"XD-3736","Improvement","Runtime",2,"Rabbit Pub/Sub Consumers Should Support Concurrency","""PubSub consumers can support concurrency since the threads are competing consumers on the queue.  """
"XD-3737","Bug","REST",1,"REST - Do not redirect after logout","""In the following PR we removed the *RestLogoutSuccessHandler*.   https://github.com/spring-projects/spring-xd/pull/1562  This is necessary, though, for REST calls and the Admin UI. Otherwise some weird UI behavior might occur due to the HTTP redirect."""
"XD-3738","Improvement","Configuration",2,"Encrypt secret information in XD configuration files","""Spring XD keeps passwords in text files such sas servers.yml, properties files, and module configuration files. Some users have requested a way to store encrypted values rather than clear text.  XD should provide a """"hook"""" for users to provide a custom component to detect encrypted property values and decrypt them during container, admin, and module initialization."""
"XD-3739","Bug","Stream Module",5,"Incorrect refresh period for groovy scripts","""All modules that allow groovy implementations (filter, script, transform, router, tcpclient) allow automatic refresh of the script when it changes. In the XD documentation it is stated that this refresh occurs every minute eg for filter at http://docs.spring.io/spring-xd/docs/1.3.0.RELEASE/reference/html/#filter """"The script is checked for updates every 60 seconds, so it may be replaced in a running system. """"   This set up can be seen in the spring xml for the modules - eg (again for filter)    However from the spring integration documentation http://docs.spring.io/spring-integration/docs/4.2.4.RELEASE/reference/html/messaging-endpoints-chapter.html#scripting-config it specifies that the refresh-check-delay parameter is actually in milliseconds - ie the above XD configuration would recheck the script every 60 milliseconds which may be a performance concern as it will be checking the lastmodified time of the script file.   Ideally this parameter would be configurable - in our case we would usually eliminate the refresh check altogether (set to -1) as our scripts will not change (or if they did a redeploy of the module would pick it up)  """
"XD-3743","Bug","Runtime",1,"Update to Spring Integration 4.2.5 When Available (Fix Metrics)","""See INT-3956"""
"XD-3744","Improvement","Stream Module",1,"Suppress DeliveryMode Header in RabbitMQ Source","""Related to XD-2567 which fixed this problem, but only in the bus.  {quote} 2016-02-19T18:25:24-0500 1.2.1.RELEASE WARN SimpleAsyncTaskExecutor-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode] {quote}"""
