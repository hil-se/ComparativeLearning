"issuekey","type","components","storypoint","title","description_text"
"MESOS-336","Epic","agent",5,"Mesos slave should cache executors","""The slave should be smarter about how it handles pulling down executors.  In our environment, executors rarely change but the slave will always pull it down from regardless HDFS.  This puts undue stress on our HDFS clusters, and is not resilient to reduced HDFS availability."""
"MESOS-564","Improvement","documentation",3,"Update Contribution Documentation","""Our contribution guide is currently fairly verbose, and it focuses on the ReviewBoard workflow for making code contributions. It would be helpful for new contributors to have a first-time contribution guide which focuses on using GitHub PRs to make small contributions, since that workflow has a smaller barrier to entry for new users."""
"MESOS-621","Bug","allocation",5,"`HierarchicalAllocatorProcess::removeSlave` doesn't properly handle framework allocations/resources","""Currently a slaveRemoved() simply removes the slave from 'slaves' map and slave's resources from 'roleSorter'. Looking at resourcesRecovered(), more things need to be done when a slave is removed (e.g., framework unallocations).    It would be nice to fix this and have a test for this."""
"MESOS-752","Bug","test",1,"SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave test is flaky","""[ RUN      ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave Checkpointing executor's forked pid 32281 to '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/meta/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3/pids/forked.pid' Fetching resources into '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3' Registered executor on localhost.localdomain Starting task 0514b52f-3c17-4ee5-ba16-635198701ca2 Forked command at 32317 sh -c 'sleep 10' tests/slave_recovery_tests.cpp:1927: Failure Mock function called more times than expected - returning directly.     Function call: statusUpdate(0x7fffae636eb0, @0x7f1590027a00 64-byte object <F0-2F D0-A1 15-7F 00-00 00-00 00-00 00-00 00-00 40-E9 01-90 15-7F 00-00 20-6B 03-90 15-7F 00-00 48-91 C3-00 00-00 00-00 B0-3B 01-90 15-7F 00-00 05-00 00-00 00-00 00-00 17-00 00-00 00-00 00-00>)          Expected: to be called once            Actual: called twice - over-saturated and active Command exited with status 0 (pid: 32317) """
"MESOS-830","Bug","test",8,"ExamplesTest.JavaFramework is flaky","""Identify the cause of the following test failure:  [ RUN      ] ExamplesTest.JavaFramework Using temporary directory '/tmp/ExamplesTest_JavaFramework_wSc7u8' Enabling authentication for the framework I1120 15:13:39.820032 1681264640 master.cpp:285] Master started on 172.25.133.171:52576 I1120 15:13:39.820180 1681264640 master.cpp:299] Master ID: 201311201513-2877626796-52576-3234 I1120 15:13:39.820194 1681264640 master.cpp:302] Master only allowing authenticated frameworks to register! I1120 15:13:39.821197 1679654912 slave.cpp:112] Slave started on 1)@172.25.133.171:52576 I1120 15:13:39.821795 1679654912 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.822855 1682337792 slave.cpp:112] Slave started on 2)@172.25.133.171:52576 I1120 15:13:39.823652 1682337792 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.825330 1679118336 master.cpp:744] The newly elected leader is master@172.25.133.171:52576 I1120 15:13:39.825445 1679118336 master.cpp:748] Elected as the leading master! I1120 15:13:39.825907 1681264640 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta' I1120 15:13:39.826127 1681264640 status_update_manager.cpp:180] Recovering status update manager I1120 15:13:39.826331 1681801216 process_isolator.cpp:317] Recovering isolator I1120 15:13:39.826738 1682874368 slave.cpp:2743] Finished recovery I1120 15:13:39.827747 1682337792 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta' I1120 15:13:39.827945 1680191488 slave.cpp:112] Slave started on 3)@172.25.133.171:52576 I1120 15:13:39.828415 1682337792 status_update_manager.cpp:180] Recovering status update manager I1120 15:13:39.828608 1680728064 sched.cpp:260] Authenticating with master master@172.25.133.171:52576 I1120 15:13:39.828606 1680191488 slave.cpp:212] Slave resources: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.828680 1682874368 slave.cpp:497] New master detected at master@172.25.133.171:52576 I1120 15:13:39.828765 1682337792 process_isolator.cpp:317] Recovering isolator I1120 15:13:39.829828 1680728064 sched.cpp:229] Detecting new master I1120 15:13:39.830288 1679654912 authenticatee.hpp:100] Initializing client SASL I1120 15:13:39.831635 1680191488 state.cpp:33] Recovering state from '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta' I1120 15:13:39.831991 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576 I1120 15:13:39.832042 1682874368 slave.cpp:524] Detecting new master I1120 15:13:39.832314 1682337792 slave.cpp:2743] Finished recovery I1120 15:13:39.832309 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(1)@172.25.133.171:52576 I1120 15:13:39.832929 1680728064 status_update_manager.cpp:180] Recovering status update manager I1120 15:13:39.833371 1681801216 slave.cpp:497] New master detected at master@172.25.133.171:52576 I1120 15:13:39.833273 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-0 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.833595 1680728064 process_isolator.cpp:317] Recovering isolator I1120 15:13:39.833859 1681801216 slave.cpp:524] Detecting new master I1120 15:13:39.833861 1682874368 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576 I1120 15:13:39.834092 1680191488 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-0 I1120 15:13:39.834486 1681264640 master.cpp:1266] Attempting to register slave on vkone.local at slave(2)@172.25.133.171:52576 I1120 15:13:39.834549 1681264640 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-1 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.834750 1680191488 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/meta/slaves/201311201513-2877626796-52576-3234-0/slave.info' I1120 15:13:39.834875 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-0 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available) I1120 15:13:39.835155 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-1 I1120 15:13:39.835458 1679118336 slave.cpp:2743] Finished recovery I1120 15:13:39.835739 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/meta/slaves/201311201513-2877626796-52576-3234-1/slave.info' I1120 15:13:39.835922 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-1 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available) I1120 15:13:39.836120 1681264640 slave.cpp:497] New master detected at master@172.25.133.171:52576 I1120 15:13:39.836340 1679118336 status_update_manager.cpp:158] New master detected at master@172.25.133.171:52576 I1120 15:13:39.836436 1681264640 slave.cpp:524] Detecting new master I1120 15:13:39.836629 1682874368 master.cpp:1266] Attempting to register slave on vkone.local at slave(3)@172.25.133.171:52576 I1120 15:13:39.836653 1682874368 master.cpp:2513] Adding slave 201311201513-2877626796-52576-3234-2 at vkone.local with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] I1120 15:13:39.836804 1680728064 slave.cpp:542] Registered with master master@172.25.133.171:52576; given slave ID 201311201513-2877626796-52576-3234-2 I1120 15:13:39.837190 1680728064 slave.cpp:555] Checkpointing SlaveInfo to '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/meta/slaves/201311201513-2877626796-52576-3234-2/slave.info' I1120 15:13:39.837569 1682874368 hierarchical_allocator_process.hpp:445] Added slave 201311201513-2877626796-52576-3234-2 (vkone.local) with cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] (and cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000] available) I1120 15:13:39.852011 1679654912 authenticatee.hpp:124] Creating new client SASL connection I1120 15:13:39.852219 1680191488 master.cpp:1734] Authenticating framework at scheduler(1)@172.25.133.171:52576 I1120 15:13:39.852577 1682337792 authenticator.hpp:83] Initializing server SASL I1120 15:13:39.856160 1682337792 authenticator.hpp:140] Creating new server SASL connection I1120 15:13:39.856334 1681264640 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5 I1120 15:13:39.856360 1681264640 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I1120 15:13:39.856421 1681264640 authenticator.hpp:243] Received SASL authentication start I1120 15:13:39.856487 1681264640 authenticator.hpp:325] Authentication requires more steps I1120 15:13:39.856531 1681264640 authenticatee.hpp:258] Received SASL authentication step I1120 15:13:39.856576 1681264640 authenticator.hpp:271] Received SASL authentication step I1120 15:13:39.856643 1681264640 authenticator.hpp:317] Authentication success I1120 15:13:39.856724 1681264640 authenticatee.hpp:298] Authentication success I1120 15:13:39.856768 1681264640 master.cpp:1774] Successfully authenticated framework at scheduler(1)@172.25.133.171:52576 I1120 15:13:39.857028 1681264640 sched.cpp:334] Successfully authenticated with master master@172.25.133.171:52576 I1120 15:13:39.857139 1681264640 master.cpp:798] Received registration request from scheduler(1)@172.25.133.171:52576 I1120 15:13:39.857306 1681264640 master.cpp:816] Registering framework 201311201513-2877626796-52576-3234-0000 at scheduler(1)@172.25.133.171:52576 I1120 15:13:39.862296 1680191488 hierarchical_allocator_process.hpp:332] Added framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.863867 1680191488 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000 Registered! ID = 201311201513-2877626796-52576-3234-0000 Launching task 0 Launching task 1 Launching task 2 I1120 15:13:39.905390 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-0 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.905825 1680191488 master.hpp:400] Adding task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:39.905886 1680191488 master.cpp:2150] Launching task 0 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:39.906422 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-1 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.906664 1680191488 master.hpp:400] Adding task 1 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:39.906721 1680191488 master.cpp:2150] Launching task 1 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:39.907171 1680191488 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-2 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.907419 1680191488 master.hpp:400] Adding task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) I1120 15:13:39.907480 1680191488 master.cpp:2150] Launching task 2 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) I1120 15:13:39.907938 1680191488 slave.cpp:722] Got assigned task 0 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.908473 1680191488 slave.cpp:833] Launching task 0 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.914427 1682874368 slave.cpp:722] Got assigned task 1 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.914594 1680728064 slave.cpp:722] Got assigned task 2 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.914844 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs I1120 15:13:39.915292 1682874368 slave.cpp:833] Launching task 1 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.915424 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs I1120 15:13:39.915685 1681801216 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs I1120 15:13:39.915828 1680728064 slave.cpp:833] Launching task 2 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.917840 1680191488 slave.cpp:943] Queuing task '0' for executor default of framework '201311201513-2877626796-52576-3234-0000 I1120 15:13:39.917935 1679118336 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4 with resources ' for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.922019 1679118336 process_isolator.cpp:163] Forked executor at 3268 I1120 15:13:39.922703 1679118336 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3268 I1120 15:13:39.929134 1682874368 slave.cpp:943] Queuing task '1' for executor default of framework '201311201513-2877626796-52576-3234-0000 I1120 15:13:39.929323 1682874368 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5 with resources ' for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.931243 1682874368 process_isolator.cpp:163] Forked executor at 3269 I1120 15:13:39.931612 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3269 E1120 15:13:39.931836 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched I1120 15:13:39.936460 1680728064 slave.cpp:943] Queuing task '2' for executor default of framework '201311201513-2877626796-52576-3234-0000 I1120 15:13:39.936619 1681801216 process_isolator.cpp:100] Launching default (/Users/vinod/workspace/apache/mesos/build/src/examples/java/test-executor) in /tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534 with resources ' for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:39.941299 1681801216 process_isolator.cpp:163] Forked executor at 3270 I1120 15:13:39.942179 1681801216 slave.cpp:2073] Monitoring executor default of framework 201311201513-2877626796-52576-3234-0000 forked at pid 3270 E1120 15:13:39.942395 1681801216 slave.cpp:2099] Failed to watch executor default of framework 201311201513-2877626796-52576-3234-0000: Already watched Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/2/slaves/201311201513-2877626796-52576-3234-2/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/2bd0e75d-a2b9-4ae6-be08-9782612309a5' Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/1/slaves/201311201513-2877626796-52576-3234-1/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/375b31a9-7093-4db1-964d-e6b425b1e4b4' Fetching resources into '/tmp/ExamplesTest_JavaFramework_wSc7u8/0/slaves/201311201513-2877626796-52576-3234-0/frameworks/201311201513-2877626796-52576-3234-0000/executors/default/runs/16d600da-da86-4614-91cb-58a7b27ab534' I1120 15:13:40.372573 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.373258 1681801216 slave.cpp:1527] Flushing queued task 1 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.388317 1681801216 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.388983 1681801216 slave.cpp:1527] Flushing queued task 0 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.398084 1679654912 slave.cpp:1406] Got registration for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.399344 1679654912 slave.cpp:1527] Flushing queued task 2 for executor 'default' of framework 201311201513-2877626796-52576-3234-0000 Registered executor on vkone.local I1120 15:13:40.491843 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.492202 1679654912 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.492424 1679654912 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 Registered executor on vkone.local I1120 15:13:40.492671 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.492735 1682337792 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 Status update: task 1 is in state TASK_RUNNING I1120 15:13:40.502235 1679654912 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f04b1852-3669-444a-906f-3675f784c14f) for task 1 of framework 201311201513-2877626796-52576-3234-0000 Registered executor on vkone.local I1120 15:13:40.531292 1679654912 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579 I1120 15:13:40.532091 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.532305 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.532776 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579 I1120 15:13:40.532951 1681801216 master.cpp:1452] Status update TASK_RUNNING (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576 Status update: task 2 is in state TASK_RUNNING I1120 15:13:40.538895 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c19b6a5a-19ce-4613-8a5a-08fe807ff27c) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.541267 1682874368 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.541555 1682874368 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.541725 1682874368 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.542196 1682874368 master.cpp:1452] Status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 I1120 15:13:40.542251 1682874368 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 Status update: task 0 is in state TASK_RUNNING I1120 15:13:40.545537 1682874368 status_update_manager.cpp:380] Received status update acknowledgement (UUID: c218b0c3-d77c-4901-8570-391c330ba117) for task 0 of framework 201311201513-2877626796-52576-3234-0000 Running task value: """"1""""  I1120 15:13:40.764219 1682337792 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.764629 1682337792 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.764698 1682337792 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.765043 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.765192 1682337792 master.hpp:418] Removing task 1 with resources cpus(*):1; mem(*):128 on slave 2Status update: task 1 is in state TASK_FINISHED Finished tasks: 1 01311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.765363 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 I1120 15:13:40.772738 1682337792 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-2 from framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.773190 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4a163594-146a-46f7-bd43-f906e76ad84c) for task 1 of framework 201311201513-2877626796-52576-3234-0000 Running task value: """"0""""  Running task value: """"2""""  I1120 15:13:40.790068 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.790411 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.790493 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.790674 1679118336 master.cpp:1452] Status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 I1120 15:13:40.790798 1679118336 master.hpp:418] Removing task 0 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.790928 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 Status update: task 0 is in state TASK_FINISHED Finished tasks: 2 I1120 15:13:40.791225 1680191488 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.794234 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 13265a94-50f1-4bc4-b2e2-9f60a1bb4086) for task 0 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.795830 1681801216 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52579 I1120 15:13:40.796111 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.796182 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.796352 1680728064 master.cpp:1452] Status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 from slave(1)@172.25.133.171:52576 I1120 15:13:40.796398 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52579 I1120 15:13:40.796466 1680728064 master.hpp:418] Removing task 2 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) I1120 15:13:40.796707 1679118336 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-0 from framework 201311201513-2877626796-52576-3234-0000 Status update: task 2 is in state TASK_FINISHED Finished tasks: 3 I1120 15:13:40.797384 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: f6f28e88-5ea6-4519-ba92-65ead6236fff) for task 2 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.824383 1681801216 master.cpp:1700] Sending 3 offers to framework 201311201513-2877626796-52576-3234-0000 Launching task 3 Launching task 4 I1120 15:13:40.826971 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-3 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827268 1679118336 master.hpp:400] Adding task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.827348 1679118336 master.cpp:2150] Launching task 3 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.827487 1680728064 slave.cpp:722] Got assigned task 3 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827857 1680728064 slave.cpp:833] Launching task 3 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827913 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-4 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.827986 1680728064 slave.cpp:968] Sending task '3' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.828126 1679118336 master.hpp:400] Adding task 4 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.828187 1679118336 master.cpp:2150] Launching task 4 of framework 201311201513-2877626796-52576-3234-0000 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.828632 1679118336 master.cpp:2026] Processing reply for offer 201311201513-2877626796-52576-3234-5 on slave 201311201513-2877626796-52576-3234-0 (vkone.local) for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.828655 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-1 for 1secs I1120 15:13:40.829005 1679118336 slave.cpp:722] Got assigned task 4 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.829027 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-2 for 1secs I1120 15:13:40.829260 1679118336 slave.cpp:833] Launching task 4 for framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.829273 1680728064 hierarchical_allocator_process.hpp:590] Framework 201311201513-2877626796-52576-3234-0000 filtered slave 201311201513-2877626796-52576-3234-0 for 1secs I1120 15:13:40.829390 1679118336 slave.cpp:968] Sending task '4' to executor 'default' of framework 201311201513-2877626796-52576-3234-0000 Running task value: """"3""""  Running task value: """"4""""  I1120 15:13:40.839279 1682337792 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.839534 1679118336 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.839705 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.839944 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 Status update: task 3 is in state TASK_RUNNING I1120 15:13:40.839947 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 I1120 15:13:40.856334 1679118336 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52578 I1120 15:13:40.856650 1679118336 status_update_manager.cpp:380] Received status update acknowledgement (UUID: a8d02ae6-3138-441c-a004-465d879b1277) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.856818 1679118336 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.856875 1679118336 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.857105 1679118336 slave.cpp:1740] Handling status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.857369 1679118336 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52578 I1120 15:13:40.857498 1680728064 status_update_manager.cpp:305] Received status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.857518 1682337792 master.cpp:1452] Status update TASK_FINISHED (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 from slave(2)@172.25.133.171:52576 I1120 15:13:40.857635 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.857630 1682337792 master.hpp:418] Removing task 3 with resources cpus(*):1; mem(*):128 on slave 201311201513-2877626796-52576-3234-1 (vkone.local) I1120 15:13:40.857843 1682337792 master.cpp:1452] Status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.858043 1680728064 hierarchical_allocator_process.hpp:637] Recovered cpus(*):1; mem(*):128 (total allocatable: cpus(*):4; mem(*):7168; disk(*):481998; ports(*):[31000-32000]) on slave 201311201513-2877626796-52576-3234-1 from framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.858098 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_RUNNING (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 Status update: task 3 is in state TASK_FINISHED Finished tasks: 4 Status update: task 4 is in state TASK_RUNNING I1120 15:13:40.858896 1682337792 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 3fc45cb8-fd7f-4bed-a21d-76f234af6b36) for task 3 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.858957 1680728064 status_update_manager.cpp:380] Received status update acknowledgement (UUID: 4f97c8df-1cc0-4eeb-8469-9d72df09ec73) for task 4 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.859905 1679654912 slave.cpp:1740] Handling status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from executor(1)@172.25.133.171:52577 I1120 15:13:40.860174 1680728064 status_update_manager.cpp:305] Received status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.860245 1680728064 status_update_manager.cpp:356] Forwarding status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to master@172.25.133.171:52576 I1120 15:13:40.860437 1679654912 master.cpp:1452] Status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 from slave(3)@172.25.133.171:52576 I1120 15:13:40.860486 1680728064 slave.cpp:1865] Sending acknowledgement for status update TASK_FINISHED (UUID: 876d7ddc-5d58-48df-a590-d82cf39d4978) for task 4 of framework 201311201513-2877626796-52576-3234-0000 to executor(1)@172.25.133.171:52577 I1120 15:13:40.860550 1679654912 master.hpp:418] Removing task 4 with resources cpus(*):1; mem(Status update: task 4 is in state TASK_FINISHED Finished tasks: 5 *):128 on slave 201311201513-2877626796-52576-3234-2 (vkone.local) I1120 15:13:40.863689 1679654912 master.cpp:996] Asked to unregister framework 201311201513-2877626796-52576-3234-0000 I1120 15:13:40.863750 1679654912 master.cpp:2385] Removing framework 201311201513-2877626796-52576-3234-0000 ../../src/tests/script.cpp:81: Failure Failed java_framework_test.sh terminated with signal 'Abort trap: 6' [  FAILED  ] ExamplesTest.JavaFramework (2688 ms) [----------] 1 test from ExamplesTest (2688 ms total)  [----------] Global test environment tear-down [==========] 1 test from 1 test case ran. (2692 ms total) [  PASSED  ] 0 tests. [  FAILED  ] 1 test, listed below: [  FAILED  ] ExamplesTest.JavaFramework """
"MESOS-934","Bug","documentation",1,"'Logging and Debugging' document is out-of-date.","""The following is no longer correct: http://mesos.apache.org/documentation/latest/logging-and-debugging/  We should either delete this document or re-write it entirely."""
"MESOS-976","Bug","test",1,"SlaveRecoveryTest/1.SchedulerFailover is flaky","""[==========] Running 1 test from 1 test case. [----------] Global test environment set-up. [----------] 1 test from SlaveRecoveryTest/1, where TypeParam = mesos::internal::slave::CgroupsIsolator [ RUN      ] SlaveRecoveryTest/1.SchedulerFailover I0206 20:18:31.525116 56447 master.cpp:239] Master ID: 2014-02-06-20:18:31-1740121354-55566-56447 Hostname: smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:31.525295 56481 master.cpp:321] Master started on 10.37.184.103:55566 I0206 20:18:31.525315 56481 master.cpp:324] Master only allowing authenticated frameworks to register! I0206 20:18:31.527093 56481 master.cpp:756] The newly elected leader is master@10.37.184.103:55566 I0206 20:18:31.527122 56481 master.cpp:764] Elected as the leading master! I0206 20:18:31.530642 56473 slave.cpp:112] Slave started on 9)@10.37.184.103:55566 I0206 20:18:31.530802 56473 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.531203 56473 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:31.531221 56473 slave.cpp:241] Slave checkpoint: true I0206 20:18:31.531991 56482 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root I0206 20:18:31.532470 56478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta' I0206 20:18:31.532698 56469 status_update_manager.cpp:188] Recovering status update manager I0206 20:18:31.533962 56472 sched.cpp:265] Authenticating with master master@10.37.184.103:55566 I0206 20:18:31.534102 56472 sched.cpp:234] Detecting new master I0206 20:18:31.534124 56484 authenticatee.hpp:124] Creating new client SASL connection I0206 20:18:31.534299 56473 master.cpp:2317] Authenticating framework at scheduler(9)@10.37.184.103:55566 I0206 20:18:31.534459 56461 authenticator.hpp:140] Creating new server SASL connection I0206 20:18:31.534572 56466 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 20:18:31.534595 56466 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 20:18:31.534667 56474 authenticator.hpp:243] Received SASL authentication start I0206 20:18:31.534732 56474 authenticator.hpp:325] Authentication requires more steps I0206 20:18:31.534814 56468 authenticatee.hpp:258] Received SASL authentication step I0206 20:18:31.534946 56466 authenticator.hpp:271] Received SASL authentication step I0206 20:18:31.535007 56466 authenticator.hpp:317] Authentication success I0206 20:18:31.535084 56471 authenticatee.hpp:298] Authentication success I0206 20:18:31.535107 56461 master.cpp:2357] Successfully authenticated framework at scheduler(9)@10.37.184.103:55566 I0206 20:18:31.535392 56476 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566 I0206 20:18:31.535512 56465 master.cpp:812] Received registration request from scheduler(9)@10.37.184.103:55566 I0206 20:18:31.535570 56465 master.cpp:830] Registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(9)@10.37.184.103:55566 I0206 20:18:31.535856 56465 hierarchical_allocator_process.hpp:332] Added framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.537802 56482 cgroups_isolator.cpp:840] Recovering isolator I0206 20:18:31.538462 56472 slave.cpp:2760] Finished recovery I0206 20:18:31.538910 56472 slave.cpp:508] New master detected at master@10.37.184.103:55566 I0206 20:18:31.539036 56478 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566 I0206 20:18:31.539223 56464 master.cpp:1834] Attempting to register slave on smfd-bkq-03-sr4.devel.twitter.com at slave(9)@10.37.184.103:55566 I0206 20:18:31.539271 56472 slave.cpp:533] Detecting new master I0206 20:18:31.539330 56464 master.cpp:2804] Adding slave 2014-02-06-20:18:31-1740121354-55566-56447-0 at smfd-bkq-03-sr4.devel.twitter.com with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.539454 56472 slave.cpp:551] Registered with master master@10.37.184.103:55566; given slave ID 2014-02-06-20:18:31-1740121354-55566-56447-0 I0206 20:18:31.539620 56472 slave.cpp:564] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/slave.info' I0206 20:18:31.539834 56475 hierarchical_allocator_process.hpp:445] Added slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available) I0206 20:18:31.540341 56472 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.543433 56472 master.cpp:1568] Processing reply for offers: [ 2014-02-06-20:18:31-1740121354-55566-56447-0 ] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.543642 56472 master.hpp:411] Adding task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:31.543781 56472 master.cpp:2441] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:31.544002 56484 slave.cpp:736] Got assigned task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.544097 56484 slave.cpp:2899] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.info' I0206 20:18:31.544272 56484 slave.cpp:2906] Checkpointing framework pid 'scheduler(9)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid' I0206 20:18:31.544617 56484 slave.cpp:845] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.546721 56484 slave.cpp:3169] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/executor.info' I0206 20:18:31.547317 56484 slave.cpp:3257] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/tasks/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/task.info' I0206 20:18:31.547514 56484 slave.cpp:955] Queuing task 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework '2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.547590 56481 cgroups_isolator.cpp:517] Launching d045a0bd-2ed2-410a-bd1f-5bd9219896e3 (/home/vinod/mesos/build/src/mesos-executor) in /tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 in cgroup mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 I0206 20:18:31.548408 56481 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.548833 56481 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.549294 56481 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.550107 56481 cgroups_isolator.cpp:1147] Updated 'memory.limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.550571 56481 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.551553 56481 cgroups_isolator.cpp:569] Forked executor at = 56671 Checkpointing executor's forked pid 56671 to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/forked.pid' I0206 20:18:31.552222 56472 slave.cpp:2098] Monitoring executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 forked at pid 56671 Fetching resources into '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986' I0206 20:18:31.604012 56472 slave.cpp:1431] Got registration for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.604167 56472 slave.cpp:1516] Checkpointing executor pid 'executor(1)@10.37.184.103:46181' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/libprocess.pid' I0206 20:18:31.605183 56472 slave.cpp:1552] Flushing queued task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 Registered executor on smfd-bkq-03-sr4.devel.twitter.com Starting task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 sh -c 'sleep 1000' Forked command at 56712 I0206 20:18:31.613098 56481 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181 I0206 20:18:31.613628 56469 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.614006 56469 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.795529 56469 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566 I0206 20:18:31.795992 56480 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181 I0206 20:18:31.796131 56471 master.cpp:2020] Status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(9)@10.37.184.103:55566 I0206 20:18:31.797099 56483 status_update_manager.cpp:392] Received status update acknowledgement (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.797165 56483 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.882767 56481 slave.cpp:394] Slave terminating I0206 20:18:31.883112 56481 master.cpp:641] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) disconnected I0206 20:18:31.883200 56476 hierarchical_allocator_process.hpp:484] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 disconnected I0206 20:18:31.888206 56473 sched.cpp:265] Authenticating with master master@10.37.184.103:55566 I0206 20:18:31.888473 56473 sched.cpp:234] Detecting new master I0206 20:18:31.888556 56469 authenticatee.hpp:124] Creating new client SASL connection I0206 20:18:31.888978 56484 master.cpp:2317] Authenticating framework at scheduler(10)@10.37.184.103:55566 I0206 20:18:31.889348 56469 authenticator.hpp:140] Creating new server SASL connection I0206 20:18:31.889925 56469 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5 I0206 20:18:31.889989 56469 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 20:18:31.890059 56469 authenticator.hpp:243] Received SASL authentication start I0206 20:18:31.890233 56469 authenticator.hpp:325] Authentication requires more steps I0206 20:18:31.890399 56468 authenticatee.hpp:258] Received SASL authentication step I0206 20:18:31.890554 56484 authenticator.hpp:271] Received SASL authentication step I0206 20:18:31.890630 56484 authenticator.hpp:317] Authentication success I0206 20:18:31.890728 56470 authenticatee.hpp:298] Authentication success I0206 20:18:31.890748 56484 master.cpp:2357] Successfully authenticated framework at scheduler(10)@10.37.184.103:55566 I0206 20:18:31.892210 56469 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566 I0206 20:18:31.892410 56473 master.cpp:900] Re-registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(10)@10.37.184.103:55566 I0206 20:18:31.892460 56473 master.cpp:926] Framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 failed over W0206 20:18:31.892691 56465 master.cpp:1048] Ignoring deactivate framework message for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from 'scheduler(9)@10.37.184.103:55566' because it is not from the registered framework 'scheduler(10)@10.37.184.103:55566' I0206 20:18:31.897049 56466 slave.cpp:112] Slave started on 10)@10.37.184.103:55566 I0206 20:18:31.897207 56466 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:31.897536 56466 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:31.897554 56466 slave.cpp:241] Slave checkpoint: true I0206 20:18:31.898388 56463 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root I0206 20:18:31.898936 56472 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta' I0206 20:18:31.901702 56465 slave.cpp:2828] Recovering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.901759 56465 slave.cpp:3020] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:31.902716 56464 status_update_manager.cpp:188] Recovering status update manager I0206 20:18:31.902884 56464 status_update_manager.cpp:196] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.475915 56463 cgroups_isolator.cpp:840] Recovering isolator I0206 20:18:34.476066 56463 cgroups_isolator.cpp:847] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.477478 56463 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.478728 56463 slave.cpp:2700] Sending reconnect request to executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at executor(1)@10.37.184.103:46181 I0206 20:18:34.480114 56476 slave.cpp:1597] Re-registering executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.480566 56476 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] I0206 20:18:34.481370 56476 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.481827 56476 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 Re-registered executor on smfd-bkq-03-sr4.devel.twitter.com I0206 20:18:34.489497 56471 slave.cpp:1713] Cleaning up un-reregistered executors I0206 20:18:34.489588 56471 slave.cpp:2760] Finished recovery I0206 20:18:34.490048 56463 slave.cpp:508] New master detected at master@10.37.184.103:55566 I0206 20:18:34.490257 56475 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566 I0206 20:18:34.490357 56463 slave.cpp:533] Detecting new master W0206 20:18:34.490603 56480 master.cpp:1878] Slave at slave(10)@10.37.184.103:55566 (smfd-bkq-03-sr4.devel.twitter.com) is being allowed to re-register with an already in use id (2014-02-06-20:18:31-1740121354-55566-56447-0) I0206 20:18:34.490927 56479 slave.cpp:601] Re-registered with master master@10.37.184.103:55566 I0206 20:18:34.491322 56461 hierarchical_allocator_process.hpp:498] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 reconnected I0206 20:18:34.491421 56468 slave.cpp:1312] Updating framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 pid to scheduler(10)@10.37.184.103:55566 I0206 20:18:34.491444 56480 master.cpp:1673] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.491488 56468 slave.cpp:1320] Checkpointing framework pid 'scheduler(10)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid' I0206 20:18:34.491497 56480 master.cpp:1707] Telling slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.491657 56468 slave.cpp:1013] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 Shutting down Killing process tree at pid 56712 Killed the following process trees: [  --- 56712 sleep 1000  ] Command terminated with signal Killed (pid: 56712) I0206 20:18:34.615216 56463 slave.cpp:1765] Handling status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181 I0206 20:18:34.615556 56483 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources  I0206 20:18:34.615624 56476 status_update_manager.cpp:314] Received status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.615701 56476 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.706945 56476 status_update_manager.cpp:367] Forwarding status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566 I0206 20:18:34.707263 56476 slave.cpp:1890] Sending acknowledgement for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181 I0206 20:18:34.707352 56469 master.cpp:2020] Status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(10)@10.37.184.103:55566 I0206 20:18:34.707620 56469 master.hpp:429] Removing task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:34.708348 56466 hierarchical_allocator_process.hpp:637] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 from framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.708673 56469 status_update_manager.cpp:392] Received status update acknowledgement (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.708749 56469 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.709411 56470 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 I0206 20:18:34.809782 56447 master.cpp:583] Master terminating I0206 20:18:34.810066 56447 master.cpp:246] Shutting down master I0206 20:18:34.810134 56482 slave.cpp:1965] master@10.37.184.103:55566 exited W0206 20:18:34.810184 56482 slave.cpp:1968] Master disconnected! Waiting for a new master to be elected I0206 20:18:34.810652 56447 master.cpp:289] Removing slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) I0206 20:18:34.813144 56447 slave.cpp:394] Slave terminating I0206 20:18:34.821583 56467 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test I0206 20:18:34.821652 56467 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test after 1 attempts I0206 20:18:34.823129 56471 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test I0206 20:18:34.823247 56471 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test I0206 20:18:34.923945 56470 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 I0206 20:18:34.924018 56470 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 after 1 attempts I0206 20:18:34.925506 56461 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 I0206 20:18:34.925580 56461 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 [       OK ] SlaveRecoveryTest/1.SchedulerFailover (3408 ms) [----------] 1 test from SlaveRecoveryTest/1 (3409 ms total)  [----------] Global test environment tear-down ../../src/tests/environment.cpp:247: Failure Failed Tests completed with child processes remaining: -+- 56447 /home/vinod/mesos/build/src/.libs/lt-mesos-tests --verbose --gtest_filter=*SlaveRecoveryTest/1.SchedulerFailover* --gtest_repeat=10   \--- 56671 () """
"MESOS-988","Bug","test",3,"ExamplesTest.PythonFramework is flaky","""Looks like a SEGFAULT during shutdown.  """
"MESOS-998","Bug","containerization",5,"Slave should wait until Containerizer::update() completes successfully","""Container resources are updated in several places in the slave and we don't check the update was successful or even wait until it completes."""
"MESOS-1010","Bug","build|python api",3,"Python extension build is broken if gflags-dev is installed","""In my environment mesos build from master results in broken python api module {{_mesos.so}}:  Unmangled version of symbol looks like this:  During {{./configure}} step {{glog}} finds {{gflags}} development files and starts using them, thus *implicitly* adding dependency on {{libgflags.so}}. This breaks Python extensions module and perhaps can break other mesos subsystems when moved to hosts without {{gflags}} installed.  This task is done when the ExamplesTest.PythonFramework test will pass on a system with gflags installed."""
"MESOS-1013","Bug","test",2,"ExamplesTest.JavaLog is flaky","""The {{ExamplesTest.JavaLog}} test framework is flaky, possibly related to a race condition between mutexes.   Full logs attached."""
"MESOS-1081","Bug","master",1,"Master should not deactivate authenticated framework/slave on new AuthenticateMessage unless new authentication succeeds.","""Master should not deactivate an authenticated framework/slave upon receiving a new AuthenticateMessage unless new authentication succeeds. As it stands now, a malicious user could spoof the pid of an authenticated framework/slave and send an AuthenticateMessage to knock a valid framework/slave off the authenticated list, forcing the valid framework/slave to re-authenticate and re-register. This could be used in a DoS attack. But how should we handle the scenario when the actual authenticated framework/slave sends an AuthenticateMessage that fails authentication?"""
"MESOS-1119","Bug","allocation",2,"Allocator should make an allocation decision per slave instead of per framework/role.","""Currently the Allocator::allocate() code loops through roles and frameworks (based on DRF sort) and allocates *all* slaves resources to the first framework.  This logic should be a bit inversed. Instead, the slave should go through each slave, allocate it a role/framework and update the DRF shares."""
"MESOS-1127","Task","framework",8,"Implement the protobufs for the scheduler API","""The default scheduler/executor interface and implementation in Mesos have a few drawbacks:  (1) The interface is fairly high-level which makes it hard to do certain things, for example, handle events (callbacks) in batch. This can have a big impact on the performance of schedulers (for example, writing task updates that need to be persisted).  (2) The implementation requires writing a lot of boilerplate JNI and native Python wrappers when adding additional API components.  The plan is to provide a lower-level API that can easily be used to implement the higher-level API that is currently provided. This will also open the door to more easily building native-language Mesos libraries (i.e., not needing the C++ shim layer) and building new higher-level abstractions on top of the lower-level API."""
"MESOS-1143","Improvement","framework|master",2,"Add a TASK_ERROR task status.","""During task validation we drop tasks that have errors and send TASK_LOST status updates. In most circumstances a framework will want to relaunch a task that has gone lost, and in the event the task is actually malformed (thus invalid) this will result in an infinite loop of sending a task and having it go lost."""
"MESOS-1148","Improvement","master",3,"Add support for rate limiting slave removal","""To safeguard against unforeseen bugs leading to widespread slave removal, it would be nice to allow for rate limiting of the decision to remove slaves and/or send TASK_LOST messages for tasks on those slaves.  Ideally this would allow an operator to be notified soon enough to intervene before causing cluster impact."""
"MESOS-1195","Bug","containerization",3,"systemd.slice + cgroup enablement fails in multiple ways. ","""When attempting to configure mesos to use systemd slices on a 'rawhide/f21' machine, it fails creating the isolator:   I0407 12:39:28.035354 14916 containerizer.cpp:180] Using isolation: cgroups/cpu,cgroups/mem Failed to create a containerizer: Could not create isolator cgroups/cpu: Failed to create isolator: The cpu subsystem is co-mounted at /sys/fs/cgroup/cpu with other subsytems  ------ details ------ /sys/fs/cgroup total 0 drwxr-xr-x. 12 root root 280 Mar 18 08:47 . drwxr-xr-x.  6 root root   0 Mar 18 08:47 .. drwxr-xr-x.  2 root root   0 Mar 18 08:47 blkio lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpu -> cpu,cpuacct lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpuacct -> cpu,cpuacct drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpu,cpuacct drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpuset drwxr-xr-x.  2 root root   0 Mar 18 08:47 devices drwxr-xr-x.  2 root root   0 Mar 18 08:47 freezer drwxr-xr-x.  2 root root   0 Mar 18 08:47 hugetlb drwxr-xr-x.  3 root root   0 Apr  3 11:26 memory drwxr-xr-x.  2 root root   0 Mar 18 08:47 net_cls drwxr-xr-x.  2 root root   0 Mar 18 08:47 perf_event drwxr-xr-x.  4 root root   0 Mar 18 08:47 systemd """
"MESOS-1219","Bug","master|webui",2,"Master should disallow frameworks that reconnect after failover timeout.","""When a scheduler reconnects after the failover timeout has exceeded, the framework id is usually reused because the scheduler doesn't know that the timeout exceeded and it is actually handled as a new framework.  The /framework/:framework_id route of the Web UI doesn't handle those cases very well because its key is reused. It only shows the terminated one.  Would it make sense to ignore the provided framework id when a scheduler reconnects to a terminated framework and generate a new id to make sure it's unique?"""
"MESOS-1303","Bug","test",1,"ExamplesTest.{TestFramework, NoExecutorFramework} flaky","""I'm having trouble reproducing this but I did observe it once on my OSX system:    when investigating a failed make check for https://reviews.apache.org/r/20971/ """
"MESOS-1332","Improvement","agent|master",3,"Improve Master and Slave metric names","""As we move the metrics to a new endpoint, we should consider revisiting the names of some of the current metrics to make them clearer.  It may also be worth considering changing some existing counter-style metrics to gauges. """
"MESOS-1347","Bug","test",2,"GarbageCollectorIntegrationTest.DiskUsage is flaky.","""From Jenkins: https://builds.apache.org/job/Mesos-Ubuntu-distcheck/79/consoleFull  """
"MESOS-1365","Bug","test",1,"SlaveRecoveryTest/0.MultipleFrameworks is flaky","""--gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure  """
"MESOS-1358","Improvement","webui",1,"Show when the leading master was elected in the webui","""This would be nice to have during debugging."""
"MESOS-1371","Task","libprocess|statistics",1,"Expose libprocess queue length from scheduler driver to metrics endpoint","""We expose the master's event queue length and we should do the same for the scheduler driver."""
"MESOS-1373","Task","framework",3,"Keep track of the principals for authenticated pids in Master.","""Need to add a 'principal' field to FrameworkInfo and verify if the Framework has the claimed principal during registration."""
"MESOS-1424","Improvement","test",1,"Mesos tests should not rely on echo","""Triggered by MESOS-1413 I would like to propose changing our tests to not rely on {{echo}} but to use {{printf}} instead.  This seems to be useful as {{echo}} is introducing an extra linefeed after the supplied string whereas {{printf}} does not. The {{-n}} switch preventing that extra linefeed is unfortunately not portable - it is not supported by the builtin {{echo}} of the BSD / OSX {{/bin/sh}}. """
"MESOS-1425","Bug","test",1,"LogZooKeeperTest.WriteRead test is flaky",""""""
"MESOS-1452","Improvement","master",3,"Improve Master::removeOffer to avoid further resource accounting bugs.","""Per comments on this review: https://reviews.apache.org/r/21750/  We've had numerous bugs around resource accounting in the master due to the trickiness of removing offers in the Master code.  There are a few ways to improve this:  1. Add multiple offer methods to differentiate semantics:   2. Add an enum to removeOffer to differentiate removal semantics: """
"MESOS-1459","Bug","build",1,"Build failure: Ubuntu 13.10/clang due to missing virtual destructor","""In file included from launcher/main.cpp:19: In file included from ./launcher/launcher.hpp:24: In file included from ../3rdparty/libprocess/include/process/future.hpp:23: ../3rdparty/libprocess/include/process/owned.hpp:188:5: error: delete called on 'mesos::internal::launcher::Operation' that is abstract but has non-virtual destructor [-Werror,-Wdelete-non-virtual-dtor]     delete t;     ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:456:8: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Data::~Data' requested here               delete __p;               ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:768:24: note: in instantiation of function template specialization 'std::__shared_count<2>::__shared_count<process::Owned<mesos::internal::launcher::Operation>::Data *>' requested here         : _M_ptr(__p), _M_refcount(__p)                        ^ /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:919:4: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here           __shared_ptr(__p).swap(*this);           ^ ../3rdparty/libprocess/include/process/owned.hpp:68:10: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::reset<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here     data.reset(new Data(t));          ^ ./launcher/launcher.hpp:101:7: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Owned' requested here   add(process::Owned<Operation>(new T()));       ^ launcher/main.cpp:26:3: note: in instantiation of function template specialization 'mesos::internal::launcher::add<mesos::internal::launcher::ShellOperation>' requested here   launcher::add<launcher::ShellOperation>();   ^ 1 error generated."""
"MESOS-1469","Bug","build|reviewbot",2,"No output from review bot on timeout","""When the mesos review build times out, likely due to a long-running failing test, we have no output to debug. We should find a way to stream the output from the build instead of waiting for the build to finish."""
"MESOS-1466","Bug","allocation|master",8,"Race between executor exited event and launch task can cause overcommit of resources","""The following sequence of events can cause an overcommit  --> Launch task is called for a task whose executor is already running  --> Executor's resources are not accounted for on the master  --> Executor exits and the event is enqueued behind launch tasks on the master  --> Master sends the task to the slave which needs to commit for resources for task and the (new) executor.  --> Master processes the executor exited event and re-offers the executor's resources causing an overcommit of resources."""
"MESOS-1472","Bug","containerization",1,"Improve child exit if slave dies during executor launch in MC","""When restarting many slaves there's a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the MesosContainerizer.  The forked child correctly detects this however rather than abort it should safely log and then exit non-zero cleanly."""
"MESOS-1471","Documentation","documentation|replicated log",5,"Document replicated log design/internals","""The replicated log could benefit from some documentation. In particular, how does it work? What do operators need to know? Possibly there is some overlap with our future maintenance documentation in MESOS-1470.  I believe [~jieyu] has some unpublished work that could be leveraged here!"""
"MESOS-1518","Improvement","documentation",2,"Update Rate Limiting Design doc to reflect the latest changes","""- Usage - Design - Implementation Notes"""
"MESOS-1527","Improvement","containerization",3,"Choose containerizer at runtime","""Currently you have to choose the containerizer at mesos-slave start time via the --isolation option.  I'd like to be able to specify the containerizer in the request to launch the job. This could be specified by a new """"Provider"""" field in the ContainerInfo proto buf."""
"MESOS-1545","Bug","test",1,"SlaveRecoveryTest/0.MultipleFrameworks is flaky",""""""
"MESOS-1559","Improvement","build",5,"Allow jenkins build machine to dump stack traces of all threads when timeout","""Many of the time, when jenkins build times out, we know that some test freezes at some place. However, most of the time, it's very hard to reproduce the deadlock on dev machines.  I would be cool if we can dump the stack traces of all threads when jenkins build times out. Some command like the following:  """
"MESOS-1567","Improvement","agent|master",1,"Add logging of the user uid when receiving SIGTERM.","""We currently do not log the user id when receiving a SIGTERM, this makes debugging a bit difficult. It's easy to get this information through sigaction."""
"MESOS-1592","Task","allocation",5,"Design inverse resource offer support","""An """"inverse"""" resource offer means that Mesos is requesting resources back from the framework, possibly within some time interval.  This can be leveraged initially to provide more automated cluster maintenance, by offering schedulers the opportunity to move tasks to compensate for planned maintenance. Operators can set a time limit on how long to wait for schedulers to relocate tasks before the tasks are forcibly terminated.  Inverse resource offers have many other potential uses, as it opens the opportunity for the allocator to attempt to move tasks in the cluster through the co-operation of the framework, possibly providing better over-subscription, fairness, etc."""
"MESOS-1587","Improvement","containerization",5,"Report disk usage from MesosContainerizer","""We should report disk usage for the executor work directory from MesosContainerizer and include in the ResourceStatistics protobuf."""
"MESOS-1586","Improvement","containerization",3,"Isolate system directories, e.g., per-container /tmp","""Ideally, tasks should not write outside their sandbox (executor work directory) but pragmatically they may need to write to /tmp, /var/tmp, or some other directory.  1) We should include any such files in disk usage and quota. 2) We should make these """"shared"""" directories private, i.e., each container has their own. 3) We should make the lifetime of any such files the same as the executor work directory."""
"MESOS-1594","Bug","test",1,"SlaveRecoveryTest/0.ReconcileKillTask is flaky","""Observed this on Jenkins.  """
"MESOS-1624","Bug","build",1,"Apache Jenkins build fails due to -lsnappy is set when building leveldb","""The failed build: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2261/consoleFull {noformat:title=the log where -lsnappy is used when compiling leveldb} gzip -d -c ../../3rdparty/leveldb.tar.gz | tar xf - test ! -e ../../3rdparty/leveldb.patch || patch -d leveldb -p1 <../../3rdparty/leveldb.patch touch leveldb-stamp cd leveldb && \           make  CC=""""gcc"""" CXX=""""g++"""" OPT=""""-g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC"""" make[5]: Entering directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb' g++ -pthread -lsnappy -shared -Wl,-soname -Wl,/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb/libleveldb.so.1 -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -fPIC db/builder.cc db/c.cc db/db_impl.cc db/db_iter.cc db/dbformat.cc db/filename.cc db/log_reader.cc db/log_writer.cc db/memtable.cc db/repair.cc db/table_cache.cc db/version_edit.cc db/version_set.cc db/write_batch.cc table/block.cc table/block_builder.cc table/filter_block.cc table/format.cc table/iterator.cc table/merger.cc table/table.cc table/table_builder.cc table/two_level_iterator.cc util/arena.cc util/bloom.cc util/cache.cc util/coding.cc util/comparator.cc util/crc32c.cc util/env.cc util/env_posix.cc util/filter_policy.cc util/hash.cc util/histogram.cc util/logging.cc util/options.cc util/status.cc  port/port_posix.cc -o libleveldb.so.1.4 ln -fs libleveldb.so.1.4 libleveldb.so ln -fs libleveldb.so.1.4 libleveldb.so.1 g++ -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -c db/builder.cc -o db/builder.o """
"MESOS-1668","Bug","agent|master",2,"Handle a temporary one-way master --> slave socket closure.","""In MESOS-1529, we realized that it's possible for a slave to remain disconnected in the master if the following occurs:   Master and Slave connected operating normally.  Temporary one-way network failure, masterslave link breaks.  Master marks slave as disconnected.  Network restored and health checking continues normally, slave is not removed as a result. Slave does not attempt to re-register since it is receiving pings once again.  Slave remains disconnected according to the master, and the slave does not try to re-register. Bad!  We were originally thinking of using a failover timeout in the master to remove these slaves that don't re-register. However, it can be dangerous when ZooKeeper issues are preventing the slave from re-registering with the master; we do not want to remove a ton of slaves in this situation.  Rather, when the slave is health checking correctly but does not re-register within a timeout, we could send a registration request from the master to the slave, telling the slave that it must re-register. This message could also be used when receiving status updates (or other messages) from slaves that are disconnected in the master."""
"MESOS-1676","Bug","test",1,"ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession is flaky","""{noformat:title=} [ RUN      ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession I0806 01:18:37.648684 17458 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 42069 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1682db0 flags=0 2014-08-06 01:18:37,656:17458(0x2b468638b700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:37,669:17458(0x2b468638b700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0000, negotiated timeout=6000 I0806 01:18:37.671725 17486 group.cpp:313] Group process (group(37)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:37.671758 17486 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:37.671771 17486 group.cpp:385] Trying to create path '/mesos' in ZooKeeper 2014-08-06 01:18:39,101:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:42,441:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0806 01:18:42.656673 17481 contender.cpp:131] Joining the ZK group I0806 01:18:42.662484 17484 contender.cpp:247] New candidate (id='0') has entered the contest for leadership I0806 01:18:42.663754 17481 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:42.663884 17481 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:42.664788 17483 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x15c00f0 flags=0 2014-08-06 01:18:42,668:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:42,672:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000 I0806 01:18:42.673542 17485 group.cpp:313] Group process (group(38)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:42.673570 17485 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:42.673580 17485 group.cpp:385] Trying to create path '/mesos' in ZooKeeper 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 131ms) 2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 115ms) 2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms 2014-08-06 01:18:46,799:17458(0x2b4687394700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 1025ms 2014-08-06 01:18:46,800:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client I0806 01:18:46.806895 17486 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:46.807857 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:47.669064 17482 contender.cpp:131] Joining the ZK group 2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2989ms 2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:47,671:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000 I0806 01:18:47.682868 17485 contender.cpp:247] New candidate (id='1') has entered the contest for leadership I0806 01:18:47.683404 17482 group.cpp:313] Group process (group(38)@127.0.1.1:55561) reconnected to ZooKeeper I0806 01:18:47.683445 17482 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:47.685998 17482 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:47.686142 17482 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:47.687289 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c0421c0 flags=0 2014-08-06 01:18:47,699:17458(0x2b4687de6700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069] 2014-08-06 01:18:47,712:17458(0x2b4687de6700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0002, negotiated timeout=6000 I0806 01:18:47.712846 17479 group.cpp:313] Group process (group(39)@127.0.1.1:55561) connected to ZooKeeper I0806 01:18:47.712873 17479 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0) I0806 01:18:47.712882 17479 group.cpp:385] Trying to create path '/mesos' in ZooKeeper I0806 01:18:47.714648 17479 detector.cpp:138] Detected a new leader: (id='0') I0806 01:18:47.714759 17479 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper I0806 01:18:47.716130 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected 2014-08-06 01:18:47,718:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response I0806 01:18:47.718889 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... 2014-08-06 01:18:47,720:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response I0806 01:18:47.720788 17484 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ... I0806 01:18:47.724663 17458 zookeeper_test_server.cpp:122] Shutdown ZooKeeperTestServer on port 42069 2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 4133ms 2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:49,720:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 33ms 2014-08-06 01:18:49,721:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:49,722:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:50,136:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:50,800:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:51,723:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:51,723:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:52,801:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:52.842553 17481 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0000) expiration I0806 01:18:52.842911 17481 group.cpp:472] ZooKeeper session expired I0806 01:18:52.843468 17485 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:52.843483 17485 detector.cpp:138] Detected a new leader: None I0806 01:18:52.843618 17485 contender.cpp:196] Membership cancelled: 0 2014-08-06 01:18:52,843:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0000  2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1349ad0 flags=0 2014-08-06 01:18:52,844:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:53,473:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:53.720684 17480 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0001) expiration I0806 01:18:53.721132 17480 group.cpp:472] ZooKeeper session expired I0806 01:18:53.721516 17479 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:53.721534 17479 detector.cpp:138] Detected a new leader: None I0806 01:18:53.721696 17479 contender.cpp:196] Membership cancelled: 1 2014-08-06 01:18:53,721:17458(0x2b46798a3700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0001  2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x16a0550 flags=0 2014-08-06 01:18:53,723:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:53,726:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client W0806 01:18:53.730258 17479 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0002) expiration I0806 01:18:53.730736 17479 group.cpp:472] ZooKeeper session expired I0806 01:18:53.731081 17481 detector.cpp:126] The current leader (id=0) is lost I0806 01:18:53.731132 17481 detector.cpp:138] Detected a new leader: None 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0002  2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@716: Client environment:host.name=lucid 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@723: Client environment:os.name=Linux 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@733: Client environment:user.name=(null) 2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins 2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src 2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c035f30 flags=0 2014-08-06 01:18:53,733:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:54,512:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:55,393:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:55,403:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:56,301:17458(0x2b468698f700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 122ms 2014-08-06 01:18:56,302:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:56,809:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:57,939:17458(0x2b4686f92700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 879ms 2014-08-06 01:18:57,940:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client 2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 870ms 2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client tests/master_contender_detector_tests.cpp:574: Failure Failed to wait 10secs for leaderReconnecting 2014-08-06 01:18:57,941:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  I0806 01:18:57.949972 17458 contender.cpp:186] Now cancelling the membership: 1 2014-08-06 01:18:57,950:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  I0806 01:18:57.950731 17458 contender.cpp:186] Now cancelling the membership: 0 2014-08-06 01:18:57,951:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0  ../3rdparty/libprocess/include/process/gmock.hpp:298: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const DispatchEvent&>()))...     Expected args: dispatch matcher (1, 16-byte object <50-20 4A-00 00-00 00-00 00-00 00-00 00-00 00-00>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession (20308 ms) {noformat}"""
"MESOS-1683","Task","documentation",2,"Create user doc for framework rate limiting feature","""Create a Markdown doc under /docs"""
"MESOS-1696","Bug","agent|master",3,"Improve reconciliation between master and slave.","""As we update the Master to keep tasks in memory until they are both terminal and acknowledged (MESOS-1410), the lifetime of tasks in Mesos will look as follows:    In the current form of reconciliation, the slave sends to the master all tasks that are not both terminal and acknowledged. At any point in the above lifecycle, the slave's re-registration message can reach the master.  Note the following properties:  *(1)* The master may have a non-terminal task, not present in the slave's re-registration message. *(2)* The master may have a non-terminal task, present in the slave's re-registration message but in a different state. *(3)* The slave's re-registration message may contain a terminal unacknowledged task unknown to the master.  In the current master / slave [reconciliation|https://github.com/apache/mesos/blob/0.19.1/src/master/master.cpp#L3146] code, the master assumes that case (1) is because a launch task message was dropped, and it sends TASK_LOST. We've seen above that (1) can happen even when the task reaches the slave correctly, so this can lead to inconsistency!  After chatting with [~vinodkone], we're considering updating the reconciliation to occur as follows:    Slave sends all tasks that are not both terminal and acknowledged, during re-registration. This is the same as before.   If the master sees tasks that are missing in the slave, the master sends the tasks that need to be reconciled to the slave for the tasks. This can be piggy-backed on the re-registration message.   The slave will send TASK_LOST if the task is not known to it. Preferably in a retried manner, unless we update socket closure on the slave to force a re-registration."""
"MESOS-1695","Bug","agent",1,"The stats.json endpoint on the slave exposes ""registered"" as a string.","""The slave is currently exposing a string value for the """"registered"""" statistic, this should be a number:    Should be a pretty straightforward fix, looks like this first originated back in 2013:  """
"MESOS-1698","Bug","test",2,"make check segfaults","""Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2331/consoleFull  It looks like the segfault happens before any tests are run. So I suspect somewhere in the setup phase of the tests.  """
"MESOS-1705","Bug","build",2,"SubprocessTest.Status sometimes flakes out","""It's a pretty rare event, but happened more then once.    [ RUN      ] SubprocessTest.Status *** Aborted at 1408023909 (unix time) try """"date -d @1408023909"""" if you are using GNU date *** PC: @       0x35700094b1 (unknown) *** SIGTERM (@0x3e8000041d8) received by PID 16872 (TID 0x7fa9ea426780) from PID 16856; stack trace: ***     @       0x3570435cb0 (unknown)     @       0x35700094b1 (unknown)     @       0x3570009d9f (unknown)     @       0x357000e726 (unknown)     @       0x3570015185 (unknown)     @           0x5ead42 process::childMain()     @           0x5ece8d std::_Function_handler<>::_M_invoke()     @           0x5eac9c process::defaultClone()     @           0x5ebbd4 process::subprocess()     @           0x55a229 process::subprocess()     @           0x55a846 process::subprocess()     @           0x54224c SubprocessTest_Status_Test::TestBody()     @     0x7fa9ea460323 (unknown)     @     0x7fa9ea455b67 (unknown)     @     0x7fa9ea455c0e (unknown)     @     0x7fa9ea455d15 (unknown)     @     0x7fa9ea4593a8 (unknown)     @     0x7fa9ea459647 (unknown)     @           0x422466 main     @       0x3570421d65 (unknown)     @           0x4260bd (unknown) [       OK ] SubprocessTest.Status (153 ms)"""
"MESOS-1712","Bug","reviewbot",2,"Automate disallowing of commits mixing mesos/libprocess/stout","""For various reasons, we don't want to mix mesos/libprocess/stout changes into a single commit. Typically, it is up to the reviewee/reviewer to catch this.   It wold be nice to automate this via the pre-commit hook ."""
"MESOS-1720","Bug","agent|master",8,"Slave should send exited executor message when the executor is never launched.","""When the slave sends TASK_LOST before launching an executor for a task, the slave does not send an exited executor message to the master.    Since the master receives no exited executor message, it still thinks the executor's resources are consumed on the slave.    One possible fix for this would be to send the exited executor message to the master in these cases."""
"MESOS-1715","Bug","agent",3,"The slave does not send pending tasks during re-registration.","""In what looks like an oversight, the pending tasks and executors in the slave (Framework::pending) are not sent in the re-registration message.  For tasks, this can lead to spurious TASK_LOST notifications being generated by the master when it falsely thinks the tasks are not present on the slave."""
"MESOS-1728","Improvement","libprocess",1,"Libprocess: report bind parameters on failure","""When you attempt to start slave or master and there's another one already running there, it is nice to report what are the actual parameters to {{bind}} call that failed."""
"MESOS-1748","Bug","test",1,"MasterZooKeeperTest.LostZooKeeperCluster is flaky","""{noformat:title=} tests/master_tests.cpp:1795: Failure Failed to wait 10secs for slaveRegisteredMessage {noformat}  Should have placed the FUTURE_MESSAGE that attempts to capture this messages before the slave starts..."""
"MESOS-1760","Bug","test",1,"MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky","""Observed this on Apache CI: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2355/changes  """
"MESOS-1758","Bug","containerization",2,"Freezer failure leads to lost task during container destruction.","""In the past we've seen numerous issues around the freezer. Lately, on the 2.6.44 kernel, we've seen issues where we're unable to freeze the cgroup:  (1) An oom occurs. (2) No indication of oom in the kernel logs. (3) The slave is unable to freeze the cgroup. (4) The task is marked as lost.    We should consider avoiding the freezer entirely in favor of a kill(2) loop. We don't have to wait for pid namespaces to remove the freezer dependency.  At the very least, when the freezer fails, we should proceed with a kill(2) loop to ensure that we destroy the cgroup."""
"MESOS-1766","Bug","test",2,"MasterAuthorizationTest.DuplicateRegistration test is flaky",""""""
"MESOS-1765","Story","containerization",5,"Use PID namespace to avoid freezing cgroup","""There is some known kernel issue when we freeze the whole cgroup upon OOM. Mesos probably can just use PID namespace so that we will only need to kill the """"init"""" of the pid namespace, instead of freezing all the processes and killing them one by one. But I am not quite sure if this would break the existing code."""
"MESOS-1778","Improvement","stout",3,"Provide an option to validate flag value in stout/flags. ","""Currently we can provide the default value for a flag, but cannot check if the flag is set to a reasonable value and, e.g., issue a warning. Passing an optional lambda checker to {{FlagBase::add()}} can be a possible solution."""
"MESOS-1782","Bug","test",1,"AllocatorTest/0.FrameworkExited is flaky","""{noformat:title=} [ RUN      ] AllocatorTest/0.FrameworkExited Using temporary directory '/tmp/AllocatorTest_0_FrameworkExited_B6WZng' I0909 08:02:35.116555 18112 leveldb.cpp:176] Opened db in 31.64686ms I0909 08:02:35.126065 18112 leveldb.cpp:183] Compacted db in 9.449823ms I0909 08:02:35.126118 18112 leveldb.cpp:198] Created db iterator in 5858ns I0909 08:02:35.126137 18112 leveldb.cpp:204] Seeked to beginning of db in 1136ns I0909 08:02:35.126150 18112 leveldb.cpp:273] Iterated through 0 keys in the db in 560ns I0909 08:02:35.126178 18112 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0909 08:02:35.126502 18133 recover.cpp:425] Starting replica recovery I0909 08:02:35.126601 18133 recover.cpp:451] Replica is in EMPTY status I0909 08:02:35.127012 18133 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I0909 08:02:35.127094 18133 recover.cpp:188] Received a recover response from a replica in EMPTY status I0909 08:02:35.127223 18133 recover.cpp:542] Updating replica status to STARTING I0909 08:02:35.226631 18133 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 99.308134ms I0909 08:02:35.226690 18133 replica.cpp:320] Persisted replica status to STARTING I0909 08:02:35.226812 18131 recover.cpp:451] Replica is in STARTING status I0909 08:02:35.227246 18131 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I0909 08:02:35.227308 18131 recover.cpp:188] Received a recover response from a replica in STARTING status I0909 08:02:35.227409 18131 recover.cpp:542] Updating replica status to VOTING I0909 08:02:35.228540 18129 master.cpp:286] Master 20140909-080235-16842879-44005-18112 (precise) started on 127.0.1.1:44005 I0909 08:02:35.228593 18129 master.cpp:332] Master only allowing authenticated frameworks to register I0909 08:02:35.228607 18129 master.cpp:337] Master only allowing authenticated slaves to register I0909 08:02:35.228620 18129 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_FrameworkExited_B6WZng/credentials' I0909 08:02:35.228754 18129 master.cpp:366] Authorization enabled I0909 08:02:35.229560 18129 master.cpp:120] No whitelist given. Advertising offers for all slaves I0909 08:02:35.229933 18129 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@127.0.1.1:44005 I0909 08:02:35.230057 18127 master.cpp:1212] The newly elected leader is master@127.0.1.1:44005 with id 20140909-080235-16842879-44005-18112 I0909 08:02:35.230129 18127 master.cpp:1225] Elected as the leading master! I0909 08:02:35.230144 18127 master.cpp:1043] Recovering from registrar I0909 08:02:35.230257 18127 registrar.cpp:313] Recovering registrar I0909 08:02:35.232461 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 4.999384ms I0909 08:02:35.232489 18131 replica.cpp:320] Persisted replica status to VOTING I0909 08:02:35.232544 18131 recover.cpp:556] Successfully joined the Paxos group I0909 08:02:35.232611 18131 recover.cpp:440] Recover process terminated I0909 08:02:35.232727 18131 log.cpp:656] Attempting to start the writer I0909 08:02:35.233012 18131 replica.cpp:474] Replica received implicit promise request with proposal 1 I0909 08:02:35.238785 18131 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.749504ms I0909 08:02:35.238818 18131 replica.cpp:342] Persisted promised to 1 I0909 08:02:35.244056 18131 coordinator.cpp:230] Coordinator attemping to fill missing position I0909 08:02:35.244580 18131 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I0909 08:02:35.250143 18131 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.382351ms I0909 08:02:35.250319 18131 replica.cpp:676] Persisted action at 0 I0909 08:02:35.250901 18131 replica.cpp:508] Replica received write request for position 0 I0909 08:02:35.251137 18131 leveldb.cpp:438] Reading position from leveldb took 18689ns I0909 08:02:35.256597 18131 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.274169ms I0909 08:02:35.256764 18131 replica.cpp:676] Persisted action at 0 I0909 08:02:35.263712 18126 replica.cpp:655] Replica received learned notice for position 0 I0909 08:02:35.269613 18126 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.417225ms I0909 08:02:35.351641 18126 replica.cpp:676] Persisted action at 0 I0909 08:02:35.351655 18126 replica.cpp:661] Replica learned NOP action at position 0 I0909 08:02:35.351889 18126 log.cpp:672] Writer started with ending position 0 I0909 08:02:35.352165 18126 leveldb.cpp:438] Reading position from leveldb took 25215ns I0909 08:02:35.353163 18126 registrar.cpp:346] Successfully fetched the registry (0B) I0909 08:02:35.353185 18126 registrar.cpp:422] Attempting to update the 'registry' I0909 08:02:35.354152 18126 log.cpp:680] Attempting to append 120 bytes to the log I0909 08:02:35.354195 18126 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0909 08:02:35.354416 18126 replica.cpp:508] Replica received write request for position 1 I0909 08:02:35.351579 18127 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.354558 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 2.984795ms I0909 08:02:35.360254 18126 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.811986ms I0909 08:02:35.360285 18126 replica.cpp:676] Persisted action at 1 I0909 08:02:35.364126 18132 replica.cpp:655] Replica received learned notice for position 1 I0909 08:02:35.369856 18132 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 5.702756ms I0909 08:02:35.369899 18132 replica.cpp:676] Persisted action at 1 I0909 08:02:35.369910 18132 replica.cpp:661] Replica learned APPEND action at position 1 I0909 08:02:35.370209 18132 registrar.cpp:479] Successfully updated 'registry' I0909 08:02:35.370311 18132 registrar.cpp:372] Successfully recovered registrar I0909 08:02:35.370477 18132 log.cpp:699] Attempting to truncate the log to 1 I0909 08:02:35.370553 18132 master.cpp:1070] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register I0909 08:02:35.370594 18132 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0909 08:02:35.371201 18127 replica.cpp:508] Replica received write request for position 2 I0909 08:02:35.376760 18127 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.264501ms I0909 08:02:35.377105 18127 replica.cpp:676] Persisted action at 2 I0909 08:02:35.377770 18127 replica.cpp:655] Replica received learned notice for position 2 I0909 08:02:35.383363 18127 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.272769ms I0909 08:02:35.383818 18127 leveldb.cpp:401] Deleting ~1 keys from leveldb took 28148ns I0909 08:02:35.384137 18127 replica.cpp:676] Persisted action at 2 I0909 08:02:35.384399 18127 replica.cpp:661] Replica learned TRUNCATE action at position 2 I0909 08:02:35.396512 18127 slave.cpp:167] Slave started on 64)@127.0.1.1:44005 I0909 08:02:35.654770 18131 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.654847 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 104933ns I0909 08:02:35.654974 18127 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/credential' I0909 08:02:35.655097 18127 slave.cpp:274] Slave using credential for: test-principal I0909 08:02:35.655203 18127 slave.cpp:287] Slave resources: cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] I0909 08:02:35.655274 18127 slave.cpp:315] Slave hostname: precise I0909 08:02:35.655285 18127 slave.cpp:316] Slave checkpoint: false I0909 08:02:35.655804 18127 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/meta' I0909 08:02:35.655913 18127 status_update_manager.cpp:193] Recovering status update manager I0909 08:02:35.656005 18127 slave.cpp:3202] Finished recovery I0909 08:02:35.656251 18127 slave.cpp:598] New master detected at master@127.0.1.1:44005 I0909 08:02:35.656285 18127 slave.cpp:672] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.656325 18127 slave.cpp:645] Detecting new master I0909 08:02:35.656358 18127 status_update_manager.cpp:167] New master detected at master@127.0.1.1:44005 I0909 08:02:35.656389 18127 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.656563 18127 master.cpp:3653] Authenticating slave(64)@127.0.1.1:44005 I0909 08:02:35.656651 18127 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.656770 18127 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.656796 18127 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.656822 18127 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.656858 18127 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.656883 18127 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.656924 18127 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.656960 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.656971 18127 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.656982 18127 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.656997 18127 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.657004 18127 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.657008 18127 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.657019 18127 authenticator.hpp:376] Authentication success I0909 08:02:35.657047 18127 authenticatee.hpp:305] Authentication success I0909 08:02:35.657073 18127 master.cpp:3693] Successfully authenticated principal 'test-principal' at slave(64)@127.0.1.1:44005 I0909 08:02:35.657145 18127 slave.cpp:729] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.657183 18127 slave.cpp:980] Will retry registration in 19.238717ms if necessary I0909 08:02:35.657276 18128 master.cpp:2843] Registering slave at slave(64)@127.0.1.1:44005 (precise) with id 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.657389 18128 registrar.cpp:422] Attempting to update the 'registry' I0909 08:02:35.658382 18130 log.cpp:680] Attempting to append 295 bytes to the log I0909 08:02:35.658432 18130 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I0909 08:02:35.658635 18130 replica.cpp:508] Replica received write request for position 3 I0909 08:02:35.660959 18112 sched.cpp:137] Version: 0.21.0 I0909 08:02:35.661093 18126 sched.cpp:233] New master detected at master@127.0.1.1:44005 I0909 08:02:35.661111 18126 sched.cpp:283] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.661175 18126 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.661306 18126 master.cpp:3653] Authenticating scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661376 18126 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.661466 18126 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.661483 18126 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.661504 18126 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.661530 18126 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.661552 18126 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.661579 18126 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.661592 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.661598 18126 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.661607 18126 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.661613 18126 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.661619 18126 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.661623 18126 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.661633 18126 authenticator.hpp:376] Authentication success I0909 08:02:35.661653 18126 authenticatee.hpp:305] Authentication success I0909 08:02:35.661672 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661730 18126 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.661741 18126 sched.cpp:476] Sending registration request to master@127.0.1.1:44005 I0909 08:02:35.661782 18126 master.cpp:1331] Received registration request from scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.661798 18126 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*' I0909 08:02:35.661917 18126 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0000 at scheduler-fd929918-7057-4fef-923a-ed9d6fd355be@127.0.1.1:44005 I0909 08:02:35.662017 18126 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.662039 18126 sched.cpp:421] Scheduler::registered took 9070ns I0909 08:02:35.662119 18126 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.662130 18126 hierarchical_allocator_process.hpp:697] No resources available to allocate! I0909 08:02:35.662135 18126 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 5558ns I0909 08:02:35.672230 18130 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 13.567526ms I0909 08:02:35.672268 18130 replica.cpp:676] Persisted action at 3 I0909 08:02:35.672483 18130 replica.cpp:655] Replica received learned notice for position 3 I0909 08:02:35.677322 18132 slave.cpp:980] Will retry registration in 14.890338ms if necessary I0909 08:02:35.677399 18132 master.cpp:2831] Ignoring register slave message from slave(64)@127.0.1.1:44005 (precise) as admission is already in progress I0909 08:02:35.680881 18130 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 8.376798ms I0909 08:02:35.680908 18130 replica.cpp:676] Persisted action at 3 I0909 08:02:35.680917 18130 replica.cpp:661] Replica learned APPEND action at position 3 I0909 08:02:35.681252 18130 registrar.cpp:479] Successfully updated 'registry' I0909 08:02:35.681330 18130 log.cpp:699] Attempting to truncate the log to 3 I0909 08:02:35.681385 18130 master.cpp:2883] Registered slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.681399 18130 master.cpp:4126] Adding slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] I0909 08:02:35.681504 18130 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I0909 08:02:35.681570 18130 slave.cpp:763] Registered with master master@127.0.1.1:44005; given slave ID 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.681689 18130 slave.cpp:2329] Received ping from slave-observer(50)@127.0.1.1:44005 I0909 08:02:35.681753 18130 hierarchical_allocator_process.hpp:442] Added slave 20140909-080235-16842879-44005-18112-0 (precise) with cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] (and cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] available) I0909 08:02:35.681808 18130 hierarchical_allocator_process.hpp:734] Offering cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.681892 18130 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20140909-080235-16842879-44005-18112-0 in 109580ns I0909 08:02:35.681968 18130 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.682014 18130 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.682443 18130 sched.cpp:544] Scheduler::resourceOffers took 254258ns I0909 08:02:35.682633 18130 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-0 with resources cpus(*):3; mem(*):1024; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.682684 18130 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-0 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.682708 18130 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I0909 08:02:35.682971 18130 replica.cpp:508] Replica received write request for position 4 I0909 08:02:35.683132 18132 master.hpp:833] Adding task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.683159 18132 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0000 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.683363 18132 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.683580 18132 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.684833 18133 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.684864 18133 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0000 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs I0909 08:02:35.686401 18132 exec.cpp:132] Version: 0.21.0 I0909 08:02:35.686848 18128 exec.cpp:182] Executor started at: executor(8)@127.0.1.1:44005 with pid 18112 I0909 08:02:35.687095 18132 slave.cpp:1231] Queuing task '0' for executor executor-1 of framework '20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.687302 18132 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0000/executors/executor-1/runs/c4458e43-94ee-4b5e-bd74-5d39a09deff6' I0909 08:02:35.687568 18132 slave.cpp:1741] Got registration for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.687893 18127 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.688789 18127 exec.cpp:218] Executor::registered took 15015ns I0909 08:02:35.688977 18132 slave.cpp:1859] Flushing queued task 0 for executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.689260 18133 exec.cpp:293] Executor asked to run task '0' I0909 08:02:35.689441 18133 exec.cpp:302] Executor::launchTask took 24599ns I0909 08:02:35.691651 18112 sched.cpp:137] Version: 0.21.0 I0909 08:02:35.691946 18131 sched.cpp:233] New master detected at master@127.0.1.1:44005 I0909 08:02:35.692126 18131 sched.cpp:283] Authenticating with master master@127.0.1.1:44005 I0909 08:02:35.692399 18131 authenticatee.hpp:128] Creating new client SASL connection I0909 08:02:35.692791 18131 master.cpp:3653] Authenticating scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.693068 18131 authenticator.hpp:156] Creating new server SASL connection I0909 08:02:35.693351 18131 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5 I0909 08:02:35.693532 18131 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5' I0909 08:02:35.693739 18131 authenticator.hpp:262] Received SASL authentication start I0909 08:02:35.693979 18131 authenticator.hpp:384] Authentication requires more steps I0909 08:02:35.694202 18131 authenticatee.hpp:265] Received SASL authentication step I0909 08:02:35.694449 18131 authenticator.hpp:290] Received SASL authentication step I0909 08:02:35.694633 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0909 08:02:35.694792 18131 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I0909 08:02:35.694980 18131 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0909 08:02:35.695158 18131 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'precise' server FQDN: 'precise' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0909 08:02:35.695369 18131 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.695724 18131 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0909 08:02:35.695907 18131 authenticator.hpp:376] Authentication success I0909 08:02:35.696117 18128 authenticatee.hpp:305] Authentication success I0909 08:02:35.698509 18130 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 15.520863ms I0909 08:02:35.698698 18130 replica.cpp:676] Persisted action at 4 I0909 08:02:35.698940 18128 sched.cpp:357] Successfully authenticated with master master@127.0.1.1:44005 I0909 08:02:35.699095 18126 master.cpp:3693] Successfully authenticated principal 'test-principal' at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.699354 18130 replica.cpp:655] Replica received learned notice for position 4 I0909 08:02:35.699973 18128 sched.cpp:476] Sending registration request to master@127.0.1.1:44005 I0909 08:02:35.700265 18128 master.cpp:1331] Received registration request from scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.700515 18128 master.cpp:1291] Authorizing framework principal 'test-principal' to receive offers for role '*' I0909 08:02:35.700809 18128 master.cpp:1390] Registering framework 20140909-080235-16842879-44005-18112-0001 at scheduler-6e711fc6-aad6-48bd-9ce7-2316b45c5482@127.0.1.1:44005 I0909 08:02:35.701037 18133 sched.cpp:407] Framework registered with 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701211 18133 sched.cpp:421] Scheduler::registered took 11991ns I0909 08:02:35.701488 18131 hierarchical_allocator_process.hpp:329] Added framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701728 18131 hierarchical_allocator_process.hpp:734] Offering cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.701992 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 297969ns I0909 08:02:35.702229 18128 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.702481 18128 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.702901 18129 sched.cpp:544] Scheduler::resourceOffers took 127949ns I0909 08:02:35.703305 18128 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-1 with resources cpus(*):1; mem(*):512; disk(*):25116; ports(*):[31000-32000] on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.703629 18128 master.cpp:2201] Processing reply for offers: [ 20140909-080235-16842879-44005-18112-1 ] on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.703908 18128 master.cpp:2284] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I0909 08:02:35.703789 18132 slave.cpp:2542] Monitoring executor 'executor-1' of framework '20140909-080235-16842879-44005-18112-0000' in container 'c4458e43-94ee-4b5e-bd74-5d39a09deff6' I0909 08:02:35.704763 18128 master.hpp:833] Adding task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.704951 18128 master.cpp:2350] Launching task 0 of framework 20140909-080235-16842879-44005-18112-0001 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 at slave(64)@127.0.1.1:44005 (precise) I0909 08:02:35.705255 18129 slave.cpp:1011] Got assigned task 0 for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.705582 18129 slave.cpp:1121] Launching task 0 for framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.707756 18129 exec.cpp:132] Version: 0.21.0 I0909 08:02:35.708035 18130 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.127072ms I0909 08:02:35.708281 18130 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28817ns I0909 08:02:35.708459 18130 replica.cpp:676] Persisted action at 4 I0909 08:02:35.708632 18130 replica.cpp:661] Replica learned TRUNCATE action at position 4 I0909 08:02:35.708869 18133 exec.cpp:182] Executor started at: executor(9)@127.0.1.1:44005 with pid 18112 I0909 08:02:35.709120 18127 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 35083ns I0909 08:02:35.709511 18129 slave.cpp:1231] Queuing task '0' for executor executor-2 of framework '20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.709707 18129 slave.cpp:552] Successfully attached file '/tmp/AllocatorTest_0_FrameworkExited_xV9Mk4/slaves/20140909-080235-16842879-44005-18112-0/frameworks/20140909-080235-16842879-44005-18112-0001/executors/executor-2/runs/7654870b-fd36-40b2-aac7-37b1bcfa821e' I0909 08:02:35.709913 18129 slave.cpp:1741] Got registration for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.710188 18129 slave.cpp:1859] Flushing queued task 0 for executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.710516 18129 slave.cpp:2542] Monitoring executor 'executor-2' of framework '20140909-080235-16842879-44005-18112-0001' in container '7654870b-fd36-40b2-aac7-37b1bcfa821e' I0909 08:02:35.710321 18130 exec.cpp:206] Executor registered on slave 20140909-080235-16842879-44005-18112-0 I0909 08:02:35.711678 18130 exec.cpp:218] Executor::registered took 14355ns I0909 08:02:35.711987 18130 exec.cpp:293] Executor asked to run task '0' I0909 08:02:35.715551 18130 exec.cpp:302] Executor::launchTask took 3.40476ms I0909 08:02:35.716006 18131 sched.cpp:745] Stopping framework '20140909-080235-16842879-44005-18112-0000' I0909 08:02:35.716292 18128 master.cpp:1640] Asked to unregister framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.716490 18127 hierarchical_allocator_process.hpp:563] Recovered mem(*):256; disk(*):25116; ports(*):[31000-32000] (total allocatable: mem(*):256; disk(*):25116; ports(*):[31000-32000]) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.716792 18127 hierarchical_allocator_process.hpp:599] Framework 20140909-080235-16842879-44005-18112-0001 filtered slave 20140909-080235-16842879-44005-18112-0 for 5secs I0909 08:02:35.717018 18128 master.cpp:3976] Removing framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.717269 18128 master.hpp:851] Removing task 0 with resources cpus(*):2; mem(*):512 on slave 20140909-080235-16842879-44005-18112-0 (precise) W0909 08:02:35.717607 18128 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0000 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING I0909 08:02:35.717470 18131 hierarchical_allocator_process.hpp:405] Deactivated framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718065 18131 hierarchical_allocator_process.hpp:563] Recovered cpus(*):2; mem(*):512 (total allocatable: mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2) on slave 20140909-080235-16842879-44005-18112-0 from framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.717438 18132 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by master@127.0.1.1:44005 I0909 08:02:35.718444 18132 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718621 18132 slave.cpp:2882] Shutting down executor 'executor-1' of framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.718843 18133 exec.cpp:379] Executor asked to shutdown I0909 08:02:35.719022 18133 exec.cpp:394] Executor::shutdown took 13745ns I0909 08:02:35.722009 18128 hierarchical_allocator_process.hpp:360] Removed framework 20140909-080235-16842879-44005-18112-0000 I0909 08:02:35.830785 18131 hierarchical_allocator_process.hpp:734] Offering mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.830940 18131 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 218030ns I0909 08:02:35.831056 18127 master.hpp:861] Adding offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.831115 18127 master.cpp:3600] Sending 1 offers to framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.831248 18127 sched.cpp:544] Scheduler::resourceOffers took 18178ns I0909 08:02:35.831387 18112 master.cpp:650] Master terminating I0909 08:02:35.831441 18112 master.hpp:851] Removing task 0 with resources cpus(*):1; mem(*):256 on slave 20140909-080235-16842879-44005-18112-0 (precise) W0909 08:02:35.831488 18112 master.cpp:4419] Removing task 0 of framework 20140909-080235-16842879-44005-18112-0001 and slave 20140909-080235-16842879-44005-18112-0 in non-terminal state TASK_STAGING I0909 08:02:35.831573 18112 master.hpp:871] Removing offer 20140909-080235-16842879-44005-18112-2 with resources mem(*):768; disk(*):25116; ports(*):[31000-32000]; cpus(*):2 on slave 20140909-080235-16842879-44005-18112-0 (precise) I0909 08:02:35.832608 18112 slave.cpp:475] Slave terminating I0909 08:02:35.832630 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0000 by @0.0.0.0:0 W0909 08:02:35.832643 18112 slave.cpp:1435] Ignoring shutdown framework 20140909-080235-16842879-44005-18112-0000 because it is terminating I0909 08:02:35.832648 18112 slave.cpp:1414] Asked to shut down framework 20140909-080235-16842879-44005-18112-0001 by @0.0.0.0:0 I0909 08:02:35.832654 18112 slave.cpp:1439] Shutting down framework 20140909-080235-16842879-44005-18112-0001 I0909 08:02:35.832664 18112 slave.cpp:2882] Shutting down executor 'executor-2' of framework 20140909-080235-16842879-44005-18112-0001 tests/allocator_tests.cpp:1444: Failure Actual function call count doesn't match EXPECT_CALL(this->allocator, resourcesRecovered(_, _, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] AllocatorTest/0.FrameworkExited, where TypeParam = mesos::internal::master::allocator::HierarchicalAllocatorProcess<mesos::internal::master::allocator::DRFSorter, mesos::internal::master::allocator::DRFSorter> (756 ms) {noformat}"""
"MESOS-1799","Bug","agent|master",3,"Reconciliation can send out-of-order updates.","""When a slave re-registers with the master, it currently sends the latest task state for all tasks that are not both terminal and acknowledged.  However, reconciliation assumes that we always have the latest unacknowledged state of the task represented in the master.  As a result, out-of-order updates are possible, e.g.  (1) Slave has task T in TASK_FINISHED, with unacknowledged updates: [TASK_RUNNING, TASK_FINISHED]. (2) Master fails over. (3) New master re-registers the slave with T in TASK_FINISHED. (4) Reconciliation request arrives, master sends TASK_FINISHED. (5) Slave sends TASK_RUNNING to master, master sends TASK_RUNNING.  I think the fix here is to preserve the task state invariants in the master, namely, that the master has the latest unacknowledged state of the task. This means when the slave re-registers, it should instead send the latest acknowledged state of each task."""
"MESOS-1808","Task","containerization",3,"Expose RTT in container stats","""As we expose the bandwidth, so we should expose the RTT as a measure of latency each container is experiencing.  We can use {{ss}} to get the per-socket statistics and filter and aggregate accordingly to get a measure of RTT."""
"MESOS-1802","Bug","test",5,"HealthCheckTest.HealthStatusChange is flaky on jenkins.","""https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/2374/consoleFull  """
"MESOS-1815","Documentation","documentation",3,"Create a guide to becoming a committer","""We have a committer's guide, but the process by which one becomes a committer is unclear. We should set some guidelines and a process by which we can grow contributors into committers."""
"MESOS-1814","Bug","test",2,"Task attempted to use more offers than requested in example jave and python frameworks",""""""
"MESOS-1813","Improvement","test",1,"Fail fast in example frameworks if task goes into unexpected state","""Most of the example frameworks launch a bunch of tasks and exit if *all* of them reach FINISHED state. But if there is a bug in the code resulting in TASK_LOST, the framework waits forever. Instead the framework should abort if an un-expected task state is encountered."""
"MESOS-1830","Story","master",5,"Expose master stats differentiating between master-generated and slave-generated LOST tasks","""The master exports a monotonically-increasing counter of tasks transitioned to TASK_LOST.  This loses fidelity of the source of the lost task.  A first step in exposing the source of lost tasks might be to just differentiate between TASK_LOST transitions initiated by the master vs the slave (and maybe bad input from the scheduler)."""
"MESOS-1844","Bug","test",1,"AllocatorTest/0.SlaveLost is flaky",""""""
"MESOS-1855","Bug","build",1,"Mesos 0.20.1 doesn't compile","""The compilation of Mesos 0.20.1 fails on Ubuntu Trusty with the following error -  slave/containerizer/mesos/containerizer.cpp  -fPIC -DPIC -o slave/containerizer/mesos/.libs/libmesos_no_3rdparty_la-containerizer.o In file included from ./linux/routing/filter/ip.hpp:36:0,                  from ./slave/containerizer/isolators/network/port_mapping.hpp:42,                  from slave/containerizer/mesos/containerizer.cpp:44: ./linux/routing/filter/filter.hpp:29:43: fatal error: linux/routing/filter/handle.hpp: No such file or directory  #include """"linux/routing/filter/handle.hpp""""                                            ^"""
"MESOS-1853","Bug","containerization",3,"Remove /proc and /sys remounts from port_mapping isolator","""/proc/net reflects a new network namespace regardless and remount doesn't actually do what we expected anyway, i.e., it's not sufficient for a new pid namespace and a new mount is required."""
"MESOS-1862","Bug","master",3,"Performance regression in the Master's http metrics.","""As part of the change to hold on to terminal unacknowledged tasks in the master, we introduced a performance regression during the following patch:  https://github.com/apache/mesos/commit/0760b007ad65bc91e8cea377339978c78d36d247   Rather than keeping a running count of allocated resources, we now compute resources on-demand. This was done in order to ignore terminal task's resources.  As a result of this change, the /stats.json and /metrics/snapshot endpoints on the master have slowed down substantially on large clusters.    {{perf top}} reveals some of the resource computation during a request to stats.json: {noformat: perf top} Events: 36K cycles  10.53%  libc-2.5.so             [.] _int_free   9.90%  libc-2.5.so             [.] malloc   8.56%  libmesos-0.21.0.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::   8.23%  libc-2.5.so             [.] _int_malloc   5.80%  libstdc++.so.6.0.8      [.] std::_Rb_tree_increment(std::_Rb_tree_node_base*)   5.33%  [kernel]                [k] _raw_spin_lock   3.13%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)   2.95%  libmesos-0.21.0.so  [.] process::SocketManager::exited(process::ProcessBase*)   2.43%  libmesos-0.21.0.so  [.] mesos::Resource::MergeFrom(mesos::Resource const&)   1.88%  libmesos-0.21.0.so  [.] mesos::internal::master::Slave::used() const   1.48%  libstdc++.so.6.0.8      [.] __gnu_cxx::__atomic_add(int volatile*, int)   1.45%  [kernel]                [k] find_busiest_group   1.41%  libc-2.5.so             [.] free   1.38%  libmesos-0.21.0.so  [.] mesos::Value_Range::MergeFrom(mesos::Value_Range const&)   1.13%  libmesos-0.21.0.so  [.] mesos::Value_Scalar::MergeFrom(mesos::Value_Scalar const&)   1.12%  libmesos-0.21.0.so  [.] mesos::Resource::SharedDtor()   1.07%  libstdc++.so.6.0.8      [.] __gnu_cxx::__exchange_and_add(int volatile*, int)   0.94%  libmesos-0.21.0.so  [.] google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)   0.92%  libstdc++.so.6.0.8      [.] operator new(unsigned long)   0.88%  libmesos-0.21.0.so  [.] mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&)   0.75%  libmesos-0.21.0.so  [.] mesos::matches(mesos::Resource const&, mesos::Resource const&) {noformat}"""
"MESOS-1865","Bug","json api",3,"Redirect to the leader master when current master is not a leader","""Some of the API endpoints, for example /master/tasks.json, will return bogus information if you query a non-leading master:    This is very hard for end-users to work around.  For example if I query """"which master is leading"""" followed by """"leader: which tasks are running"""" it is possible that the leader fails over in between, leaving me with an incorrect answer and no way to know that this happened.  In my opinion the API should return the correct response (by asking the current leader?) or an error (500 Not the leader?) but it's unacceptable to return a successful wrong answer. """
"MESOS-1941","Improvement","agent|containerization",3,"Make executor's user owner of executor's cgroup directory","""Currently, when cgroups are enabled, and executor is spawned, it's mounted under, for ex: /sys/fs/cgroup/cpu/mesos/<mesos-id>. This directory in current implementation is only writable by root user. This prevents process launched by executor to mount its child processes under this cgroup, because the cgroup directory is only writable by root.  To enable a executor spawned process to mount it's child processes under it's cgroup directory, the cgroup directory should be made writable by the user which spawns the executor."""
"MESOS-2008","Bug","test",2,"MasterAuthorizationTest.DuplicateReregistration is flaky","""{noformat:title=} [ RUN      ] MasterAuthorizationTest.DuplicateReregistration Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX' I1029 08:25:26.021766 32232 leveldb.cpp:176] Opened db in 3.066621ms I1029 08:25:26.022734 32232 leveldb.cpp:183] Compacted db in 935019ns I1029 08:25:26.022766 32232 leveldb.cpp:198] Created db iterator in 4350ns I1029 08:25:26.022785 32232 leveldb.cpp:204] Seeked to beginning of db in 902ns I1029 08:25:26.022799 32232 leveldb.cpp:273] Iterated through 0 keys in the db in 387ns I1029 08:25:26.022831 32232 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1029 08:25:26.023305 32248 recover.cpp:437] Starting replica recovery I1029 08:25:26.023598 32248 recover.cpp:463] Replica is in EMPTY status I1029 08:25:26.025059 32260 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1029 08:25:26.025320 32247 recover.cpp:188] Received a recover response from a replica in EMPTY status I1029 08:25:26.025585 32256 recover.cpp:554] Updating replica status to STARTING I1029 08:25:26.026546 32249 master.cpp:312] Master 20141029-082526-3142697795-40696-32232 (pomona.apache.org) started on 67.195.81.187:40696 I1029 08:25:26.026561 32261 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 694444ns I1029 08:25:26.026592 32249 master.cpp:358] Master only allowing authenticated frameworks to register I1029 08:25:26.026592 32261 replica.cpp:320] Persisted replica status to STARTING I1029 08:25:26.026605 32249 master.cpp:363] Master only allowing authenticated slaves to register I1029 08:25:26.026639 32249 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateReregistration_DLOmYX/credentials' I1029 08:25:26.026877 32249 master.cpp:392] Authorization enabled I1029 08:25:26.026901 32260 recover.cpp:463] Replica is in STARTING status I1029 08:25:26.027498 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:26.027541 32248 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40696 I1029 08:25:26.028055 32252 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1029 08:25:26.028451 32247 recover.cpp:188] Received a recover response from a replica in STARTING status I1029 08:25:26.028733 32249 master.cpp:1242] The newly elected leader is master@67.195.81.187:40696 with id 20141029-082526-3142697795-40696-32232 I1029 08:25:26.028764 32249 master.cpp:1255] Elected as the leading master! I1029 08:25:26.028781 32249 master.cpp:1073] Recovering from registrar I1029 08:25:26.028904 32246 recover.cpp:554] Updating replica status to VOTING I1029 08:25:26.029163 32257 registrar.cpp:313] Recovering registrar I1029 08:25:26.029556 32251 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 485711ns I1029 08:25:26.029588 32251 replica.cpp:320] Persisted replica status to VOTING I1029 08:25:26.029726 32253 recover.cpp:568] Successfully joined the Paxos group I1029 08:25:26.029932 32253 recover.cpp:452] Recover process terminated I1029 08:25:26.030436 32250 log.cpp:656] Attempting to start the writer I1029 08:25:26.032152 32248 replica.cpp:474] Replica received implicit promise request with proposal 1 I1029 08:25:26.032778 32248 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 597030ns I1029 08:25:26.032807 32248 replica.cpp:342] Persisted promised to 1 I1029 08:25:26.033481 32254 coordinator.cpp:230] Coordinator attemping to fill missing position I1029 08:25:26.035429 32247 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1029 08:25:26.036154 32247 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 690208ns I1029 08:25:26.036181 32247 replica.cpp:676] Persisted action at 0 I1029 08:25:26.037344 32249 replica.cpp:508] Replica received write request for position 0 I1029 08:25:26.037395 32249 leveldb.cpp:438] Reading position from leveldb took 22607ns I1029 08:25:26.038074 32249 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 647429ns I1029 08:25:26.038105 32249 replica.cpp:676] Persisted action at 0 I1029 08:25:26.038683 32247 replica.cpp:655] Replica received learned notice for position 0 I1029 08:25:26.039378 32247 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 664911ns I1029 08:25:26.039407 32247 replica.cpp:676] Persisted action at 0 I1029 08:25:26.039433 32247 replica.cpp:661] Replica learned NOP action at position 0 I1029 08:25:26.040045 32252 log.cpp:672] Writer started with ending position 0 I1029 08:25:26.041378 32251 leveldb.cpp:438] Reading position from leveldb took 25625ns I1029 08:25:26.044642 32246 registrar.cpp:346] Successfully fetched the registry (0B) in 15.433984ms I1029 08:25:26.044742 32246 registrar.cpp:445] Applied 1 operations in 16444ns; attempting to update the 'registry' I1029 08:25:26.047538 32256 log.cpp:680] Attempting to append 139 bytes to the log I1029 08:25:26.156330 32247 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1029 08:25:26.158460 32261 replica.cpp:508] Replica received write request for position 1 I1029 08:25:26.159277 32261 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 782308ns I1029 08:25:26.159328 32261 replica.cpp:676] Persisted action at 1 I1029 08:25:26.160267 32255 replica.cpp:655] Replica received learned notice for position 1 I1029 08:25:26.161070 32255 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 750259ns I1029 08:25:26.161100 32255 replica.cpp:676] Persisted action at 1 I1029 08:25:26.161125 32255 replica.cpp:661] Replica learned APPEND action at position 1 I1029 08:25:26.162199 32253 registrar.cpp:490] Successfully updated the 'registry' in 117.40416ms I1029 08:25:26.162400 32253 registrar.cpp:376] Successfully recovered registrar I1029 08:25:26.162724 32249 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1029 08:25:26.162757 32253 log.cpp:699] Attempting to truncate the log to 1 I1029 08:25:26.162919 32256 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1029 08:25:26.163949 32250 replica.cpp:508] Replica received write request for position 2 I1029 08:25:26.164589 32250 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 603175ns I1029 08:25:26.164618 32250 replica.cpp:676] Persisted action at 2 I1029 08:25:26.165385 32251 replica.cpp:655] Replica received learned notice for position 2 I1029 08:25:26.166007 32251 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 594003ns I1029 08:25:26.166056 32251 leveldb.cpp:401] Deleting ~1 keys from leveldb took 23309ns I1029 08:25:26.166077 32251 replica.cpp:676] Persisted action at 2 I1029 08:25:26.166100 32251 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1029 08:25:26.178493 32232 sched.cpp:137] Version: 0.21.0 I1029 08:25:26.179029 32256 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:26.179078 32256 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:26.179424 32246 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:26.179678 32259 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.179970 32250 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:26.180165 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:26.180191 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:26.180272 32250 authenticator.hpp:267] Received SASL authentication start I1029 08:25:26.180378 32250 authenticator.hpp:389] Authentication requires more steps I1029 08:25:26.180557 32260 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:26.180704 32254 authenticator.hpp:295] Received SASL authentication step I1029 08:25:26.180737 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:26.180748 32254 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:26.180780 32254 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:26.180804 32254 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:26.180816 32254 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.180824 32254 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.180841 32254 authenticator.hpp:381] Authentication success I1029 08:25:26.180937 32259 authenticatee.hpp:310] Authentication success I1029 08:25:26.180991 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.181422 32259 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:26.181449 32259 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:26.181697 32260 master.cpp:1362] Received registration request for framework 'default' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.181758 32260 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:26.182063 32260 master.cpp:1426] Registering framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.182430 32248 hierarchical_allocator_process.hpp:329] Added framework 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:26.182462 32248 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:26.182462 32261 sched.cpp:407] Framework registered with 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:26.182473 32248 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 15372ns I1029 08:25:26.182554 32261 sched.cpp:421] Scheduler::registered took 60059ns I1029 08:25:26.185515 32260 sched.cpp:227] Scheduler::disconnected took 16607ns I1029 08:25:26.185538 32260 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:26.185567 32260 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:26.185783 32246 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:26.186218 32250 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.186456 32247 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:26.186594 32250 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:26.186621 32250 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:26.186745 32259 authenticator.hpp:267] Received SASL authentication start I1029 08:25:26.186800 32259 authenticator.hpp:389] Authentication requires more steps I1029 08:25:26.186936 32260 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:26.187062 32249 authenticator.hpp:295] Received SASL authentication step I1029 08:25:26.187095 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:26.187108 32249 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:26.187137 32249 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:26.187162 32249 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:26.187175 32249 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.187182 32249 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:26.187199 32249 authenticator.hpp:381] Authentication success I1029 08:25:26.187327 32249 authenticatee.hpp:310] Authentication success I1029 08:25:26.187366 32260 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:26.187631 32249 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:26.187659 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:27.028445 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:28.045682 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 1.017231941secs I1029 08:25:28.045760 32249 sched.cpp:476] Sending registration request to master@67.195.81.187:40696 I1029 08:25:28.045900 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.045989 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:28.046455 32253 master.cpp:1499] Received re-registration request from framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.046529 32253 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1029 08:25:28.050155 32247 sched.cpp:233] New master detected at master@67.195.81.187:40696 I1029 08:25:28.050217 32247 sched.cpp:283] Authenticating with master master@67.195.81.187:40696 I1029 08:25:28.050405 32252 master.cpp:1552] Re-registering framework 20141029-082526-3142697795-40696-32232-0000 (default)  at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.050509 32253 authenticatee.hpp:133] Creating new client SASL connection I1029 08:25:28.050566 32252 master.cpp:1592] Allowing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 to re-register with an already used id I1029 08:25:28.051084 32257 sched.cpp:449] Framework re-registered with 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:28.051151 32252 master.cpp:3853] Authenticating scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.051167 32257 sched.cpp:463] Scheduler::reregistered took 52801ns I1029 08:25:28.051723 32261 authenticator.hpp:161] Creating new server SASL connection I1029 08:25:28.052042 32249 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1029 08:25:28.052077 32249 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1029 08:25:28.052170 32249 master.cpp:1534] Dropping re-registration request of framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 because new authentication attempt is in progress I1029 08:25:28.052218 32257 authenticator.hpp:267] Received SASL authentication start I1029 08:25:28.052325 32257 authenticator.hpp:389] Authentication requires more steps I1029 08:25:28.052428 32257 authenticatee.hpp:270] Received SASL authentication step I1029 08:25:28.052641 32246 authenticator.hpp:295] Received SASL authentication step I1029 08:25:28.052685 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1029 08:25:28.052701 32246 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1029 08:25:28.052739 32246 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1029 08:25:28.052767 32246 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1029 08:25:28.052779 32246 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1029 08:25:28.052788 32246 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1029 08:25:28.052804 32246 authenticator.hpp:381] Authentication success I1029 08:25:28.052947 32252 authenticatee.hpp:310] Authentication success I1029 08:25:28.053020 32246 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:28.053462 32247 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40696 I1029 08:25:29.046855 32261 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:29.046880 32261 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 35632ns I1029 08:25:30.047458 32253 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:30.047487 32253 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 43031ns I1029 08:25:31.028373 32261 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:31.048673 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:31.048702 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 44769ns I1029 08:25:32.049576 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:32.049604 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 51919ns I1029 08:25:33.050864 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:33.050896 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38019ns I1029 08:25:34.051961 32251 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:34.051993 32251 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 64619ns I1029 08:25:35.052196 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:35.052223 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 34475ns I1029 08:25:36.029101 32259 master.cpp:120] No whitelist given. Advertising offers for all slaves I1029 08:25:36.053067 32249 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:36.053095 32249 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38354ns I1029 08:25:37.053506 32259 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:37.053536 32259 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38249ns tests/master_authorization_tests.cpp:877: Failure Failed to wait 10secs for frameworkReregisteredMessage I1029 08:25:38.053241 32259 master.cpp:768] Framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 disconnected I1029 08:25:38.053375 32259 master.cpp:1731] Disconnecting framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.053426 32259 master.cpp:1747] Deactivating framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.053932 32259 master.cpp:790] Giving framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 0ns to failover I1029 08:25:38.054072 32257 hierarchical_allocator_process.hpp:405] Deactivated framework 20141029-082526-3142697795-40696-32232-0000 I1029 08:25:38.054208 32257 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1029 08:25:38.054236 32257 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 38534ns I1029 08:25:38.054508 32258 master.cpp:3665] Framework failover timeout, removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.054549 32258 master.cpp:4201] Removing framework 20141029-082526-3142697795-40696-32232-0000 (default) at scheduler-9ba6b803-40b4-48b9-bcef-45a329f6b2a4@67.195.81.187:40696 I1029 08:25:38.055179 32252 master.cpp:677] Master terminating I1029 08:25:38.055181 32254 hierarchical_allocator_process.hpp:360] Removed framework 20141029-082526-3142697795-40696-32232-0000 ../3rdparty/libprocess/include/process/gmock.hpp:345: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <B8-BD 01-88 4A-2B 00-00>, 1-byte object <95>, 1-byte object <30>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] MasterAuthorizationTest.DuplicateReregistration (12042 ms) {noformat}"""
"MESOS-2007","Bug","test",2,"AllocatorTest/0.SlaveReregistersFirst is flaky","""{noformat:title=} [ RUN      ] AllocatorTest/0.SlaveReregistersFirst Using temporary directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d' I1028 23:48:22.360447 31190 leveldb.cpp:176] Opened db in 2.192575ms I1028 23:48:22.361253 31190 leveldb.cpp:183] Compacted db in 760753ns I1028 23:48:22.361320 31190 leveldb.cpp:198] Created db iterator in 22188ns I1028 23:48:22.361340 31190 leveldb.cpp:204] Seeked to beginning of db in 1950ns I1028 23:48:22.361351 31190 leveldb.cpp:273] Iterated through 0 keys in the db in 345ns I1028 23:48:22.361403 31190 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1028 23:48:22.362185 31217 recover.cpp:437] Starting replica recovery I1028 23:48:22.362764 31219 recover.cpp:463] Replica is in EMPTY status I1028 23:48:22.363955 31210 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1028 23:48:22.364320 31217 recover.cpp:188] Received a recover response from a replica in EMPTY status I1028 23:48:22.364820 31211 recover.cpp:554] Updating replica status to STARTING I1028 23:48:22.365365 31215 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418991ns I1028 23:48:22.365391 31215 replica.cpp:320] Persisted replica status to STARTING I1028 23:48:22.365617 31217 recover.cpp:463] Replica is in STARTING status I1028 23:48:22.366328 31206 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043 I1028 23:48:22.366377 31206 master.cpp:358] Master only allowing authenticated frameworks to register I1028 23:48:22.366391 31206 master.cpp:363] Master only allowing authenticated slaves to register I1028 23:48:22.366402 31206 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials' I1028 23:48:22.366708 31206 master.cpp:392] Authorization enabled I1028 23:48:22.366886 31209 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1028 23:48:22.367311 31208 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:22.367312 31207 recover.cpp:188] Received a recover response from a replica in STARTING status I1028 23:48:22.367686 31211 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043 I1028 23:48:22.367863 31212 recover.cpp:554] Updating replica status to VOTING I1028 23:48:22.368477 31218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 375527ns I1028 23:48:22.368505 31218 replica.cpp:320] Persisted replica status to VOTING I1028 23:48:22.368517 31204 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190 I1028 23:48:22.368549 31204 master.cpp:1255] Elected as the leading master! I1028 23:48:22.368567 31204 master.cpp:1073] Recovering from registrar I1028 23:48:22.368621 31215 recover.cpp:568] Successfully joined the Paxos group I1028 23:48:22.368716 31219 registrar.cpp:313] Recovering registrar I1028 23:48:22.369000 31215 recover.cpp:452] Recover process terminated I1028 23:48:22.369523 31208 log.cpp:656] Attempting to start the writer I1028 23:48:22.370909 31205 replica.cpp:474] Replica received implicit promise request with proposal 1 I1028 23:48:22.371266 31205 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 325016ns I1028 23:48:22.371290 31205 replica.cpp:342] Persisted promised to 1 I1028 23:48:22.371979 31218 coordinator.cpp:230] Coordinator attemping to fill missing position I1028 23:48:22.373378 31210 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1028 23:48:22.373746 31210 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 329018ns I1028 23:48:22.373772 31210 replica.cpp:676] Persisted action at 0 I1028 23:48:22.374897 31214 replica.cpp:508] Replica received write request for position 0 I1028 23:48:22.374951 31214 leveldb.cpp:438] Reading position from leveldb took 26002ns I1028 23:48:22.375272 31214 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 289094ns I1028 23:48:22.375298 31214 replica.cpp:676] Persisted action at 0 I1028 23:48:22.375886 31204 replica.cpp:655] Replica received learned notice for position 0 I1028 23:48:22.376258 31204 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346650ns I1028 23:48:22.376277 31204 replica.cpp:676] Persisted action at 0 I1028 23:48:22.376298 31204 replica.cpp:661] Replica learned NOP action at position 0 I1028 23:48:22.376843 31215 log.cpp:672] Writer started with ending position 0 I1028 23:48:22.378056 31205 leveldb.cpp:438] Reading position from leveldb took 28265ns I1028 23:48:22.380323 31217 registrar.cpp:346] Successfully fetched the registry (0B) in 11.55584ms I1028 23:48:22.380466 31217 registrar.cpp:445] Applied 1 operations in 50632ns; attempting to update the 'registry' I1028 23:48:22.382472 31217 log.cpp:680] Attempting to append 139 bytes to the log I1028 23:48:22.382715 31210 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1028 23:48:22.383463 31210 replica.cpp:508] Replica received write request for position 1 I1028 23:48:22.383857 31210 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 363758ns I1028 23:48:22.383875 31210 replica.cpp:676] Persisted action at 1 I1028 23:48:22.384397 31218 replica.cpp:655] Replica received learned notice for position 1 I1028 23:48:22.384840 31218 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 420161ns I1028 23:48:22.384862 31218 replica.cpp:676] Persisted action at 1 I1028 23:48:22.384882 31218 replica.cpp:661] Replica learned APPEND action at position 1 I1028 23:48:22.385684 31211 registrar.cpp:490] Successfully updated the 'registry' in 5.158144ms I1028 23:48:22.385818 31211 registrar.cpp:376] Successfully recovered registrar I1028 23:48:22.385912 31214 log.cpp:699] Attempting to truncate the log to 1 I1028 23:48:22.386101 31218 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1028 23:48:22.386124 31211 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1028 23:48:22.387398 31209 replica.cpp:508] Replica received write request for position 2 I1028 23:48:22.387758 31209 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 334969ns I1028 23:48:22.387776 31209 replica.cpp:676] Persisted action at 2 I1028 23:48:22.388272 31204 replica.cpp:655] Replica received learned notice for position 2 I1028 23:48:22.388453 31204 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 159390ns I1028 23:48:22.388501 31204 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30409ns I1028 23:48:22.388516 31204 replica.cpp:676] Persisted action at 2 I1028 23:48:22.388531 31204 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1028 23:48:22.400737 31207 slave.cpp:169] Slave started on 34)@67.195.81.190:50043 I1028 23:48:22.400786 31207 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/credential' I1028 23:48:22.400996 31207 slave.cpp:276] Slave using credential for: test-principal I1028 23:48:22.401304 31207 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:22.401413 31207 slave.cpp:318] Slave hostname: pietas.apache.org I1028 23:48:22.401520 31207 slave.cpp:319] Slave checkpoint: false W1028 23:48:22.401535 31207 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I1028 23:48:22.402349 31207 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/meta' I1028 23:48:22.402678 31207 status_update_manager.cpp:197] Recovering status update manager I1028 23:48:22.403048 31211 slave.cpp:3456] Finished recovery I1028 23:48:22.403815 31215 slave.cpp:602] New master detected at master@67.195.81.190:50043 I1028 23:48:22.403852 31215 slave.cpp:665] Authenticating with master master@67.195.81.190:50043 I1028 23:48:22.403875 31206 status_update_manager.cpp:171] Pausing sending status updates I1028 23:48:22.403961 31215 slave.cpp:638] Detecting new master I1028 23:48:22.404016 31211 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:22.404230 31204 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043 I1028 23:48:22.404464 31205 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:22.404613 31211 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:22.404649 31211 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:22.404734 31211 authenticator.hpp:267] Received SASL authentication start I1028 23:48:22.404783 31211 authenticator.hpp:389] Authentication requires more steps I1028 23:48:22.404898 31215 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:22.404999 31215 authenticator.hpp:295] Received SASL authentication step I1028 23:48:22.405030 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:22.405047 31215 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:22.405086 31215 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:22.405109 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:22.405122 31215 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.405129 31215 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.405146 31215 authenticator.hpp:381] Authentication success I1028 23:48:22.405243 31213 authenticatee.hpp:310] Authentication success I1028 23:48:22.405253 31214 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043 I1028 23:48:22.405505 31213 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:22.405619 31213 slave.cpp:1050] Will retry registration in 17.050994ms if necessary I1028 23:48:22.405819 31215 master.cpp:3032] Registering slave at slave(34)@67.195.81.190:50043 (pietas.apache.org) with id 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.406262 31216 registrar.cpp:445] Applied 1 operations in 52647ns; attempting to update the 'registry' I1028 23:48:22.406697 31190 sched.cpp:137] Version: 0.21.0 I1028 23:48:22.407083 31211 sched.cpp:233] New master detected at master@67.195.81.190:50043 I1028 23:48:22.407114 31211 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:22.407290 31214 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:22.407424 31214 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.407659 31207 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:22.407757 31207 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:22.407774 31207 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:22.407830 31207 authenticator.hpp:267] Received SASL authentication start I1028 23:48:22.407868 31207 authenticator.hpp:389] Authentication requires more steps I1028 23:48:22.407927 31207 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:22.408015 31212 authenticator.hpp:295] Received SASL authentication step I1028 23:48:22.408037 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:22.408046 31212 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:22.408072 31212 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:22.408092 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:22.408100 31212 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.408105 31212 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:22.408116 31212 authenticator.hpp:381] Authentication success I1028 23:48:22.408192 31210 authenticatee.hpp:310] Authentication success I1028 23:48:22.408210 31217 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.408419 31210 sched.cpp:357] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:22.408460 31210 sched.cpp:476] Sending registration request to master@67.195.81.190:50043 I1028 23:48:22.408568 31217 master.cpp:1362] Received registration request for framework 'default' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.408617 31217 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*' I1028 23:48:22.408937 31214 master.cpp:1426] Registering framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.409265 31213 sched.cpp:407] Framework registered with 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.409267 31212 hierarchical_allocator_process.hpp:329] Added framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.409312 31212 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1028 23:48:22.409324 31215 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:22.409333 31213 sched.cpp:421] Scheduler::registered took 38591ns I1028 23:48:22.409327 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 24107ns I1028 23:48:22.409518 31205 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I1028 23:48:22.410127 31206 replica.cpp:508] Replica received write request for position 3 I1028 23:48:22.410706 31206 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554098ns I1028 23:48:22.410725 31206 replica.cpp:676] Persisted action at 3 I1028 23:48:22.411151 31217 replica.cpp:655] Replica received learned notice for position 3 I1028 23:48:22.411499 31217 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 326572ns I1028 23:48:22.411519 31217 replica.cpp:676] Persisted action at 3 I1028 23:48:22.411533 31217 replica.cpp:661] Replica learned APPEND action at position 3 I1028 23:48:22.412292 31219 registrar.cpp:490] Successfully updated the 'registry' in 5.972992ms I1028 23:48:22.412518 31218 log.cpp:699] Attempting to truncate the log to 3 I1028 23:48:22.412621 31213 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4 I1028 23:48:22.412734 31219 slave.cpp:2522] Received ping from slave-observer(38)@67.195.81.190:50043 I1028 23:48:22.412787 31206 master.cpp:3086] Registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:22.412858 31219 slave.cpp:756] Registered with master master@67.195.81.190:50043; given slave ID 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.412994 31210 status_update_manager.cpp:178] Resuming sending status updates I1028 23:48:22.413014 31211 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available) I1028 23:48:22.413159 31211 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] on slave 20141028-234822-3193029443-50043-31190-S0 to framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.413290 31208 replica.cpp:508] Replica received write request for position 4 I1028 23:48:22.413421 31211 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 346658ns I1028 23:48:22.413650 31208 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 336067ns I1028 23:48:22.413668 31208 replica.cpp:676] Persisted action at 4 I1028 23:48:22.413797 31216 master.cpp:3795] Sending 1 offers to framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:22.414077 31212 replica.cpp:655] Replica received learned notice for position 4 I1028 23:48:22.414356 31212 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260401ns I1028 23:48:22.414403 31212 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28541ns I1028 23:48:22.414417 31212 replica.cpp:676] Persisted action at 4 I1028 23:48:22.414446 31212 replica.cpp:661] Replica learned TRUNCATE action at position 4 I1028 23:48:22.414422 31207 sched.cpp:544] Scheduler::resourceOffers took 310278ns I1028 23:48:22.415086 31214 master.cpp:2321] Processing reply for offers: [ 20141028-234822-3193029443-50043-31190-O0 ] on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) for framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 W1028 23:48:22.415163 31214 master.cpp:1969] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases. W1028 23:48:22.415186 31214 master.cpp:1980] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases. I1028 23:48:22.415256 31214 master.cpp:2417] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins' I1028 23:48:22.416033 31219 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) I1028 23:48:22.416084 31219 master.cpp:2480] Launching task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.416317 31214 slave.cpp:1081] Got assigned task 0 for framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.416679 31215 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.416721 31215 hierarchical_allocator_process.hpp:599] Framework 20141028-234822-3193029443-50043-31190-0000 filtered slave 20141028-234822-3193029443-50043-31190-S0 for 5secs I1028 23:48:22.416724 31214 slave.cpp:1191] Launching task 0 for framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.418534 31214 slave.cpp:3871] Launching executor default of framework 20141028-234822-3193029443-50043-31190-0000 in work directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.420557 31214 exec.cpp:132] Version: 0.21.0 I1028 23:48:22.420755 31213 exec.cpp:182] Executor started at: executor(22)@67.195.81.190:50043 with pid 31190 I1028 23:48:22.420903 31214 slave.cpp:1317] Queuing task '0' for executor default of framework '20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.420997 31214 slave.cpp:555] Successfully attached file '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.421058 31214 slave.cpp:1849] Got registration for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043 I1028 23:48:22.421295 31214 slave.cpp:1968] Flushing queued task 0 for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.421391 31205 exec.cpp:206] Executor registered on slave 20141028-234822-3193029443-50043-31190-S0 I1028 23:48:22.421495 31214 slave.cpp:2802] Monitoring executor 'default' of framework '20141028-234822-3193029443-50043-31190-0000' in container 'd593f433-3c16-4678-8f76-4038fe2841c4' I1028 23:48:22.422873 31205 exec.cpp:218] Executor::registered took 19148ns I1028 23:48:22.422991 31205 exec.cpp:293] Executor asked to run task '0' I1028 23:48:22.423085 31205 exec.cpp:302] Executor::launchTask took 76519ns I1028 23:48:22.424541 31205 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.424724 31205 slave.cpp:2202] Handling status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043 I1028 23:48:22.424932 31213 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.424963 31213 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425122 31213 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to the slave I1028 23:48:22.425257 31205 slave.cpp:2442] Forwarding the update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to master@67.195.81.190:50043 I1028 23:48:22.425398 31205 slave.cpp:2369] Status update manager successfully handled status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425420 31205 slave.cpp:2375] Sending acknowledgement for status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to executor(22)@67.195.81.190:50043 I1028 23:48:22.425583 31212 master.cpp:3410] Forwarding status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425621 31206 exec.cpp:339] Executor received status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.425786 31212 master.cpp:3382] Status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.425832 31212 master.cpp:4617] Updating the latest state of task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to TASK_RUNNING I1028 23:48:22.425885 31208 sched.cpp:635] Scheduler::statusUpdate took 49727ns I1028 23:48:22.426082 31208 master.cpp:2882] Forwarding status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 to slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.426360 31206 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.426623 31206 slave.cpp:1789] Status update manager successfully handled status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.426893 31210 master.cpp:677] Master terminating W1028 23:48:22.427028 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING I1028 23:48:22.427397 31209 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000 I1028 23:48:22.427512 31210 master.cpp:4705] Removing executor 'default' with resources  of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:22.428129 31206 slave.cpp:2607] master@67.195.81.190:50043 exited W1028 23:48:22.428153 31206 slave.cpp:2610] Master disconnected! Waiting for a new master to be elected I1028 23:48:22.434645 31190 leveldb.cpp:176] Opened db in 2.551453ms I1028 23:48:22.437157 31190 leveldb.cpp:183] Compacted db in 2.484612ms I1028 23:48:22.437203 31190 leveldb.cpp:198] Created db iterator in 19171ns I1028 23:48:22.437235 31190 leveldb.cpp:204] Seeked to beginning of db in 18300ns I1028 23:48:22.437306 31190 leveldb.cpp:273] Iterated through 3 keys in the db in 59465ns I1028 23:48:22.437347 31190 replica.cpp:741] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned I1028 23:48:22.437827 31216 recover.cpp:437] Starting replica recovery I1028 23:48:22.438127 31216 recover.cpp:463] Replica is in VOTING status I1028 23:48:22.438443 31216 recover.cpp:452] Recover process terminated I1028 23:48:22.439877 31212 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043 I1028 23:48:22.439916 31212 master.cpp:358] Master only allowing authenticated frameworks to register I1028 23:48:22.439931 31212 master.cpp:363] Master only allowing authenticated slaves to register I1028 23:48:22.439946 31212 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials' I1028 23:48:22.440142 31212 master.cpp:392] Authorization enabled I1028 23:48:22.440439 31218 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:22.440901 31213 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043 I1028 23:48:22.441395 31206 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190 I1028 23:48:22.441421 31206 master.cpp:1255] Elected as the leading master! I1028 23:48:22.441457 31206 master.cpp:1073] Recovering from registrar I1028 23:48:22.441623 31205 registrar.cpp:313] Recovering registrar I1028 23:48:22.442172 31219 log.cpp:656] Attempting to start the writer I1028 23:48:22.443235 31219 replica.cpp:474] Replica received implicit promise request with proposal 2 I1028 23:48:22.443685 31219 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 427888ns I1028 23:48:22.443703 31219 replica.cpp:342] Persisted promised to 2 I1028 23:48:22.444371 31213 coordinator.cpp:230] Coordinator attemping to fill missing position I1028 23:48:22.444687 31209 log.cpp:672] Writer started with ending position 4 I1028 23:48:22.445754 31215 leveldb.cpp:438] Reading position from leveldb took 47909ns I1028 23:48:22.445826 31215 leveldb.cpp:438] Reading position from leveldb took 30611ns I1028 23:48:22.446941 31218 registrar.cpp:346] Successfully fetched the registry (277B) in 5.213184ms I1028 23:48:22.447118 31218 registrar.cpp:445] Applied 1 operations in 42362ns; attempting to update the 'registry' I1028 23:48:22.449329 31204 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:22.449477 31218 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5 I1028 23:48:22.450187 31215 replica.cpp:508] Replica received write request for position 5 I1028 23:48:22.450767 31215 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554400ns I1028 23:48:22.450788 31215 replica.cpp:676] Persisted action at 5 I1028 23:48:22.451561 31215 replica.cpp:655] Replica received learned notice for position 5 I1028 23:48:22.451979 31215 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 397219ns I1028 23:48:22.452000 31215 replica.cpp:676] Persisted action at 5 I1028 23:48:22.452020 31215 replica.cpp:661] Replica learned APPEND action at position 5 I1028 23:48:22.452993 31213 registrar.cpp:490] Successfully updated the 'registry' in 5.816832ms I1028 23:48:22.453136 31213 registrar.cpp:376] Successfully recovered registrar I1028 23:48:22.453238 31208 log.cpp:699] Attempting to truncate the log to 5 I1028 23:48:22.453384 31214 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6 I1028 23:48:22.453518 31215 master.cpp:1100] Recovered 1 slaves from the Registry (277B) ; allowing 10mins for slaves to re-register I1028 23:48:22.454116 31207 replica.cpp:508] Replica received write request for position 6 I1028 23:48:22.454570 31207 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 427424ns I1028 23:48:22.454589 31207 replica.cpp:676] Persisted action at 6 I1028 23:48:22.455095 31219 replica.cpp:655] Replica received learned notice for position 6 I1028 23:48:22.455399 31219 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 282466ns I1028 23:48:22.455462 31219 leveldb.cpp:401] Deleting ~2 keys from leveldb took 43939ns I1028 23:48:22.455478 31219 replica.cpp:676] Persisted action at 6 I1028 23:48:22.455494 31219 replica.cpp:661] Replica learned TRUNCATE action at position 6 I1028 23:48:22.465553 31213 status_update_manager.cpp:171] Pausing sending status updates I1028 23:48:22.465566 31216 slave.cpp:602] New master detected at master@67.195.81.190:50043 I1028 23:48:22.465612 31216 slave.cpp:665] Authenticating with master master@67.195.81.190:50043 I1028 23:48:23.441506 31206 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1028 23:48:27.441004 31214 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:30.101379 31206 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 6.659877806secs I1028 23:48:30.101568 31216 slave.cpp:638] Detecting new master I1028 23:48:30.101632 31214 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:30.102021 31218 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043 I1028 23:48:30.102329 31212 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:30.102505 31216 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:30.102545 31216 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:30.102638 31216 authenticator.hpp:267] Received SASL authentication start I1028 23:48:30.102709 31216 authenticator.hpp:389] Authentication requires more steps I1028 23:48:30.102812 31216 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:30.102957 31204 authenticator.hpp:295] Received SASL authentication step I1028 23:48:30.102982 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:30.102993 31204 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:30.103032 31204 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:30.103049 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1028 23:48:30.103056 31204 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1028 23:48:30.103061 31204 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1028 23:48:30.103073 31204 authenticator.hpp:381] Authentication success I1028 23:48:30.103149 31209 authenticatee.hpp:310] Authentication success I1028 23:48:30.103153 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043 I1028 23:48:30.103371 31209 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043 I1028 23:48:30.103773 31209 slave.cpp:1050] Will retry registration in 12.861518ms if necessary I1028 23:48:30.104068 31219 master.cpp:3210] Re-registering slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:30.104760 31216 registrar.cpp:445] Applied 1 operations in 71655ns; attempting to update the 'registry' I1028 23:48:30.107877 31205 log.cpp:680] Attempting to append 316 bytes to the log I1028 23:48:30.108070 31219 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 7 I1028 23:48:30.109110 31211 replica.cpp:508] Replica received write request for position 7 I1028 23:48:30.109434 31211 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 281545ns I1028 23:48:30.109484 31211 replica.cpp:676] Persisted action at 7 I1028 23:48:30.110124 31219 replica.cpp:655] Replica received learned notice for position 7 I1028 23:48:30.110903 31219 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 750414ns I1028 23:48:30.110927 31219 replica.cpp:676] Persisted action at 7 I1028 23:48:30.110950 31219 replica.cpp:661] Replica learned APPEND action at position 7 I1028 23:48:30.112160 31205 registrar.cpp:490] Successfully updated the 'registry' in 7.33824ms I1028 23:48:30.112529 31217 log.cpp:699] Attempting to truncate the log to 7 I1028 23:48:30.112714 31207 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 8 I1028 23:48:30.112870 31210 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) W1028 23:48:30.113136 31210 master.cpp:4394] Possibly orphaned task 0 of framework 20141028-234822-3193029443-50043-31190-0000 running on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) I1028 23:48:30.113198 31219 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043 I1028 23:48:30.113340 31210 master.cpp:3278] Re-registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] I1028 23:48:30.113499 31219 slave.cpp:824] Re-registered with master master@67.195.81.190:50043 I1028 23:48:30.113636 31219 replica.cpp:508] Replica received write request for position 8 I1028 23:48:30.113652 31210 status_update_manager.cpp:178] Resuming sending status updates I1028 23:48:30.113759 31212 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] available) I1028 23:48:30.113904 31212 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 74698ns I1028 23:48:30.114116 31219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 452165ns I1028 23:48:30.114142 31219 replica.cpp:676] Persisted action at 8 I1028 23:48:30.114786 31213 replica.cpp:655] Replica received learned notice for position 8 I1028 23:48:30.115337 31213 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 525187ns I1028 23:48:30.115399 31213 leveldb.cpp:401] Deleting ~2 keys from leveldb took 37689ns I1028 23:48:30.115418 31213 replica.cpp:676] Persisted action at 8 I1028 23:48:30.115484 31213 replica.cpp:661] Replica learned TRUNCATE action at position 8 I1028 23:48:30.116603 31212 sched.cpp:227] Scheduler::disconnected took 16969ns I1028 23:48:30.116624 31212 sched.cpp:233] New master detected at master@67.195.81.190:50043 I1028 23:48:30.116657 31212 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:30.116870 31205 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:30.117084 31207 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:30.117279 31212 authenticator.hpp:161] Creating new server SASL connection I1028 23:48:30.117410 31210 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1028 23:48:30.117507 31210 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1028 23:48:30.117604 31214 authenticator.hpp:267] Received SASL authentication start I1028 23:48:30.117652 31214 authenticator.hpp:389] Authentication requires more steps I1028 23:48:30.117738 31210 authenticatee.hpp:270] Received SASL authentication step I1028 23:48:30.117905 31208 authenticator.hpp:295] Received SASL authentication step I1028 23:48:30.117935 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1028 23:48:30.117947 31208 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1028 23:48:30.117979 31208 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1028 23:48:30.118001 31208 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I../../src/tests/allocator_tests.cpp:2405: Failure 1028 23:48:30.118013 31208 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true Failed to wait 10secs for resourceOffers2 I1028 23:48:31.101976 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 1 slaves in 124354ns I1028 23:48:58.775811 31208 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true W1028 23:48:35.117725 31214 sched.cpp:378] Authentication timed out W1028 23:48:35.117784 31219 master.cpp:3911] Authentication timed out I1028 23:48:45.114322 31213 slave.cpp:2522] Received ping from slave-observer(39)@67.195.81.190:50043 I1028 23:48:35.102212 31206 master.cpp:120] No whitelist given. Advertising offers for all slaves I1028 23:48:58.775874 31208 authenticator.hpp:381] Authentication success I1028 23:48:58.776267 31214 sched.cpp:338] Failed to authenticate with master master@67.195.81.190:50043: Authentication discarded ../../src/tests/allocator_tests.cpp:2396: Failure Actual function call count doesn't match EXPECT_CALL(allocator2, frameworkAdded(_, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1028 23:48:58.776526 31204 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 I1028 23:48:58.776626 31214 sched.cpp:283] Authenticating with master master@67.195.81.190:50043 I1028 23:48:58.776928 31204 authenticatee.hpp:133] Creating new client SASL connection I1028 23:48:58.777194 31210 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 W1028 23:48:58.777528 31210 master.cpp:3888] Failed to authenticate scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043: Failed to communicate with authenticatee ../../src/tests/allocator_tests.cpp:2399: Failure Actual function call count doesn't match EXPECT_CALL(sched, resourceOffers(&driver, _))...          Expected: to be called once            Actual: never called - unsatisfied and active ../../src/tests/allocator_tests.cpp:2394: Failure Actual function call count doesn't match EXPECT_CALL(sched, registered(&driver, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1028 23:48:58.778053 31205 slave.cpp:591] Re-detecting master I1028 23:48:58.778084 31205 slave.cpp:638] Detecting new master I1028 23:48:58.778115 31207 status_update_manager.cpp:171] Pausing sending status updates F1028 23:48:58.778115 31205 logging.cpp:57] RAW: Pure virtual method called I1028 23:48:58.778724 31210 master.cpp:677] Master terminating W1028 23:48:58.778919 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING *** Aborted at 1414540138 (unix time) try """"date -d @1414540138"""" if you are using GNU date *** PC: @           0x91bc86 process::PID<>::PID() *** SIGSEGV (@0x0) received by PID 31190 (TID 0x2b20a6d95700) from PID 0; stack trace: ***     @     0x2b20a41ff340 (unknown)     @     0x2b20a1f2a188  google::LogMessage::Fail()     @     0x2b20a1f2f87c  google::RawLog__()     @           0x91bc86 process::PID<>::PID()     @           0x91bf24 process::Process<>::self()     @     0x2b20a15d5c06  __cxa_pure_virtual     @     0x2b20a1877752  mesos::internal::slave::Slave::detected()     @     0x2b20a1671f24 process::dispatch<>()     @     0x2b20a18b35f9  _ZZN7process8dispatchIN5mesos8internal5slave5SlaveERKNS_6FutureI6OptionINS1_10MasterInfoEEEES9_EEvRKNS_3PIDIT_EEMSD_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESM_     @     0x2b20a1663217 mesos::internal::master::allocator::Allocator::resourcesRecovered()     @     0x2b20a1650d01 mesos::internal::master::Master::removeTask()     @     0x2b20a162fb41 mesos::internal::master::Master::finalize()     @     0x2b20a1eb69a1 process::ProcessBase::visit()     @     0x2b20a1ec0464 process::TerminateEvent::visit()     @           0x8e0812 process::ProcessBase::serve()     @     0x2b20a18da89e  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveERKNS0_6FutureI6OptionINS5_10MasterInfoEEEESD_EEvRKNS0_3PIDIT_EEMSH_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_     @     0x2b20a1eb1ca0 process::ProcessManager::resume()     @     0x2b20a1ea8365 process::schedule()     @     0x2b20a41f7182 start_thread     @     0x2b20a4507fbd (unknown) make[3]: *** [check-local] Segmentation fault {noformat}"""
"MESOS-2017","Bug","test",5,"Segfault with ""Pure virtual method called"" when tests fail","""The most recent one:  {noformat:title=DRFAllocatorTest.DRFAllocatorProcess} [ RUN      ] DRFAllocatorTest.DRFAllocatorProcess Using temporary directory '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j' I1030 05:55:06.934813 24459 leveldb.cpp:176] Opened db in 3.175202ms I1030 05:55:06.935925 24459 leveldb.cpp:183] Compacted db in 1.077924ms I1030 05:55:06.935976 24459 leveldb.cpp:198] Created db iterator in 16460ns I1030 05:55:06.935995 24459 leveldb.cpp:204] Seeked to beginning of db in 2018ns I1030 05:55:06.936005 24459 leveldb.cpp:273] Iterated through 0 keys in the db in 335ns I1030 05:55:06.936039 24459 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I1030 05:55:06.936705 24480 recover.cpp:437] Starting replica recovery I1030 05:55:06.937023 24480 recover.cpp:463] Replica is in EMPTY status I1030 05:55:06.938158 24475 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request I1030 05:55:06.938859 24482 recover.cpp:188] Received a recover response from a replica in EMPTY status I1030 05:55:06.939486 24474 recover.cpp:554] Updating replica status to STARTING I1030 05:55:06.940249 24489 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 591981ns I1030 05:55:06.940274 24489 replica.cpp:320] Persisted replica status to STARTING I1030 05:55:06.940752 24481 recover.cpp:463] Replica is in STARTING status I1030 05:55:06.940820 24489 master.cpp:312] Master 20141030-055506-3142697795-40429-24459 (pomona.apache.org) started on 67.195.81.187:40429 I1030 05:55:06.940871 24489 master.cpp:358] Master only allowing authenticated frameworks to register I1030 05:55:06.940891 24489 master.cpp:363] Master only allowing authenticated slaves to register I1030 05:55:06.940908 24489 credentials.hpp:36] Loading credentials for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_BI905j/credentials' I1030 05:55:06.941215 24489 master.cpp:392] Authorization enabled I1030 05:55:06.941751 24475 master.cpp:120] No whitelist given. Advertising offers for all slaves I1030 05:55:06.942227 24474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request I1030 05:55:06.942401 24476 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.187:40429 I1030 05:55:06.942895 24483 recover.cpp:188] Received a recover response from a replica in STARTING status I1030 05:55:06.943035 24474 master.cpp:1242] The newly elected leader is master@67.195.81.187:40429 with id 20141030-055506-3142697795-40429-24459 I1030 05:55:06.943063 24474 master.cpp:1255] Elected as the leading master! I1030 05:55:06.943079 24474 master.cpp:1073] Recovering from registrar I1030 05:55:06.943313 24480 registrar.cpp:313] Recovering registrar I1030 05:55:06.943455 24475 recover.cpp:554] Updating replica status to VOTING I1030 05:55:06.944144 24474 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 536365ns I1030 05:55:06.944172 24474 replica.cpp:320] Persisted replica status to VOTING I1030 05:55:06.944355 24489 recover.cpp:568] Successfully joined the Paxos group I1030 05:55:06.944576 24489 recover.cpp:452] Recover process terminated I1030 05:55:06.945155 24486 log.cpp:656] Attempting to start the writer I1030 05:55:06.947013 24473 replica.cpp:474] Replica received implicit promise request with proposal 1 I1030 05:55:06.947854 24473 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 806463ns I1030 05:55:06.947883 24473 replica.cpp:342] Persisted promised to 1 I1030 05:55:06.948547 24481 coordinator.cpp:230] Coordinator attemping to fill missing position I1030 05:55:06.950269 24479 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2 I1030 05:55:06.950933 24479 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 603843ns I1030 05:55:06.950961 24479 replica.cpp:676] Persisted action at 0 I1030 05:55:06.952180 24476 replica.cpp:508] Replica received write request for position 0 I1030 05:55:06.952239 24476 leveldb.cpp:438] Reading position from leveldb took 28437ns I1030 05:55:06.952896 24476 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 623980ns I1030 05:55:06.952926 24476 replica.cpp:676] Persisted action at 0 I1030 05:55:06.953543 24485 replica.cpp:655] Replica received learned notice for position 0 I1030 05:55:06.954082 24485 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 511807ns I1030 05:55:06.954107 24485 replica.cpp:676] Persisted action at 0 I1030 05:55:06.954128 24485 replica.cpp:661] Replica learned NOP action at position 0 I1030 05:55:06.954710 24473 log.cpp:672] Writer started with ending position 0 I1030 05:55:06.956215 24478 leveldb.cpp:438] Reading position from leveldb took 33085ns I1030 05:55:06.959481 24475 registrar.cpp:346] Successfully fetched the registry (0B) in 16.11904ms I1030 05:55:06.959616 24475 registrar.cpp:445] Applied 1 operations in 28239ns; attempting to update the 'registry' I1030 05:55:06.962514 24487 log.cpp:680] Attempting to append 139 bytes to the log I1030 05:55:06.962646 24474 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I1030 05:55:06.964146 24486 replica.cpp:508] Replica received write request for position 1 I1030 05:55:06.964962 24486 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 743389ns I1030 05:55:06.964993 24486 replica.cpp:676] Persisted action at 1 I1030 05:55:06.965895 24473 replica.cpp:655] Replica received learned notice for position 1 I1030 05:55:06.966531 24473 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 607242ns I1030 05:55:06.966555 24473 replica.cpp:676] Persisted action at 1 I1030 05:55:06.966578 24473 replica.cpp:661] Replica learned APPEND action at position 1 I1030 05:55:06.967706 24481 registrar.cpp:490] Successfully updated the 'registry' in 8.036096ms I1030 05:55:06.967895 24481 registrar.cpp:376] Successfully recovered registrar I1030 05:55:06.967993 24482 log.cpp:699] Attempting to truncate the log to 1 I1030 05:55:06.968258 24479 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I1030 05:55:06.968268 24475 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register I1030 05:55:06.969156 24476 replica.cpp:508] Replica received write request for position 2 I1030 05:55:06.969678 24476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 491913ns I1030 05:55:06.969703 24476 replica.cpp:676] Persisted action at 2 I1030 05:55:06.970459 24478 replica.cpp:655] Replica received learned notice for position 2 I1030 05:55:06.971060 24478 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 573076ns I1030 05:55:06.971124 24478 leveldb.cpp:401] Deleting ~1 keys from leveldb took 35339ns I1030 05:55:06.971145 24478 replica.cpp:676] Persisted action at 2 I1030 05:55:06.971168 24478 replica.cpp:661] Replica learned TRUNCATE action at position 2 I1030 05:55:06.980211 24459 containerizer.cpp:100] Using isolation: posix/cpu,posix/mem I1030 05:55:06.984153 24473 slave.cpp:169] Slave started on 203)@67.195.81.187:40429 I1030 05:55:07.055308 24473 credentials.hpp:84] Loading credential for authentication from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/credential' I1030 05:55:06.988750 24459 sched.cpp:137] Version: 0.21.0 I1030 05:55:07.055521 24473 slave.cpp:276] Slave using credential for: test-principal I1030 05:55:07.055726 24473 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):0; ports(*):[31000-32000] I1030 05:55:07.055865 24473 slave.cpp:318] Slave hostname: pomona.apache.org I1030 05:55:07.055881 24473 slave.cpp:319] Slave checkpoint: false W1030 05:55:07.055889 24473 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I1030 05:55:07.056172 24485 sched.cpp:233] New master detected at master@67.195.81.187:40429 I1030 05:55:07.056222 24485 sched.cpp:283] Authenticating with master master@67.195.81.187:40429 I1030 05:55:07.056717 24485 state.cpp:33] Recovering state from '/tmp/DRFAllocatorTest_DRFAllocatorProcess_wULx31/meta' I1030 05:55:07.056851 24475 authenticatee.hpp:133] Creating new client SASL connection I1030 05:55:07.057003 24473 status_update_manager.cpp:197] Recovering status update manager I1030 05:55:07.057252 24488 master.cpp:3853] Authenticating scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.057502 24489 containerizer.cpp:281] Recovering containerizer I1030 05:55:07.057524 24475 authenticator.hpp:161] Creating new server SASL connection I1030 05:55:07.057688 24475 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1030 05:55:07.057719 24475 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1030 05:55:07.057919 24481 authenticator.hpp:267] Received SASL authentication start I1030 05:55:07.057968 24481 authenticator.hpp:389] Authentication requires more steps I1030 05:55:07.058070 24473 authenticatee.hpp:270] Received SASL authentication step I1030 05:55:07.058199 24485 authenticator.hpp:295] Received SASL authentication step I1030 05:55:07.058223 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1030 05:55:07.058233 24485 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1030 05:55:07.058259 24485 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1030 05:55:07.058290 24485 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1030 05:55:07.058302 24485 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.058307 24485 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.058320 24485 authenticator.hpp:381] Authentication success I1030 05:55:07.058467 24480 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.058493 24485 slave.cpp:3456] Finished recovery I1030 05:55:07.058593 24478 authenticatee.hpp:310] Authentication success I1030 05:55:07.058838 24478 sched.cpp:357] Successfully authenticated with master master@67.195.81.187:40429 I1030 05:55:07.058861 24478 sched.cpp:476] Sending registration request to master@67.195.81.187:40429 I1030 05:55:07.058969 24475 slave.cpp:602] New master detected at master@67.195.81.187:40429 I1030 05:55:07.058969 24487 status_update_manager.cpp:171] Pausing sending status updates I1030 05:55:07.059026 24475 slave.cpp:665] Authenticating with master master@67.195.81.187:40429 I1030 05:55:07.059061 24481 master.cpp:1362] Received registration request for framework 'framework1' at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.059131 24481 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role 'role1' I1030 05:55:07.059171 24475 slave.cpp:638] Detecting new master I1030 05:55:07.059214 24482 authenticatee.hpp:133] Creating new client SASL connection I1030 05:55:07.059550 24481 master.cpp:3853] Authenticating slave(203)@67.195.81.187:40429 I1030 05:55:07.059787 24487 authenticator.hpp:161] Creating new server SASL connection I1030 05:55:07.059922 24481 master.cpp:1426] Registering framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:07.059996 24474 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5 I1030 05:55:07.060034 24474 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5' I1030 05:55:07.060117 24474 authenticator.hpp:267] Received SASL authentication start I1030 05:55:07.060165 24474 authenticator.hpp:389] Authentication requires more steps I1030 05:55:07.060377 24476 hierarchical_allocator_process.hpp:329] Added framework 20141030-055506-3142697795-40429-24459-0000 I1030 05:55:07.060394 24488 sched.cpp:407] Framework registered with 20141030-055506-3142697795-40429-24459-0000 I1030 05:55:07.060403 24476 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1030 05:55:07.060431 24476 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 29857ns I1030 05:55:07.060443 24488 sched.cpp:421] Scheduler::registered took 19407ns I1030 05:55:07.060545 24478 authenticatee.hpp:270] Received SASL authentication step I1030 05:55:07.060645 24478 authenticator.hpp:295] Received SASL authentication step I1030 05:55:07.060673 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I1030 05:55:07.060685 24478 auxprop.cpp:153] Looking up auxiliary property '*userPassword' I1030 05:55:07.060714 24478 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I1030 05:55:07.060740 24478 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pomona.apache.org' server FQDN: 'pomona.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I1030 05:55:07.060760 24478 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.060770 24478 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I1030 05:55:07.060788 24478 authenticator.hpp:381] Authentication success I1030 05:55:07.060920 24474 authenticatee.hpp:310] Authentication success I1030 05:55:07.060945 24485 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(203)@67.195.81.187:40429 I1030 05:55:07.061388 24489 slave.cpp:722] Successfully authenticated with master master@67.195.81.187:40429 I1030 05:55:07.061504 24489 slave.cpp:1050] Will retry registration in 4.778336ms if necessary I1030 05:55:07.061718 24480 master.cpp:3032] Registering slave at slave(203)@67.195.81.187:40429 (pomona.apache.org) with id 20141030-055506-3142697795-40429-24459-S0 I1030 05:55:07.062119 24489 registrar.cpp:445] Applied 1 operations in 53691ns; attempting to update the 'registry' I1030 05:55:07.065182 24479 log.cpp:680] Attempting to append 316 bytes to the log I1030 05:55:07.065337 24487 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3 I1030 05:55:07.066359 24474 replica.cpp:508] Replica received write request for position 3 I1030 05:55:07.066643 24474 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 249579ns I1030 05:55:07.066671 24474 replica.cpp:676] Persisted action at 3 I../../src/tests/allocator_tests.cpp:120: Failure Failed to wait 10secs for offers1 1030 05:55:07.067101 24477 slave.cpp:1050] Will retry registration in 24.08243ms if necessary I1030 05:55:07.067140 24473 master.cpp:3020] Ignoring register slave message from slave(203)@67.195.81.187:40429 (pomona.apache.org) as admission is already in progress I1030 05:55:07.067395 24488 replica.cpp:655] Replica received learned notice for position 3 I1030 05:55:07.943416 24478 hierarchical_allocator_process.hpp:697] No resources available to allocate! I1030 05:55:19.804687 24478 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 11.861261123secs I1030 05:55:11.942713 24474 master.cpp:120] No whitelist given. Advertising offers for all slaves I1030 05:55:19.805850 24488 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 1.067224ms I1030 05:55:19.806012 24488 replica.cpp:676] Persisted action at 3 ../../src/tests/allocator_tests.cpp:115: Failure Actual function call count doesn't match EXPECT_CALL(sched1, resourceOffers(_, _))...          Expected: to be called once            Actual: never called - unsatisfied and active I1030 05:55:19.806144 24488 replica.cpp:661] Replica learned APPEND action at position 3 I1030 05:55:19.806695 24473 master.cpp:768] Framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 disconnected I1030 05:55:19.806726 24473 master.cpp:1731] Disconnecting framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.806751 24473 master.cpp:1747] Deactivating framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.806967 24473 master.cpp:790] Giving framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 0ns to failover ../../src/tests/allocator_tests.cpp:94: Failure Actual function call count doesn't match EXPECT_CALL(allocator, slaveAdded(_, _, _))...          Expected: to be called once            Actual: never called - unsatisfied and active F1030 05:55:19.806967 24480 logging.cpp:57] RAW: Pure virtual method called I1030 05:55:19.807348 24488 master.cpp:3665] Framework failover timeout, removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 I1030 05:55:19.807370 24488 master.cpp:4201] Removing framework 20141030-055506-3142697795-40429-24459-0000 (framework1) at scheduler-c98e7aac-d03f-464a-aa75-61208600e196@67.195.81.187:40429 *** Aborted at 1414648519 (unix time) try """"date -d @1414648519"""" if you are using GNU date *** PC: @           0x91bc86 process::PID<>::PID() *** SIGSEGV (@0x0) received by PID 24459 (TID 0x2b86c919a700) from PID 0; stack trace: *** I1030 05:55:19.808631 24489 registrar.cpp:490] Successfully updated the 'registry' in 12.746377984secs     @     0x2b86c55fc340 (unknown) I1030 05:55:19.808938 24473 log.cpp:699] Attempting to truncate the log to 3     @     0x2b86c3327174  google::LogMessage::Fail() I1030 05:55:19.809084 24481 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4     @           0x91bc86 process::PID<>::PID()     @     0x2b86c332c868  google::RawLog__() I1030 05:55:19.810191 24479 replica.cpp:508] Replica received write request for position 4 I1030 05:55:19.810899 24479 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 678090ns I1030 05:55:19.810919 24479 replica.cpp:676] Persisted action at 4     @           0x91bf24 process::Process<>::self() I1030 05:55:19.811635 24485 replica.cpp:655] Replica received learned notice for position 4 I1030 05:55:19.812180 24485 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 523927ns I1030 05:55:19.812228 24485 leveldb.cpp:401] Deleting ~2 keys from leveldb took 29523ns I1030 05:55:19.812242 24485 replica.cpp:676] Persisted action at 4 I    @     0x2b86c29d2a36  __cxa_pure_virtual 1030 05:55:19.812258 24485 replica.cpp:661] Replica learned TRUNCATE action at position 4     @          0x1046936  testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith() I1030 05:55:19.829655 24474 slave.cpp:1050] Will retry registration in 31.785967ms if necessary     @           0x9c0633  testing::internal::FunctionMockerBase<>::InvokeWith()     @           0x9b6152  testing::internal::FunctionMocker<>::Invoke()     @           0x9abdeb  mesos::internal::tests::MockAllocatorProcess<>::frameworkDeactivated()     @           0x91c78f  _ZZN7process8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS1_11FrameworkIDES6_EEvRKNS_3PIDIT_EEMSA_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESJ_     @           0x959ad7  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal6master9allocator16AllocatorProcessERKNS5_11FrameworkIDESA_EEvRKNS0_3PIDIT_EEMSE_FvT0_ET1_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_     @     0x2b86c32d174f  std::function<>::operator()()     @     0x2b86c32b2a17  process::ProcessBase::visit()     @     0x2b86c32bd34c  process::DispatchEvent::visit()     @           0x8e0812  process::ProcessBase::serve()     @     0x2b86c32aec8c  process::ProcessManager::resume() I1030 05:55:22.050081 24478 slave.cpp:1050] Will retry registration in 25.327301ms if necessary     @     0x2b86c32a5351  process::schedule()     @     0x2b86c55f4182  start_thread     @     0x2b86c5904fbd  (unknown) {noformat}"""
"MESOS-2035","Improvement","agent",5,"Add reason to containerizer proto Termination","""When an isolator kills a task, the reason is unknown. As part of MESOS-1830, the reason is set to a general one but ideally we would have the termination reason to pass through to the status update."""
"MESOS-2032","Task","agent|framework|master",13,"Update Maintenance design to account for persistent resources.","""With persistent resources and dynamic reservations, frameworks need to know how long the resources will be unavailable for maintenance operations.  This is because for persistent resources, the framework needs to understand how long the persistent resource will be unavailable. For example, if there will be a 10 minute reboot for a kernel upgrade, the framework will not want to re-replicate all of it's persistent data on the machine. Rather, tolerating one unavailable replica for the maintenance window would be preferred.  I'd like to do a revisit of the design to ensure it works well for persistent resources as well."""
"MESOS-2030","Task","master",3,"Maintain persistent disk resources in master memory.","""Maintain an in-memory data structure to track persistent disk resources on each slave. Update this data structure when slaves register/re-register/disconnect, etc."""
"MESOS-2043","Bug","agent|master|scheduler driver|security",5,"Framework auth fail with timeout error and never get authenticated","""I'm facing this issue in master as of https://github.com/apache/mesos/commit/74ea59e144d131814c66972fb0cc14784d3503d4  As [~adam-mesos] mentioned in IRC, this sounds similar to MESOS-1866. I'm running 1 master and 1 scheduler (aurora). The framework authentication fail due to time out:  error on mesos master:    scheduler error:   Looks like 2 instances {{scheduler-20f88a53-5945-4977-b5af-28f6c52d3c94}} & {{scheduler-d2d4437b-d375-4467-a583-362152fe065a}} of same framework is trying to authenticate and fail.   Restarting master and scheduler didn't fix it.   This particular issue happen with 1 master and 1 scheduler after MESOS-1866 is fixed."""
"MESOS-2052","Bug","agent|containerization",1,"RunState::recover should always recover 'completed'","""RunState::recover() will return partial state if it cannot find or open the libprocess pid file. Specifically, it does not recover the 'completed' flag.  However, if the slave has removed the executor (because launch failed or the executor failed to register) the sentinel flag will be set and this fact should be recovered. This ensures that container recovery is not attempted later.  This was discovered when the LinuxLauncher failed to recover because it was asked to recover two containers with the same forkedPid. Investigation showed the executors both OOM'ed before registering, i.e., no libprocess pid file was present. However, the containerizer had detected the OOM, destroyed the container, and notified the slave which cleaned everything up: failing the task and calling removeExecutor (which writes the completed sentinel file.)"""
"MESOS-2062","Task","c++ api",3,"Add InverseOffer to Event/Call API.","""The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add this is to tack it on to the OFFERS Event:  """
"MESOS-2058","Task","agent|master",1,"Deprecate stats.json endpoints for Master and Slave","""With the introduction of the libprocess {{/metrics/snapshot}} endpoint, metrics are now duplicated in the Master and Slave between this and {{stats.json}}. We should deprecate the {{stats.json}} endpoints.  Manual inspection of {{stats.json}} shows that all metrics are now covered by the new endpoint for Master and Slave."""
"MESOS-2057","Improvement","agent|fetcher",8,"Concurrency control for fetcher cache","""Having added a URI flag to CommandInfo messages (in MESOS-2069) that indicates caching, caching files downloaded by the fetcher in a repository, now ensure that when a URI is """"cached"""", it is only ever downloaded once for the same user on the same slave as long as the slave keeps running.   This even holds if multiple tasks request the same URI concurrently. If multiple requests for the same URI occur, perform only one of them and reuse the result. Make concurrent requests for the same URI wait for the one download.   Different URIs from different CommandInfos can be downloaded concurrently.  No cache eviction, cleanup or failover will be handled for now. Additional tickets will be filed for these enhancements. (So don't use this feature in production until the whole epic is complete.)  Note that implementing this does not suffice for production use. This ticket contains the main part of the fetcher logic, though. See the epic MESOS-336 for the rest of the features that lead to a fully functional fetcher cache.  The proposed general approach is to keep all bookkeeping about what is in which stage of being fetched and where it resides in the slave's MesosContainerizerProcess, so that all concurrent access is disambiguated and controlled by an """"actor"""" (aka libprocess """"process"""").  Depends on MESOS-2056 and MESOS-2069. """
"MESOS-2056","Improvement","agent|fetcher",1,"Refactor fetcher code in preparation for fetcher cache","""Refactor/rearrange fetcher-related code so that cache functionality can be dropped in. One could do both together in one go. This is splitting up reviews into smaller chunks. It will not immediately be obvious how this change will be used later, but it will look better-factored and still do the exact same thing as before. In particular, a download routine to be reused several times in launcher/fetcher will be factored out and the remainder of fetcher-related code can be moved from the containerizer realm into fetcher.cpp."""
"MESOS-2078","Bug","java api",3,"Scheduler driver may ACK status updates when the scheduler threw an exception","""[~vinodkone] discovered that this can happen if the scheduler calls {{SchedulerDriver#stop}} before or while handling {{Scheduler#statusUpdate}}.  In src/sched/sched.cpp: The driver invokes {{statusUpdate}} and later checks the {{aborted}} flag to determine whether to send an ACK.   In src/java/jni/org_apache_mesos_MesosSchedulerDriver.cpp: The {{statusUpdate}} implementation checks for an exception and invokes {{driver->abort()}}.   In src/sched/sched.cpp: The {{abort()}} implementation exits early if {{status != DRIVER_RUNNING}}, and *does not set the aborted flag*.   As a result, the code will ACK despite an exception being thrown."""
"MESOS-2076","Task","master",13,"Implement maintenance primitives in the Master.","""The master will need to do a number of things to implement the maintenance primitives:  # For machines that have a maintenance window: #* Disambiguate machines to agents. #* For unused resources, offers must be augmented with an Unavailability. #* For used resources, inverse offers must be sent. # For inverse offers: #* Filter them before sending them again. #* For declined inverse offers, do something with the reason (store or log). # Recover the maintenance information upon failover.  Note: Some amount of this logic will need to be placed in the allocator."""
"MESOS-2075","Task","master",13,"Add maintenance information to the replicated registry.","""To achieve fault-tolerance for the maintenance primitives, we will need to add the maintenance information to the registry.  The registry currently stores all of the slave information, which is quite large (~ 17MB for 50,000 slaves from my testing), which results in a protobuf object that is extremely expensive to copy.  As far as I can tell, reads / writes to maintenance information is independent of reads / writes to the existing 'registry' information. So there are two approach here:  h4. Add maintenance information to 'maintenance' key:  # The advantage of this approach is that we don't further grow the large Registry object. # This approach assumes that writes to 'maintenance' are independent of writes to the 'registry'. -If these writes are not independent, this approach requires that we add transactional support to the State abstraction.- # -This approach requires adding compaction to LogStorage.- # This approach likely requires some refactoring to the Registrar.  h4. Add maintenance information to 'registry' key: (This is the chosen method.) # The advantage of this approach is that it's the easiest to implement. # This will further grow the single 'registry' object, but doesn't preclude it being split apart in the future. # This approach may require using the diff support in LogStorage and/or adding compression support to LogStorage snapshots to deal with the increased size of the registry."""
"MESOS-2074","Improvement","agent|fetcher",5,"Fetcher cache test fixture","""To accelerate providing good test coverage for the fetcher cache (MESOS-336), we can provide a framework that canonicalizes creating and running a number of tasks and allows easy parametrization with combinations of the following: - whether to cache or not - whether make what has been downloaded executable or not - whether to extract from an archive or not - whether to download from a file system, http, or...  We can create a simple HHTP server in the test fixture to support the latter.  Furthermore, the tests need to be robust wrt. varying numbers of StatusUpdate messages. An accumulating update message sink that reports the final state is needed.  All this has already been programmed in this patch, just needs to be rebased: https://reviews.apache.org/r/21316/"""
"MESOS-2072","Improvement","agent|fetcher",8,"Fetcher cache eviction","""Delete files from the fetcher cache so that a given cache size is never exceeded. Succeed in doing so while concurrent downloads are on their way and new requests are pouring in.  Idea: measure the size of each download before it begins, make enough room before the download. This means that only download mechanisms that divulge the size before the main download will be supported. AFAWK, those in use so far have this property.   The calculation of how much space to free needs to be under concurrency control, accumulating all space needed for competing, incomplete download requests. (The Python script that performs fetcher caching for Aurora does not seem to implement this. See https://gist.github.com/zmanji/f41df77510ef9d00265a, imagine several of these programs running concurrently, each one's _cache_eviction() call succeeding, each perceiving the SAME free space being available.)  Ultimately, a conflict resolution strategy is needed if just the downloads underway already exceed the cache capacity. Then, as a fallback, direct download into the work directory will be used for some tasks. TBD how to pick which task gets treated how.   At first, only support copying of any downloaded files to the work directory for task execution. This isolates the task life cycle after starting a task from cache eviction considerations.   (Later, we can add symbolic links that avoid copying. But then eviction of fetched files used by ongoing tasks must be blocked, which adds complexity. another future extension is MESOS-1667 """"Extract from URI while downloading into work dir""""). """
"MESOS-2070","Improvement","agent|fetcher",2,"Implement simple slave recovery behavior for fetcher cache","""Clean the fetcher cache completely upon slave restart/recovery. This implements correct, albeit not ideal behavior. More efficient schemes that restore knowledge about cached files or even resume downloads can be added later. """
"MESOS-2069","Improvement","agent|fetcher",8,"Basic fetcher cache functionality","""Add a flag to CommandInfo URI protobufs that indicates that files downloaded by the fetcher shall be cached in a repository. To be followed by MESOS-2057 for concurrency control.  Also see MESOS-336 for the overall goals for the fetcher cache."""
"MESOS-2067","Task","master",8,"Add HTTP API to the master for maintenance operations.","""Based on MESOS-1474, we'd like to provide an HTTP API on the master for the maintenance primitives in mesos.  For the MVP, we'll want something like this for manipulating the schedule:  (Note: The slashes in URLs might not be supported yet.)  A schedule might look like:   There should be firewall settings such that only those with access to master can use these endpoints."""
"MESOS-2065","Task","python api",5,"Add InverseOffer to Python Scheduler API.","""The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add these to the Python Scheduler API is to add a new callback:    Egg / libmesos compatibility will need to be figured out here.  We may want to leave the Python binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers."""
"MESOS-2064","Task","java api",5,"Add InverseOffer to Java Scheduler API.","""The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add these to the Java Scheduler API is to add a new callback:    JAR / libmesos compatibility will need to be figured out here.  We may want to leave the Java binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers."""
"MESOS-2063","Task","c++ api",5,"Add InverseOffer to C++ Scheduler API.","""The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.  One way to add these to the C++ Scheduler API is to add a new callback:    libmesos compatibility will need to be figured out here.  We may want to leave the C++ binding untouched in favor of Event/Call, in order to not break API compatibility for schedulers."""
"MESOS-2083","Documentation","documentation",8,"Add documentation for maintenance primitives.","""We should provide some guiding documentation around the upcoming maintenance primitives in Mesos.  Specifically, we should ensure that general users, framework developers, and operators understand the notion of maintenance in Mesos. Some guidance and recommendations for the latter two audiences will be necessary."""
"MESOS-2082","Task","webui",5,"Update the webui to include maintenance information.","""The simplest thing here would probably be to include another tab in the header for maintenance information.  We could also consider adding maintenance information inline to the slaves table. Depending on how this is done, the maintenance tab could actually be a subset of the slaves table; only those slaves for which there is maintenance information."""
"MESOS-2104","Improvement","containerization",3,"Correct naming of cgroup memory statistics","""mem_rss_bytes is *not* RSS but is the total memory usage (memory.usage_in_bytes) of the cgroup, including file cache etc. Actual RSS is reported as mem_anon_bytes. These, and others, should be consistently named."""
"MESOS-2103","Improvement","containerization",2,"Expose number of processes and threads in a container","""The CFS cpu statistics (cpus_nr_throttled, cpus_nr_periods, cpus_throttled_time) are difficult to interpret. 1) nr_throttled is the number of intervals where *any* throttling occurred 2) throttled_time is the aggregate time *across all runnable tasks* (tasks in the Linux sense).  For example, in a typical 60 second sampling interval: nr_periods = 600, nr_throttled could be 60, i.e., 10% of intervals, but throttled_time could be much higher than (60/600) * 60 = 6 seconds if there is more than one task that is runnable but throttled. *Each* throttled task contributes to the total throttled time.  Small test to demonstrate throttled_time > nr_periods * quota_interval:  5 x {{'openssl speed'}} running with quota=100ms:  All 10 intervals throttled (100%) for total time of 2.8 seconds in 1 second (""""more than 100%"""" of the time interval)   It would be helpful to expose the number of processes and tasks in the container cgroup. This would be at a very coarse granularity but would give some guidance."""
"MESOS-2100","Task","agent|master",8,"Implement master to slave protocol for persistent disk resources.","""We need to do the following: 1) Slave needs to send persisted resources when registering (or re-registering). 2) Master needs to send total persisted resources to slave by either re-using RunTask/UpdateFrameworkInfo or introduce new type of messages (like UpdateResources)."""
"MESOS-2099","Task","allocation",8,"Support acquiring/releasing resources with DiskInfo in allocator.","""The allocator needs to be changed because the resources are changing while we acquiring or releasing persistent disk resources (resources with DiskInfo). For example, when we release a persistent disk resource, we are changing the release with DiskInfo to a resource with the DiskInfo."""
"MESOS-2110","Improvement","agent|master",8,"Configurable Ping Timeouts","""After a series of ping-failures, the master considers the slave lost and calls shutdownSlave, requiring such a slave that reconnects to kill its tasks and re-register as a new slaveId. On the other side, after a similar timeout, the slave will consider the master lost and try to detect a new master. These timeouts are currently hardcoded constants (5 * 15s), which may not be well-suited for all scenarios. - Some clusters may tolerate a longer slave process restart period, and wouldn't want tasks to be killed upon reconnect. - Some clusters may have higher-latency networks (e.g. cross-datacenter, or for volunteer computing efforts), and would like to tolerate longer periods without communication.  We should provide flags/mechanisms on the master to control its tolerance for non-communicative slaves, and (less importantly?) on the slave to tolerate missing masters."""
"MESOS-2127","Improvement","master",3,"killTask() should perform reconciliation for unknown tasks.","""Currently, {{killTask}} uses its own reconciliation logic, which has diverged from the {{reconcileTasks}} logic. Specifically, when the task is unknown and a non-strict registry is in use, {{killTask}} will not send TASK_LOST whereas {{reconcileTask}} will.  We should make these consistent. """
"MESOS-2136","Improvement","containerization",5,"Expose per-cgroup memory pressure","""The cgroup memory controller can provide information on the memory pressure of a cgroup. This is in the form of an event based notification where events of (low, medium, critical) are generated when the kernel makes specific actions to allocate memory. This signal is probably more informative than comparing memory usage to memory limit. """
"MESOS-2128","Bug","containerization",2,"Turning on cgroups_limit_swap effectively disables memory isolation","""Our test runs show that enabling cgroups_limit_swap effectively disables memory isolation altogether.  Per: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html  """"It is important to set the memory.limit_in_bytes parameter before setting the memory.memsw.limit_in_bytes parameter: attempting to do so in the reverse order results in an error. This is because memory.memsw.limit_in_bytes becomes available only after all memory limitations (previously set in memory.limit_in_bytes) are exhausted.""""  Looks like the flag sets """"memory.memsw.limit_in_bytes"""" if true and """"memory.limit_in_bytes"""" if false, but should always set """"memory.limit_in_bytes"""" and in addition set """"memory.memsw.limit_in_bytes"""" if true. Otherwise the limits won't be set and enforced.  See: https://github.com/apache/mesos/blob/c8598f7f5a24a01b6a68e0f060b79662ee97af89/src/slave/containerizer/isolators/cgroups/mem.cpp#L365 """
"MESOS-2144","Bug","test",8,"Segmentation Fault in ExamplesTest.LowLevelSchedulerPthread","""Occured on review bot review of: https://reviews.apache.org/r/28262/#review62333  The review doesn't touch code related to the test (And doesn't break libprocess in general)  [ RUN      ] ExamplesTest.LowLevelSchedulerPthread ../../src/tests/script.cpp:83: Failure Failed low_level_scheduler_pthread_test.sh terminated with signal Segmentation fault [  FAILED  ] ExamplesTest.LowLevelSchedulerPthread (7561 ms)  The test """
"MESOS-2139","Task","master",5,"Enable the master to handle reservation operations","""master's {{_accept}} function currently only handles {{Create}} and {{Destroy}} operations which exist for persistent volumes. We need to handle the {{Reserve}} and {{Unreserve}} operations for dynamic reservations as well.  In addition, we need to add {{validate}} functions for the reservation operations."""
"MESOS-2176","Bug","allocation",5,"Hierarchical allocator inconsistently accounts for reserved resources. ","""Looking through the allocator code for MESOS-2099, I see an issue with respect to accounting reserved resources in the sorters:  Within {{HierarchicalAllocatorProcess::allocate}}, only unreserved resources are accounted for in the sorters, whereas everywhere else (add/remove framework, add/remove slave) we account for both reserved and unreserved.  From git blame, it looks like this issue was introduced over a long course of refactoring and fixes to the allocator. My guess is that this was never caught due to the lack of unit-testability of the allocator (unnecessarily requires a master PID to use an allocator).  From my understanding, the two levels of the hierarchical sorter should have the following semantics:  # Level 1 sorts across roles. Only unreserved resources are shared across roles, and therefore the """"role sorter"""" for level 1 should only account for the unreserved resource pool. # Level 2 sorts across frameworks, within a role. Both unreserved and reserved resources are shared across frameworks within a role, and therefore the """"framework sorters"""" for level 2 should each account for the reserved resource pool for the role, as well as the unreserved resources _allocated_ inside the role."""
"MESOS-2184","Task","agent",1,"deprecate unused flag 'cgroups_subsystems'","""cgroups_subsystems is a slave flag that is no longer used and should be deprecated."""
"MESOS-2182","Bug","libprocess",3,"Performance issue in libprocess SocketManager.","""Noticed an issue in production under which the master is slow to respond after failover for ~15 minutes.  After looking at some perf data, the top offender is:    It appears that in the SocketManager, whenever an internal Process exits, we loop over all the links unnecessarily:    On clusters with 10,000s of slaves, this means we hold the socket manager lock for a very expensive loop erasing nothing from a set! This is because, the master contains links from the Master Process to each slave. However, when a random ephemeral Process terminates, we don't need to loop over each slave link.  While we hold this lock, the following calls will block:    As a result, the slave observers and the master can block calling send()!  Short term, we will try to fix this issue by removing the unnecessary looping. Longer term, it would be nice to avoid all this locking when sending on independent sockets."""
"MESOS-2191","Wish","containerization",3,"Add ContainerId to the TaskStatus message","""{{TaskStatus}} provides the frameworks with certain information ({{executorId}}, {{slaveId}}, etc.) which is useful when collecting statistics about cluster performance; however, it is difficult to associate tasks to the container it is executed since this information stays always within mesos itself. Therefore it would be good to provide the framework scheduler with this information, adding a new field in the {{TaskStatus}} message.  See comments for a use case."""
"MESOS-2199","Bug","test",2,"Failing test: SlaveTest.ROOT_RunTaskWithCommandInfoWithUser","""Appears that running the executor as {{nobody}} is not supported.    [~nnielsen] can you take a look?    Executor log:      Test output:  """
"MESOS-2201","Bug","test",3,"ReplicaTest.Restore fails with leveldb greater than v1.7.","""I wanted to configure Mesos with system provided leveldb libraries when I ran into this issue. Apparently,  if one does {{../configure --with-leveldb=/path/to/leveldb}}, compilation succeeds, however the """"ReplicaTest_Restore"""" test fails with the following back trace:    The bundled version of leveldb is v1.4. I tested version 1.5 and that seems to work.  However, v1.6 had some build issues and us unusable with Mesos. The next version v1.7, allows Mesos to compile fine but results in the above error."""
"MESOS-2205","Documentation","documentation|framework",2,"Add user documentation for reservations","""Add a user guide for reservations which describes basic usage of them, how ACLs are used to specify who can unreserve whose resources, and few advanced usage cases."""
"MESOS-2215","Bug","docker",8,"The Docker containerizer attempts to recover any task when checkpointing is enabled, not just docker tasks.","""Once the slave restarts and recovers the task, I see this error in the log for all tasks that were recovered every second or so.  Note, these were NOT docker tasks:  W0113 16:01:00.790323 773142 monitor.cpp:213] Failed to get resource usage for  container 7b729b89-dc7e-4d08-af97-8cd1af560a21 for executor thermos-1421085237813-slipstream-prod-agent-3-8f769514-1835-4151-90d0-3f55dcc940dd of framework 20150109-161713-715350282-5050-290797-0000: Failed to 'docker inspect mesos-7b729b89-dc7e-4d08-af97-8cd1af560a21': exit status = exited with status 1 stderr = Error: No such image or container: mesos-7b729b89-dc7e-4d08-af97-8cd1af560a21 However the tasks themselves are still healthy and running.  The slave was launched with --containerizers=mesos,docker  ----- More info: it looks like the docker containerizer is a little too ambitious about recovering containers, again this was not a docker task: I0113 15:59:59.476145 773142 docker.cpp:814] Recovering container '7b729b89-dc7e-4d08-af97-8cd1af560a21' for executor 'thermos-1421085237813-slipstream-prod-agent-3-8f769514-1835-4151-90d0-3f55dcc940dd' of framework 20150109-161713-715350282-5050-290797-0000  Looking into the source, it looks like the problem is that the ComposingContainerize runs recover in parallel, but neither the docker containerizer nor mesos containerizer check if they should recover the task or not (because they were the ones that launched it).  Perhaps this needs to be written into the checkpoint somewhere?"""
"MESOS-2228","Bug","test",3,"SlaveTest.MesosExecutorGracefulShutdown is flaky","""Observed this on internal CI  """
"MESOS-2226","Bug","test",3,"HookTest.VerifySlaveLaunchExecutorHook is flaky","""Observed this on internal CI  """
"MESOS-2225","Bug","test",2,"FaultToleranceTest.ReregisterFrameworkExitedExecutor is flaky","""Observed this on internal CI.  """
"MESOS-2232","Bug","test",3,"Suppress MockAllocator::transformAllocation() warnings.","""After transforming allocated resources feature was added to allocator, a number of warnings are popping out for allocator tests. Commits leading to this behaviour: {{dacc88292cc13d4b08fe8cda4df71110a96cb12a}} {{5a02d5bdc75d3b1149dcda519016374be06ec6bd}} corresponding reviews: https://reviews.apache.org/r/29083 https://reviews.apache.org/r/29084  Here is an example: """
"MESOS-2230","Improvement","libprocess",3,"Update RateLimiter to allow the acquired future to be discarded","""Currently there is no way for the future returned by RateLimiter's acquire() to be discarded by the user of the limiter. This is useful in cases where the user is no longer interested in the permit. See MESOS-1148 for an example use case."""
"MESOS-2241","Bug","test",1,"DiskUsageCollectorTest.SymbolicLink test is flaky","""Observed this on a local machine running linux w/ sudo.  """
"MESOS-2273","Improvement","build",1,"Add ""tests"" target to Makefile for building-but-not-running tests.","""'make check' allows one to build and run the test suite. However, often we just want to build the tests.  Currently, this is done by setting GTEST_FILTER to an empty string.  It will be nice to have a dedicated target such as 'make tests' that allows one to build the test suite without running it."""
"MESOS-2281","Improvement","agent|master",3,"Deprecate plain text Credential format.","""Currently two formats of credentials are supported: JSON    And a new line file:   We should deprecate the new line format and remove support for the old format."""
"MESOS-2283","Bug","test",1,"SlaveRecoveryTest.ReconcileKillTask is flaky.","""Saw this on an internal CI:  """
"MESOS-2306","Bug","test",1,"MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky.","""Good run:    Bad run:  """
"MESOS-2302","Bug","test",1,"FaultToleranceTest.SchedulerFailoverFrameworkMessage is flaky.","""Bad Run:   Good Run: """
"MESOS-2319","Bug","agent",2,"Unable to set --work_dir to a non /tmp device","""When starting mesos-slave with --work_dir set to a directory which is not the same device as /tmp results in mesos-slave throwing a core dump:   Removing the --work_dir option results in the slave starting successfully."""
"MESOS-2324","Bug","test",1,"MasterAllocatorTest/0.OutOfOrderDispatch is flaky",""" {noformat:title=} [ RUN      ] MasterAllocatorTest/0.OutOfOrderDispatch Using temporary directory '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b' I0206 07:55:44.084333 15065 leveldb.cpp:175] Opened db in 25.006293ms I0206 07:55:44.089635 15065 leveldb.cpp:182] Compacted db in 5.256332ms I0206 07:55:44.089695 15065 leveldb.cpp:197] Created db iterator in 23534ns I0206 07:55:44.089710 15065 leveldb.cpp:203] Seeked to beginning of db in 2175ns I0206 07:55:44.089720 15065 leveldb.cpp:272] Iterated through 0 keys in the db in 417ns I0206 07:55:44.089781 15065 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0206 07:55:44.093750 15086 recover.cpp:448] Starting replica recovery I0206 07:55:44.094044 15086 recover.cpp:474] Replica is in EMPTY status I0206 07:55:44.095473 15086 replica.cpp:640] Replica in EMPTY status received a broadcasted recover request I0206 07:55:44.095724 15086 recover.cpp:194] Received a recover response from a replica in EMPTY status I0206 07:55:44.096097 15086 recover.cpp:565] Updating replica status to STARTING I0206 07:55:44.106575 15086 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 10.289939ms I0206 07:55:44.106613 15086 replica.cpp:322] Persisted replica status to STARTING I0206 07:55:44.108144 15086 recover.cpp:474] Replica is in STARTING status I0206 07:55:44.109122 15086 replica.cpp:640] Replica in STARTING status received a broadcasted recover request I0206 07:55:44.110879 15091 recover.cpp:194] Received a recover response from a replica in STARTING status I0206 07:55:44.117267 15087 recover.cpp:565] Updating replica status to VOTING I0206 07:55:44.124771 15087 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.66794ms I0206 07:55:44.124814 15087 replica.cpp:322] Persisted replica status to VOTING I0206 07:55:44.124948 15087 recover.cpp:579] Successfully joined the Paxos group I0206 07:55:44.125095 15087 recover.cpp:463] Recover process terminated I0206 07:55:44.126204 15087 master.cpp:344] Master 20150206-075544-16842879-38895-15065 (utopic) started on 127.0.1.1:38895 I0206 07:55:44.126268 15087 master.cpp:390] Master only allowing authenticated frameworks to register I0206 07:55:44.126281 15087 master.cpp:395] Master only allowing authenticated slaves to register I0206 07:55:44.126307 15087 credentials.hpp:35] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b/credentials' I0206 07:55:44.126683 15087 master.cpp:439] Authorization enabled I0206 07:55:44.129329 15086 master.cpp:1350] The newly elected leader is master@127.0.1.1:38895 with id 20150206-075544-16842879-38895-15065 I0206 07:55:44.129361 15086 master.cpp:1363] Elected as the leading master! I0206 07:55:44.129389 15086 master.cpp:1181] Recovering from registrar I0206 07:55:44.129653 15088 registrar.cpp:312] Recovering registrar I0206 07:55:44.130859 15088 log.cpp:659] Attempting to start the writer I0206 07:55:44.132334 15088 replica.cpp:476] Replica received implicit promise request with proposal 1 I0206 07:55:44.135187 15088 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.825465ms I0206 07:55:44.135390 15088 replica.cpp:344] Persisted promised to 1 I0206 07:55:44.138062 15091 coordinator.cpp:229] Coordinator attemping to fill missing position I0206 07:55:44.139576 15091 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2 I0206 07:55:44.142156 15091 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 2.545543ms I0206 07:55:44.142189 15091 replica.cpp:678] Persisted action at 0 I0206 07:55:44.143414 15091 replica.cpp:510] Replica received write request for position 0 I0206 07:55:44.143468 15091 leveldb.cpp:437] Reading position from leveldb took 28872ns I0206 07:55:44.145982 15091 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 2.480277ms I0206 07:55:44.146015 15091 replica.cpp:678] Persisted action at 0 I0206 07:55:44.147050 15089 replica.cpp:657] Replica received learned notice for position 0 I0206 07:55:44.154364 15089 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 7.281644ms I0206 07:55:44.154400 15089 replica.cpp:678] Persisted action at 0 I0206 07:55:44.154422 15089 replica.cpp:663] Replica learned NOP action at position 0 I0206 07:55:44.155506 15091 log.cpp:675] Writer started with ending position 0 I0206 07:55:44.156746 15091 leveldb.cpp:437] Reading position from leveldb took 30248ns I0206 07:55:44.173681 15091 registrar.cpp:345] Successfully fetched the registry (0B) in 43.977984ms I0206 07:55:44.173821 15091 registrar.cpp:444] Applied 1 operations in 30768ns; attempting to update the 'registry' I0206 07:55:44.176213 15086 log.cpp:683] Attempting to append 119 bytes to the log I0206 07:55:44.176426 15086 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 1 I0206 07:55:44.177608 15088 replica.cpp:510] Replica received write request for position 1 I0206 07:55:44.180059 15088 leveldb.cpp:342] Persisting action (136 bytes) to leveldb took 2.415145ms I0206 07:55:44.180094 15088 replica.cpp:678] Persisted action at 1 I0206 07:55:44.181324 15084 replica.cpp:657] Replica received learned notice for position 1 I0206 07:55:44.183831 15084 leveldb.cpp:342] Persisting action (138 bytes) to leveldb took 2.473724ms I0206 07:55:44.183866 15084 replica.cpp:678] Persisted action at 1 I0206 07:55:44.183887 15084 replica.cpp:663] Replica learned APPEND action at position 1 I0206 07:55:44.185510 15084 registrar.cpp:489] Successfully updated the 'registry' in 11.619072ms I0206 07:55:44.185678 15086 log.cpp:702] Attempting to truncate the log to 1 I0206 07:55:44.186111 15086 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 2 I0206 07:55:44.186944 15086 replica.cpp:510] Replica received write request for position 2 I0206 07:55:44.187492 15084 registrar.cpp:375] Successfully recovered registrar I0206 07:55:44.188016 15087 master.cpp:1208] Recovered 0 slaves from the Registry (83B) ; allowing 10mins for slaves to re-register I0206 07:55:44.189678 15086 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 2.702559ms I0206 07:55:44.189713 15086 replica.cpp:678] Persisted action at 2 I0206 07:55:44.190620 15086 replica.cpp:657] Replica received learned notice for position 2 I0206 07:55:44.193383 15086 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 2.737088ms I0206 07:55:44.193455 15086 leveldb.cpp:400] Deleting ~1 keys from leveldb took 37762ns I0206 07:55:44.193475 15086 replica.cpp:678] Persisted action at 2 I0206 07:55:44.193496 15086 replica.cpp:663] Replica learned TRUNCATE action at position 2 I0206 07:55:44.200028 15065 containerizer.cpp:102] Using isolation: posix/cpu,posix/mem I0206 07:55:44.212924 15088 slave.cpp:172] Slave started on 46)@127.0.1.1:38895 I0206 07:55:44.213762 15088 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/credential' I0206 07:55:44.214251 15088 slave.cpp:281] Slave using credential for: test-principal I0206 07:55:44.214653 15088 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] I0206 07:55:44.214918 15088 slave.cpp:328] Slave hostname: utopic I0206 07:55:44.215116 15088 slave.cpp:329] Slave checkpoint: false W0206 07:55:44.215332 15088 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag I0206 07:55:44.217061 15090 state.cpp:32] Recovering state from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/meta' I0206 07:55:44.235409 15088 status_update_manager.cpp:196] Recovering status update manager I0206 07:55:44.235601 15088 containerizer.cpp:299] Recovering containerizer I0206 07:55:44.236486 15088 slave.cpp:3526] Finished recovery I0206 07:55:44.237709 15087 status_update_manager.cpp:170] Pausing sending status updates I0206 07:55:44.237890 15088 slave.cpp:620] New master detected at master@127.0.1.1:38895 I0206 07:55:44.241575 15088 slave.cpp:683] Authenticating with master master@127.0.1.1:38895 I0206 07:55:44.247459 15088 slave.cpp:688] Using default CRAM-MD5 authenticatee I0206 07:55:44.248617 15089 authenticatee.hpp:137] Creating new client SASL connection I0206 07:55:44.249099 15089 master.cpp:3788] Authenticating slave(46)@127.0.1.1:38895 I0206 07:55:44.249137 15089 master.cpp:3799] Using default CRAM-MD5 authenticator I0206 07:55:44.249728 15089 authenticator.hpp:169] Creating new server SASL connection I0206 07:55:44.250285 15089 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5 I0206 07:55:44.250496 15089 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 07:55:44.250452 15088 slave.cpp:656] Detecting new master I0206 07:55:44.251063 15091 authenticator.hpp:275] Received SASL authentication start I0206 07:55:44.251124 15091 authenticator.hpp:397] Authentication requires more steps I0206 07:55:44.251256 15089 authenticatee.hpp:274] Received SASL authentication step I0206 07:55:44.251451 15090 authenticator.hpp:303] Received SASL authentication step I0206 07:55:44.251575 15090 authenticator.hpp:389] Authentication success I0206 07:55:44.251687 15090 master.cpp:3846] Successfully authenticated principal 'test-principal' at slave(46)@127.0.1.1:38895 I0206 07:55:44.253306 15089 authenticatee.hpp:314] Authentication success I0206 07:55:44.258015 15089 slave.cpp:754] Successfully authenticated with master master@127.0.1.1:38895 I0206 07:55:44.258468 15089 master.cpp:2913] Registering slave at slave(46)@127.0.1.1:38895 (utopic) with id 20150206-075544-16842879-38895-15065-S0 I0206 07:55:44.259028 15089 registrar.cpp:444] Applied 1 operations in 88902ns; attempting to update the 'registry' I0206 07:55:44.269492 15065 sched.cpp:149] Version: 0.22.0 I0206 07:55:44.270539 15090 sched.cpp:246] New master detected at master@127.0.1.1:38895 I0206 07:55:44.270614 15090 sched.cpp:302] Authenticating with master master@127.0.1.1:38895 I0206 07:55:44.270634 15090 sched.cpp:309] Using default CRAM-MD5 authenticatee I0206 07:55:44.270900 15090 authenticatee.hpp:137] Creating new client SASL connection I0206 07:55:44.272300 15089 log.cpp:683] Attempting to append 285 bytes to the log I0206 07:55:44.272552 15089 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 3 I0206 07:55:44.273609 15086 master.cpp:3788] Authenticating scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.273643 15086 master.cpp:3799] Using default CRAM-MD5 authenticator I0206 07:55:44.273955 15086 authenticator.hpp:169] Creating new server SASL connection I0206 07:55:44.274617 15090 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5 I0206 07:55:44.274813 15090 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 07:55:44.275171 15088 authenticator.hpp:275] Received SASL authentication start I0206 07:55:44.275215 15088 authenticator.hpp:397] Authentication requires more steps I0206 07:55:44.275408 15090 authenticatee.hpp:274] Received SASL authentication step I0206 07:55:44.275696 15084 authenticator.hpp:303] Received SASL authentication step I0206 07:55:44.275774 15084 authenticator.hpp:389] Authentication success I0206 07:55:44.275876 15084 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.277593 15090 authenticatee.hpp:314] Authentication success I0206 07:55:44.278201 15086 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895 I0206 07:55:44.278548 15086 master.cpp:1568] Received registration request for framework 'framework1' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.278642 15086 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*' I0206 07:55:44.279157 15086 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.280081 15086 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.280320 15086 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.281411 15089 replica.cpp:510] Replica received write request for position 3 I0206 07:55:44.282289 15085 master.cpp:2901] Ignoring register slave message from slave(46)@127.0.1.1:38895 (utopic) as admission is already in progress I0206 07:55:44.284984 15089 leveldb.cpp:342] Persisting action (304 bytes) to leveldb took 3.368213ms I0206 07:55:44.285020 15089 replica.cpp:678] Persisted action at 3 I0206 07:55:44.285893 15089 replica.cpp:657] Replica received learned notice for position 3 I0206 07:55:44.288350 15089 leveldb.cpp:342] Persisting action (306 bytes) to leveldb took 2.430449ms I0206 07:55:44.288384 15089 replica.cpp:678] Persisted action at 3 I0206 07:55:44.288405 15089 replica.cpp:663] Replica learned APPEND action at position 3 I0206 07:55:44.290154 15089 registrar.cpp:489] Successfully updated the 'registry' in 31.046912ms I0206 07:55:44.290307 15085 log.cpp:702] Attempting to truncate the log to 3 I0206 07:55:44.290671 15085 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 4 I0206 07:55:44.291482 15085 replica.cpp:510] Replica received write request for position 4 I0206 07:55:44.292559 15087 master.cpp:2970] Registered slave 20150206-075544-16842879-38895-15065-S0 at slave(46)@127.0.1.1:38895 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] I0206 07:55:44.292940 15087 slave.cpp:788] Registered with master master@127.0.1.1:38895; given slave ID 20150206-075544-16842879-38895-15065-S0 I0206 07:55:44.293298 15087 hierarchical_allocator_process.hpp:450] Added slave 20150206-075544-16842879-38895-15065-S0 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] available) I0206 07:55:44.293684 15087 status_update_manager.cpp:177] Resuming sending status updates I0206 07:55:44.294085 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.299957 15085 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 8.442691ms I0206 07:55:44.300165 15085 replica.cpp:678] Persisted action at 4 I0206 07:55:44.300698 15065 sched.cpp:1468] Asked to stop the driver I0206 07:55:44.301127 15090 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0000' I0206 07:55:44.301503 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.301535 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895 I0206 07:55:44.302376 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0000 by master@127.0.1.1:38895 W0206 07:55:44.302407 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.302814 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.302947 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.309281 15086 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0000 I0206 07:55:44.310158 15084 replica.cpp:657] Replica received learned notice for position 4 I0206 07:55:44.313246 15084 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 3.055049ms I0206 07:55:44.313328 15084 leveldb.cpp:400] Deleting ~2 keys from leveldb took 45270ns I0206 07:55:44.313349 15084 replica.cpp:678] Persisted action at 4 I0206 07:55:44.313374 15084 replica.cpp:663] Replica learned TRUNCATE action at position 4 I0206 07:55:44.329591 15065 sched.cpp:149] Version: 0.22.0 I0206 07:55:44.330258 15088 sched.cpp:246] New master detected at master@127.0.1.1:38895 I0206 07:55:44.330346 15088 sched.cpp:302] Authenticating with master master@127.0.1.1:38895 I0206 07:55:44.330368 15088 sched.cpp:309] Using default CRAM-MD5 authenticatee I0206 07:55:44.330652 15088 authenticatee.hpp:137] Creating new client SASL connection I0206 07:55:44.331403 15088 master.cpp:3788] Authenticating scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.331717 15088 master.cpp:3799] Using default CRAM-MD5 authenticator I0206 07:55:44.332293 15088 authenticator.hpp:169] Creating new server SASL connection I0206 07:55:44.332655 15088 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5 I0206 07:55:44.332684 15088 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5' I0206 07:55:44.332792 15088 authenticator.hpp:275] Received SASL authentication start I0206 07:55:44.332835 15088 authenticator.hpp:397] Authentication requires more steps I0206 07:55:44.332903 15088 authenticatee.hpp:274] Received SASL authentication step I0206 07:55:44.332983 15088 authenticator.hpp:303] Received SASL authentication step I0206 07:55:44.333056 15088 authenticator.hpp:389] Authentication success I0206 07:55:44.333153 15088 authenticatee.hpp:314] Authentication success I0206 07:55:44.333297 15091 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.334326 15087 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895 I0206 07:55:44.334645 15087 master.cpp:1568] Received registration request for framework 'framework2' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.334722 15087 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*' I0206 07:55:44.335153 15087 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.336019 15087 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.336156 15087 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.336796 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 I0206 07:55:44.337725 15065 sched.cpp:1468] Asked to stop the driver I0206 07:55:44.338002 15086 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0001' I0206 07:55:44.338297 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.338353 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895 ../../src/tests/master_allocator_tests.cpp:300: Failure Mock function called more times than expected - taking default action specified at: ../../src/tests/mesos.hpp:713:     Function call: deactivateFramework(@0x7fdb74008d70 20150206-075544-16842879-38895-15065-0001)          Expected: to be called once            Actual: called twice - over-saturated and active ../../src/tests/master_allocator_tests.cpp:312: Failure Mock function called more times than expected - taking default action specified at: ../../src/tests/mesos.hpp:753:     Function call: recoverResources(@0x7fdb74013040 20150206-075544-16842879-38895-15065-0001, @0x7fdb74013060 20150206-075544-16842879-38895-15065-S0, @0x7fdb74013080 { cpus(*):2, mem(*):1024, disk(*):24988, ports(*):[31000-32000] }, @0x7fdb74013098 16-byte object <01-00 00-00 DB-7F 00-00 00-00 00-00 00-00 00-00>)          Expected: to be called once            Actual: called twice - over-saturated and active I0206 07:55:44.339527 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0001 by master@127.0.1.1:38895 W0206 07:55:44.339558 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.339954 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.340095 15090 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.340181 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0001 I0206 07:55:44.340852 15085 master.cpp:781] Master terminating I0206 07:55:44.345564 15086 slave.cpp:2680] master@127.0.1.1:38895 exited W0206 07:55:44.345593 15086 slave.cpp:2683] Master disconnected! Waiting for a new master to be elected I0206 07:55:44.393707 15065 slave.cpp:502] Slave terminating [  FAILED  ] MasterAllocatorTest/0.OutOfOrderDispatch, where TypeParam = mesos::master::allocator::HierarchicalAllocatorProcess<mesos::master::allocator::DRFSorter, mesos::master::allocator::DRFSorter> (360 ms) {noformat}"""
"MESOS-2340","Improvement","leader election",5,"Add ability to decode JSON serialized MasterInfo from ZK","""Currently to discover the master a client needs the ZK node location and access to the MasterInfo protobuf so it can deserialize the binary blob in the node.  I think it would be nice to publish JSON (like Twitter's ServerSets) so clients are not tied to protobuf to do service discovery.  This ticket is an intermediate (compatibility) step: we add in {{0.23}} the ability for the {{Detector}} to """"understand"""" JSON **alongside** Protobuf serialized format; this makes it compatible with both earlier versions, as well a future one (most likely, {{0.24}}) that will write the {{MasterInfo}} information in JSON format."""
"MESOS-2337","Bug","build|python api",2,"__init__.py not getting installed in $PREFIX/lib/pythonX.Y/site-packages/mesos","""When doing a {{make install}}, the src/python/native/src/mesos/__init__.py file is not getting installed in {{$PREFIX/lib/pythonX.Y/site-packages/mesos/}}.    This makes it impossible to do the following import when {{PYTHONPATH}} is set to the {{site-packages}} directory.    The directories {{$PREFIX/lib/pythonX.Y/site-packages/mesos/interface, native}} do have their corresponding {{__init__.py}} files.  Reproducing the bug: """
"MESOS-2335","Improvement","agent|master|modules",0.5,"Mesos Lifecycle Modules","""A new kind of module that receives callbacks at significant life cycle events of its host libprocess process. Typically the latter is a Mesos slave or master and the life time of the libprocess process coincides with the underlying OS process.   h4. Motivation and Use Cases  We want to add customized and experimental capabilities that concern the life time of Mesos components without protruding into Mesos source code and without creating new build process dependencies for everybody.   Example use cases: 1. A slave or master life cycle module that gathers fail-over incidents and reports summaries thereof to a remote data sink. 2. A slave module that observes host computer metrics and correlates these with task activity. This can be used to find resources leaks and to prevent, respectively guide, oversubscription. 3. Upgrades and provisioning that require shutdown and restart.  h4. Specifics  The specific life cycle events that we want to get notified about and want to be able to act upon are:  - Process is spawning/initializing - Process is terminating/finalizing  In all these cases, a reference to the process is passed as a parameter, giving the module access for inspection and reaction.   h4. Module Classification  Unlike other named modules, a life cycle module does not directly replace or provide essential Mesos functionality (such as an Isolator module does). Unlike a decorator module it does not directly add or inject data into Mesos core either."""
"MESOS-2332","Improvement","containerization",5,"Report per-container metrics for network bandwidth throttling","""Export metrics from the network isolation to identify scope and duration of container throttling.    Packet loss can be identified from the overlimits and requeues fields of the htb qdisc report for the virtual interface, e.g.    Note that since a packet can be examined multiple times before transmission, overlimits can exceed total packets sent.    Add to the port_mapping isolator usage() and the container statistics protobuf. Carefully consider the naming (esp tx/rx) + commenting of the protobuf fields so it's clear what these represent and how they are different to the existing dropped packet counts from the network stack."""
"MESOS-2350","Improvement","containerization",5,"Add support for MesosContainerizerLaunch to chroot to a specified path","""In preparation for the MesosContainerizer to support a filesystem isolator the MesosContainerizerLauncher must support chrooting. Optionally, it should also configure the chroot environment by (re-)mounting special filesystems such as /proc and /sys and making device nodes such as /dev/zero, etc., such that the chroot environment is functional."""
"MESOS-2349","Improvement","containerization",5,"Provide a way to execute an arbitrary process in a MesosContainerizer container context","""Include a separate binary that when provided with a container_id, path to an executable, and optional arguments will find the container context, enter it, and exec the executable.  e.g.,   This need only support (initially) containers created with the MesosContainerizer and will support all isolators shipped with Mesos, i.e., it should find and enter the cgroups and namespaces for the running executor of the specified container."""
"MESOS-2347","Improvement","c++ api|java api|python api",8,"Add ability for schedulers to explicitly acknowledge status updates on the driver.","""In order for schedulers to be able to handle status updates in a scalable manner, they need the ability to send acknowledgements through the driver. This enables optimizations in schedulers (e.g. process status updates asynchronously w/o backing up the driver, processing/acking updates in batch).  Without this, an implicit reconciliation can overload a scheduler (hence the motivation for MESOS-2308)."""
"MESOS-2353","Improvement","master",5,"Improve performance of the state.json endpoint for large clusters.","""The master's state.json endpoint consistently takes a long time to compute the JSON result, for large clusters:    This can cause the master to get backlogged if there are many state.json requests in flight.  Looking at {{perf}} data, it seems most of the time is spent doing memory allocation / de-allocation. This ticket will try to capture any low hanging fruit to speed this up. Possibly we can leverage moves if they are not already being used by the compiler."""
"MESOS-2373","Bug","allocation",2,"DRFSorter needs to distinguish resources from different slaves.","""Currently the {{DRFSorter}} aggregates total and allocated resources across multiple slaves, which only works for scalar resources. We need to distinguish resources from different slaves.  Suppose we have 2 slaves and 1 framework. The framework is allocated all resources from both slaves.    To provide some context, this issue came up while trying to reserve all unreserved resources from every offer.    Suppose the slave resources are the same as above:  {quote} Slave1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} Slave2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} {quote}  Initial (incorrect) total resources in the DRFSorter is:  {quote} {{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}} {quote}  We receive 2 offers, 1 from each slave:  {quote} Offer1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} Offer2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}} {quote}  At this point, the resources allocated for the framework is:  {quote} {{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}} {quote}  After first {{RESERVE}} operation with Offer1:  The allocated resources for the framework becomes:  {quote} {{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}} {quote}  During second {{RESERVE}} operation with Offer2:  {code:title=HierarchicalAllocatorProcess::updateAllocation}   // ...    FrameworkSorter* frameworkSorter =     frameworkSorters[frameworks\[frameworkId\].role];    Resources allocation = frameworkSorter->allocation(frameworkId.value());    // Update the allocated resources.   Try<Resources> updatedAllocation = allocation.apply(operations);   CHECK_SOME(updatedAllocation);    // ... {code}  {{allocation}} in the above code is:  {quote} {{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}} {quote}  We try to {{apply}} a {{RESERVE}} operation and we fail to find {{ports(\*):\[31000-32000\]}} which leads to the {{CHECK}} fail at {{CHECK_SOME(updatedAllocation);}}"""
"MESOS-2367","Bug","agent",5,"Improve slave resiliency in the face of orphan containers ","""Right now there's a case where a misbehaving executor can cause a slave process to flap:  {panel:title=Quote From [~jieyu]} {quote} 1) User tries to kill an instance 2) Slave sends {{KillTaskMessage}} to executor 3) Executor sends kill signals to task processes 4) Executor sends {{TASK_KILLED}} to slave 5) Slave updates container cpu limit to be 0.01 cpus 6) A user-process is still processing the kill signal 7) the task process cannot exit since it has too little cpu share and is throttled 8) Executor itself terminates 9) Slave tries to destroy the container, but cannot because the user-process is stuck in the exit path. 10) Slave restarts, and is constantly flapping because it cannot kill orphan containers {quote} {panel}  The slave's orphan container handling should be improved to deal with this case despite ill-behaved users (framework writers)."""
"MESOS-2387","Bug","test",1,"SlaveTest.TaskLaunchContainerizerUpdateFails is flaky","""Observed on internal CI  """
"MESOS-2382","Bug","build",1,"replace unsafe ""find | xargs"" with ""find -exec""","""The problem exists in  1194:src/Makefile.am  47:src/tests/balloon_framework_test.sh  The current """"find | xargs rm -rf"""" in the Makefile could potentially destroy data if mesos source was in a folder with a space in the name. E.g. if you for some reason checkout mesos to """"/ mesos"""" the command in src/Makefile.am would turn into a rm -rf /  """"find | xargs"""" should be NUL delimited with """"find -print0 | xargs -0"""" for safer execution or can just be replaced with the find build-in option """"find -exec '{}' \+"""" which behaves similar to xargs.  There was a second occurrence of this in a test script, though in that case it would only rmdir empty folders, so is less critical.  I submitted a PR here: https://github.com/apache/mesos/pull/36 """
"MESOS-2392","Improvement","master",3,"Rate limit slaves removals during master recovery.","""Much like we rate limit slave removals in the common path (MESOS-1148), we need to rate limit slave removals that occur during master recovery. When a master recovers and is using a strict registry, slaves that do not re-register within a timeout will be removed.  Currently there is a safeguard in place to abort when too many slaves have not re-registered. However, in the case of a transient partition, we don't want to remove large sections of slaves without rate limiting."""
"MESOS-2403","Bug","test",2,"MasterAllocatorTest/0.FrameworkReregistersFirst is flaky",""""""
"MESOS-2401","Bug","test",1,"MasterTest.ShutdownFrameworkWhileTaskRunning is flaky","""Looks like the executorShutdownTimeout() was called immediately after executorShutdown() was called!  """
"MESOS-2408","Task","agent",5,"Slave should reclaim storage for destroyed persistent volumes.","""At present, destroying a persistent volume does not cleanup any filesystem space that was used by the volume (it just removes the Mesos-level metadata about the volume). This effectively leads to a storage leak, which is bad. For task sandboxes, we do """"garbage collection"""" to remove the sandbox at a later time to facilitate debugging failed tasks; for volumes, because they are explicitly deleted and are not tied to the lifecycle of a task, removing the associated storage immediately seems best.  To implement this safely, we'll either need to ensure that libprocess messages are delivered in-order, or else add some extra safe-guards to ensure that out-of-order {{CheckpointResources}} messages don't lead to accidental data loss."""
"MESOS-2428","Task","python api",2,"Add Python bindings for the acceptOffers API.","""We introduced the new acceptOffers API in C++ driver. We need to provide Python binding for this API as well."""
"MESOS-2427","Task","java api",2,"Add Java binding for the acceptOffers API.","""We introduced the new acceptOffers API in C++ driver. We need to provide Java binding for this API as well."""
"MESOS-2438","Improvement","libprocess",8,"Improve support for streaming HTTP Responses in libprocess.","""Currently libprocess' HTTP::Response supports a PIPE construct for doing streaming responses:    This interface is too low level and difficult to program against:  * Connection closure is signaled with SIGPIPE, which is difficult for callers to deal with (must suppress SIGPIPE locally or globally in order to get EPIPE instead). * Pipes are generally for inter-process communication, and the pipe has finite size. With a blocking pipe the caller must deal with blocking when the pipe's buffer limit is exceeded. With a non-blocking pipe, the caller must deal with retrying the write.  We'll want to consider a few use cases: # Sending an HTTP::Response with streaming data. # Making a request with http::get and http::post in which the data is returned in a streaming manner. # Making a request in which the request content is streaming.  This ticket will focus on 1 as it is required for the HTTP API."""
"MESOS-2462","Improvement","containerization",3,"Add option for Subprocess to set a death signal for the forked child","""Currently, children forked by the slave, including those through Subprocess, will continue running if the slave exits. For some processes, including helper processes like the fetcher, du, or perf, we'd like them to be terminated when the slave exits.  Add support to Subprocess to optionally set a DEATHSIG for the child, e.g., setting SIGTERM would mean the child would get SIGTERM when the slave terminates.  This can be done (*after forking*) with PR_SET_DEATHSIG. See """"man prctl"""". It is preserved through an exec call."""
"MESOS-2461","Improvement","containerization",1,"Slave should provide details on processes running in its cgroups","""The slave can optionally be put into its own cgroups for a list of subsystems, e.g., for monitoring of memory and cpu. See the slave flag: --slave_subsystems  It currently refuses to start if there are any processes in its cgroups - this could be another slave or some subprocess started by a previous slave - and only logs the pids of those processes.  Improve this to log details about the processes: suggest at least the process command, uid running it, and perhaps its start time."""
"MESOS-2485","Improvement","master",3,"Add ability to distinguish slave removals metrics by reason.","""Currently we only expose a single removal metric ({{""""master/slave_removals""""}}) which makes it difficult to distinguish between removal reasons in the alerting.  Currently, a slave can be removed for the following reasons:  # Health checks failed. # Slave unregistered. # Slave was replaced by a new slave (on the same endpoint).  In the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures."""
"MESOS-2491","Task","agent|master",5,"Persist the reservation state on the slave","""h3. Goal  The goal for this task is to persist the reservation state stored on the master on the corresponding slave. The {{needCheckpointing}} predicate is used to capture the condition for which a resource needs to be checkpointed. Currently the only condition is {{isPersistentVolume}}. We'll update this to include dynamically reserved resources.  h3. Expected Outcome  * The dynamically reserved resources will be persisted on the slave."""
"MESOS-2501","Documentation","libprocess",1,"Doxygen style for libprocess","""Create a description of the Doxygen style to use for libprocess documentation.   It is expected that this will later also become the Doxygen style for stout and Mesos, but we are working on libprocess only for now.  Possible outcome: a file named docs/doxygen-style.md  We hope for much input and expect a lot of discussion! """
"MESOS-2500","Documentation","libprocess",2,"Doxygen setup for libprocess","""Goals:  - Initial doxygen setup.  - Enable interested developers to generate already available doxygen content locally in their workspace and view it. - Form the basis for future contributions of more doxygen content.  1. Devise a way to use Doxygen with Mesos source code. (For example, solve this by adding optional brew/apt-get installation to the """"Getting Started"""" doc.) 2. Create a make target for libprocess documentation that can be manually triggered. 3. Create initial library top level documentation. 4. Enhance one header file with Doxygen. Make sure the generated output has all necessary links to navigate from the lib to the file and back, etc. """
"MESOS-2507","Improvement","master",5,"Performance issue in the master when a large number of slaves are registering.","""For large clusters, when a lot of slaves are registering, the master gets backlogged processing registration requests. {{perf}} revealed the following:    This is likely because we loop over all the slaves for each registration:  """
"MESOS-2534","Bug","test",2,"PerfTest.ROOT_SampleInit test fails.","""From MESOS-2300 as well, it looks like this test is not reliable:    It looks like this test samples PID 1, which is either {{init}} or {{systemd}}. Per a chat with [~idownes] this should probably sample something that is guaranteed to be consuming cycles."""
"MESOS-2545","Documentation","libprocess",2,"Developer guide for libprocess","""Create a developer guide for libprocess that explains the philosophy behind it and explains the most important features as well as the prevalent use patterns in Mesos with examples.   This could be similar to stout/README.md. """
"MESOS-2559","Bug","framework",1,"Do not use RunTaskMessage.framework_id.","""Assume that FrameworkInfo.id is always set and so need to read/set RunTaskMessage.framework_id.  This should land after https://issues.apache.org/jira/browse/MESOS-2558 has been shipped."""
"MESOS-2591","Improvement","containerization|test",2,"Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse","""Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse"""
"MESOS-2598","Bug","statistics",3,"Slave state.json frameworks.executors.queued_tasks wrong format?","""queued_tasks.executor_id is expected to be a string and not a complete json object. It should have the very same format as the tasks array on the same level.  Example, directly taken from slave  """
"MESOS-2596","Task","allocation|documentation|modules",2,"Update allocator docs","""Once Allocator interface changes, so does the way of writing new allocators. This should be reflected in Mesos docs. The modules doc should mention how to write and use allocator modules. Configuration doc should mention the new {{--allocator}} flag."""
"MESOS-2595","Improvement","docker",8,"Create docker executor","""Currently we're reusing the command executor to wait on the progress of the docker executor, but has the following drawback:  - We need to launch a seperate docker log process just to forward logs, where we can just simply reattach stdout/stderr if we create a specific executor for docker - In general, Mesos slave is assuming that the executor is the one starting the actual task. But the current docker containerizer, the containerizer is actually starting the docker container first then launches the command executor to wait on it. This can cause problems if the container failed before the command executor was able to launch, as slave will try to update the limits of the containerizer on executor registration but then the docker containerizer will fail to do so since the container failed.   Overall it's much simpler to tie the container lifecycle with the executor and simplfies logic and log management."""
"MESOS-2600","Task","master",5,"Add /reserve and /unreserve endpoints on the master for dynamic reservation","""Enable operators to manage dynamic reservations by Introducing the {{/reserve}} and {{/unreserve}} HTTP endpoints on the master."""
"MESOS-2613","Improvement","containerization|docker",2,"Change docker rm command","""Right now it seems Mesos is using docker rm f ID to delete containers so bind mounts are not deleted. This means thousands of dirs in /var/lib/docker/vfs/dir   I would like to have the option to change it to docker rm f v ID This deletes bind mounts but not persistant volumes.  Best,  Mike"""
"MESOS-2615","Task","allocation|master",1,"Pipe 'updateFramework' path from master to Allocator to support framework re-registration","""Pipe the 'updateFramework' call from the master through the allocator, as described in the design doc in the epic: MESOS-703"""
"MESOS-2633","Task","master",1,"Move implementations of Framework struct functions out of master.hpp.","""To help reduce compile time and keep the header easy to read, let's move the implementations of the Framework struct functions out of master.hpp"""
"MESOS-2622","Documentation","documentation",1,"Document the semantic change in decorator return values","""In order to enable decorator modules to _remove_ metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks.  The Result<T> return values means:  ||State||Before||After|| |Error|Error is propagated to the call-site|No change| |None|The result of the decorator is not applied|No change| |Some|The result of the decorator is *appended*|The result of the decorator *overwrites* the final labels/environment object|"""
"MESOS-2665","Bug","containerization",5,"Fix queuing discipline wrapper in linux/routing/queueing ","""qdisc search function is dependent on matching a single hard coded handle and does not correctly test for interface, making the implementation fragile.  Additionally, the current setup scripts (using dynamically created shell commands) do not match the hard coded handles.  """
"MESOS-2680","Improvement","documentation|modules",1,"Update modules doc with hook usage example","""Modules doc states the possibility of using hooks, but doesn't refer to necessary flags and usage example."""
"MESOS-2696","Task","containerization",5,"Explore exposing stats from kernel","""Exploratory work.  Additional tickets to follow."""
"MESOS-2702","Task","containerization",3,"Compare split/flattened cgroup hierarchy for CPU oversubscription","""Investigate if a flat hierarchy is sufficient for oversubscription of CPU or if a two-way split is necessary/preferred."""
"MESOS-2701","Task","containerization",8,"Implement bi-level cpu.shares subtrees in cgroups/cpu isolator.","""See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.  # Configurable bias # Change cgroup layout ** Implement roll-forward migration path in isolator recover ** Document roll-back migration path"""
"MESOS-2700","Task","containerization",13,"Determine CFS behavior with biased cpu.shares subtrees","""See this [ticket|https://issues.apache.org/jira/browse/MESOS-2652] for context.  * Understand the relationship between cpu.shares and CFS quota. * Determine range of possible bias splits * Determine how to achieve bias, e.g., should 20:1 be 20480:1024 or ~1024:50 * Rigorous testing of behavior with varying loads, particularly the combination of latency sensitive loads for high biased tasks (non-revokable), and cpu intensive loads for the low biased tasks (revokable). * Discover any performance edge cases?"""
"MESOS-2709","Improvement","java api",3,"Design Master discovery functionality for HTTP-only clients","""When building clients that do not bind to {{libmesos}} and only use the HTTP API (via """"pure"""" language bindings - eg, Java-only) there is no simple way to discover the Master's IP address to connect to.  Rather than relying on 'out-of-band' configuration mechanisms, we would like to enable the ability of interrogating the ZooKeeper ensemble to discover the Master's IP address (and, possibly, other information) to which the HTTP API requests can be addressed to."""
"MESOS-2726","Task","containerization",13,"Add support for enabling network namespace without enabling the network isolator","""Following the discussion Kapil started, it is currently not possible to enable the linux network namespace for a container without enabling the network isolator (which requires certain kernel capabilities and dependencies). Following the pattern of enabling pid namespaces (--isolation=""""namespaces/pid""""). One possible solution could be to add another one for network i.e. """"namespaces/network"""".  """
"MESOS-2737","Documentation","documentation",3,"Add documentation for maintainers.","""In order to scale the number of committers in the project, we proposed the concept of maintainers here:  http://markmail.org/thread/cjmdn3d7qfzbxhpm  To follow up on that proposal, we'll need some documentation to capture the concept of maintainers. Both how contributors can benefit from maintainer feedback and the expectations of """"maintainer-ship"""".  In order to not enforce an excessive amount of process, maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback."""
"MESOS-2743","Improvement","json api",3,"Include ExecutorInfos for custom executors in master/state.json","""The slave/state.json already reports executorInfos: https://github.com/apache/mesos/blob/0.22.1/src/slave/http.cpp#L215-219  Would be great to see this in the master/state.json as well, so external tools don't have to query each slave to find out executor resources, sandbox directories, etc."""
"MESOS-2753","Task","containerization|master",3,"Master should validate tasks using oversubscribed resources","""Current implementation out for [review|https://reviews.apache.org/r/34310] only supports setting the priority of containers with revocable CPU if it's specified in the initial executor info resources. This should be enforced at the master.  Also master should make sure that oversubscribed resources used by the task are valid."""
"MESOS-2750","Bug","containerization",3,"Extend queueing discipline wrappers to expose network isolator statistics","""Export Traffic Control statistics in queueing library to enable reporting out impact of network bandwidth statistics."""
"MESOS-2757","Improvement","libprocess|stout",3,"Add -> operator for Option<T>, Try<T>, Result<T>, Future<T>.","""Let's add operator overloads to Option<T> to allow access to the underlying T using the `->` operator. """
"MESOS-2783","Documentation","documentation",5,"document the fetcher","""For framework developers specifically, Mesos provides a fetcher to move binaries. This needs MVP documentation.  - What is it - How does it help - What protocols or schemas are supported - Can it be extended  This is important to get framework developers over the hump of learning to code against Mesos and grow the ecosystem."""
"MESOS-2784","Improvement","documentation",1,"Added constexpr to C++11 whitelist.","""constexpr is currently used to eliminate initialization dependency issues for non-POD objects.  We should add it to the whitelist of acceptable c++11 features in the style guide."""
"MESOS-2796","Improvement","containerization",5,"Implement AppC image provisioner.","""Implement a filesystem provisioner that can provision container images compliant with the Application Container Image (aci) [specification|https://github.com/appc/spec]."""
"MESOS-2795","Improvement","containerization",5,"Introduce filesystem provisioner abstraction","""Optional filesystem provisioner component for the Mesos containerizer that can provision per-container filesystems.  This is different to a filesystem isolators because it just provisions a root filesystem for a container and doesn't actually do any isolation (e.g., through a mount namespace + pivot or chroot)."""
"MESOS-2794","Improvement","containerization",13,"Implement filesystem isolators","""Move persistent volume support from Mesos containerizer to separate filesystem isolators, including support for container rootfs, where possible.  Use symlinks for posix systems without container rootfs. Use bind mounts for Linux with/without container rootfs."""
"MESOS-2793","Improvement","containerization",1,"Add support for container rootfs to Mesos isolators","""Mesos containers can have a different rootfs to the host. Update Isolator interface to pass rootfs during Isolator::prepare(). Update Isolators where  necessary."""
"MESOS-2801","Improvement","libprocess",3,"Remove dynamic allocation from Future<T>","""Remove the dynamic allocation of `T*` inside `Future::Data`"""
"MESOS-2800","Improvement","stout",3,"Rename Option<T>::get(const T& _t) to getOrElse() and refactor the original function","""As suggested, if we want to change the name then we should refactor the original function as opposed to having 2 copies.  If we did have 2 versions of the same function, would it make more sense to delegate one of them to the other.  As of today, there is only one file need to be refactor: 3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp at line 151, 161"""
"MESOS-2807","Task","leader election",3,"As a developer I need an easy way to convert MasterInfo protobuf to/from JSON","""As a preliminary to MESOS-2340, this requires the implementation of a simple (de)serialization mechanism to JSON from/to {{MasterInfo}} protobuf."""
"MESOS-2805","Improvement","libprocess|stout",8,"Make synchronized as primary form of synchronization.","""Re-organize Synchronized to allow {{synchronized(m)}} to work on:   1. {{std::mutex}}   2. {{std::recursive_mutex}}   3. {{std::atomic_flag}}  Move synchronized.hpp into stout, so that developers don't think it's part of the utility suite for actors in libprocess.  Remove references to internal.hpp and replace them with {{std::atomic_flag}} synchronization."""
"MESOS-2804","Improvement","master",1,"Log framework capabilities in the master.","""Now that {{Capabilities}} has been added to FrameworkInfo, we should log these in the master when a framework (re-)registers (i.e. which capabilities are enabled and disabled). This would make debugging easier for framework developers.  Ideally, folding in the old {{checkpoint}} capability and logging that as well. In the past, the fact that {{checkpoint}} defaults to false has tripped up a lot of developers."""
"MESOS-2817","Improvement","containerization",3,"Support revocable/non-revocable CPU updates in Mesos containerizer","""MESOS-2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container. Improve this to support updates to/from revocable cpu. Note, *any* revocable cpu will result in the entire container's cpu being treated as revocable at the cpu isolator level. Higher level logic is responsible for adding/removing based on some policy."""
"MESOS-2815","Bug","fetcher",2,"Flaky test: FetcherCacheHttpTest.HttpCachedSerialized","""FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:  [ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized  GMOCK WARNING: Uninteresting mock function call - returning directly.     Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <D0-E1 59-49 10-2B 00-00 00-00 00-00 00-00 00-00 10-A1 00-68 10-2B 00-00 A0-DE 00-68 10-2B 00-00 20-DF 00-68 10-2B 00-00 40-9C 00-68 10-2B 00-00 90-2D 00-68 10-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2B 00-00 00-00 00-00 0F-00 00-00> }) Stack trace: F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers *** Check failure stack trace: ***     @     0x2b10488ff6c0  google::LogMessage::Fail()     @     0x2b10488ff60c  google::LogMessage::SendToLog()     @     0x2b10488ff00e  google::LogMessage::Flush()     @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()     @           0x9721e4  _CheckFatal::~_CheckFatal()     @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()     @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()     @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x114e1df  testing::Test::Run()     @          0x114e902  testing::TestInfo::Run()     @          0x114ee8a  testing::TestCase::Run()     @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()     @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported<>()     @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported<>()     @          0x1152a60  testing::UnitTest::Run()     @           0xcbc50f  main     @     0x2b104af78ec5  (unknown)     @           0x867559  (unknown) make[4]: *** [check-local] Aborted make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[3]: *** [check-am] Error 2 make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[2]: *** [check] Error 2 make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src' make[1]: *** [check-recursive] Error 1 make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build' make: *** [distcheck] Error 1 """
"MESOS-2830","Wish","agent",8,"Add an endpoint to slaves to allow launching system administration tasks","""As a System Administrator often times I need to run a organization-mandated task on every machine in the cluster. Ideally I could do this within the framework of mesos resources if it is a """"cleanup"""" or auditing task, but sometimes I just have to run something, and run it now, regardless if a machine has un-accounted resources  (Ex: Adding/removing a user).  Currently to do this I have to completely bypass Mesos and SSH to the box. Ideally I could tell a mesos slave (With proper authentication) to run a container with the limited special permissions needed to get the task done."""
"MESOS-2829","Bug","fetcher",0,"FetcherCacheTest.CachedFallback test is flaky","""Observed this on internal CI  """
"MESOS-2834","Improvement","containerization",3,"Support different perf output formats","""The output format of perf changes in 3.14 (inserting an additional field) and in again in 4.1 (appending additional) fields. See kernel commits: 410136f5dd96b6013fe6d1011b523b1c247e1ccb d73515c03c6a2706e088094ff6095a3abefd398b  Update the perf::parse() function to understand all these formats."""
"MESOS-2850","Improvement","containerization",3,"Implement Docker image provisioner","""Provisions a Docker image (provisions all its dependent layers), fetch an image from persistent store, and also destroy an image.   Done when tested for local discovery and copy backend. """
"MESOS-2849","Improvement","containerization",5,"Implement Docker local image store","""Given a local Docker image name and path to the image or image tarball, fetches the image's dependent layers, untarring if necessary. It will also parse the image layers' configuration json and place the layers and image into persistent store.  Done when a Docker image can be successfully stored and retrieved using 'put' and 'get' methods. """
"MESOS-2848","Improvement","containerization",2,"Local filesystem docker image discovery","""Given a docker image name and the local directory where images can be found, creates a URI with a path to the corresponding image.  Done when system successfully checks for the image, untars the image if necessary, and returns the proper URI to the image."""
"MESOS-2862","Bug","fetcher",2,"mesos-fetcher won't fetch uris which begin with a "" ""","""Discovered while running mesos with marathon on top. If I launch a marathon task with a URI which is """" http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz"""" mesos will log to stderr:    It would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed. """
"MESOS-2860","Story","master",3,"Create the basic infrastructure to handle /scheduler endpoint","""This is the first basic step in ensuring the basic {{/call}} functionality: processing a   and returning:  - {{202}} if all goes well; - {{401}} if not authorized; and - {{403}} if the request is malformed.  We'll get more sophisticated as the work progressed (eg, supporting {{415}} if the content-type is not of the right kind)."""
"MESOS-2857","Bug","fetcher|test",1,"FetcherCacheTest.LocalCachedExtract is flaky.","""From jenkins:    [~bernd-mesos] not sure if there's a ticket capturing this already, sorry if this is a duplicate."""
"MESOS-2866","Bug","agent",3,"Slave should send oversubscribed resource information after master failover.","""After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this."""
"MESOS-2874","Bug","containerization|test",2,"Convert PortMappingStatistics to use automatic JSON encoding/decoding","""Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.  This change will simplify the implementation of MESOS-2332."""
"MESOS-2883","Improvement","modules",2,"Do not call hook manager if no hooks installed","""Hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator, environment decorator, etc.  Often the call into such a decorator hooks results in a new copy of labels, environment, etc., being returned to the call site. This is an unnecessary overhead if there are no hooks installed.  The proper way would be to call decorators via the hook manager only if there are some hooks installed. This would prevent unnecessary copying overhead if no hooks are available."""
"MESOS-2879","Bug","libprocess",1,"Random recursive_mutex errors in when running make check","""While running make check on OS X, from time to time {{recursive_mutex}} errors appear after running all the test successfully. Just one of the experience messages actually stops {{make check}} reporting an error.  The following error messages have been experienced:      """
"MESOS-2893","Task","allocation|master",1,"Add queue size metrics for the allocator.","""In light of the performance regression in MESOS-2891, we'd like to have visibility into the queue size of the allocator. This will enable alerting on performance problems.  We currently have no metrics in the allocator.  I will also look into MESOS-1286 now that we have gcc 4.8, current queue size gauges require a trip through the Process' queue."""
"MESOS-2892","Task","allocation|master",3,"Add benchmark for hierarchical allocator.","""In light of the performance regression in MESOS-2891, we'd like to have a synthetic benchmark of the allocator code, in order to analyze and direct improvements."""
"MESOS-2891","Bug","allocation|master",3,"Performance regression in hierarchical allocator.","""For large clusters, the 0.23.0 allocator cannot keep up with the volume of slaves. After the following slave was re-registered, it took the allocator a long time to work through the backlog of slaves to add:  {noformat:title=45 minute delay} I0618 18:55:40.738399 10172 master.cpp:3419] Re-registered slave 20150422-211121-2148346890-5050-3253-S4695 I0618 19:40:14.960636 10164 hierarchical.hpp:496] Added slave 20150422-211121-2148346890-5050-3253-S4695 {noformat}  Empirically, [addSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L462] and [updateSlave|https://github.com/apache/mesos/blob/dda49e688c7ece603ac7a04a977fc7085c713dd1/src/master/allocator/mesos/hierarchical.hpp#L533] have become expensive.  Some timings from a production cluster reveal that the allocator spending in the low tens of milliseconds for each call to {{addSlave}} and {{updateSlave}}, when there are tens of thousands of slaves this amounts to the large delay seen above.  We also saw a slow steady increase in memory consumption, hinting further at a queue backup in the allocator.  A synthetic benchmark like we did for the registrar would be prudent here, along with visibility into the allocator's queue size."""
"MESOS-2890","Bug","webui",3,"Sandbox URL doesn't work in web-ui when using SSL","""The links to the sandbox in the web ui don't work when ssl is enabled.  This can happen if the certificate for the master and the slave do not match. This is a consequence of the redirection that happens when serving files. The resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves."""
"MESOS-2888","Improvement","libprocess",5,"Add SSL socket tests","""commit beac384c77d4a9c235a813e9286716f4509bdd55 Author: Joris Van Remoortere <joris.van.remoortere@gmail.com> Date:   Fri Jun 26 18:30:12 2015 -0700      Add SSL tests.          Review: https://reviews.apache.org/r/35889"""
"MESOS-2886","Documentation","documentation|test",1,"Capture some testing patterns we use in a doc","""In Mesos tests we use some tricks and patterns to express certain expectations. These are not always obvious and not documented. The intent of the ticket is to kick-start the document with the description of those tricks for posterity."""
"MESOS-2884","Task","containerization",5,"Allow isolators to specify required namespaces","""Currently, the LinuxLauncher looks into SlaveFlags to compute the namespaces that should be enabled when launching the executor. This means that a custom Isolator module doesn't have any way to specify dependency on a set of namespaces.  The proposed solution is to extend the Isolator interface to also export the namespaces dependency. This way the MesosContainerizer can directly query all loaded Isolators (inbuilt and custom modules) to compute the set of namespaces required by the executor. This set of namespaces is then passed on to the LinuxLauncher. """
"MESOS-2902","Improvement","agent|master|modules",5,"Enable Mesos to use arbitrary script / module to figure out IP, HOSTNAME","""Currently Mesos tries to guess the IP, HOSTNAME by doing a reverse DNS lookup. This doesn't work on a lot of clouds as we want things like public IPs (which aren't the default DNS), there aren't FQDN names (Azure), or the correct way to figure it out is to call some cloud-specific endpoint.  If Mesos / Libprocess could load a mesos-module (Or run a script) which is provided per-cloud, we can figure out perfectly the IP / Hostname for the given environment. It also means we can ship one identical set of files to all hosts in a given provider which doesn't happen to have the DNS scheme + hostnames that libprocess/Mesos expects. Currently we have to generate host-specific config files which Mesos uses to guess.  The host-specific files break / fall apart if machines change IP / hostname without being reinstalled."""
"MESOS-2904","Bug","agent|statistics",1,"Add slave metric to count container launch failures","""We have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state (for example, unexpected network configuration).   Adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state."""
"MESOS-2903","Bug","containerization",3,"Network isolator should not fail when target state already exists","""Network isolator has multiple instances of the following pattern:    These failures have occurred in operation due to the failure to recover or delete an orphan, causing the slave to remain on line but unable to create new resources.    We should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested."""
"MESOS-2917","Bug","build|containerization",1,"Specify correct libnl version for configure check","""Currently configure.ac lists 3.2.24 as the required libnl version. However, https://reviews.apache.org/r/31503 caused the minimum required version to be bumped to 3.2.26. The configure check thus fails to error out during execution and the dependency is captured only during the build step."""
"MESOS-2925","Bug","libprocess",1,"Invalid usage of ATOMIC_FLAG_INIT in member initialization","""The C++ specification states:  The macro ATOMIC_FLAG_INIT shall be defined in such a way that it can be used to initialize an object of type atomic_flag to the clear state. The macro can be used in the form: """"atomic_flag guard = ATOMIC_FLAG_INIT; """"It is unspecified whether the macro can be used in other initialization contexts.""""   Clang catches this (although reports it erroneously as a braced scaled init issue) and refuses to compile libprocess."""
"MESOS-2922","Improvement","libprocess",3,"Add move constructors / assignment to Future.","""Now that we have C++11, let's add move constructors and move assignment operators for Future, similarly to what was done for Option. There is currently one move constructor for Future<T>, but not for T, U, and no assignment operator."""
"MESOS-2921","Improvement","stout",3,"Add move constructors / assignment to Result.","""Now that we have C++11, let's add move constructors and move assignment operators for Result, similarly to what was done for Option."""
"MESOS-2920","Improvement","stout",3,"Add move constructors / assignment to Try.","""Now that we have C++11, let's add move constructors and move assignment operators for Try, similarly to what was done for Option."""
"MESOS-2940","Improvement","master",3,"Reconciliation is expensive for large numbers of tasks.","""We've observed that both implicit and explicit reconciliation are expensive for large numbers of tasks:  {noformat: title=Explicit O(100,000) tasks: 70secs} I0625 20:55:23.716320 21937 master.cpp:3863] Performing explicit task state reconciliation for N tasks of framework F (NAME) at S@IP:PORT I0625 20:56:34.812464 21937 master.cpp:5041] Removing task T with resources R of framework F on slave S at slave(1)@IP:PORT (HOST)   Let's add a benchmark to see if there are any bottlenecks here, and to guide improvements."""
"MESOS-2938","Bug","docker",1,"Linux docker inspect crashes","""On linux,  when a simple task is being executed on docker container executor, the sandbox stderr shows a backtrace:  *** Aborted at 1435254156 (unix time) try """"date -d @1435254156"""" if you are using GNU date *** PC: @     0x7ffff2b1364d (unknown) *** SIGSEGV (@0xfffffffffffffff8) received by PID 88424 (TID 0x7fffe88fb700) from PID 18446744073709551608; stack trace: ***     @     0x7ffff25a4340 (unknown)     @     0x7ffff2b1364d (unknown)     @     0x7ffff2b724df (unknown)     @           0x4a6466 Docker::Container::~Container()     @     0x7ffff5bfa49a Option<>::~Option()     @     0x7ffff5c15989 Option<>::operator=()     @     0x7ffff5c09e9f Try<>::operator=()     @     0x7ffff5c09ee3 Result<>::operator=()     @     0x7ffff5c0a938 process::Future<>::set()     @     0x7ffff5bff412 process::Promise<>::set()     @     0x7ffff5be53e3 Docker::___inspect()     @     0x7ffff5be3cf8 _ZZN6Docker9__inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationENS2_6FutureISsEERKNS2_10SubprocessEENKUlRKSG_E1_clESL_     @     0x7ffff5be91e9 _ZZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferEENUlSM_E_clESM_     @     0x7ffff5be9d9d _ZNSt17_Function_handlerIFvRKN7process6FutureISsEEEZNKS2_5onAnyIZN6Docker9__inspectERKSsRKNS0_5OwnedINS0_7PromiseINS7_9ContainerEEEEERK6OptionI8DurationES2_RKNS0_10SubprocessEEUlS4_E1_vEES4_OT_NS2_6PreferEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_     @     0x7ffff5c1eadd std::function<>::operator()()     @     0x7ffff5c15e07 process::Future<>::onAny()     @     0x7ffff5be93a1 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferE     @     0x7ffff5be87f6 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_EESM_OT_     @     0x7ffff5be459c Docker::__inspect()     @     0x7ffff5be337c _ZZN6Docker8_inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationEENKUlvE_clEv     @     0x7ffff5be8c5a _ZZNK7process6FutureI6OptionIiEE5onAnyIZN6Docker8_inspectERKSsRKNS_5OwnedINS_7PromiseINS5_9ContainerEEEEERKS1_I8DurationEEUlvE_vEERKS3_OT_NS3_10LessPreferEENUlSL_E_clESL_     @     0x7ffff5be9b36 _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyIZN6Docker8_inspectERKSsRKNS0_5OwnedINS0_7PromiseINS9_9ContainerEEEEERKS2_I8DurationEEUlvE_vEES6_OT_NS4_10LessPreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_     @     0x7ffff5c1e9b3 std::function<>::operator()()     @     0x7ffff6184a1a _ZN7process8internal3runISt8functionIFvRKNS_6FutureI6OptionIiEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_     @     0x7ffff617e64d process::Future<>::set()     @     0x7ffff6752e46 process::Promise<>::set()     @     0x7ffff675faec process::internal::cleanup()     @     0x7ffff6765293 _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EE6__callIvIS6_EILm0ELm1ELm2EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE     @     0x7ffff6764bcd _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EEclIJS6_EvEET0_DpOT_     @     0x7ffff67642a5 _ZZNK7process6FutureI6OptionIiEE5onAnyISt5_BindIFPFvRKS3_PNS_7PromiseIS2_EERKNS_10SubprocessEESt12_PlaceholderILi1EESA_SB_EEvEES7_OT_NS3_6PreferEENUlS7_E_clES7_     @     0x7ffff676531d _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyISt5_BindIFPFvS6_PNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EESC_SD_EEvEES6_OT_NS4_6PreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_     @     0x7ffff5c1e9b3 std::function<>::operator()() (END)  """
"MESOS-2937","Documentation","documentation",5,"Initial design document for Quota support in Allocator.","""Create a design document for the Quota feature support in the built-in Hierarchical DRF allocator to be shared with the Mesos community."""
"MESOS-2936","Documentation","documentation",8,"Create a design document for Quota support in Master","""Create a design document for the Quota feature support in Mesos Master (excluding allocator) to be shared with the Mesos community.  Design Doc: https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I/edit?usp=sharing"""
"MESOS-2951","Improvement","containerization|docker",3,"Inefficient container usage collection","""docker containerizer currently collects usage statistics by calling os's process statistics (eg ps ). There is scope for making this efficient, say by querying cgroups file system.  """
"MESOS-2950","Task","master|security",8,"Implement current mesos Authorizer in terms of generalized Authorizer interface","""In order to maintain compatibility with existent versions of Mesos, as well as to prove the flexibility of the generalized {{mesos::Authorizer}} design, the current authorization mechanism through ACL definitions needs to run under the updated interface without any changes being noticeable by the current authorization users."""
"MESOS-2949","Task","master|security",3,"Draft design for generalized Authorizer interface","""As mentioned in MESOS-2948 the current {{mesos::Authorizer}} interface is rather inflexible if new _Actions_ or _Objects_ need to be added.  A new API needs to be designed in a way that allows for arbitrary _Actions_ and _Objects_ to be added to the authorization mechanism without having to recompile mesos."""
"MESOS-2944","Bug","test",1,"Use of EXPECT in test and relying on the checked condition afterwards.","""In docker_containerizer_test we have the following pattern.    As we rely on the value afterwards we should use ASSERT_NE instead. In that case the test will fail immediately. """
"MESOS-2943","Bug","libprocess",2,"mesos fails to compile under mac when libssl and libevent are enabled","""../configure --enable-debug --enable-libevent --enable-ssl && make  produces the following error:  poll.cpp' || echo '../../../3rdparty/libprocess/'`src/libevent_poll.cpp libtool: compile:  g++ -DPACKAGE_NAME=\""""libprocess\"""" -DPACKAGE_TARNAME=\""""libprocess\"""" -DPACKAGE_VERSION=\""""0.0.1\"""" """"-DPACKAGE_STRING=\""""libprocess 0.0.1\"""""""" -DPACKAGE_BUGREPORT=\""""\"""" -DPACKAGE_URL=\""""\"""" -DPACKAGE=\""""libprocess\"""" -DVERSION=\""""0.0.1\"""" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"""".libs/\"""" -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBCURL=1 -DHAVE_EVENT2_EVENT_H=1 -DHAVE_LIBEVENT=1 -DHAVE_EVENT2_THREAD_H=1 -DHAVE_LIBEVENT_PTHREADS=1 -DHAVE_OPENSSL_SSL_H=1 -DHAVE_LIBSSL=1 -DHAVE_LIBCRYPTO=1 -DHAVE_EVENT2_BUFFEREVENT_SSL_H=1 -DHAVE_LIBEVENT_OPENSSL=1 -DUSE_SSL_SOCKET=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBDL=1 -I. -I../../../3rdparty/libprocess -I../../../3rdparty/libprocess/include -I../../../3rdparty/libprocess/3rdparty/stout/include -I3rdparty/boost-1.53.0 -I3rdparty/libev-4.15 -I3rdparty/picojson-4f93734 -I3rdparty/glog-0.3.3/src -I3rdparty/ry-http-parser-1c3624a -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0 -g1 -O0 -std=c++11 -stdlib=libc++ -DGTEST_USE_OWN_TR1_TUPLE=1 -MT libprocess_la-libevent_poll.lo -MD -MP -MF .deps/libprocess_la-libevent_poll.Tpo -c ../../../3rdparty/libprocess/src/libevent_poll.cpp  -fno-common -DPIC -o libprocess_la-libevent_poll.o mv -f .deps/libprocess_la-socket.Tpo .deps/libprocess_la-socket.Plo mv -f .deps/libprocess_la-subprocess.Tpo .deps/libprocess_la-subprocess.Plo mv -f .deps/libprocess_la-libevent.Tpo .deps/libprocess_la-libevent.Plo mv -f .deps/libprocess_la-metrics.Tpo .deps/libprocess_la-metrics.Plo In file included from ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:11: In file included from ../../../3rdparty/libprocess/include/process/queue.hpp:9: ../../../3rdparty/libprocess/include/process/future.hpp:849:7: error: no viable conversion from 'const process::Future<const process::Future<process::network::Socket> >' to 'const process::network::Socket'  set(u);      ^ ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:769:10: note: in instantiation of function template specialization 'process::Future<process::network::Socket>::Future<process::Future<const process::Future<process::network::Socket> > >' requested here  return accept_queue.get()         ^ ../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to      'process::network::Socket &&' for 1st argument class Socket      ^ ../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to      'const process::network::Socket &' for 1st argument class Socket      ^ ../../../3rdparty/libprocess/include/process/future.hpp:411:21: note: passing argument to parameter '_t' here  bool set(const T& _t);                    ^ 1 error generated. make[4]: *** [libprocess_la-libevent_ssl_socket.lo] Error 1 make[4]: *** Waiting for unfinished jobs.... mv -f .deps/libprocess_la-libevent_poll.Tpo .deps/libprocess_la-libevent_poll.Plo mv -f .deps/libprocess_la-openssl.Tpo .deps/libprocess_la-openssl.Plo mv -f .deps/libprocess_la-process.Tpo .deps/libprocess_la-process.Plo make[3]: *** [all-recursive] Error 1 make[2]: *** [all-recursive] Error 1 make[1]: *** [all] Error 2 make: *** [all-recursive] Error 1"""
"MESOS-2957","Improvement","master",1,"Add version to MasterInfo","""This will help schedulers figure out the version of the master that they are interacting with. See MESOS-2736 for additional context."""
"MESOS-2967","Improvement","libprocess",5,"Missing doxygen documentation for libprocess socket interface ","""Convert existing comments to doxygen format.  """
"MESOS-2966","Improvement","libprocess",5,"socket::peer() and socket::address() might fail with SSL enabled","""libevent SSL currently uses a secondary FD so we need to virtualize the get() function on socket interface. """
"MESOS-2965","Improvement","stout",2,"Add implicit cast to string operator to Path.","""For example:   does not have an overload for   The implementation should be something like:  """
"MESOS-2964","Improvement","libprocess",3,"libprocess io does not support peek()","""Finally, I so wish we could just do:    from: https://reviews.apache.org/r/31207/"""
"MESOS-2962","Bug","agent",1,"Slave fails with Abort stacktrace when DNS cannot resolve hostname","""If the DNS cannot resolve the hostname-to-IP for a slave node, we correctly return an {{Error}} object, but we then fail with a segfault.  This code adds a more user-friendly message and exits normally (with an {{EXIT_FAILURE}} code).  For example, forcing {{net::getIp()}} to always return an {{Error}}, now causes the slave to exit like this:  """
"MESOS-2961","Task","containerization|docker",2,"Add cpuacct subsystem utils to cgroups","""Current cgroups implementation does not have a cpuacct subsystem implementation. This subsystem reports important metrics like user and system CPU ticks spent by a process. """"cgroups"""" namespace has subsystem specific utilities for """"cpu"""", """"memory"""" etc. It could use other subsystems specific utils (eg. cpuacct).  In the future, we could also view cgroups as a mesos-subsystem with  features like event notifications.  Although refactoring cgroups would be a different epic, listing the possible tasks:   -  Have hierarchies, subsystems abstracted to represent the domain    - Create  """"cgroups service""""   -  """"cgroups service"""" listen to update events from the OS on files like stats. This would be an interrupt based system(maybe use linux fsnotify)   - """"cgroups service"""" services events to mesos (containers for example).  """
"MESOS-2973","Bug","libprocess",3,"SSL tests don't work with --gtest_repeat","""commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f Author: Joris Van Remoortere <joris.van.remoortere@gmail.com> Date:   Wed Jul 1 16:16:52 2015 -0700      MESOS-2973: Allow SSL tests to run using gtest_repeat.          The SSL ctx object carried some settings between reinitialize()     calls. Re-construct the object to avoid this state transition.          Review: https://reviews.apache.org/r/36074"""
"MESOS-2968","Improvement","containerization",3,"Implement shared copy based provisioner backend","""Currently Appc and Docker both implemented its own copy backend, but most of the logic is the same where the input is just a image name with its dependencies. We can refactor both so that we just have one implementation that is shared between both provisioners, so appc and docker can reuse the shared copy backend."""
"MESOS-2986","Bug","docker",1,"Docker version output is not compatible with Mesos","""We currently use docker version to get Docker version, in Docker master branch and soon in Docker 1.8 [1] the output for this command changes. The solution for now will be to use the unchanged docker --version output, in the long term we should consider stop using the cli and use the API instead.   [1] https://github.com/docker/docker/pull/14047"""
"MESOS-2993","Bug","containerization|documentation",3,"Document  per container unique egress flow and network queueing statistics","""Document new network isolation capabilities in 0.23"""
"MESOS-2991","Bug","stout|test",1,"Compilation Error on Mac OS 10.10.4 with clang 3.5.0","""Compiling 0.23.0 (rc1) produces compilation errors on Mac OS 10.10.4 with {{g++}} based on LLVM 3.5. It looks like the issue was introduced in {{a5640ad813e6256b548fca068f04fd9fa3a03eda}}, https://reviews.apache.org/r/32838. In contrast to the commit message, compiling the rc with gcc4.4 on CentOS worked fine for me.   According to 0.23 release notes and MESOS-2604, we should support clang 3.5.     Compiler version:  """
"MESOS-3008","Improvement","libprocess",8,"Libevent SSL doesn't use EPOLL","""we currently disable to epoll in libevent to allow SSL to work. It would be more scalable if we didn't have to do that."""
"MESOS-3005","Bug","libprocess",3,"SSL tests can fail depending on hostname configuration","""Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate. We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test."""
"MESOS-3004","Improvement","containerization",5,"Design support running the command executor with provisioned image for running a task in a container","""Mesos Containerizer uses the command executor to actually launch the user defined command, and the command executor then can communicate with the slave about the process lifecycle. When we provision a new container with the user specified image, we also need to be able to run the command executor in the container to support the same semantics. One approach is to dynamically mount in a static binary of the command executor with all its dependencies in a special directory so it doesn't interfere with the provisioned root filesystem and configure the mesos containerizer to run the command executor in that directory."""
"MESOS-3002","Bug","containerization",1,"Rename Option<T>::get(const T& _t) to getOrElse() broke network isolator","""Change to Option from get() to getOrElse() breaks network isolator.  Building with '../configure --with-network-isolator' generates the following error:  ../../src/slave/containerizer/isolators/network/port_mapping.cpp: In static member function 'static Try<mesos::slave::Isolator*> mesos::internal::slave::PortMappingIsolatorProcess::create(const mesos::internal::slave::Flags&)': ../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: error: no matching function for call to 'Option<std::basic_string<char> >::get(const char [1]) const'        flags.resources.get(""""""""),                              ^ ../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: note: candidates are: In file included from ../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp:26:0,                  from ../../3rdparty/libprocess/include/process/check.hpp:19,                  from ../../3rdparty/libprocess/include/process/collect.hpp:7,                  from ../../src/slave/containerizer/isolators/network/port_mapping.cpp:30: ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note: const T& Option<T>::get() const [with T = std::basic_string<char>]    const T& get() const { assert(isSome()); return t; }             ^ ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note:   candidate expects 0 arguments, 1 provided ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note: T& Option<T>::get() [with T = std::basic_string<char>]    T& get() { assert(isSome()); return t; }       ^ ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note:   candidate expects 0 arguments, 1 provided make[2]: *** [slave/containerizer/isolators/network/libmesos_no_3rdparty_la-port_mapping.lo] Error 1 make[2]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src' make[1]: *** [check] Error 2 make[1]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src' make: *** [check-recursive] Error 1 """
"MESOS-3001","Bug","framework",8,"Create a ""demo"" HTTP API client","""We want to create a simple """"demo"""" HTTP API Client (in Java, Python or Go) that can serve as an """"example framework"""" for people who will want to use the new API for their Frameworks.  The scope should be fairly limited (eg, launching a simple Container task?) but sufficient to exercise most of the new API endpoint messages/capabilities.  Scope: TBD  Non-Goals:   - create a """"best-of-breed"""" Framework to deliver any specific functionality; - create an Integration Test for the HTTP API."""
"MESOS-2997","Bug","libprocess",3,"SSL connection failure causes failed CHECK.",""""""
"MESOS-3024","Bug","master|security",8,"HTTP endpoint authN is enabled merely by specifying --credentials","""If I set `--credentials` on the master, framework and slave authentication are allowed, but not required. On the other hand, http authentication is now required for authenticated endpoints (currently only `/shutdown`). That means that I cannot enable framework or slave authentication without also enabling http endpoint authentication. This is undesirable.  Framework and slave authentication have separate flags (`\--authenticate` and `\--authenticate_slaves`) to require authentication for each. It would be great if there was also such a flag for http authentication. Or maybe we get rid of these flags altogether and rely on ACLs to determine which unauthenticated principals are even allowed to authenticate for each endpoint/action."""
"MESOS-3032","Documentation","containerization",3,"Document containerizer launch ","""We currently dont have enough documentation for the containerizer component. This task adds documentation for containerizer launch sequence. The mail goals are: - Have diagrams (state, sequence, class etc) depicting the containerizer launch process. - Make the documentation newbie friendly. - Usable for future design discussions."""
"MESOS-3046","Bug","stout",3,"Stout's UUID re-seeds a new random generator during each call to UUID::random.","""Per [~StephanErb] and [~kevints]'s observations on MESOS-2940, stout's UUID abstraction is re-seeding the random generator during each call to {{UUID::random()}}, which is really expensive.  This is confirmed in the perf graph from MESOS-2940."""
"MESOS-3045","Task","master",3,"Maintenance information is not populated in case of failover","""When a master starts up, or after a master has failed, it must re-populate maintenance information (i.e. from the registry to the local state).  Particularly, {{Master::recover}} in {{src/master/master.cpp}} should be changed to process maintenance information."""
"MESOS-3044","Task","master",8,"Slaves are not deactivated upon reaching a maintenance window","""After a maintenance window is reached, the slave should be deactivated to prevent further tasks from utilizing it.   * For slaves that have completely drained, simply deactivate the slave.  See Master::deactivate(Slave*). * For tasks which have not explicitly declined the InverseOffers (i.e. they've accepted them or do not understand InverseOffers), send kill signals.  See Master::killTask * If a slave has tasks that have declined the InverseOffers, do not deactivate the slave.  Possible test(s): * SlaveDrainedTest ** Start master, slave. ** Set maintenance to now. ** Check that slave gets deactivated * InverseOfferAgnosticTest ** Start master, slave, framework. ** Have a task run on the slave (ignores InverseOffers). ** Set maintenance to now. ** Check that task gets killed. ** Check that slave gets deactivated. * InverseOfferAcceptanceTest ** Start master, slave, framework. ** Run a task on the slave. ** Set maintenance to future. ** Have task accept InverseOffer. ** Check task gets killed, slave gets deactivated. * InverseOfferDeclinedTest ** Start master, slave, framework. ** Run task on slave. ** Set maintenance to future. ** Have task decline maintenance with reason. ** Check task lives, slave still active."""
"MESOS-3043","Task","master",3,"Master does not handle InverseOffers in the Accept call (Event/Call API)","""InverseOffers are similar to Offers in that they are Accepted or Declined based on their OfferID.    Some additional logic may be neccesary in Master::accept (src/master/master.cpp) to gracefully handle the acceptance of InverseOffers. * The InverseOffer needs to be removed from the set of pending InverseOffers. * The InverseOffer should not result any errors/warnings.    Note: accepted InverseOffers do not preclude further InverseOffers from being sent to the framework.  Instead, an accepted InverseOffer merely signifies that the framework is _currently_ fine with the expected downtime."""
"MESOS-3042","Task","allocation|master",8,"Master/Allocator does not send InverseOffers to resources to be maintained","""Offers are currently sent from master/allocator to framework via ResourceOffersMessage's.  InverseOffers, which are roughly equivalent to negative Offers, can be sent in the same package. In src/messages/messages.proto   Sent InverseOffers can be tracked in the master's local state: i.e. In src/master/master.hpp:   One actor (master or allocator) should populate the new InverseOffers field. * In master (src/master/master.cpp) ** Master::offer is where the ResourceOffersMessage and Offer object is constructed. ** The same method could also check for maintenance and send InverseOffers. * In the allocator (src/master/allocator/mesos/hierarchical.hpp) ** HierarchicalAllocatorProcess::allocate is where slave resources are aggregated an sent off to the frameworks. ** InverseOffers (i.e. negative resources) allocation could be calculated in this method. ** A change to Master::offer (i.e. the """"offerCallback"""") may be necessary to account for the negative resources.  Possible test(s): * InverseOfferTest ** Start master, slave, framework. ** Accept resource offer, start task. ** Set maintenance schedule to the future. ** Check that InverseOffer(s) are sent to the framework. ** Decline InverseOffer. ** Check that more InverseOffer(s) are sent. ** Accept InverseOffer. ** Check that more InverseOffer(s) are sent."""
"MESOS-3038","Task","allocation|master",8,"Resource offers do not contain Unavailability, given a maintenance schedule","""Given a schedule, defined elsewhere, any resource offers to affected slaves must include an Unavailability field.  The maintenance schedule for a single slave should be held in [persistent storage|MESOS-2075] and locally by the master.  i.e. In src/master/master.hpp:  The new field should be populated via an API call (see [MESOS-2067]).  The Unavailability field can be added to Master::offer (src/master/master.cpp).   Possible test(s): * PendingUnavailibilityTest ** Start master, slave. ** Check unavailability of offer == none. ** Set unavailability to the future. ** Check offer has unavailability. """
"MESOS-3051","Bug","allocation",8,"performance issues with port ranges comparison","""Testing in an environment with lots of frameworks (>200), where the frameworks permanently decline resources they don't need. The allocator ends up spending a lot of time figuring out whether offers are refused (the code path through {{HierarchicalAllocatorProcess::isFiltered()}}.  In profiling a synthetic benchmark, it turns out that comparing port ranges is very expensive, involving many temporary allocations. 61% of Resources::contains() run time is in operator -= (Resource). 35% of Resources::contains() run time is in Resources::_contains().  The heaviest call chain through {{Resources::_contains}} is:    mesos::coalesce(Value_Ranges) gets done a lot and ends up being really expensive. The heaviest parts of the inverted call chain are:   """
"MESOS-3050","Bug","containerization|docker|test",5,"Failing ROOT_ tests on CentOS 7.1","""Running `sudo make check` on CentOS 7.1 for Mesos 0.23.0-rc3 causes several several failures/errors:   ...  ... """
"MESOS-3069","Task","master|replicated log",8,"Registry operations do not exist for manipulating maintanence schedules","""In order to modify the maintenance schedule in the replicated registry, we will need Operations (src/master/registrar.hpp).  The operations will likely correspond to the HTTP API: * UpdateMaintenanceSchedule: Given a blob representing a maintenance schedule, perform some verification on the blob.  Write the blob to the registry.   * StartMaintenance:  Given a set of machines, verify then transition machines from Draining to Deactivated. * StopMaintenance:  Given a set of machines, verify then transition machines from Deactivated to Normal.  Remove affected machines from the schedule(s)."""
"MESOS-3068","Task","master|replicated log",5,"Registry operations are hardcoded for a single key (Registry object)","""This is primarily a refactoring.  The prototype for modifying the registry is currently:   In order to support Maintenance schedules (possibly Quotas as well), there should be an alternate prototype for Maintenance.  Something like:   The existing RegistrarProcess::update (src/master/registrar.cpp) should be refactored to allow for more than one key.  If necessary, refactor existing operations defined in src/master/master.hpp (AdminSlave, ReadminSlave,  RemoveSlave)."""
"MESOS-3066","Task","master|replicated log",3,"Replicated registry needs a representation of maintenance schedules","""In order to persist maintenance schedules across failovers of the master, the schedule information must be kept in the replicated registry.  This means adding an additional message in the Registry protobuf in src/master/registry.proto.  The status of each individual slave's maintenance will also be persisted in this way.   Note: There can be multiple SlaveID's attached to a single hostname."""
"MESOS-3062","Task","master",2,"Add authorization for dynamic reservation","""Dynamic reservations should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Reserve}} and {{Unreserve}} into the ACL.    When a framework/operator reserves resources, """"reserve"""" ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to reserve the specified resources. If not authorized, the reserve operation is rejected.  When a framework/operator unreserves resources, """"unreserve"""" ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to unreserve the resources reserved by a framework or operator ({{Resource.ReservationInfo.principal}}). If not authorized, the unreserve operation is rejected."""
"MESOS-3060","Bug","fetcher",1,"FTP response code for success not recognized by fetcher.","""The response code for successful HTTP requests is 200, the response code for successful FTP file transfers is 226. The fetcher currently only checks for a response code of 200 even for FTP URIs. This results in failed fetching even though the resource gets downloaded successfully. This has been found by a dedicated external test using an FTP server. """
"MESOS-3077","Task","master|replicated log",5,"Registry recovery does not recover the maintenance object.","""Persisted info is fetched from the registry when a master is elected or after failover.  Currently, this process involves 3 steps: * Fetch the """"registry"""". * Start an operation to add the new master to the fetched registry. * Check the success of the operation and finish recovering. These methods can be found in src/master/registrar.cpp   Since the maintenance schedule is stored in a separate key, the recover process must also fetch a new """"maintenance"""" object.  This object needs to be passed along to the master along with the existing """"registry"""" object.  Possible test(s): * src/tests/registrar_tests.cpp ** Change the """"Recovery"""" test to include checks for the new object."""
"MESOS-3072","Improvement","modules",3,"Unify initialization of modularized components","""h1.Introduction  As it stands right now, default implementations of modularized components are required to have a non parametrized {{create()}} static method. This allows to write tests which can cover default implementations and modules based on these default implementations on a uniform way.  For example, with the interface {{Foo}}:    With a default implementation:    This allows to create typed tests which look as following:    The test will be applied to each of types in the template parameters of {{FooTestTypes}}. This allows to test different implementation of an interface. In our code, it tests default implementations and a module which uses the same default implementation.  The class {{tests::Module<typename T, ModuleID N>}} needs a little explanation, it is a wrapper around {{ModuleManager}} which allows the tests to encode information about the requested module in the type itself instead of passing a string to the factory method. The wrapper around create, the real important method looks as follows:    h1.The Problem  Consider the following implementation of {{Foo}}:    As it can be seen, this implementation cannot be used as a default implementation since its create API does not match the one of {{test::Module<>}}: {{create()}} has a different signature for both types. It is still a common situation to require initialization parameters for objects, however this constraint (keeping both interfaces alike) forces default implementations of modularized components to have default constructors, therefore the tests are forcing the design of the interfaces.  Implementations which are supposed to be used as modules only, i.e. non default implementations are allowed to have constructor parameters, since the actual signature of their factory method is, this factory method's function is to decode the parameters and call the appropriate constructor:    where parameters is just an array of key-value string pairs whose interpretation is left to the specific module. Sadly, this call is wrapped by  {{ModuleManager}} which only allows module parameters to be passed from the command line and does not offer a programmatic way to feed construction parameters to modules.  h1.The Ugly Workaround  With the requirement of a default constructor and parameters devoid {{create()}} factory function, a common pattern (see [Authenticator|https://github.com/apache/mesos/blob/9d4ac11ed757aa5869da440dfe5343a61b07199a/include/mesos/authentication/authenticator.hpp]) has been introduced to feed construction parameters into default implementation, this leads to adding an {{initialize()}} call to the public interface, which will have {{Foo}} become:    {{ParameterFoo}} will thus look as follows:    Look that this {{initialize()}} method now has to be implemented by all descendants of {{Foo}}, even if there's a {{DatabaseFoo}} which takes is return value for {{hello()}} from a DB, it will need to support {{int}} as an initialization parameter.  The problem is more severe the more specific the parameter to {{initialize()}} is. For example, if there is a very complex structure implementing ACLs, all implementations of an authorizer will need to import this structure even if they can completely ignore it.  In the {{Foo}} example if {{ParameterFoo}} were to become the default implementation of {{Foo}}, the tests would look as follows:  """
"MESOS-3098","Task","containerization",13,"Implement WindowsContainerizer and WindowsDockerContainerizer","""The MVP for Windows support is a containerizer that (1) runs on Windows, and (2) runs and passes all the tests that are relevant to the Windows platform (_e.g._, not the tests that involve cgroups). To do this we require at least a `WindowsContainerizer` (to be implemented alongside the `MesosContainerizer`), which provides no meaningful (_e.g._) process namespacing (much like the default unix containerizer). In the long term (hopefully before MesosCon) we want to support also the Windows container API. This will require implementing a separate containerizer, maybe called `WindowsDockerContainerizer`.  Since the Windows container API is actually officially supported through the Docker interface (_i.e._, MSFT actually ported the Docker engine to Windows, and that is the official API), the interfaces (like the fetcher) shouldn't change much. The tests probably will have to change, as we don't have access to any isolation primitives like cgroups for those tests.  Outstanding TODO([~hausdorff]): Flesh out this description when more details are available, regarding: * The container API for Windows (when we know them) * The nuances of Windows vs Linux (when we know them) * etc."""
"MESOS-3097","Story","libprocess|stout",13,"OS-specific code touched by the containerizer tests is not Windows compatible","""In the process of adding the Cmake build system, [~hausdorff] noted and stubbed out all OS-specific code. That sweep (mostly of libprocess and stout) is here: https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52  Instead of having inline {{#if defined(...)}}, the OS-specific code will be separated into directories. The Windows code will be stubbed out."""
"MESOS-3093","Improvement","containerization|docker",3,"Support HTTPS requests in libprocess","""In order to pull images from Docker registries, https calls are needed to securely communicate with the registry hosts. Currently, only http requests are supported through libprocess. Now that SSL sockets are available through libprocess, support for https can be added."""
"MESOS-3087","Documentation","documentation",1,"Typos in oversubscription doc","""* In docs/oversubscription.md: there are three cases where """"revocable"""" is written as """"recovable"""", including the name of a JSON field.    * Also in `docs/oversubscription.md`: the last sentence doesn't make sense    Maybe should say """"To select a custom..."""" or """"To install a custom..."""""""
"MESOS-3122","Improvement","stout",2,"Add configurable UNIMPLEMENTED macro to stout","""During the transition to support for windows, it would be great if we had the ability to use a macro that marks functions as un-implemented. To support being able to find all the unimplemented functions easily at compile time, while also being able to run the tests at the same time, we can add a configuration flag that controls whether this macro aborts or expands to a static assertion."""
"MESOS-3121","Bug","libprocess",2,"Always disable SSLV2","""The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise."""
"MESOS-3112","Improvement","fetcher",8,"Fetcher should perform cache eviction based on cache file usage patterns.","""Currently, the fetcher uses a trivial strategy to select eviction victims: it picks the first cache file it finds in linear iteration. This means that potentially a file that has just been used gets evicted the next moment. This performance loss can be avoided by even the simplest enhancement of the selection procedure.  Proposed approach: determine an effective yet relatively uncomplex and quick algorithm and implement it in `FetcherProcess::Cache::selectVictims(const Bytes& requiredSpace)`. Suggestion: approximate MRU-retention somehow.  Unit-test what actually happens!"""
"MESOS-3110","Task","cmake",3,"Harden the CMake system-dependency-locating routines","""Currently the Mesos project has two flavors of dependency: (1) the dependencies we expect are already on the system (_e.g._, apr, libsvn), and (2) the dependencies that are historically bundled with Mesos (_e.g._, glog).  Dependency type (1) requires solid modules that will locate them on any system: Linux, BSD, or Windows. This would come for free if we were using CMake 3.0, but we're using CMake 2.8 so that Ubuntu users can install it out of the box, instead of upgrading CMake first.  This is additionally useful for dependency type (2), where we will expect to have to use these routines when we support both the rebundled dependencies in the `3rdparty/` folder, and system installations of those dependencies."""
"MESOS-3109","Task","cmake",3,"Expand CMake build system to support building the containerizer and associated components","""In other tasks in epic MESOS-898, we implement a CMake-based build system that allows us to build process library, the process tests, and the stout tests.  For the CMake build system MVP, it's important that we expand this to build the containerizer, associated modules, and all related tests."""
"MESOS-3107","Task","cmake",3,"Define CMake style guide","""The short story is that it is important to be principled about how the CMake build system is maintained, because there CMake language makes it difficult to statically verify that a configuration is correct. It is not unique in this regard, but (make is arguably even worse) but it is something that's important to make sure we get right.  The longer story is, CMake's language is dynamically scoped and often has somewhat odd defaults for variable values (_e.g._, IIRC, target names passed to ExternalProject_Add default to """"PREFIX"""" instead of erroring out). This means that it is rare to get a configuration-time error (_i.e._, CMake usually doesn't say something like """"hey this variable isn't defined""""), and in large projects, this can make it very difficult to know where definitions come from, or whether it's important that one config routine runs before another. Dynamic scoping also makes it particularly easy to write spaghetti code, which is clearly undesirable for something as important as a build system.  Thus, it is particularly important that we lay down our expectations for how the CMake system is to be structured. This might include:  * Function naming (_e.g._, making it easy to tell whether a function was defined by us, and where it was defined; so we might say that we want our functions to have an underscore to start, and start with the package the come from, like libprocess, so that we know where to look for the definition.) * What assertions we want to check variable values against, so that we can replace subtle errors (_e.g._, a library is accidentally named something silly like """"PREFIX.0.0.1"""") with an obvious ones (_e.g._, """"You have failed to define your target name, so CMake has defaulted to 'PREFIX'; please check your configuration routines"""") * Decisions of what goes where. (_e.g._, the most complex parts of the CMake MVPs is in the configuration routines, like `MesosConfigure.cmake`; to curb this, we should have strict rules about what goes in that file vs other files, and how we know what is to be run before what. Part of this should probably be prominent comments explaining the structure of the project, so that people aren't confused!) * And so on."""
"MESOS-3106","Task","cmake",5,"Extend CMake build system to support building against third-party libraries from either the system or the local Mesos rebundling","""Currently Mesos has third-party dependencies of two types: (1) those that are expected to be on the system (such as APR, libsvn, _etc_.), and (2) those that have been historically bundled as tarballs inside the Mesos repository, and are not expected to be on the system when Mesos is installed (these are located in the `3rdparty/` directory, and includes things like boost and glog).  For type (2), the MVP of the CMake-based build system will always pull down a fresh tarball from an external source, instead of using the bundled tarballs in the `3rdparty/` folder.  However, many CI systems do not have Internet access, so in the long term, we need to provide many options for getting these dependencies."""
"MESOS-3103","Task","libprocess",5,"Separate OS-specific code in the libprocess library","""This issue tracks changes for all files under {{3rdparty/libprocess/include/}} and {{3rdparty/libprocess/src}}.  The changes will be based on this commit: https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c"""
"MESOS-3102","Task","stout",5,"Separate OS-specific code in the stout library","""This issue tracks changes for all files under {{3rdparty/libprocess/3rdparty/stout/}}  The changes will be based on this commit: https://github.com/hausdorff/mesos/commit/b862f66c6ff58c115a009513621e5128cb734d52#diff-a6d038bad64b154996452bec020cfa7c"""
"MESOS-3101","Task","stout",3,"Standardize separation of Windows/Linux-specific OS code","""There are 50+ files that must be touched to separate OS-specific code.  First, we will standardize the changes by using stout/abort.hpp as an example. The review/discussion can be found here: https://reviews.apache.org/r/36625/"""
"MESOS-3129","Task","containerization",2,"Move all MesosContainerizer related files under src/slave/containerizer/mesos","""Currently, some MesosContainerizer specific files are not in the correct location. For example:   They should be put under src/slave/containerizer/mesos/"""
"MESOS-3127","Improvement","documentation",1,"Improve task reconciliation documentation.","""Include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary."""
"MESOS-3139","Task","cmake",13,"Incorporate CMake into standard documentation","""Right now it's anyone's guess how to build with CMake. If we want people to use it, we should put up documentation. The central challenge is that the CMake instructions will be slightly different for different platforms.  For example, on Linux, the gist of the build is basically the same as autotools; you pull down the system dependencies (like APR, _etc_.), and then:  ``` ./bootstrap mkdir build-cmake && cd build-cmake cmake .. make ```  But, on Windows, it will be somewhat more complicated. There is no bootstrap step, for example, because Windows doesn't have bash natively. And even when we put that in, you'll still have to build the glog stuff out-of-band because CMake has no way of booting up Visual Studio and calling """"build.""""  So practically, we need to figure out:  * What our build story is for different platforms * Write specific instructions for our """"core"""" target platforms."""
"MESOS-3135","Story","master",2,"Publish MasterInfo to ZK using JSON","""Following from MESOS-2340, which now allows Master to correctly decode JSON information ({{MasterInfo}}) published to Zookeeper, we can now enable the Master Leader Contender to serialize it too in JSON."""
"MESOS-3134","Bug","cmake",5,"Port bootstrap to CMake","""Bootstrap does a lot of significant things, like setting up the git commit hooks. We will want something like bootstrap to run also on systems that don't have bash -- ideally this should just run in CMake itself."""
"MESOS-3132","Task","agent|master",5,"Allow slave to forward messages through the master for HTTP schedulers.","""The master currently has no install handler for {{ExecutorToFramework}} messages and the slave directly sends these messages to the scheduler driver, bypassing the master entirely.  We need to preserve this behavior for the driver, but HTTP schedulers will not have a libprocess 'pid'. We'll have to ensure that the {{RunTaskMessage}} and {{UpdateFrameworkMessage}} have an optional pid. For now the master will continue to set the pid, but 0.24.0 slaves will know to send messages through the master when the 'pid' is not available."""
"MESOS-3143","Bug","agent|libprocess|master",2,"Disable endpoints rule fails to recognize HTTP path delegates","""In mesos, one can use the flag {{--firewall_rules}} to disable endpoints. Disabled endpoints will return a _403 Forbidden_ response whenever someone tries to access endpoints.  Libprocess support adding one default delegate for endpoints, which is the default process id which handles endpoints if no process id was given. For example, the default id of the master libprocess process is {{master}} which is also set as the delegate for the master system process, so a request to the endpoint {{http://master-address:5050/state.json}} will effectively be resolved by {{http://master-address:5050/master/state.json}}. But if one disables  {{/state.json}} because of how delegates work, it can still access {{/master/state.json}}.  The only workaround is to disabled both enpoints."""
"MESOS-3142","Story","stout",2,"As a Developer I want a better way to run shell commands","""When reviewing the code in [r/36425|https://reviews.apache.org/r/36425/] [~benjaminhindman] noticed that there is a better abstraction that is possible to introduce for {{os::shell()}} that will simplify the caller's life.  Instead of having to handle all possible outcomes, we propose to refactor {{os::shell()}} as follows:    where the returned string is {{stdout}} and, should the program be signaled, or exit with a non-zero exit code, we will simply return a {{Failure}} with an error message that will encapsulate both the returned/signaled state, and, possibly {{stderr}}.  And some test driven development:   Alternatively, the caller can ask to have {{stderr}} conflated with {{stdout}}:   However, {{stderr}} will be ignored by default:   An analysis of existing usage shows that in almost all cases, the caller only cares {{if not error}}; in fact, the actual exit code is read only once, and even then, in a test case.  We believe this will simplify the API to the caller, and will significantly reduce the length and complexity at the calling sites (<6 LOC against the current 20+)."""
"MESOS-3158","Improvement","libprocess",3,"Libprocess Process: Join runqueue workers during finalization","""The lack of synchronization between ProcessManager destruction and the thread pool threads running the queued processes means that the shared state that is part of the ProcessManager gets destroyed prematurely. Synchronizing the ProcessManager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction."""
"MESOS-3154","Story","agent",1,"Enable Mesos Agent Node to use arbitrary script / module to figure out IP, HOSTNAME","""Following from MESOS-2902 we want to enable the same functionality in the Mesos Agents too.  This is probably best done once we implement the new {{os::shell}} semantics, as described in MESOS-3142."""
"MESOS-3168","Bug","test",2,"MesosZooKeeperTest fixture can have side effects across tests","""MesosZooKeeperTest fixture doesn't restart the ZooKeeper server for each test. This means if a test shuts down the ZooKeeper server, the next test (using the same fixture) might fail.   For an example see https://reviews.apache.org/r/36807/"""
"MESOS-3166","Bug","containerization",3,"Design doc for docker image registry client","""Create design document for the docker registry Authenticator component so that we have a baseline for the implementation. """
"MESOS-3165","Task","master|replicated log",5,"Persist and recover quota to/from Registry","""To persist quotas across failovers, the Master should save them in the registry. To support this, we shall: * Introduce a Quota state variable in registry.proto; * Extend the Operation interface so that it supports a Quota accumulator (see src/master/registrar.hpp); * Introduce AddQuota / RemoveQuota operations; * Recover quotas from the registry on failover to the Masters internal::master::Role struct; * Extend RegistrarTest with quota-specific tests.  NOTE: Registry variable can be rather big for production clusters (see MESOS-2075). While it should be fine for MVP to add quota information to registry, we should consider storing Quota separately, as this does not need to be in sync with slaves update. However, currently adding more variable is not supported by the registrar.  While the Agents are reregistering (note they may fail to do so), the information about what part of the quota is allocated is only partially available to the Master. In other words, the state of the quota allocation is reconstructed as Agents reregister. During this period, some roles may be under quota from the perspective of the newly elected Master.  The same problem exists on the allocator side: it may think the cluster is under quota and may eagerly try to satisfy quotas before enough Agents reregister, which may result in resources being allocated to frameworks beyond their quota. To address this issue and also to avoid panicking and generating under quota alerts, the Master should give a certain amount of time for the majority (e.g. 80%) of the Agents to reregister before reporting any quota status and notifying the allocator about granted quotas."""
"MESOS-3164","Task","master",3,"Introduce QuotaInfo message","""A {{QuotaInfo}} protobuf message is internal representation for quota related information (e.g. for persisting quota). The protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals. It may also be used to pass quota information to allocators."""
"MESOS-3162","Task","libprocess",3,"Provide a means to check http connection equality for streaming connections.","""If one uses an http::Pipe::Writer to stream a response, one cannot compare the writer with another to see if the connection has changed.  This is useful for example, in the master's http api when there is asynchronous disconnection logic. When we handle the disconnection, it's possible for the scheduler to have re-subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action."""
"MESOS-3173","Improvement","stout",1,"Mark Path::basename, Path::dirname as const functions.","""The functions Path::basename and Path::dirname in stout/path.hpp are not marked const, although they could. Marking them const would remove some ambiguities in the usage of these functions."""
"MESOS-3169","Bug","master",2,"FrameworkInfo should only be updated if the re-registration is valid","""See Ben Mahler's comment in https://reviews.apache.org/r/32961/ FrameworkInfo should not be updated if the re-registration is invalid. This can happen in a few cases under the branching logic, so this requires some refactoring. Notice that a  can be generated  both inside  as well as from inside """
"MESOS-3174","Bug","fetcher",1,"Fetcher logs erroneous message when successfully extracting an archive","""When fetching an asset while not using the cache, the fetcher may erroneously report this: """"Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: """".  This message appears in the stderr log in the sandbox no matter whether extraction succeeded or not. It should be absent after successful extraction. """
"MESOS-3185","Bug","agent",3,"Refactor Subprocess logic in linux/perf.cpp to use common subroutine","""MESOS-2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions.  In order to achieve this, it requires to execute the """"perf --version"""" command.   We should decompose the existing Subcommand processing in perf so that we can share the implementation between the multiple uses of perf."""
"MESOS-3183","Documentation","documentation",3,"Documentation images do not load","""Any images which are referenced from the generated docs ({{docs/*.md}}) do not show up on the website.  For example: * [Architecture|http://mesos.apache.org/documentation/latest/architecture/] * [External Containerizer|http://mesos.apache.org/documentation/latest/external-containerizer/] * [Fetcher Cache Internals|http://mesos.apache.org/documentation/latest/fetcher-cache-internals/] * [Maintenance|http://mesos.apache.org/documentation/latest/maintenance/]   * [Oversubscription|http://mesos.apache.org/documentation/latest/oversubscription/] """
"MESOS-3189","Bug","libprocess",2,"TimeTest.Now fails with --enable-libevent","""[ RUN      ] TimeTest.Now ../../../3rdparty/libprocess/src/tests/time_tests.cpp:50: Failure Expected: (Microseconds(10)) < (Clock::now() - t1), actual: 8-byte object <10-27 00-00 00-00 00-00> vs 0ns [  FAILED  ] TimeTest.Now (0 ms)"""
"MESOS-3203","Bug","test",1,"MasterAuthorizationTest.DuplicateRegistration test is flaky","""[ RUN      ] MasterAuthorizationTest.DuplicateRegistration Using temporary directory '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7' I0804 22:16:01.578500 26185 leveldb.cpp:176] Opened db in 2.188338ms I0804 22:16:01.579172 26185 leveldb.cpp:183] Compacted db in 645075ns I0804 22:16:01.579211 26185 leveldb.cpp:198] Created db iterator in 15766ns I0804 22:16:01.579227 26185 leveldb.cpp:204] Seeked to beginning of db in 1658ns I0804 22:16:01.579238 26185 leveldb.cpp:273] Iterated through 0 keys in the db in 313ns I0804 22:16:01.579282 26185 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned I0804 22:16:01.579787 26212 recover.cpp:449] Starting replica recovery I0804 22:16:01.580075 26212 recover.cpp:475] Replica is in EMPTY status I0804 22:16:01.581014 26205 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request I0804 22:16:01.581357 26211 recover.cpp:195] Received a recover response from a replica in EMPTY status I0804 22:16:01.581761 26207 recover.cpp:566] Updating replica status to STARTING I0804 22:16:01.582334 26218 master.cpp:377] Master 20150804-221601-2550141356-59302-26185 (d6d349cd895b) started on 172.17.0.152:59302 I0804 22:16:01.582355 26218 master.cpp:379] Flags at startup: --acls="""""""" --allocation_interval=""""1secs"""" --allocator=""""HierarchicalDRF"""" --authenticate=""""true"""" --authenticate_slaves=""""true"""" --authenticators=""""crammd5"""" --credentials=""""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials"""" --framework_sorter=""""drf"""" --help=""""false"""" --initialize_driver_logging=""""true"""" --log_auto_initialize=""""true"""" --logbufsecs=""""0"""" --logging_level=""""INFO"""" --max_slave_ping_timeouts=""""5"""" --quiet=""""false"""" --recovery_slave_removal_limit=""""100%"""" --registry=""""replicated_log"""" --registry_fetch_timeout=""""1mins"""" --registry_store_timeout=""""25secs"""" --registry_strict=""""true"""" --root_submissions=""""true"""" --slave_ping_timeout=""""15secs"""" --slave_reregister_timeout=""""10mins"""" --user_sorter=""""drf"""" --version=""""false"""" --webui_dir=""""/mesos/mesos-0.24.0/_inst/share/mesos/webui"""" --work_dir=""""/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/master"""" --zk_session_timeout=""""10secs"""" I0804 22:16:01.582711 26218 master.cpp:424] Master only allowing authenticated frameworks to register I0804 22:16:01.582722 26218 master.cpp:429] Master only allowing authenticated slaves to register I0804 22:16:01.582728 26218 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_DuplicateRegistration_NKT3f7/credentials' I0804 22:16:01.582929 26204 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421543ns I0804 22:16:01.582950 26204 replica.cpp:323] Persisted replica status to STARTING I0804 22:16:01.583032 26218 master.cpp:468] Using default 'crammd5' authenticator I0804 22:16:01.583132 26211 recover.cpp:475] Replica is in STARTING status I0804 22:16:01.583154 26218 master.cpp:505] Authorization enabled I0804 22:16:01.583356 26214 whitelist_watcher.cpp:79] No whitelist given I0804 22:16:01.583411 26217 hierarchical.hpp:346] Initialized hierarchical allocator process I0804 22:16:01.583976 26213 replica.cpp:641] Replica in STARTING status received a broadcasted recover request I0804 22:16:01.584187 26209 recover.cpp:195] Received a recover response from a replica in STARTING status I0804 22:16:01.584581 26213 master.cpp:1495] The newly elected leader is master@172.17.0.152:59302 with id 20150804-221601-2550141356-59302-26185 I0804 22:16:01.584609 26213 master.cpp:1508] Elected as the leading master! I0804 22:16:01.584627 26213 master.cpp:1278] Recovering from registrar I0804 22:16:01.584656 26204 recover.cpp:566] Updating replica status to VOTING I0804 22:16:01.584770 26212 registrar.cpp:313] Recovering registrar I0804 22:16:01.585261 26218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 370526ns I0804 22:16:01.585285 26218 replica.cpp:323] Persisted replica status to VOTING I0804 22:16:01.585412 26216 recover.cpp:580] Successfully joined the Paxos group I0804 22:16:01.585667 26216 recover.cpp:464] Recover process terminated I0804 22:16:01.586047 26213 log.cpp:661] Attempting to start the writer I0804 22:16:01.587164 26211 replica.cpp:477] Replica received implicit promise request with proposal 1 I0804 22:16:01.587549 26211 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 358261ns I0804 22:16:01.587568 26211 replica.cpp:345] Persisted promised to 1 I0804 22:16:01.588173 26209 coordinator.cpp:230] Coordinator attemping to fill missing position I0804 22:16:01.589316 26208 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2 I0804 22:16:01.589700 26208 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 351778ns I0804 22:16:01.589721 26208 replica.cpp:679] Persisted action at 0 I0804 22:16:01.590698 26213 replica.cpp:511] Replica received write request for position 0 I0804 22:16:01.590754 26213 leveldb.cpp:438] Reading position from leveldb took 31557ns I0804 22:16:01.591147 26213 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 321842ns I0804 22:16:01.591167 26213 replica.cpp:679] Persisted action at 0 I0804 22:16:01.591790 26217 replica.cpp:658] Replica received learned notice for position 0 I0804 22:16:01.592133 26217 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 315281ns I0804 22:16:01.592155 26217 replica.cpp:679] Persisted action at 0 I0804 22:16:01.592180 26217 replica.cpp:664] Replica learned NOP action at position 0 I0804 22:16:01.592686 26211 log.cpp:677] Writer started with ending position 0 I0804 22:16:01.593729 26205 leveldb.cpp:438] Reading position from leveldb took 26394ns I0804 22:16:01.596165 26209 registrar.cpp:346] Successfully fetched the registry (0B) in 11.343104ms I0804 22:16:01.596281 26209 registrar.cpp:445] Applied 1 operations in 26242ns; attempting to update the 'registry' I0804 22:16:01.598415 26212 log.cpp:685] Attempting to append 178 bytes to the log I0804 22:16:01.598563 26215 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1 I0804 22:16:01.599324 26215 replica.cpp:511] Replica received write request for position 1 I0804 22:16:01.599778 26215 leveldb.cpp:343] Persisting action (197 bytes) to leveldb took 420523ns I0804 22:16:01.599800 26215 replica.cpp:679] Persisted action at 1 I0804 22:16:01.600349 26204 replica.cpp:658] Replica received learned notice for position 1 I0804 22:16:01.600684 26204 leveldb.cpp:343] Persisting action (199 bytes) to leveldb took 310315ns I0804 22:16:01.600706 26204 replica.cpp:679] Persisted action at 1 I0804 22:16:01.600723 26204 replica.cpp:664] Replica learned APPEND action at position 1 I0804 22:16:01.601632 26213 registrar.cpp:490] Successfully updated the 'registry' in 5.287936ms I0804 22:16:01.601747 26213 registrar.cpp:376] Successfully recovered registrar I0804 22:16:01.601826 26215 log.cpp:704] Attempting to truncate the log to 1 I0804 22:16:01.601948 26210 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2 I0804 22:16:01.602145 26208 master.cpp:1305] Recovered 0 slaves from the Registry (139B) ; allowing 10mins for slaves to re-register I0804 22:16:01.602859 26219 replica.cpp:511] Replica received write request for position 2 I0804 22:16:01.603181 26219 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 284713ns I0804 22:16:01.603209 26219 replica.cpp:679] Persisted action at 2 I0804 22:16:01.603984 26211 replica.cpp:658] Replica received learned notice for position 2 I0804 22:16:01.604313 26211 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 302445ns I0804 22:16:01.604365 26211 leveldb.cpp:401] Deleting ~1 keys from leveldb took 29354ns I0804 22:16:01.604387 26211 replica.cpp:679] Persisted action at 2 I0804 22:16:01.604408 26211 replica.cpp:664] Replica learned TRUNCATE action at position 2 I0804 22:16:01.616402 26185 sched.cpp:164] Version: 0.24.0 I0804 22:16:01.616902 26209 sched.cpp:262] New master detected at master@172.17.0.152:59302 I0804 22:16:01.617000 26209 sched.cpp:318] Authenticating with master master@172.17.0.152:59302 I0804 22:16:01.617019 26209 sched.cpp:325] Using default CRAM-MD5 authenticatee I0804 22:16:01.617324 26212 authenticatee.cpp:115] Creating new client SASL connection I0804 22:16:01.617550 26209 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.617641 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(259)@172.17.0.152:59302 I0804 22:16:01.617858 26208 authenticator.cpp:92] Creating new server SASL connection I0804 22:16:01.618140 26216 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0804 22:16:01.618191 26216 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0804 22:16:01.618324 26213 authenticator.cpp:197] Received SASL authentication start I0804 22:16:01.618413 26213 authenticator.cpp:319] Authentication requires more steps I0804 22:16:01.618557 26216 authenticatee.cpp:252] Received SASL authentication step I0804 22:16:01.618664 26216 authenticator.cpp:225] Received SASL authentication step I0804 22:16:01.618703 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0804 22:16:01.618719 26216 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0804 22:16:01.618778 26216 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0804 22:16:01.618820 26216 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0804 22:16:01.618834 26216 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.618839 26216 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.618857 26216 authenticator.cpp:311] Authentication success I0804 22:16:01.618954 26219 authenticatee.cpp:292] Authentication success I0804 22:16:01.619035 26204 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.619083 26219 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(259)@172.17.0.152:59302 I0804 22:16:01.619309 26208 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302 I0804 22:16:01.619335 26208 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302 I0804 22:16:01.619494 26208 sched.cpp:746] Will retry registration in 439203ns if necessary I0804 22:16:01.619627 26217 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.619695 26217 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*' I0804 22:16:01.620848 26217 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.152:59302 I0804 22:16:01.620929 26217 sched.cpp:746] Will retry registration in 2.099193326secs if necessary I0804 22:16:01.621036 26210 master.cpp:1812] Received SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.621083 26210 master.cpp:1534] Authorizing framework principal 'test-principal' to receive offers for role '*' I0804 22:16:01.621727 26217 master.cpp:1876] Subscribing framework default with checkpointing disabled and capabilities [  ] I0804 22:16:01.621981 26208 sched.cpp:262] New master detected at master@172.17.0.152:59302 I0804 22:16:01.622131 26208 sched.cpp:318] Authenticating with master master@172.17.0.152:59302 I0804 22:16:01.622153 26208 sched.cpp:325] Using default CRAM-MD5 authenticatee I0804 22:16:01.622323 26212 authenticatee.cpp:115] Creating new client SASL connection I0804 22:16:01.622324 26210 hierarchical.hpp:391] Added framework 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:01.622369 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:01.622386 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 28592ns I0804 22:16:01.622511 26210 sched.cpp:640] Framework registered with 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:01.622586 26210 sched.cpp:654] Scheduler::registered took 48005ns I0804 22:16:01.622592 26208 master.cpp:4405] Authenticating scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.622673 26212 authenticator.cpp:406] Starting authentication session for crammd5_authenticatee(260)@172.17.0.152:59302 I0804 22:16:01.622923 26205 authenticator.cpp:92] Creating new server SASL connection I0804 22:16:01.623112 26204 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5 I0804 22:16:01.623133 26216 master.cpp:1870] Dropping SUBSCRIBE call for framework 'default' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302: Re-authentication in progress I0804 22:16:01.623144 26204 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5' I0804 22:16:01.623258 26215 authenticator.cpp:197] Received SASL authentication start I0804 22:16:01.623313 26215 authenticator.cpp:319] Authentication requires more steps I0804 22:16:01.623394 26215 authenticatee.cpp:252] Received SASL authentication step I0804 22:16:01.623512 26212 authenticator.cpp:225] Received SASL authentication step I0804 22:16:01.623546 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false  I0804 22:16:01.623564 26212 auxprop.cpp:174] Looking up auxiliary property '*userPassword' I0804 22:16:01.623603 26212 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5' I0804 22:16:01.623622 26212 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: 'd6d349cd895b' server FQDN: 'd6d349cd895b' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true  I0804 22:16:01.623631 26212 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.623636 26212 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true I0804 22:16:01.623649 26212 authenticator.cpp:311] Authentication success I0804 22:16:01.623777 26212 authenticatee.cpp:292] Authentication success I0804 22:16:01.623846 26212 master.cpp:4435] Successfully authenticated principal 'test-principal' at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:01.623913 26212 authenticator.cpp:424] Authentication session cleanup for crammd5_authenticatee(260)@172.17.0.152:59302 I0804 22:16:01.624130 26212 sched.cpp:407] Successfully authenticated with master master@172.17.0.152:59302 I0804 22:16:02.583772 26218 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:02.583818 26218 hierarchical.hpp:908] Performed allocation for 0 slaves in 80538ns I0804 22:16:03.585110 26211 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:03.585156 26211 hierarchical.hpp:908] Performed allocation for 0 slaves in 69272ns I0804 22:16:04.586539 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:04.586586 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 79232ns I0804 22:16:05.587239 26209 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:05.587293 26209 hierarchical.hpp:908] Performed allocation for 0 slaves in 85128ns I0804 22:16:06.587935 26212 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:06.587985 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 78141ns I0804 22:16:07.588817 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:07.588865 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 81433ns I0804 22:16:08.589857 26214 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:08.589906 26214 hierarchical.hpp:908] Performed allocation for 0 slaves in 71929ns I0804 22:16:09.591085 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:09.591133 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 78223ns I0804 22:16:10.591737 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:10.591785 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71894ns I0804 22:16:11.593166 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:11.593221 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 89782ns I0804 22:16:12.593647 26212 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:12.593689 26212 hierarchical.hpp:908] Performed allocation for 0 slaves in 69426ns I0804 22:16:13.594154 26210 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:13.594202 26210 hierarchical.hpp:908] Performed allocation for 0 slaves in 70581ns I0804 22:16:14.594712 26207 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:14.594758 26207 hierarchical.hpp:908] Performed allocation for 0 slaves in 71201ns I0804 22:16:15.595412 26219 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:15.595464 26219 hierarchical.hpp:908] Performed allocation for 0 slaves in 85183ns I0804 22:16:16.596201 26217 hierarchical.hpp:1008] No resources available to allocate! I0804 22:16:16.596247 26217 hierarchical.hpp:908] Performed allocation for 0 slaves in 95132ns ../../src/tests/master_authorization_tests.cpp:794: Failure Failed to wait 15secs for frameworkRegisteredMessage I0804 22:16:16.624354 26212 master.cpp:966] Framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 disconnected I0804 22:16:16.624398 26212 master.cpp:2092] Disconnecting framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.624445 26212 master.cpp:2116] Deactivating framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.624686 26212 master.cpp:988] Giving framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 0ns to failover I0804 22:16:16.625641 26219 hierarchical.hpp:474] Deactivated framework 20150804-221601-2550141356-59302-26185-0000 I0804 22:16:16.626688 26218 master.cpp:4180] Framework failover timeout, removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.626734 26218 master.cpp:4759] Removing framework 20150804-221601-2550141356-59302-26185-0000 (default) at scheduler-ac5e7b68-e2d2-441c-a5f5-60c1ff8cf00c@172.17.0.152:59302 I0804 22:16:16.627074 26218 master.cpp:858] Master terminating I0804 22:16:16.627218 26215 hierarchical.hpp:428] Removed framework 20150804-221601-2550141356-59302-26185-0000 ../../3rdparty/libprocess/include/process/gmock.hpp:365: Failure Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...     Expected args: message matcher (8-byte object <98-98 02-AC 54-2B 00-00>, 1-byte object <97>, 1-byte object <D2>)          Expected: to be called once            Actual: never called - unsatisfied and active [  FAILED  ] MasterAuthorizationTest.DuplicateRegistration (15056 ms) """
"MESOS-3201","Bug","libprocess",3,"Libev handle_async can deadlock with run_in_event_loop","""Due to the arbitrary nature of the functions that are executed in handle_async, invoking them under the (A) {{watchers_mutex}} can lead to deadlocks if (B) is acquired before calling {{run_in_event_loop}} and (B) is also acquired within the arbitrary function.   This was introduced in https://github.com/apache/mesos/commit/849fc4d361e40062073324153ba97e98e294fdf2"""
"MESOS-3200","Task","libprocess|stout",1,"Remove unused 'fatal' and 'fatalerror' macros","""There exist {{fatal}} and {{fatalerror}} macros in both {{libprocess}} and {{stout}}. None of them are currently used as we favor {{glog}}'s {{LOG(FATAL)}}, and therefore should be removed."""
"MESOS-3197","Bug","test",2,"MemIsolatorTest/{0,1}.MemUsage fails on OS X","""Looks like this is due to {{mlockall}} being unimplemented on OS X.  """
"MESOS-3213","Task","containerization|docker",2,"Design doc for docker registry token manager","""Create design document for describing the component and interaction between Docker Registry Client and remote Docker Registry for token based authorization."""
"MESOS-3207","Bug","project website|webui",1,"C++ style guide is not rendered correctly (code section syntax disregarded)","""Some paragraphs at the bottom of docs/mesos-c++-style-guide.md containing code sections are not rendered correctly by the web site generator. It looks fine in a github gist and apparently the syntax used is correct.   """
"MESOS-3222","Task","containerization|docker",5,"Implement docker registry client","""Implement the following functionality:  - fetch manifest from remote registry based on authorization method dictated by the registry. - fetch image layers from remote registry  based on authorization method dictated by the registry.. """
"MESOS-3252","Bug","containerization",2,"Ignore no statistics condition for containers with no qdisc","""In PortMappingStatistics::execute, we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container.    This can occur because of an error reading the qdisc (statistics function return an error) or because the qdisc does not exist (function returns none).    We should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting.  We do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created."""
"MESOS-3251","Bug","libprocess",1,"http::get API evaluates ""host"" wrongly","""Currently libprocess http API sets the """"Host"""" header field from the peer socket address (IP:port). The problem is that socket address might not be right HTTP server and might be just a proxy. """
"MESOS-3254","Bug","test",2,"Cgroup CHECK fails test harness","""CHECK in clean up of ContainerizerTest causes test harness to abort rather than fail or skip only perf related tests.  [ RUN      ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch [       OK ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch (628 ms) [----------] 24 tests from SlaveRecoveryTest/0 (38986 ms total)  [----------] 4 tests from MesosContainerizerSlaveRecoveryTest [ RUN      ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics ../../src/tests/mesos.cpp:720: Failure cgroups::mount(hierarchy, subsystem): 'perf_event' is already attached to another hierarchy ------------------------------------------------------------- We cannot run any cgroups tests that require a hierarchy with subsystem 'perf_event' because we failed to find an existing hierarchy or create a new one (tried '/tmp/mesos_test_cgroup/perf_event'). You can either remove all existing hierarchies, or disable this test case (i.e., --gtest_filter=-MesosContainerizerSlaveRecoveryTest.*). ------------------------------------------------------------- F0811 17:23:43.874696 12955 mesos.cpp:774] CHECK_SOME(cgroups): '/tmp/mesos_test_cgroup/perf_event' is not a valid hierarchy  *** Check failure stack trace: ***     @     0x7fb2fb4835fd  google::LogMessage::Fail()     @     0x7fb2fb48543d  google::LogMessage::SendToLog()     @     0x7fb2fb4831ec  google::LogMessage::Flush()     @     0x7fb2fb485d39  google::LogMessageFatal::~LogMessageFatal()     @           0x4e3f98  _CheckFatal::~_CheckFatal()     @           0x82f25a  mesos::internal::tests::ContainerizerTest<>::TearDown()     @           0xc030e3  testing::internal::HandleExceptionsInMethodIfSupported<>()     @           0xbf9050  testing::Test::Run()     @           0xbf912e  testing::TestInfo::Run()     @           0xbf9235  testing::TestCase::Run()     @           0xbf94e8  testing::internal::UnitTestImpl::RunAllTests()     @           0xbf97a4  testing::UnitTest::Run()     @           0x4a9df3  main     @     0x7fb2f9371ec5  (unknown)     @           0x4b63ee  (unknown) Build step 'Execute shell' marked build as failure"""
"MESOS-3265","Task","agent|master",8,"Starting maintenance needs to deactivate agents and kill tasks.","""After using the {{/maintenance/start}} endpoint to begin maintenance on a machine, agents running on said machine should: * Be deactivated such that no offers are sent from that agent.  (Investigate if {{Master::deactivate(Slave*)}} can be used or modified for this purpose.) * Kill all tasks still running on the agent (See MESOS-1475). * Prevent other agents on that machine from registering or sending out offers.  This will likely involve some modifications to {{Master::register}} and {{Master::reregister}}.  """
"MESOS-3266","Task","agent|master",5,"Stopping/Completing maintenance needs to reactivate agents.","""After using the {{/maintenance/stop}} endpoint to end maintenance on a machine, any deactivated agents must be reactivated and allowed to register with the master."""
"MESOS-3273","Bug","HTTP API",5,"EventCall Test Framework is flaky","""Observed this on ASF CI. h/t [~haosdent@gmail.com]  Looks like the HTTP scheduler never sent a SUBSCRIBE request to the master.  """
"MESOS-3284","Bug","stout",3,"JSON representation of Protobuf should use base64 encoding for 'bytes' fields.","""Currently we encode 'bytes' fields as UTF-8 strings, which is lossy for binary data due to invalid byte sequences! In order to encode binary data in a lossless fashion, we can encode 'bytes' fields in base64.  Note that this is also how proto3 does its encoding (see [here|https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json]), so this would make migration easier as well."""
"MESOS-3280","Bug","master|replicated log",8,"Master fails to access replicated log after network partition","""In a 5 node cluster with 3 masters and 2 slaves, and ZK on each node, when a network partition is forced, all the masters apparently lose access to their replicated log. The leading master halts. Unknown reasons, but presumably related to replicated log access. The others fail to recover from the replicated log. Unknown reasons. This could have to do with ZK setup, but it might also be a Mesos bug.   This was observed in a Chronos test drive scenario described in detail here: https://github.com/mesos/chronos/issues/511  With setup instructions here: https://github.com/mesos/chronos/issues/508  """
"MESOS-3289","Task","docker",5,"Add DockerRegistry unit tests","""Add unit tests suite for docker registry implementation.  This could include:  - Creating mock docker registry server - Using openssl library for digest functions."""
"MESOS-3288","Task","docker",5,"Implement docker registry client","""Implement the docker registry client as per design document:  https://docs.google.com/document/d/1kE-HXPQl4lQgamPIiaD4Ytdr-N4HeQc4fnE93WHR4X4/edit"""
"MESOS-3297","Bug","containerization|docker|test",5,"Failing ROOT_ tests on CentOS 7.1 - MesosContainerizerLaunchTest","""h2. MesosContainerizerLaunchTest  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  """
"MESOS-3296","Bug","containerization|docker|test",5,"Failing ROOT_ tests on CentOS 7.1 - LinuxFilesystemIsolatorTest","""h2. LinuxFilesystemIsolatorTest  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  """
"MESOS-3295","Bug","containerization|docker|test",5,"Failing ROOT_ tests on CentOS 7.1 - ContainerizerTest","""h2. ContainerizerTest.ROOT_CGROUPS_BalloonFramework  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  """
"MESOS-3294","Bug","containerization|docker|test",5,"Failing ROOT_ tests on CentOS 7.1 - UserCgroupIsolatorTest","""h2. UserCgroupIsolatorTest  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  """
"MESOS-3293","Bug","containerization|docker|test",5,"Failing ROOT_ tests on CentOS 7.1 - LimitedCpuIsolatorTest","""h2. LimitedCpuIsolatorTest.ROOT_CGROUPS_Pids_and_Tids  This is one of several ROOT failing tests: we want to track them *individually* and for each of them decide whether to:  * fix; * remove; OR * redesign.  (full verbose logs attached)  h2. Steps to Reproduce  Completely cleaned the build, removed directory, clean pull from {{master}} (SHA: {{fb93d93}}) - same results, 9 failed tests:  """
"MESOS-3311","Bug","test",2,"SlaveTest.HTTPSchedulerSlaveRestart","""Observed on ASF CI  """
"MESOS-3321","Bug","fetcher",1,"Spurious fetcher message about extracting an archive","""The fetcher emits a spurious log message about not extracting an archive with """".tgz"""" extension, even though the tarball is extracted correctly.  """
"MESOS-3332","Task","libprocess",8,"Support HTTP Pipelining in libprocess (http::post)","""Currently , {{http::post}} in libprocess, does not support HTTP pipelining. Each call as of know sends in the {{Connection: close}} header, thereby, signaling to the server to close the TCP socket after the response.  We either need to create a new interface for supporting HTTP pipelining , or modify the existing {{http::post}} to do so.  This is needed for the Scheduler/Executor library implementations to make sure """"Calls"""" are sent in order to the master. Currently, in order to do so, we send in the next request only after we have received a response for an earlier call that results in degraded performance.  """
"MESOS-3345","Task","stout",5,"Expand the range of integer precision when converting into/out of json.","""For [MESOS-3299], we added some protobufs to represent time with integer precision.  However, this precision is not maintained through protobuf <-> JSON conversion, because of how our JSON encoders/decoders convert numbers to floating point.  To maintain precision, we can try one of the following: * Try using a {{long double}} to represent a number. * Add logic to stringify/parse numbers without loss when possible. * Try representing {{int64_t}} as a string and parse it as such? * Update PicoJson and add a compiler flag, i.e. {{-DPICOJSON_USE_INT64}}   In all cases, we'll need to make sure that: * Integers are properly stringified without loss. * The JSON decoder parses the integer without loss. * We have some unit tests for big (close to {{INT32_MAX}}/{{INT64_MAX}}) and small integers."""
"MESOS-3340","Improvement","stout",2,"Command-line flags should take precedence over OS Env variables","""Currently, it appears that re-defining a flag on the command-line that was already defined via a OS Env var ({{MESOS_*}}) causes the Master to fail with a not very helpful message.  For example, if one has {{MESOS_QUORUM}} defined, this happens:   which is not very helpful.  Ideally, we would parse the flags with a """"well-known"""" priority (command-line first, environment last) - but at the very least, the error message should be more helpful in explaining what the issue is."""
"MESOS-3339","Task","test",3,"Implement filtering mechanism for (Scheduler API Events) Testing","""Currently, our testing infrastructure does not have a mechanism of filtering/dropping HTTP events of a particular type from the Scheduler API response stream.  We need a {{DROP_HTTP_CALLS}} abstraction that can help us to filter a particular event type.    This helper code is duplicated in at least two places currently, Scheduler Library/Maintenance Primitives tests.  - The solution can be as trivial as moving this helper function to a common test-header. - Implement a {{DROP_HTTP_CALLS}} similar to what we do for other protobufs via {{DROP_CALLS}}."""
"MESOS-3338","Bug","allocation|master",3,"Dynamic reservations are not counted as used resources in the master","""Dynamically reserved resources should be considered used or allocated and hence reflected in Mesos bookkeeping structures and {{state.json}}.  I expanded the {{ReservationTest.ReserveThenUnreserve}} test with the following section: {code}   // Check that the Master counts the reservation as a used resource.   {     Future<process::http::Response> response =       process::http::get(master.get(), """"state.json"""");     AWAIT_READY(response);      Try<JSON::Object> parse = JSON::parse<JSON::Object>(response.get().body);     ASSERT_SOME(parse);      Result<JSON::Number> cpus =       parse.get().find<JSON::Number>(""""slaves[0].used_resources.cpus"""");      ASSERT_SOME_EQ(JSON::Number(1), cpus);   } {code} and got   Idea for new resources states: https://docs.google.com/drawings/d/1aquVIqPY8D_MR-cQjZu-wz5nNn3cYP3jXqegUHl-Kzc/edit"""
"MESOS-3356","Task","documentation",5,"Scope out approaches to deal with logging to finite disks (i.e. log rotation|capped-size logging).","""For the background, see the parent story [MESOS-3348].  For the work/design/discussion, see the linked design document (below).  """
"MESOS-3349","Bug","test",5,"Removing mount point fails with EBUSY in LinuxFilesystemIsolator.","""When running the tests as root, we found PersistentVolumeTest.AccessPersistentVolume fails consistently on some platforms.    Turns out that the 'rmdir' after the 'umount' fails with EBUSY because there's still some references to the mount.  FYI [~jieyu] [~mcypark]"""
"MESOS-3366","Improvement","agent",3,"Allow resources/attributes discovery","""In heterogeneous clusters, tasks sometimes have strong constraints on the type of hardware they need to execute on. The current solution is to use custom resources and attributes on the agents. Detecting non-standard resources/attributes requires wrapping the """"mesos-slave"""" binary behind a script and use custom code to probe the agent. Unfortunately, this approach doesn't allow composition. The solution would be to provide a hook/module mechanism to allow users to use custom code performing resources/attributes discovery.  Please review the detailed document below: https://docs.google.com/document/d/15OkebDezFxzeyLsyQoU0upB0eoVECAlzEkeg0HQAX9w  Feel free to express comments/concerns by annotating the document or by replying to this issue. """
"MESOS-3375","Task","HTTP API",1,"Add executor protobuf to v1","""A new protobuf for Executor was introduced in Mesos for the HTTP API, it needs to be added to /v1 so it reflects changes made on v1/mesos.proto. This protobuf is ought to be changed as the executor HTTP API design evolves."""
"MESOS-3378","Documentation","documentation|test",3,"Document a test pattern for expediting event firing","""We use {{Clock::advance()}} extensively in tests to expedite event firing and minimize overall {{make check}} time. Document this pattern for posterity."""
"MESOS-3393","Task","HTTP API",1,"Remove unused executor protobuf","""The executor protobuf definition living outside the v1/ directory is unused, it should  be removed to avoid confusion."""
"MESOS-3413","Bug","agent|containerization|docker",5,"Docker containerizer does not symlink persistent volumes into sandbox","""For the ArangoDB framework I am trying to use the persistent primitives. nearly all is working, but I am missing a crucial piece at the end: I have successfully created a persistent disk resource and have set the persistence and volume information in the DiskInfo message. However, I do not see any way to find out what directory on the host the mesos slave has reserved for us. I know it is ${MESOS_SLAVE_WORKDIR}/volumes/roles/<myRole>/<NAME>_<UUID> but we have no way to query this information anywhere. The docker containerizer does not automatically mount this directory into our docker container, or symlinks it into our sandbox. Therefore, I have essentially no access to it. Note that the mesos containerizer (which I cannot use for other reasons) seems to create a symlink in the sandbox to the actual path for the persistent volume. With that, I could mount the volume into our docker container and all would be well."""
"MESOS-3417","Improvement","replicated log",2,"Log source address replicated log recieved broadcasts","""Currently Mesos doesn't log what machine a replicated log status broadcast was recieved from:   It would be really useful for debugging replicated log startup issues to have info about where the message came from (libprocess address, ip, or hostname) the message came from"""
"MESOS-3426","Bug","libprocess",3,"process::collect and process::await do not perform discard propagation.","""When aggregating futures with collect, one may discard the outer future:    Discard requests should propagate down into the inner futures being collected."""
"MESOS-3424","Task","containerization",5,"Support fetching AppC images into the store","""So far AppC store is read only and depends on out of band mechanisms to get the images. We need to design a way to support fetching in a native way.  As commented on MESOS-2824: It's unacceptable to have either have: * the slave to be blocked for extended period of time (minutes) which delays the communication between the executor and scheduler, or * the first task that uses this image to be blocked for a long time to wait for the container image to be ready.  The solution needs to enable the operator to prefetch a list of """"preferred images"""" without introducing the above problems."""
"MESOS-3423","Bug","agent|containerization",3,"Perf event isolator stops performing sampling if a single timeout occurs.","""Currently the perf event isolator times out a sample after a fixed extra time of 2 seconds on top of the sample time elapses:    This should be based on the reap interval maximum.  Also, the code stops sampling altogether when a single timeout occurs. We've observed time outs during normal operation, so it would be better for the isolator to continue performing perf sampling in the case of timeouts. It may also make sense to continue sampling in the case of errors, since these may be transient."""
"MESOS-3443","Task","stout",2,"Windows: Port protobuf_tests.hpp","""We have ported `stout/protobuf.hpp`, but to make the `protobuf_tests.cpp` file to work, we need to port `stout/uuid.hpp`."""
"MESOS-3459","Task","master",1,"Change /machine/up and /machine/down endpoints to take an array","""With [MESOS-3312] committed, the {{/machine/up}} and {{/machine/down}} endpoints should also take an input as an array.  It is important to change this before maintenance primitives are released: https://reviews.apache.org/r/38011/  Also, a minor change to the error message from these endpoints: https://reviews.apache.org/r/37969/"""
"MESOS-3458","Bug","master",1,"Segfault when accepting or declining inverse offers","""Discovered while writing a test for filters (in regards to inverse offers).  Fix here: https://reviews.apache.org/r/38470/"""
"MESOS-3492","Task","documentation",1,"Expose maintenance user doc via the documentation home page","""The committed docs can be found here: http://mesos.apache.org/documentation/latest/maintenance/  We need to add a link to {{docs/home.md}} Also, the doc needs some minor formatting tweaks."""
"MESOS-3490","Bug","stout",1,"Mesos UI fails to represent JSON entities","""The Mesos UI is broken, it seems to fail to represent JSON from /state. This may have been introduced with https://reviews.apache.org/r/38028 """
"MESOS-3485","Improvement","modules",3,"Make hook execution order deterministic","""Currently, when using multiple hooks of the same type, the execution order is implementation-defined.   This is because in src/hook/manager.cpp, the list of available hooks is stored in a {{hashmap<string, Hook*>}}. A hashmap is probably unnecessary for this task since the number of hooks should remain reasonable. A data structure preserving ordering should be used instead to allow the user to predict the execution order of the hooks. I suggest that the execution order should be the order in which hooks are specified with {{--hooks}} when starting an agent/master.  This will be useful when combining multiple hooks after MESOS-3366 is done."""
"MESOS-3497","Task","containerization",3,"Add implementation for sha256 based file content verification.","""https://reviews.apache.org/r/38747/"""
"MESOS-3496","Task","containerization",2,"Create interface for digest verifier","""Add interface for digest verifier so that we can add implementations for digest types like sha256, sha512 etc"""
"MESOS-3513","Bug","agent|test",1,"Cgroups Test Filters aborts tests on Centos 6.6 ","""Running make check on centos 6.6 causes all tests to abort due to CHECK_SOME test in CgroupsFIlter:  """
"MESOS-3517","Bug","build",2,"Building mesos from source fails when OS language is not English","""Line 963 of mesos/3rdparty/libprocess/3rdparty/stout/tests/os_tests.cpp contains the following:    EXPECT_TRUE(strings::contains(result.get(), """"No such file or directory""""));  But this does not match when your locale is not English. When changing it to what my terminal gives me: """"Bestand of map bestaat niet"""" then it works just fine. """
"MESOS-3525","Task","build",3,"Figure out how to enforce 64-bit builds on Windows.","""We need to make sure people don't try to compile Mesos on 32-bit architectures. We don't want a Windows repeat of something like this:  https://issues.apache.org/jira/browse/MESOS-267"""
"MESOS-3540","Bug","libprocess",2,"Libevent termination triggers Broken Pipe","""When the libevent loop terminates and we unblock the {{SIGPIPE}} signal, the pending {{SIGPIPE}} instantly triggers and causes a broken pipe when the test binary stops running. """
"MESOS-3552","Task","master",3,"CHECK failure due to floating point precision on reservation request","""result.cpus() == cpus() check is failing due to ( double == double ) comparison problem.    Root Cause :   Framework requested 0.1 cpu reservation for the first task. So far so good. Next Reserve operation  lead to double operations resulting in following double values :   results.cpus() : 23.9999999999999964472863211995 cpus() : 24  And the check ( result.cpus() == cpus() ) failed.    The double arithmetic operations caused results.cpus() value to be :  23.9999999999999964472863211995 and hence ( 23.9999999999999964472863211995 == 24 ) failed.   """
"MESOS-3551","Bug","libprocess|stout",3,"Replace use of strerror with thread-safe alternatives strerror_r / strerror_l.","""{{strerror()}} is not required to be thread safe by POSIX and is listed as unsafe on Linux:  http://pubs.opengroup.org/onlinepubs/9699919799/ http://man7.org/linux/man-pages/man3/strerror.3.html  I don't believe we've seen any issues reported due to this. We should replace occurrences of strerror accordingly, possibly offering a wrapper in stout to simplify callsites."""
"MESOS-3560","Bug","master",1,"JSON-based credential files do not work correctly","""Specifying the following credentials file:   Then hitting a master endpoint with:   Does not work. This is contrary to the text-based credentials file which works:   Currently, the password in a JSON-based credentials file needs to be base64-encoded in order for it to work: """
"MESOS-3556","Bug","cli",1,"mesos.cli broken in 0.24.x","""The issue was initially reported on the mailing list: http://www.mail-archive.com/user@mesos.apache.org/msg04670.html  The format of the master data stored in zookeeper has changed but the mesos.cli does not reflect these changes causing tools like {{mesos-tail}} and {{mesos-ps}} to fail.  Example error from {{mesos-tail}}:    The problem exists in https://github.com/mesosphere/mesos-cli/blob/master/mesos/cli/master.py#L107. The code should be along the lines of:    This causes the master address to come back correctly."""
"MESOS-3554","Improvement","allocation",3,"Allocator changes trigger large re-compiles","""Due to the templatized nature of the allocator, even small changes trigger large recompiles of the code-base. This make iterating on changes expensive for developers."""
"MESOS-3571","Task","containerization",5,"Refactor registry_client","""Refactor registry client component to:  - Make methods shorter for readability - Pull out structs not related to registry client into common namespace."""
"MESOS-3575","Bug","HTTP API",2,"V1 API java/python protos are not generated","""The java/python protos for the V1 api should be generated according to the Makefile; however, they do not show up in the generated build directory."""
"MESOS-3573","Bug","agent|docker",5,"Mesos does not kill orphaned docker containers","""After upgrade to 0.24.0 we noticed hanging containers appearing. Looks like there were changes between 0.23.0 and 0.24.0 that broke cleanup.  Here's how to trigger this bug:  1. Deploy app in docker container. 2. Kill corresponding mesos-docker-executor process 3. Observe hanging container  Here are the logs after kill:    Another issue: if you restart mesos-slave on the host with orphaned docker containers, they are not getting killed. This was the case before and I hoped for this trick to kill hanging containers, but it doesn't work now.  Marking this as critical because it hoards cluster resources and blocks scheduling."""
"MESOS-3579","Bug","fetcher|test",2,"FetcherCacheTest.LocalUncachedExtract is flaky","""From ASF CI: https://builds.apache.org/job/Mesos/866/COMPILER=clang,CONFIGURATION=--verbose,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/console   """
"MESOS-3586","Bug","test",1,"MemoryPressureMesosTest.CGROUPS_ROOT_Statistics and CGROUPS_ROOT_SlaveRecovery are flaky","""I am install Mesos 0.24.0 on 4 servers which have very similar hardware and software configurations.   After performing {{../configure}}, {{make}}, and {{make check}} some servers have completed successfully and other failed on test {{[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics}}.  Is there something I should check in this test?   """
"MESOS-3584","Bug","libprocess",1,"rename libprocess tests to ""libprocess-tests""","""Stout tests are in a binary named {{stout-tests}}, Mesos tests are in {{mesos-tests}}, but libprocess tests are just {{tests}}. It would be helpful to name them {{libprocess-tests}} """
"MESOS-3581","Documentation","documentation",2,"License headers show up all over doxygen documentation.","""Currently license headers are commented in something resembling Javadoc style,    Since we use Javadoc-style comment blocks for doxygen documentation all license headers appear in the generated documentation, potentially and likely hiding the actual documentation.  Using {{/*}} to start the comment blocks would be enough to hide them from doxygen, but would likely also result in a largish (though mostly uninteresting) patch."""
"MESOS-3595","Bug","scheduler driver",3,"Framework process hangs after master failover when number frameworks > libprocess thread pool size","""When running multi framework instances per process, if the number of framework created exceeds the libprocess threads then during master failover the zookeeper updates can cause deadlock. E.g. On a machine with 24 cpus, if the framework instance count exceeds 24 ( per process)  then when the master fails over all the libprocess threads block updating the cache ( GroupProcess) leading to deadlock. Below is the stack trace of one the libprocess thread :    Solution:   Create master detector per url instead of per framework. Will send the review request.   """
"MESOS-3639","Task","stout",5,"Implement stout/os/windows/killtree.hpp","""killtree() is implemented using Windows Job Objects. The processes created by the  executor are associated with a job object using `create_job'. killtree() is simply terminating the job object.   Helper functions: `create_job` function creates a job object whose name is derived from the `pid` and associates the `pid` process with the job object. Every process started by the process which is part of the job object becomes part of the job object. The job name should match the name used in `kill_job`. The jobs should be create with JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE and allow the caller to decide how to handle the returned handle.   `kill_job` function assumes the process identified by `pid` is associated with a job object whose name is derive from it. Every process started by the process which is part of the job object becomes part of the job object. Destroying the task will close all such processes."""
"MESOS-3624","Task","containerization",3,"Port slave/containerizer/mesos/launch.cpp to Windows","""Important subset of the dependency tree follows:  slave/containerizer/mesos/launch.cpp: os, protobuf, launch launch: subcommand subcommand: flags flags.hpp: os.hpp, path.hpp, fetch.hpp"""
"MESOS-3623","Task","containerization",3,"Port slave/containerizer/mesos/containerizer.cpp to Windows","""Important subset of the dependency tree follows:  slave/containerizer/mesos/containerizer.cpp: isolator, collect, defer, io, metrics, reap, subprocess, fs, os, path, protobuf_utils, paths, slave, containerizer, fetcher, launcher, posix, disk, containerizer, launch, provisioner"""
"MESOS-3620","Task","containerization",3,"Create slave/containerizer/isolators/filesystem/windows.cpp","""Should look a lot like the posix.cpp flavor. Important subset of the dependency tree follows for the posix flavor:  slave/containerizer/isolators/filesystem/posix.cpp: filesystem/posix, fs, os, path filesystem/posix: flags, isolator"""
"MESOS-3619","Task","containerization",3,"Port slave/containerizer/isolator.cpp to Windows","""Important subset of the dependency tree follows:  isolator.hpp: dispatch.hpp, path.hpp isolator: process dispatch.hpp: process.hpp """
"MESOS-3618","Task","containerization",3,"Port slave/containerizer/fetcher.cpp","""Important subset of the dependency tree follows:  slave/containerizer/fetcher.cpp: slave, fetcher, collect, dispatch, net collect: future, defer, process fetcher: type_utils, future, process, subprocess dispatch.hpp: process.hpp net.hpp: ip, networking stuff future.hpp: pid.hpp defer.hpp: deferred.hpp, dispatch.hpp deferred.hpp: dispatch.hpp, pid.hpp type_utils.hpp: uuid.hpp subprocess: os, future"""
"MESOS-3615","Task","agent",3,"Port slave/state.cpp","""Important subset of changes this depends on:  slave/state.cpp: pid, os, path, protobuf, paths, state pid.hpp: address.hpp, ip.hpp address.hpp: ip.hpp, net.hpp net.hpp: ip, networking stuff state: type_utils, pid, os, path, protobuf, uuid type_utils.hpp: uuid.hpp"""
"MESOS-3613","Task","agent",1,"Port slave/paths.cpp to Windows","""Important subset of dependency tree of changes necessary:  slave/paths.cpp: os, path"""
"MESOS-3604","Bug","test",3,"ExamplesTest.PersistentVolumeFramework does not work in OS X El Capitan","""The example persistent volume framework test does not pass in OS X El Capitan. It seems to be executing the {{<build_dir>/src/.libs/mesos-executor}} directly while it should be executing the wrapper script at {{<build_dir>/src/mesos-executor}} instead. The no-executor framework passes however, which seem to have a very similar configuration with the persistent volume framework. The following is the output that shows the {{dyld}} load error:  """
"MESOS-3692","Documentation","documentation",1,"Clarify error message 'could not chown work directory'","""When deploying a framework I encountered the error message 'could not chown work directory'.  It took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the Docker container and the agent was running as root.  I suggest to clarify this message by pointing out to either set {{--switch-user}}  to {{false}} or to run the framework as the same user as the agent."""
"MESOS-3705","Bug","libprocess",3,"HTTP Pipelining doesn't keep order of requests","""[HTTP 1.1 Pipelining|https://en.wikipedia.org/wiki/HTTP_pipelining] describes a mechanism by which multiple HTTP request can be performed over a single socket. The requirement here is that responses should be send in the same order as requests are being made.  Libprocess has some mechanisms built in to deal with pipelining when multiple HTTP requests are made, it is still, however, possible to create a situation in which responses are scrambled respected to the requests arrival.  Consider the situation in which there are two libprocess processes, {{processA}} and {{processB}}, each running in a different thread, {{thread2}} and {{thread3}} respectively. The [{{ProcessManager}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L374] runs in {{thread1}}.  {{processA}} is of type {{ProcessA}} which looks roughly as follows:    {{processB}} is from type {{ProcessB}} which is just like {{ProcessA}} but routes {{""""bar""""}} instead of {{""""foo""""}}.  The situation in which the bug arises is the following:  # Two requests, one for {{""""http://server_uri/(1)/foo""""}} and one for {{""""http://server_uri/(2)//bar""""}} are made over the same socket. # The first request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. This one creates an {{HttpEvent}} and delivers to the handler, in this case {{processA}}. # [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processA}} queue. This happens in {{thread1}}. # The second request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. Another {{HttpEvent}} is created and delivered to the handler, in this case {{processB}}. # [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processB}} queue. This happens in {{thread1}}. # {{Thread2}} is blocked, so {{processA}} cannot handle the first request, it is stuck in the queue. # {{Thread3}} is idle, so it picks up the request to {{processB}} immediately. # [{{ProcessBase::visit(HttpEvent)}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3073] is called in {{thread3}}, this one in turn [dispatches|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3106] the response's future to the {{HttpProxy}} associated with the socket where the request came.  At the last point, the bug is evident, the request to {{processB}} will be send before the request to {{processA}} even if the handler takes a long time and the {{processA::bar()}} actually finishes before. The responses are not send in the order the requests are done.  h1. Reproducer  The following is a test which successfully reproduces the issue:  {code:title=3rdparty/libprocess/src/tests/http_tests.cpp} #include <process/latch.hpp  using process::Latch; using testing::InvokeWithoutArgs;  // This tests tries to force a situation in which HTTP Pipelining is scrambled. // It does so by having two actors to which three requests are made, the first // two requests to the first actor and a third request to the second actor. // The first request will block the first actor long enough to allow the second // actor to process the third request. Since the first actor will not be able to // handle any event until it is done processing the first request, the third // request is finished before the second even starts. // The ultimate goal of the test is to alter the order in which // `ProcessBase::visit(HttpEvent)` is executed for the different events // respect to the order in which the requests arrived. TEST(HTTPConnectionTest, ComplexPipelining) {   Http server1, server2;    Future<http::Request> get1, get2, get3;   Latch latch;    EXPECT_CALL(*server1.process, get(_))     .WillOnce(DoAll(FutureArg<0>(&get1),                     InvokeWithoutArgs([&latch]() { latch.await(); }),                     Return(http::OK(""""1""""))))     .WillOnce(DoAll(FutureArg<0>(&get2),                     Return(http::OK(""""2""""))));    EXPECT_CALL(*server2.process, get(_))     .WillOnce(DoAll(FutureArg<0>(&get3),                     Return(http::OK(""""3""""))));      auto url1 = http::URL(       """"http"""",       server1.process->self().address.ip,       server1.process->self().address.port,       server1.process->self().id + """"/get"""");     auto url2 = http::URL(       """"http"""",       server1.process->self().address.ip,       server1.process->self().address.port,       server2.process->self().id + """"/get"""");    // Create a connection to the server for HTTP pipelining.   Future<http::Connection> connect = http::connect(url1);    AWAIT_READY(connect);    http::Connection connection = connect.get();    http::Request request1;   request1.method = """"GET"""";   request1.url = url1;   request1.keepAlive = true;   request1.body = """"1"""";   Future<http::Response> response1 = connection.send(request1);    http::Request request2 = request1;   request2.body = """"2"""";   Future<http::Response> response2 = connection.send(request2);    http::Request request3;   request3.method = """"GET"""";   request3.url = url2;   request3.keepAlive = true;   request3.body = """"3"""";   Future<http::Response> response3 = connection.send(request3);    // Verify that request1 arrived at server1 and it is the right request.   // Now server1 is blocked processing request1 and cannot pick up more events   // in the queue.   AWAIT_READY(get1);   EXPECT_EQ(request1.body, get1->body);    // Verify that request3 arrived at server2 and it is the right request.   AWAIT_READY(get3);   EXPECT_EQ(request3.body, get3->body);    // Request2 hasn't been picked up since server1 is still blocked serving   // request1.   EXPECT_TRUE(get2.isPending());    // Free server1 so it can serve request2.   latch.trigger();    // Verify that request2 arrived at server1 and it is the right request.   AWAIT_READY(get2);   EXPECT_EQ(request2.body, get2->body);    // Wait for all responses.   AWAIT_READY(response1);   AWAIT_READY(response2);   AWAIT_READY(response3);    // If pipelining works as expected, even though server2 finished processing   // its request before server1 even began with request2, the responses should   // arrive in the order they were made.   EXPECT_EQ(request1.body, response1->body);   EXPECT_EQ(request2.body, response2->body);   EXPECT_EQ(request3.body, response3->body);    AWAIT_READY(connection.disconnect());   AWAIT_READY(connection.disconnected()); } {code}"""
"MESOS-3723","Task","master",5,"Prototype quota request authorization","""When quotas are requested they should authorize their roles.  This ticket will authorize quota requests with ACLs. The existing authorization support that has been implemented in MESOS-1342 will be extended to add a `request_quotas` ACL."""
"MESOS-3722","Task","master",5,"Prototype quota request authentication","""Quota requests need to be authenticated.  This ticket will authenticate quota requests using credentials provided by the `Authorization` field of the HTTP request. This is similar to how authentication is implemented in `Master::Http`."""
"MESOS-3720","Improvement","master",5,"Tests for Quota support in master","""Allocator-agnostic tests for quota support in the master. They can be divided into several groups: * Heuristic check; * Master failover; * Functionality and quota guarantees."""
"MESOS-3718","Bug","allocation",5,"Implement Quota support in allocator","""The built-in Hierarchical DRF allocator should support Quota. This includes (but not limited to): adding, updating, removing and satisfying quota; avoiding both overcomitting resources and handing them to non-quota'ed roles in presence of master failover.  A [design doc for Quota support in Allocator|https://issues.apache.org/jira/browse/MESOS-2937] provides an overview of a feature set required to be implemented."""
"MESOS-3717","Task","master",5,"Master recovery in presence of quota","""Quota complicates master failover in several ways. The new master should determine if it is possible to satisfy the total quota and notify an operator in case it's not (imagine simultaneous failovers of multiple agents). The new master should hint the allocator how many agents might reconnect in the future to help it decide how to satisfy quota before the majority of agents reconnect."""
"MESOS-3716","Bug","allocation",3,"Update Allocator interface to support quota","""An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota."""
"MESOS-3732","Improvement","test",1,"Speed up FaultToleranceTest.FrameworkReregister test","""FaultToleranceTest.FrameworkReregister test takes more than one second to complete:   There must be a {{1s}} timeout somewhere which we should mitigate via {{Clock::advance()}}."""
"MESOS-3743","Bug","fetcher",2,"Provide diagnostic output in agent log when fetching fails","""When fetching fails, the fetcher has written log output to stderr in the task sandbox, but it is not easy to get to. It may even be impossible to get to if one only has the agent log available and no more access to the sandbox. This is for instance the case when looking at output from a CI run.  The fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not. When it exits with an error code, we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log.  This is similar to this patch: https://reviews.apache.org/r/37813/  The difference is that the output of the latter is triggered by test failures outside the fetcher, whereas what is proposed here is triggering upon failures inside the fetcher."""
"MESOS-3739","Bug","HTTP API",2,"Mesos does not set Content-Type for 400 Bad Request","""While integrating with the HTTP Scheduler API I encountered the following scenario.  The message below was serialized to protobuf and sent as the POST body {code:title=message} call {   type: ACKNOWLEDGE,   acknowledge: {     uuid: <bytes>,     agentID: { value: """"20151012-182734-16777343-5050-8978-S2"""" },     taskID: { value: """"task-1"""" }   } }   I received the following response {code:title=Response Headers} HTTP/1.1 400 Bad Request Date: Wed, 14 Oct 2015 23:21:36 GMT Content-Length: 74  Failed to validate Scheduler::Call: Expecting 'framework_id' to be present {code}  Even though my accept header made no mention of {{text/plain}} the message body returned to me is {{text/plain}}. Additionally, there is no {{Content-Type}} header set on the response so I can't even do anything intelligently in my response handler."""
"MESOS-3753","Story","framework|HTTP API|test",13,"Test the HTTP Scheduler library with SSL enabled","""Currently, the HTTP Scheduler library does not support SSL-enabled Mesos.   (You can manually test this by spinning up an SSL-enabled master and attempt to run the event-call framework example against it.)  We need to add tests that check the HTTP Scheduler library against SSL-enabled Mesos: * with downgrade support, * with required framework/client-side certifications, * with/without verification of certificates (master-side), * with/without verification of certificates (framework-side), * with a custom certificate authority (CA)  These options should be controlled by the same environment variables found on the [SSL user doc|http://mesos.apache.org/documentation/latest/ssl/].  Note: This issue will be broken down into smaller sub-issues as bugs/problems are discovered."""
"MESOS-3751","Bug","containerization",2,"MESOS_NATIVE_JAVA_LIBRARY not set on MesosContainerize tasks with --executor_environmnent_variables","""When using --executor_environment_variables, and having MESOS_NATIVE_JAVA_LIBRARY in the environment of mesos-slave, the mesos containerizer does not set MESOS_NATIVE_JAVA_LIBRARY itself.  Relevant code: https://github.com/apache/mesos/blob/14f7967ef307f3d98e3a4b93d92d6b3a56399b20/src/slave/containerizer/containerizer.cpp#L281  It sees that the variable is in the mesos-slave's environment (os::getenv), rather than checking if it is set in the environment variable set."""
"MESOS-3749","Documentation","documentation",1,"Configuration docs are missing --enable-libevent and --enable-ssl","""The {{\-\-enable-libevent}} and {{\-\-enable-ssl}} config flags are currently not documented in the """"Configuration"""" docs with the rest of the flags. They should be added."""
"MESOS-3748","Bug","framework|HTTP API",1,"HTTP scheduler library does not gracefully parse invalid resource identifiers","""If you pass a nonsense string for """"master"""" into a framework using the C++ HTTP scheduler library, the framework segfaults.  For example, using the example frameworks:  {code:title=Scheduler Driver} build/src/test-framework --master=""""asdf://127.0.0.1:5050""""  Failed to create a master detector for 'asdf://127.0.0.1:5050': Failed to parse 'asdf://127.0.0.1:5050'  Results in   {code:title=Stack Trace} * thread #2: tid = 0x28b6bb, 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213, stop reason = EXC_BAD_ACCESS (code=1, address=0x0)   * frame #0: 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213     frame #1: 0x0000000100ad05f2 libmesos-0.26.0.dylib`virtual thunk to mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 34 at scheduler.cpp:210     frame #2: 0x00000001022b60f3 libmesos-0.26.0.dylib`::resume() + 931 at process.cpp:2449     frame #3: 0x00000001022c131c libmesos-0.26.0.dylib`::operator()() + 268 at process.cpp:2174     frame #4: 0x00000001022c0fa2 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35) &, const std::__1::atomic<bool> &> + 27 at __functional_base:415     frame #5: 0x00000001022c0f87 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __apply_functor<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::tuple<std::__1::reference_wrapper<const std::__1::atomic<bool> > >, 0, std::__1::tuple<> > + 55 at functional:2060     frame #6: 0x00000001022c0f50 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] operator()<> + 41 at functional:2123     frame #7: 0x00000001022c0f27 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 14 at __functional_base:415     frame #8: 0x00000001022c0f19 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __thread_execute<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 25 at thread:337     frame #9: 0x00000001022c0f00 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() + 368 at thread:347     frame #10: 0x00007fff964c705a libsystem_pthread.dylib`_pthread_body + 131     frame #11: 0x00007fff964c6fd7 libsystem_pthread.dylib`_pthread_start + 176     frame #12: 0x00007fff964c43ed libsystem_pthread.dylib`thread_start + 13 {code}"""
"MESOS-3762","Task","test",3,"Refactor SSLTest fixture such that MesosTest can use the same helpers.","""In order to write tests that exercise SSL with other components of Mesos, such as the HTTP scheduler library, we need to use the setup/teardown logic found in the {{SSLTest}} fixture.  Currently, the test fixtures have separate inheritance structures like this:  where {{::testing::Test}} is a gtest class.  The plan is the following: # Change {{SSLTest}} to inherit from {{TemporaryDirectoryTest}}.  This will require moving the setup (generation of keys and certs) from {{SetUpTestCase}} to {{SetUp}}.  At the same time, *some* of the cleanup logic in the SSLTest will not be needed. # Move the logic of generating keys/certs into helpers, so that individual tests can call them when needed, much like {{MesosTest}}. # Write a child class of {{SSLTest}} which has the same functionality as the existing {{SSLTest}}, for use by the existing tests that rely on {{SSLTest}} or the {{RegistryClientTest}}. # Have {{MesosTest}} inherit from {{SSLTest}} (which might be renamed during the refactor).  If Mesos is not compiled with {{--enable-ssl}}, then {{SSLTest}} could be {{#ifdef}}'d into any empty class.  The resulting structure should be like: """
"MESOS-3759","Improvement","documentation",3,"Document messages.proto","""The messages we pass between Mesos components are largely undocumented.  See this [TODO|https://github.com/apache/mesos/blob/19f14d06bac269b635657960d8ea8b2928b7830c/src/messages/messages.proto#L23]."""
"MESOS-3756","Task","modules",13,"Generalized HTTP Authentication Modules","""Libprocess is going to factor out an authentication interface: MESOS-3231  Here we propose that Mesos can provide implementations for this interface as Mesos modules."""
"MESOS-3771","Bug","HTTP API",2,"Mesos JSON API creates invalid JSON due to lack of binary data / non-ASCII handling","""Spark encodes some binary data into the ExecutorInfo.data field.  This field is sent as a """"bytes"""" Protobuf value, which can have arbitrary non-UTF8 data.  If you have such a field, it seems that it is splatted out into JSON without any regards to proper character encoding:    I suspect this is because the HTTP api emits the executorInfo.data directly:    I think this may be because the custom JSON processing library in stout seems to not have any idea of what a byte array is.  I'm guessing that some implicit conversion makes it get written as a String instead, but:    Thank you for any assistance here.  Our cluster is currently entirely down -- the frameworks cannot handle parsing the invalid JSON produced (it is not even valid utf-8) """
"MESOS-3785","Improvement","fetcher",5,"Use URI content modification time to trigger fetcher cache updates.","""Instead of using checksums to trigger fetcher cache updates, we can for starters use the content modification time (mtime), which is available for a number of download protocols, e.g. HTTP and HDFS.  Proposal: Instead of just fetching the content size, we fetch both size  and mtime together. As before, if there is no size, then caching fails and we fall back on direct downloading to the sandbox.   Assuming a size is given, we compare the mtime from the fetch URI with the mtime known to the cache. If it differs, we update the cache. (As a defensive measure, a difference in size should also trigger an update.)   Not having an mtime available at the fetch URI is simply treated as a unique valid mtime value that differs from all others. This means that when initially there is no mtime, cache content remains valid until there is one. Thereafter,  anew lack of an mtime invalidates the cache once. In other words: any change from no mtime to having one or back is the same as encountering a new mtime.  Note that this scheme does not require any new protobuf fields. """
"MESOS-3795","Improvement","libprocess",2,"process::io::write takes parameter as void* which could be const","""In libprocess we have    which expects a non-{{const}} {{void*}} for its {{data}} parameter. Under the covers {{data}} appears to be handled as a {{const}} (like one would expect from the signature its inspiration {{::write}}).  This function is not used too often, but since it expects a non-{{const}} value for {{data}} automatic conversions to {{void*}} from other pointer types are disabled; instead callers seem cast manually to {{void*}} -- often with C-style casts.  We should sync this method's signature with that of {{::write}}.  In addition to following the expected semantics of {{::write}}, having this work without casts with any pointer value {{data}} would make it easier to interface this with character literals, or raw data ptrs from STL containers (e.g. {{Container::data}}). It would probably also indirectly eliminate temptation to use C-casts."""
"MESOS-3820","Epic","libprocess|test",3,"Test-only libprocess reinitialization","""*Background* Libprocess initialization includes the spawning of a variety of global processes and the creation of the server socket which listens for incoming requests.  Some properties of the server socket are configured via environment variables, such as the IP and port or the SSL configuration.  In the case of tests, libprocess is initialized once per test binary.  This means that testing different configurations (SSL in particular) is cumbersome as a separate process would be needed for every test case.  *Proposal* # Add some optional code between some tests like:  See [MESOS-3863] for more on {{process::finalize}}."""
"MESOS-3831","Documentation","documentation",3,"Document operator HTTP endpoints","""These are not exhaustively documented; they probably should be.  Some endpoints have docs: e.g., {{/reserve}} and {{/unreserve}} are described in the reservation doc page. But it would be good to have a single page that lists all the endpoints and their semantics."""
"MESOS-3833","Bug","HTTP API",2,"/help endpoints do not work for nested paths","""Mesos displays the list of all supported endpoints starting at a given path prefix using the {{/help}} suffix, e.g. {{master:5050/help}}.  It seems that the {{help}} functionality is broken for URL's having nested paths e.g. {{master:5050/help/master/machine/down}}. The response returned is: {quote} Malformed URL, expecting '/help/id/name/' {quote}"""
"MESOS-3849","Bug","build",1,"Corrected style in Makefiles","""Order of files in Makefiles is not strictly alphabetic"""
"MESOS-3851","Bug","containerization",2,"Investigate recent crashes in Command Executor","""Post https://reviews.apache.org/r/38900 i.e. updating CommandExecutor to support rootfs. There seem to be some tests showing frequent crashes due to assert violations.  {{FetcherCacheTest.SimpleEviction}} failed due to the following log:    The reason seems to be a race between the executor receiving a {{RunTaskMessage}} before {{ExecutorRegisteredMessage}} leading to the {{CHECK_SOME(executorInfo)}} failure.  Link to complete log: https://issues.apache.org/jira/browse/MESOS-2831?focusedCommentId=14995535&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14995535  Another related failure from {{ExamplesTest.PersistentVolumeFramework}}    Full logs at: https://builds.apache.org/job/Mesos/1191/COMPILER=gcc,CONFIGURATION=--verbose,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull"""
"MESOS-3864","Task","libprocess",1,"Simplify and/or document the libprocess initialization synchronization logic","""Tracks this [TODO|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L749].  The [synchronization logic of libprocess|https://github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff-128d3b56fc8c9ec0176fdbadcfd11fc2] [predates abstractions|https://github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff-0eebc8689450916990abe080d86c2acb] like {{process::Once}}, which is used in almost all other one-time initialization blocks.    The logic should be documented.  It can also be simplified (see the [review description|https://reviews.apache.org/r/39949/]).  Or it can be replaced with {{process::Once}}."""
"MESOS-3863","Task","libprocess|test",2,"Investigate the requirements of programmatically re-initializing libprocess","""This issue is for investigating what needs to be added/changed in {{process::finalize}} such that {{process::initialize}} will start on a clean slate.  Additional issues will be created once done.  Also see [the parent issue|MESOS-3820].  {{process::finalize}} should cover the following components: * {{__s__}} (the server socket) ** {{delete}} should be sufficient.  This closes the socket and thereby prevents any further interaction from it. * {{process_manager}} ** Related prior work: [MESOS-3158] ** Cleans up the garbage collector, help, logging, profiler, statistics, route processes (including [this one|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L963], which currently leaks a pointer). ** Cleans up any other {{spawn}} 'd process. ** Manages the {{EventLoop}}. * {{Clock}} ** The goal here is to clear any timers so that nothing can deference {{process_manager}} while we're finalizing/finalized.  It's probably not important to execute any remaining timers, since we're """"shutting down"""" libprocess.  This means: *** The clock should be {{paused}} and {{settled}} before the clean up of {{process_manager}}. *** Processes, which might interact with the {{Clock}}, should be cleaned up next. *** A new {{Clock::finalize}} method would then clear timers, process-specific clocks, and {{tick}} s; and then {{resume}} the clock. * {{__address__}} (the advertised IP and port) ** Needs to be cleared after {{process_manager}} has been cleaned up.  Processes use this to communicate events.  If cleared prematurely, {{TerminateEvents}} will not be sent correctly, leading to infinite waits. * {{socket_manager}} ** The idea here is to close all sockets and deallocate any existing {{HttpProxy}} or {{Encoder}} objects. ** All sockets are created via {{__s__}}, so cleaning up the server socket prior will prevent any new activity. * {{mime}} ** This is effectively a static map. ** It should be possible to statically initialize it. * Synchronization atomics {{initialized}} & {{initializing}}. ** Once cleanup is done, these should be reset.  *Summary*: * Implement {{Clock::finalize}}.  [MESOS-3882] * Implement {{~SocketManager}}.  [MESOS-3910] * Make sure the {{MetricsProcess}} and {{ReaperProcess}} are reinitialized.  [MESOS-3934] * (Optional) Clean up {{mime}}. * Wrap everything up in {{process::finalize}}."""
"MESOS-3862","Task","master",5,"Authorize set quota requests.","""When quotas are requested they should authorize their roles. This ticket will authorize quota requests with ACLs. The existing authorization support that has been implemented in MESOS-1342 will be extended to add a `request_quotas` ACL."""
"MESOS-3861","Task","master",3,"Authenticate quota requests","""Quota requests need to be authenticated. This ticket will authenticate quota requests using credentials provided by the {{Authorization}} field of the HTTP request. This is similar to how authentication is implemented in {{Master::Http}}."""
"MESOS-3854","Task","security",5,"Finalize design for generalized Authorizer interface","""Finalize the structure the interface and achieve consensus on the design doc proposed in MESOS-2949.  https://docs.google.com/document/d/1-XARWJFUq0r_TgRHz_472NvLZNjbqE4G8c2JL44OSMQ/edit"""
"MESOS-3882","Task","libprocess|test",3,"Libprocess: Implement process::Clock::finalize","""Tracks this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L974-L975].  The {{Clock}} is initialized with a callback that, among other things, will dereference the global {{process_manager}} object.  When libprocess is shutting down, the {{process_manager}} is cleaned up.  Between cleanup and termination of libprocess, there is some chance that a {{Timer}} will time out and result in dereferencing {{process_manager}}.  *Proposal*  * Implement {{Clock::finalize}}.  This would clear: ** existing timers ** process-specific clocks ** ticks * Change {{process::finalize}}. *# Resume the clock.  (The clock is only paused during some tests.)  When the clock is not paused, the callback does not dereference {{process_manager}}. *# Clean up {{process_manager}}.  This terminates all the processes that would potentially interact with {{Clock}}. *# Call {{Clock::finalize}}."""
"MESOS-3877","Task","documentation",5,"Draft operator documentation for quota","""Draft an operator guide for quota which describes basic usage of the endpoints and few basic and advanced usage cases."""
"MESOS-3875","Task","allocation|master",3,"Account dynamic reservations towards quota.","""Dynamic reservationswhether allocated or notshould be accounted towards role's quota. This requires update in at least two places: * The built-in allocator, which actually satisfies quota; * The sanity check in the master."""
"MESOS-3874","Task","allocation",3,"Investigate recovery for the Hierarchical allocator","""The built-in Hierarchical allocator should implement the recovery (in the presence of quota)."""
"MESOS-3873","Task","allocation",3,"Enhance allocator interface with the recovery() method","""There are some scenarios (e.g. quota is set for some roles) when it makes sense to notify an allocator about the recovery. Introduce a method into the allocator interface that allows for this."""
"MESOS-3900","Task","build",3,"Enable mesos-reviewbot project on jenkins to use docker","""As a first step to adding capability for building multiple configurations on reviewbot, we need to change the build scripts to use docker. """
"MESOS-3899","Bug","documentation",1,"Wrong syntax and inconsistent formatting of JSON examples in flag documentation","""The JSON examples in the documentation of the commandline flags ({{mesos-master.sh --help}} and {{mesos-slave.sh --help}}) don't have a consistent formatting. Furthermore, some examples aren't even compliant JSON because they have trailing commas were they shouldn't."""
"MESOS-3884","Improvement","allocation",1,"Corrected style in hierarchical allocator","""The built-in allocator code has some style issues (namespaces in the .cpp file, unfortunate formatting) which should be corrected for readability."""
"MESOS-3905","Documentation","docker|documentation",1,"Five new docker-related slave flags are not covered by the configuration documentation.","""These flags were added to """"slave/flags.cpp"""", but are not mentioned in """"docs/configuration.md"""":    add(&Flags::docker_auth_server,       """"docker_auth_server"""",       """"Docker authentication server"""",       """"auth.docker.io"""");    add(&Flags::docker_auth_server_port,       """"docker_auth_server_port"""",       """"Docker authentication server port"""",       """"443"""");   add(&Flags::docker_puller_timeout_secs,       """"docker_puller_timeout"""",       """"Timeout value in seconds for pulling images from Docker registry"""",       """"60"""");    add(&Flags::docker_registry,       """"docker_registry"""",       """"Default Docker image registry server host"""",       """"registry-1.docker.io"""");   add(&Flags::docker_registry_port,       """"docker_registry_port"""",       """"Default Docker registry server port"""",       """"443""""); """
"MESOS-3923","Bug","framework|HTTP API|master",5,"Implement AuthN handling in Master for the Scheduler endpoint","""If authentication(AuthN) is enabled on a master, frameworks attempting to use the HTTP Scheduler API can't register.    Authorization(AuthZ) is already supported for HTTP based frameworks."""
"MESOS-3912","Task","master",3,"Rescind offers in order to satisfy quota","""When a quota request comes in, we may need to rescind a certain amount of outstanding offers in order to satisfy it. Because resources are allocated in the allocator, there can be a race between rescinding and allocating. This race makes it hard to determine the exact amount of offers that should be rescinded in the master."""
"MESOS-3911","Task","master",1,"Add a `--force` flag to disable sanity check in quota","""There are use cases when an operator may want to disable the sanity check for quota endpoints (MESOS-3074), even if this renders the cluster under quota. For example, an operator sets quota before adding more agents in order to make sure that no non-quota allocations from new agents are made. """
"MESOS-3910","Task","libprocess|test",5,"Libprocess: Implement cleanup of the SocketManager in process::finalize","""The {{socket_manager}} and {{process_manager}} are intricately tied together.  Currently, only the {{process_manager}} is cleaned up by {{process::finalize}}.  To clean up the {{socket_manager}}, we must close all sockets and deallocate any existing {{HttpProxy}} or {{Encoder}} objects.  And we should prevent further objects from being created/tracked by the {{socket_manager}}.  *Proposal* # Clean up all processes other than {{gc}}.  This will clear all links and delete all {{HttpProxy}} s while {{socket_manager}} still exists. # Close all sockets via {{SocketManager::close}}.  All of {{socket_manager}} 's state is cleaned up via {{SocketManager::close}}, including termination of {{HttpProxy}} (termination is idempotent, meaning that killing {{HttpProxy}} s via {{process_manager}} is safe). # At this point, {{socket_manager}} should be empty and only the {{gc}} process should be running.  (Since we're finalizing, assume there are no threads trying to spawn processes.)  {{socket_manager}} can be deleted. # {{gc}} can be deleted.  This is currently a leaked pointer, so we'll also need to track and delete that. # {{process_manager}} should be devoid of processes, so we can proceed with cleanup (join threads, stop the {{EventLoop}}, etc)."""
"MESOS-3909","Bug","c++ api|modules",3,"isolator module headers depend on picojson headers","""When trying to build an isolator module, stout headers end up depending on {{picojson.hpp}} which is not installed.  """
"MESOS-3934","Task","libprocess|test",3,"Libprocess: Unify the initialization of the MetricsProcess and ReaperProcess","""Related to this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L949-L950].  The {{MetricsProcess}} and {{ReaperProcess}} are global processes (singletons) which are initialized upon first use.  The two processes could be initialized alongside the {{gc}}, {{help}}, {{logging}}, {{profiler}}, and {{system}} (statistics) processes inside {{process::initialize}}.  This is also necessary for libprocess re-initialization."""
"MESOS-3939","Bug","stout",2,"ubsan error in net::IP::create(sockaddr const&): misaligned address","""Running ubsan from GCC 5.2 on the current Mesos unit tests yields this, among other problems:  """
"MESOS-3936","Documentation","documentation",5,"Document possible task state transitions for framework authors","""We should document the possible ways in which the state of a task can evolve over time; what happens when an agent is partitioned from the master; and more generally, how we recommend that framework authors develop fault-tolerant schedulers and do task state reconciliation."""
"MESOS-3949","Bug","containerization",3,"User CGroup Isolation tests fail on Centos 6.","""UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup and UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup fail on CentOS 6.6 with similar output when libevent and SSL are enabled.      """
"MESOS-3960","Improvement","master",3,"Standardize quota endpoints","""To be consistent with other operator endpoints, require a single JSON object in the request as opposed to key-value pairs encoded in a string."""
"MESOS-3976","Bug","framework|HTTP API",3,"C++ HTTP Scheduler Library does not work with SSL enabled","""The C++ HTTP scheduler library does not work against Mesos when SSL is enabled (without downgrade).  The fix should be simple: * The library should detect if SSL is enabled. * If SSL is enabled, connections should be made with HTTPS instead of HTTP."""
"MESOS-3965","Bug","master",3,"Ensure resources in `QuotaInfo` protobuf do not contain `role`","""{{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}"""
"MESOS-3964","Bug","containerization|test",2,"LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8.","""sudo ./bin/mesos-test.sh --gtest_filter=""""LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs""""   """
"MESOS-3994","Improvement","containerization",3,"Refactor registry client/puller to avoid JSON and struct.","""We should get rid of all JSON and struct for message passing as function returned type. By using the methods provided by spec.hpp to refactor all unnecessary JSON message and struct in registry client and registry puller. Also, remove all redundant check in registry client that are already checked by spec validation. """
"MESOS-3983","Task","master|test",3,"Tests for quota request validation","""Tests should include: * JSON validation; * Absence of irrelevant fields; * Semantic validation."""
"MESOS-3981","Task","allocation",3,"Implement recovery in the Hierarchical allocator","""The built-in Hierarchical allocator should implement the recovery (in the presence of quota)."""
"MESOS-3979","Improvement","allocation",3,"Replace `QuotaInfo` with `Quota` in allocator interface","""After introduction of C++ wrapper `Quota` for `QuotaInfo`, all allocator methods using `QuotaInfo` should be updated."""
"MESOS-4005","Improvement","containerization",2,"Support workdir runtime configuration from image ","""We need to support workdir runtime configuration returned from image such as Dockerfile."""
"MESOS-4004","Improvement","containerization",3,"Support default entrypoint and command runtime config in Mesos containerizer","""We need to use the entrypoint and command runtime configuration returned from image to be used in Mesos containerizer."""
"MESOS-4014","Task","master",3,"Introduce remove endpoint for quota","""This endpoint is for removing quotas via the DELETE method."""
"MESOS-4013","Task","master",5,"Introduce status endpoint for quota","""This endpoint is for querying quota status via the GET method."""
"MESOS-4009","Bug","test",1,"RegistryClientTest.SimpleRegistryPuller doesn't compile with GCC 5.1.1","""GCC 5.1.1 has {{-Werror=sign-compare}} in {{-Wall}} and stumbles over a comparison between signed and unsigned int in {{provisioner_docker_tests.cpp}}."""
"MESOS-4021","Improvement","master",1,"Remove quota from Registry for quota remove request","""When a remove quota requests hits the endpoint and passes validation, quota should be removed from the registry before the allocator is notified about the change."""
"MESOS-4047","Bug","test",1,"MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky","""{code:title=Output from passed test} [----------] 1 test from MemoryPressureMesosTest 1+0 records in 1+0 records out 1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s [ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0 I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 Registered executor on ubuntu Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162 sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done' Forked command at 5085 I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0 Re-registered executor on ubuntu Shutting down Sending SIGTERM to process tree at pid 5085 Killing the following process trees: [  -+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done   \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp  ] [       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms)   Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent."""
"MESOS-4046","Improvement","docker",3,"Enable `Env` specified in docker image can be returned from docker pull","""Currently docker pull only return an image structure, which only contains entrypoint info. We have docker inspect as a subprocess inside docker pull, which contains many other useful information of a docker image. We should be able to support returning environment variables information from the image."""
"MESOS-4066","Task","agent",3,"Agent should not return partial state when a request is made to /state endpoint during recovery.","""Currently when a user is hitting /state.json on the agent, it may return partial state if the agent has failed over and is recovering. There is currently no clear way to tell if this is the case when looking at a response, so the user may incorrectly interpret the agent as being empty of tasks.  We could consider exposing the 'state' enum of the agent in the endpoint:    This may be a bit tricky to maintain as far as backwards-compatibility of the endpoint, if we were to alter this enum.  Exposing this would allow users to be more informed about the state of the agent."""
"MESOS-4058","Improvement","master",1,"Do not use `Resource.role` for resources in quota request.","""To be consistent with other operator endpoints and to adhere to the principal of least surprise, move role from each {{Resource}} in quota set request to the request itself.   {{Resource.role}} is used for reserved resources. Since quota is not a direct reservation request, to avoid confusion we shall not reuse this field for communicating the role for which quota should be reserved.  Food for thought: Shall we try to keep internal storage protobufs as close as possible to operator's JSON to provide some sort of a schema or decouple those two for the sake of flexibility?"""
"MESOS-4069","Bug","libprocess",8,"libevent_ssl_socket assertion fails ","""Have been seeing the following socket  receive error frequently:    In this case this was a HTTP get over SSL. The url being:  https://dseasb33srnrn.cloudfront.net:443/registry-v2/docker/registry/v2/blobs/sha256/44/44be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58f/data?Expires=1449259252&Signature=Q4CQdr1LbxsiYyVebmetrx~lqDgQfHVkGxpbMM3PoISn6r07DXIzBX6~tl1iZx9uXdfr~5awH8Kxwh-y8b0dTV3mLTZAVlneZlHbhBAX9qbYMd180-QvUvrFezwOlSmX4B3idvo-zK0CarUu3Ev1hbJz5y3olwe2ZC~RXHEwzkQ_&Key-Pair-Id=APKAJECH5M7VWIS5YZ6Q   *Steps to reproduce:*  1. Run master 2. Run slave from your build directory as  as:     3. Run mesos-execute from your build directory as :  """
"MESOS-4090","Improvement","build",5,"Create light-weight executor only and scheduler only mesos eggs","""Currently, when running tasks in docker containers, if the executor uses the mesos.native python library, the execution environment inside the container (OS, native libs, etc) must match the execution environment outside the container fairly closely in order to load the mesos.so library.  The solution here can be to introduce a much lighter weight python egg, mesos.executor, which only includes code (and dependencies) needed to create and run an MesosExecutorDriver.  Executors can then use this native library instead of mesos.native."""
"MESOS-4088","Task","modules",2,"Modularize existing plain-file logging for executor/task logs launched with the Mesos Containerizer","""Once a module for executor/task output logging has been introduced, the default module will mirror the existing behavior.  Executor/task stdout/stderr is piped into files within the executor's sandbox directory.  The files are exposed in the web UI, via the {{/files}} endpoint."""
"MESOS-4087","Task","containerization|modules",5,"Introduce a module for logging executor/task output","""Existing executor/task logs are logged to files in their sandbox directory, with some nuances based on which containerizer is used (see background section in linked document).  A logger for executor/task logs has the following requirements: * The logger is given a command to run and must handle the stdout/stderr of the command. * The handling of stdout/stderr must be resilient across agent failover.  Logging should not stop if the agent fails. * Logs should be readable, presumably via the web UI, or via some other module-specific UI."""
"MESOS-4099","Bug","build|libprocess",1,"parallel make tests does not build all test targets","""When inside 3rdparty/libprocess: Running {{make -j8 tests}} from a clean build does not yield the {{libprocess-tests}} binary. Running it a subsequent time triggers more compilation and ends up yielding the {{libprocess-tests}} binary. This suggests the {{test}} target is not being built correctly."""
"MESOS-4102","Bug","allocation",5,"Quota doesn't allocate resources on slave joining.","""See attached patch. {{framework1}} is not allocated any resources, despite the fact that the resources on {{agent2}} can safely be allocated to it without risk of violating {{quota1}}. If I understand the intended quota behavior correctly, this doesn't seem intended.  Note that if the framework is added _after_ the slaves are added, the resources on {{agent2}} are allocated to {{framework1}}."""
"MESOS-4112","Task","libprocess|test",2,"Clean up libprocess gtest macros","""This ticket is regarding the libprocess gtest helpers in {{3rdparty/libprocess/include/process/gtest.hpp}}.  The pattern in this file seems to be a set of macros:  * {{AWAIT_ASSERT_<STATE>_FOR}} * {{AWAIT_ASSERT_<STATE>}} -- default of 15 seconds * {{AWAIT_<STATE>\_FOR}} -- alias for {{AWAIT_ASSERT_<STATE>_FOR}} * {{AWAIT_<STATE>}} -- alias for {{AWAIT_ASSERT_<STATE>}} * {{AWAIT_EXPECT_<STATE>_FOR}} * {{AWAIT_EXPECT_<STATE>}} -- default of 15 seconds  (1) {{AWAIT_EQ_FOR}} should be added for completeness.  (2) In {{gtest}}, we've got {{EXPECT_EQ}} as well as the {{bool}}-specific versions: {{EXPECT_TRUE}} and {{EXPECT_FALSE}}.  We should adopt this pattern in these helpers as well. Keeping the pattern above in mind, the following are missing:  * {{AWAIT_ASSERT_TRUE_FOR}} * {{AWAIT_ASSERT_TRUE}} * {{AWAIT_ASSERT_FALSE_FOR}} * {{AWAIT_ASSERT_FALSE}} * {{AWAIT_EXPECT_TRUE_FOR}} * {{AWAIT_EXPECT_FALSE_FOR}}  (3) There are HTTP response related macros at the bottom of the file, e.g. {{AWAIT_EXPECT_RESPONSE_STATUS_EQ}}, however these are missing their {{ASSERT}} counterparts.  -(4) The reason for (3) presumably is because we reach for {{EXPECT}} over {{ASSERT}} in general due to the test suite crashing behavior of {{ASSERT}}. If this is the case, it would be worthwhile considering whether macros such as {{AWAIT_READY}} should alias {{AWAIT_EXPECT_READY}} rather than {{AWAIT_ASSERT_READY}}.-  (5) There are a few more missing macros, given {{AWAIT_EQ_FOR}} and {{AWAIT_EQ}} which aliases to {{AWAIT_ASSERT_EQ_FOR}} and {{AWAIT_ASSERT_EQ}} respectively, we should also add {{AWAIT_TRUE_FOR}}, {{AWAIT_TRUE}}, {{AWAIT_FALSE_FOR}}, and {{AWAIT_FALSE}} as well."""
"MESOS-4110","Bug","stout",5,"Implement `WindowsError` to correspond with `ErrnoError`.","""In the C standard library, `errno` records the last error on a thread. You can pretty-print it with `strerror`.  In Stout, we report these errors with `ErrnoError`.  The Windows API has something similar, called `GetLastError()`. The way to pretty-print this is hilariously unintuitive and terrible, so in this case it is actually very beneficial to wrap it with something similar to `ErrnoError`, maybe called `WindowsError`."""
"MESOS-4109","Bug","libprocess|test",1,"HTTPConnectionTest.ClosingResponse is flaky","""Output of the test: """
"MESOS-4108","Bug","stout",5,"Implement `os::mkdtemp` for Windows","""Used basically exclusively for testing, this insecure and otherwise-not-quite-suitable-for-prod function needs to work to run what will eventually become the FS tests."""
"MESOS-4107","Bug","stout",1,"`os::strerror_r` breaks the Windows build","""`os::strerror_r` does not exist on Windows."""
"MESOS-4137","Task","docker|modules",3,"Modularize plain-file logging for executor/task logs launched with the Docker Containerizer","""Adding a hook inside the Docker containerizer is slightly more involved than the Mesos containerizer.  Docker executors/tasks perform plain-file logging in different places depending on whether the agent is in a Docker container itself || Agent || Code || | Not in container | {{DockerContainerizerProcess::launchExecutorProcess}} | | In container | {{Docker::run}} in a {{mesos-docker-executor}} process |  This means a {{ContainerLogger}} will need to be loaded or hooked into the {{mesos-docker-executor}}.  Or we will need to change how piping in done in {{mesos-docker-executor}}."""
"MESOS-4136","Improvement","modules",3,"Add a ContainerLogger module that restrains log sizes","""One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).  We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.  This will be a non-default module which will also serve as an example for how to implement the module."""
"MESOS-4130","Documentation","fetcher",1,"Document how the fetcher can reach across a proxy connection.","""The fetcher uses libcurl for downloading content from HTTP, HTTPS, etc. There is no source code in the pertinent parts of """"net.hpp"""" that deals with proxy settings. However, libcurl automatically picks up certain environment variables and adjusts its settings accordingly. See """"man libcurl-tutorial"""" for details. See section """"Proxies"""", subsection """"Environment Variables"""". If you follow this recipe in your Mesos agent startup script, you can use a proxy.   We should document this in the fetcher (cache) doc (http://mesos.apache.org/documentation/latest/fetcher/). """
"MESOS-4128","Improvement","allocation",3,"Refactor sorter factories in allocator and improve comments around them.","""For clarity we want to refactor the factory section in the allocator and explain the purpose (and necessity) of all sorters."""
"MESOS-4143","Bug","master",2,"Reserve/UnReserve Dynamic Reservation Endpoints allow reservations on non-existing roles","""When working with Dynamic reservations via the /reserve and /unreserve endpoints, it is possible to reserve resources for roles that have not been specified via the --roles flag on the master.  However, these roles are not usable because the roles have not been defined, nor are they added to the list of roles available.   Per the mailing list, changing roles after the fact is not possible at this time. (That may be another JIRA), more importantly, the /reserve and /unreserve end points should not allow reservation of roles not specified by --roles.  """
"MESOS-4150","Task","modules",3,"Implement container logger module metadata recovery","""The {{ContainerLoggers}} are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.  For default {{ContainerLogger}} s, like the {{SandboxContainerLogger}} and the (tentatively named) {{TruncatingSandboxContainerLogger}}, the log files are exposed during agent recovery regardless.  For non-default {{ContainerLogger}} s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  This can be implemented as part of {{Containerizer::recover}}."""
"MESOS-4154","Bug","security",2,"Rename shutdown_frameworks to teardown_frameworks","""The mesos is now using teardown framework to shutdown a framework but the acls are still using shutdown_framework, it is better to rename shutdown_framework to teardown_framework for acl to keep consistent.  This is a post review request for https://reviews.apache.org/r/40829/"""
"MESOS-4183","Improvement","build",3,"Move operator<< definitions to .cpp files and include <iosfwd> in .hpp where possible.","""We often include complex headers like {{<ostream>}} in """".hpp"""" files to define {{operator<<()}} inline (e.g. """"mesos/authorizer/authorizer.hpp""""). Instead, we can move definitions to corresponding """".cpp"""" files and replace stream headers with {{iosfwd}}, for example, this is partially done for {{URI}} in """"mesos/uri/uri.hpp""""."""
"MESOS-4192","Bug","documentation",3,"Add documentation for API Versioning","""Currently, we don't have any documentation for:  - How Mesos implements API versioning ? - How are protobufs versioned and how does mesos handle them internally ? - What do contributors need to do when they make a change to a external user facing protobuf ?  The relevant design doc: https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b """
"MESOS-4190","Documentation","documentation",3,"Create a Design Doc for dynamic weights.","""A short design doc for dynamic weights, it will focus on /weights API and the changes to the allocator API."""
"MESOS-4186","Improvement","containerization",2,"Serialize docker v1 image spec as protobuf","""Currently we only support v2 docker manifest serialization method. When we read docker image spec locally from disk, we should be able to parse v1 docker manifest as protobuf, which will make it easier to gather runtime config and other necessary info."""
"MESOS-4206","Documentation","documentation",3,"Write new logging-related documentation","""This should include: * Default logging behavior for master, agent, framework, executor, task. * Master/agent: ** A summary of log-related flags. ** {{glog}} specific options. * Separation of master/agent logs from container logs. * The {{ContainerLogger}} module."""
"MESOS-4200","Task","allocation|test",2,"Test case(s) for weights + allocation behavior","""As far as I can see, we currently have NO test cases for behavior when weights are defined."""
"MESOS-4209","Documentation","documentation",3,"Document ""how to program with dynamic reservations and persistent volumes""","""Specifically, some of the gotchas around:  * Retrying reservation attempts after a timeout * Fuzzy-matching resources to determine whether a reservation/PV is successful * Represent client state as a state machine and repeatedly move """"toward"""" successful terminate stats  Should also point to persistent volume example framework. We should also ask Gabriel and others (Arango?) who have built frameworks with PVs/DRs for feedback."""
"MESOS-4207","Documentation","documentation",2,"Add an example bug due to a lack of defer() to the defer() documentation","""In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation."""
"MESOS-4229","Bug","test",3,"Docker containers left running on disk after reviewbot builds","""The Mesos Reviewbot builds recently failed due to Docker containers being left running on the disk, eventually leading to a full disk: https://issues.apache.org/jira/browse/INFRA-10984  These containers should be automatically cleaned up to avoid this problem in the future."""
"MESOS-4227","Improvement","containerization",1,"Enable passing docker image cmd runtime config to provisioner","""Cmd is the command to run when starting a container. We should be able to collect Cmd config information from a docker image, and pass it back to provisioner."""
"MESOS-4226","Improvement","containerization",1,"Enable passing docker image environment variables runtime config to provisioner","""Collect environment variables runtime config information from a docker image, and save as a map. Pass it back to provisioner, and handling environment variables merge issue."""
"MESOS-4225","Improvement","containerization",2,"Exposed docker/appc image manifest to mesos containerizer.","""Collect docker image manifest from disk(which contains all runtime configurations), and pass it back to provisioner, so that mesos containerizer can grab all necessary info from provisioner."""
"MESOS-4222","Documentation","containerization",3,"Document containerizer from user perspective.","""Add documentation that covers:  * Purpose of containerizers from a use case perspective. * What purpose does each containerizer (mesos. docker, compose) serve. * What criteria could be used to choose a containerizer."""
"MESOS-4241","Improvement","containerization",3,"Consolidate docker store slave flags","""Currently there are too many slave flags for configuring the docker store/puller. We can remove the following flags:  docker_auth_server_port docker_local_archives_dir docker_registry_port docker_puller  And consolidate them into the existing flags."""
"MESOS-4262","Improvement","containerization",5,"Enable net_cls subsytem in cgroup infrastructure","""Currently the control group infrastructure within mesos supports only the memory and CPU subsystems. We need to enhance this infrastructure to support the net_cls subsystem as well. Details of the net_cls subsystem and its use-cases can be found here: https://www.kernel.org/doc/Documentation/cgroups/net_cls.txt  Enabling the net_cls will allow us to provide operators to, potentially, regulate framework traffic on a per-container basis.  """
"MESOS-4261","Improvement","containerization",3,"Remove docker auth server flag","""We currently use a configured docker auth server from a slave flag to get token auth for docker registry. However this doesn't work for private registries as docker registry supports sending down the correct auth server to contact.  We should remove docker auth server flag completely and ask the docker registry for auth server."""
"MESOS-4289","Task","containerization",5,"Design doc for simple appc image discovery","""Create a design document describing the following:  - Model and abstraction of the Discoverer - Workflow of the discovery process """
"MESOS-4285","Bug","containerization",3,"Mesos command task doesn't support volumes with image","""Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. """
"MESOS-4284","Story","master",8,"Draft design doc for multi-role frameworks","""Create a document that describes the problems with having only single-role frameworks and proposes an MVP solution and implementation approach."""
"MESOS-4282","Bug","containerization",2,"Update isolator prepare function to use ContainerLaunchInfo","""Currently we have the isolator's prepare function returning ContainerPrepareInfo protobuf. We should enable ContainerLaunchInfo (contains environment variables, namespaces, etc.) to be returned which will be used by Mesos containerize to launch containers.   By doing this (ContainerPrepareInfo -> ContainerLaunchInfo), we can select any necessary information and passing then to launcher."""
"MESOS-4279","Bug","containerization|docker",5,"Docker executor truncates task's output when the task is killed.","""I'm implementing a graceful restarts of our mesos-marathon-docker setup and I came to a following issue:  (it was already discussed on https://github.com/mesosphere/marathon/issues/2876 and guys form mesosphere got to a point that its probably a docker containerizer problem...) To sum it up:  When i deploy simple python script to all mesos-slaves: {code} #!/usr/bin/python  from time import sleep import signal import sys import datetime  def sigterm_handler(_signo, _stack_frame):     print """"got %i"""" % _signo     print datetime.datetime.now().time()     sys.stdout.flush()     sleep(2)     print datetime.datetime.now().time()     print """"ending""""     sys.stdout.flush()     sys.exit(0)  signal.signal(signal.SIGTERM, sigterm_handler) signal.signal(signal.SIGINT, sigterm_handler)  try:     print """"Hello""""     i = 0     while True:         i += 1         print datetime.datetime.now().time()         print """"Iteration #%i"""" % i         sys.stdout.flush()         sleep(1) finally:     print """"Goodbye"""" {code}  and I run it through Marathon like {code:javascript} data = {  args: [""""/tmp/script.py""""],  instances: 1,  cpus: 0.1,  mem: 256,  id: """"marathon-test-api"""" } {code}  During the app restart I get expected result - the task receives sigterm and dies peacefully (during my script-specified 2 seconds period)  But when i wrap this python script in a docker: {code} FROM node:4.2  RUN mkdir /app ADD . /app WORKDIR /app ENTRYPOINT [] {code} and run appropriate application by Marathon: {code:javascript} data = {  args: [""""./script.py""""],  container: {   type: """"DOCKER"""",   docker: {    image: """"bydga/marathon-test-api""""   },   forcePullImage: yes  },  cpus: 0.1,  mem: 256,  instances: 1,  id: """"marathon-test-api"""" } {code}  The task during restart (issued from marathon) dies immediately without having a chance to do any cleanup. """
"MESOS-4275","Bug","stout",2,"Duration uses fixed-width types inconsistently","""The implementation of the {{Duration}} class correctly uses fixed-width types (here {{int64_t}}) for portability internally, but uses {{long}} types in a few places (in particular {{LLONG_MIN}} and {{LLONG_MAX}}). This is inconsistent on 64-bit platforms, and probably incorrect on 32-bit as there {{long}} is 32 bit wide.  Additionally, the longer {{Duration}} types ({{Minutes}}, {{Hours}}, {{Days}}, and {{Weeks}}) construct from {{int32_t}}, while shorter ones take {{int64_t}}. Probably as a left-over this is matched with a redundant {{Duration}} constructor taking an {{int32_t}} value where the other one taking an {{int64_t}} value would be sufficient. It should be safe to just construct from {{int64_t}} in all places."""
"MESOS-4273","Improvement","stout",1,"Replace variadic List constructor with one taking a initializer_list","""{{List}} provides a variadic constructor currently implemented with some preprocessor magic. Given that we already require C++11 we can replace that one with a much simpler one just taking a {{std::initializer_list}}. This would change the invocations,   This addresses an existing {{TODO}}. """
"MESOS-4272","Bug","test",1,"DurationTest.Arithmetic performs inexact float calculation in test","""{{DurationTest.Arithmetic}} does a calculation with not exactly representable floating point values and also performs an equality check,  Here neither the value {{3.3}} nor {{0.33}} cannot be represented exactly as a floating point number so the check might fail incorrectly (as it does e.g. when compiling and executing the test under 32-bit on Debian8).  Instead we should just use exactly representable values to make sure the test will succeed as long as the implementation behaves as expected."""
"MESOS-4295","Task","documentation",3,"Change documentation links to ""*.md""","""Right now, links either use the form  or . We should probably switch to using the latter form consistently -- it previews better on Github, and it will make it easier to have multiple versions of the docs on the website at once in the future."""
"MESOS-4294","Bug","stout",1,"Protobuf parse should support parsing JSON object containing JSON Null.","""(This bug was exposed by MESOS-4184, when serializing docker v1 image manifest as protobuf).  Currently protobuf::parse returns failures when parsing any JSON containing JSON::Null. If we have any protobuf field set as `JSON::Null`, any other non-repeated field cannot capture their value. For example, assuming we have a protobuf message:   If there exists any field containing JSON::Null, like below:  When we do protobuf::parse, it would return the following failure: """
"MESOS-4292","Task","test",3,"Tests for quota with implicit roles.","""With the introduction of implicit roles (MESOS-3988), we should make sure quota can be set for an inactive role (unknown to the master) and maybe transition it to the active state."""
"MESOS-4301","Bug","master",1,"Accepting an inverse offer prints misleading logs","""Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs:   Inverse offers should not trigger this warning."""
"MESOS-4300","Task","master|security",3,"Add AuthN and AuthZ to maintenance endpoints.","""Maintenance endpoints are currently only restricted by firewall settings.  They should also support authentication/authorization like other HTTP endpoints."""
"MESOS-4304","Bug","fetcher",1,"hdfs operations fail due to prepended / on path for non-hdfs hadoop clients.","""This bug was resolved for the hdfs protocol for MESOS-3602 but since the process checks for the """"hdfs"""" protocol at the beginning of the URI, the fix does not extend itself to non-hdfs hadoop clients.   After a brief chat with [~jieyu], it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it."""
"MESOS-4311","Bug","stout",1,"Protobuf parse should pass error messages when parsing nested JSON.","""Currently when protobuf::parse handles nested JSON objects, it cannot pass any error message out. We should enable showing those error messages."""
"MESOS-4336","Documentation","documentation|fetcher",1,"Document supported file types for archive extraction by fetcher","""The Mesos fetcher extracts specified URIs if requested to do so by the scheduler. However, the documentation at http://mesos.apache.org/documentation/latest/fetcher/ doesn't list the file types /extensions that will be extracted by the fetcher.  [The relevant code|https://github.com/apache/mesos/blob/master/src/launcher/fetcher.cpp#L63] specifies an exhaustive list of extensions that will be extracted, the documentation should be updated to match."""
"MESOS-4333","Improvement","containerization",2,"Refactor Appc provisioner tests  ","""Current tests can be refactored so that we can reuse some common tasks like test image creation. This will benefit future tests like appc image puller tests."""
"MESOS-4329","Bug","test",1,"SlaveTest.LaunchTaskInfoWithContainerInfo cannot be execute in isolation","""Executing {{SlaveTest.LaunchTaskInfoWithContainerInfo}} from {{468b8ec}} under OS X 10.10.5 in isolation fails due to missing cleanup,  """
"MESOS-4349","Bug","test",1,"GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor","""  Occurs non-deterministically for me on OSX 10.10, perhaps one run in ten."""
"MESOS-4348","Bug","test",1,"GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator","""  Occurs non-deterministically for me. OSX 10.10."""
"MESOS-4347","Bug","test",1,"GMock warning in ReservationTest.ACLMultipleOperations","""  Seems to occur non-deterministically for me, maybe once per 50 runs or so. OSX 10.10"""
"MESOS-4345","Task","containerization",3,"Implement a network-handle manager for net_cls cgroup subsystem","""As part of implementing the net_cls cgroup isolator we need a mechanism to manage the minor handles that will be allocated to containers when they are associated with a net_cls cgroup. The network-handle manager needs to provide the following functionality:  a) During normal operation keep track of the free and allocated network handles. There can be a total of 64K such network handles. b) On startup, learn the allocated network handle by walking the net_cls cgroup tree for mesos and build a map of free network handles available to the agent. """
"MESOS-4344","Improvement","containerization",1,"Allow operators to assign net_cls major handles to mesos agents","""The net_cls cgroup associates a 16-bit major and 16-bit minor network handle to packets originating from tasks associated with a specific net_cls cgroup. In mesos we need to give the operator the ability to fix the 16-bit major handle used in an agent (the minor handle will be allocated by the agent. See MESOS-4345). Fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy (say DENY ALL) for all container traffic till the container is allocated a minor handle.   A simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup. """
"MESOS-4338","Bug","containerization",5,"Create utilities for common shell commands used.","""We spawn shell for command line utilities like tar, untar, sha256 etc. Would be great for resuse if we can create a common utilities class/file for all these utilities.  """
"MESOS-4358","Task","containerization",2,"Expose net_cls network handles in agent's state endpoint","""We need to expose net_cls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy.   In order to achieve the above we need to add a new field in the `NetworkInfo` protobuf (say NetHandles) and update this field when a container gets assigned to a net_cls cgroup. The `ContainerStatus` protobuf already has the `NetworkInfo` protobuf as a nested message, and the `ContainerStatus` itself is exposed to operators as part of TaskInfo (for tasks associated with the container) in an agent's state.json. """
"MESOS-4357","Bug","test",1,"GMock warning in RoleTest.ImplicitRoleStaticReservation",""""""
"MESOS-4353","Improvement","libprocess",1,"Limit the number of processes created by libprocess","""Currently libprocess will create {{max(8, number of CPU cores)}} processes during the initialization, see https://github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#L2146 for details. This should be OK for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an IBM Power machine may have 192 cores), this will cause too much worker threads which are not necessary.  And since libprocess is widely used in Mesos (master, agent, scheduler, executor), it may also cause some performance issue. For example, when user creates a Docker container via Mesos in a Mesos agent which is running on a powerful machine with 192 cores, the DockerContainerizer in Mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. And if user creates 1000 Docker containers in that machine, then there will be 1000 executors, i.e., 1000 * 192 worker threads which is a large number and may thrash the OS.  """
"MESOS-4385","Bug","framework|master",2,"Offers and InverseOffers cannot be accepted in the same ACCEPT call","""*Problem* * In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call. * If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)  Here's a regression test: https://reviews.apache.org/r/42092/  *Proprosal* The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.  Arguments for mixing: * The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers. * Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.  Arguments against mixing: * Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a """"reason""""? * What happens if we presumably add a third type of offer? * Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?"""
"MESOS-4383","Bug","containerization",2,"Support docker runtime configuration env var from image.","""We need to support env var configuration returned from docker image in mesos containerizer."""
"MESOS-4381","Documentation","documentation",3,"Improve upgrade compatibility documentation.","""Investigate and document upgrade compatibility for 0.27 release."""
"MESOS-4377","Documentation","documentation",1,"Document units associated with resource types","""We should document the units associated with memory and disk resources."""
"MESOS-4376","Documentation","documentation|scheduler driver",2,"Document semantics of `slaveLost`","""We should clarify the semantics of this callback:  * Is it always invoked, or just a hint? * Can a slave ever come back from `slaveLost`? * What happens to persistent resources on a lost slave?  The new HA framework development guide might be a good place to put (some of?) this information. """
"MESOS-4368","Improvement","allocation",3,"Make HierarchicalAllocatorProcess set a Resource's active role during allocation","""The concrete implementation here depends on the implementation strategy used to solve MESOS-4367."""
"MESOS-4367","Improvement","master",5,"Add tracking of the role a Resource was offered for","""If a framework can have multiple roles, we need a way to identify for which of the framework's role a resource was offered for (e.g., for resource recovery and reconciliation)."""
"MESOS-4365","Improvement","master",3,"Add internal migration from role to roles to master","""If only the {{role}} field is given, add it as single entry to {{roles}}. Add a note to {{CHANGELOG}}/release notes on deprecation of the existing {{role}} field. File a JIRA issue for removal of that migration code once the deprecation cycle is over. """
"MESOS-4364","Improvement","master",5,"Add roles validation code to master","""A {{FrameworkInfo}} can only have one of role or roles. A natural location for this appears to be under {{validation::operation::validate}}."""
"MESOS-4363","Improvement","framework|master",1,"Add a roles field to FrameworkInfo","""To represent multiple roles per framework a new repeated string field for roles is needed."""
"MESOS-4411","Bug","allocation",3,"Traverse all roles for quota allocation.","""There might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. We need to investigate this behavior."""
"MESOS-4410","Improvement","master",3,"Introduce protobuf for quota set request.","""To document quota request JSON schema and simplify request processing, introduce a {{QuotaRequest}} protobuf wrapper."""
"MESOS-4417","Bug","allocation",3,"Prevent allocator from crashing on successful recovery.","""There might be a bug that may crash the master as pointed out by [~bmahler] in https://reviews.apache.org/r/42222/: """
"MESOS-4421","Task","documentation|master",3,"Document that /reserve, /create-volumes endpoints can return misleading ""success""","""The docs for the {{/reserve}} endpoint say:    This is not true: the master returns {{200}} when the request has been validated and a {{CheckpointResourcesMessage}} has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. Same behavior applies to {{/unreserve}}, {{/create-volumes}}, and {{/destroy-volumes}}.  We should _either_:  1. Accurately document what {{200}} return code means. 2. Change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the HTTP client."""
"MESOS-4434","Bug","build|modules",3,"Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos","""Mesos modules depend on having these packages installed with the exact version as Mesos was compiled with."""
"MESOS-4425","Bug","libprocess",3,"Introduce filtering test abstractions for HTTP events to libprocess","""We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.  The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}."""
"MESOS-4439","Task","containerization",1,"Fix appc CachedImage image validation","""Currently image validation is done assuming that the image's filename will have  digest (SHA-512) information. This is not part of the spec     (https://github.com/appc/spec/blob/master/spec/discovery.md).          The spec specifies the tuple <image name, labels> as unique identifier for  discovering an image. """
"MESOS-4438","Task","containerization",1,"Add 'dependency' message to 'AppcImageManifest' protobuf.","""AppcImageManifest protobuf currently lacks 'dependencies' which is necessary for image discovery."""
"MESOS-4437","Task","containerization",1,"Disable the test RegistryClientTest.BadTokenServerAddress.","""As we are retiring registry client, disable this test which looks flaky."""
"MESOS-4435","Task","master",3,"Update `Master::Http::stateSummary` to use `jsonify`.","""Update {{state-summary}} to use {{jsonify}} to stay consistent with {{state}} HTTP endpoint."""
"MESOS-4452","Documentation","documentation",2,"Improve documentation around roles, principals, authz, and reservations","""* What is the difference between a role and a principal? * Why do some ACL entities reference """"roles"""" but others reference """"principals""""? In a typical organization, what real-world entities would my roles vs. principals map to? The ACL documentation could use more information about the motivation of ACLs and examples of configuring ACLs to meet real-world security policies. * We should give some examples of making reservations when the role and principal are different, and why you would want to do that * We should add an example to the ACL page that includes setting ACLs for reservations and/or persistent volumes"""
"MESOS-4454","Bug","containerization",2,"Create common sha512 compute utility function.","""Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. """
"MESOS-4490","Improvement","containerization",3,"Get container status information in slave. ","""As part of MESOS-4487 an interface will be introduce into the `Containerizer` to allow agents to retrieve container state information. The agent needs to use this interface to retrieve container state information during status updates from the executor. The container state information can be then use by the agent to expose various isolator specific configuration (for e.g., IP address allocated by network isolators, net_cls handles allocated by `cgroups/net_cls` isolator), that has been applied to the container, in the state.json endpoint.  """
"MESOS-4489","Improvement","containerization",1,"The `cgroups/net_cls` isolator needs to expose handles in the ContainerStatus","""The `cgroup/net_cls` isolator is responsible for allocating network handles to containers launched within a net_cls cgroup. The `cgroup/net_cls` isolator needs to expose these handles to the containerizer as part of the `ContainerStatus` when the containerizer queries the status() method of the isolator. The information itself will go as part of a `CgroupInfo` protobuf that will be defined as part of MESOS-4488 .  """
"MESOS-4488","Improvement","containerization",1,"Define a CgroupInfo protobuf to expose cgroup isolator configuration.","""Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem. The isolators apply subsystem specific configuration on the containers before launching the containers. For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares.   Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer. """
"MESOS-4487","Improvement","containerization",2,"Introduce status() interface in `Containerizer`","""In the Containerizer, during container isolation, the isolators end up modifying the state of the containers. Examples would be IP address allocation to a container by the 'network isolator, or net_cls handle allocation by the cgroup/net_cls isolator.   Often times the state of the container, needs to be exposed to operators through the state.json end-point. For e.g. operators or frameworks might want to know the IP-address configured on a particular container, or the net_cls handle associated with a container to configure the right TC rules. However, at present, there is no clean interface for the slave to retrieve the state of a container from the Containerizer for any of the launched containers. Thus, we need to introduce a `status` interface in the `Containerizer` base class, in order for the slave to expose container state information in its state.json.   """
"MESOS-4505","Improvement","allocation",3,"Hierarchical allocator performance is slow due to Quota","""Since we do not strip the non-scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.  One approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. This is valid as quota can currently only be set for scalar resource types."""
"MESOS-4520","Improvement","containerization",1,"Introduce a status() interface for isolators","""While launching a container mesos isolators end up configuring/modifying various properties of the container. For e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. Similary network isolator (net-modules, port mapping) configure the IP address and ports associated with a container.   Currently, there are not interface in the isolator to extract the run time state of these properties for a given container. Therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. """
"MESOS-4517","Bug","containerization",3,"Introduce docker runtime isolator.","""Currently docker image default configuration are included in `ProvisionInfo`. We should grab necessary config from `ProvisionInfo` into `ContainerInfo`, and handle all these runtime informations inside of docker runtime isolator. Return a `ContainerLaunchInfo` containing `working_dir`, `env` and merged `commandInfo`, etc."""
"MESOS-4512","Bug","master",3,"Render quota status consistently with other endpoints.","""Currently quota status endpoint returns a collection of {{QuotaInfo}} protos converted to JSON. An example response looks like this:   Presence of some fields, e.g. """"role"""", is misleading. To address this issue and make the output more informative, we should probably introduce a  {{model()}} function for {{QuotaStatus}}."""
"MESOS-4535","Bug","modules",1,"Logrotate ContainerLogger may not handle FD ownership correctly","""One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.  The way the logrotate module uses this is slightly incorrect: # The module starts a subprocess with an output {{Subprocess::PIPE()}}. # That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}. # When the second subprocess starts, the pipe's FD is closed in the parent. # When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD."""
"MESOS-4530","Bug","containerization",1,"NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky","""While running the command  One eventually gets the following output: """
"MESOS-4529","Task","allocation",2,"Update the allocator to not offer unreserved resources beyond quota.","""Eventually, we will want to offer unreserved resources as revocable beyond the role's quota. Rather than offering non-revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota."""
"MESOS-4528","Task","allocation",2,"Account for reserved resources in the quota guarantee check.","""Reserved resources should be accounted for in the quota guarantee check so that frameworks cannot continually reserve resources to pull them out of the quota pool."""
"MESOS-4527","Task","allocation",5,"Roles can exceed limit allocation via reservations.","""Since unallocated reservations are not accounted towards the guarantee (which today is also a limit), we might exceed the limit."""
"MESOS-4526","Task","allocation",1,"Include the allocated portion of reserved resources in the role sorter for DRF.","""Reserved resources should be accounted for fairness calculation whether they are allocated or not, since they model a long or forever running task. That is, the effect of reserving resources is equivalent to launching a task in that the resources that make up the reservation are not available to other roles as non-revocable.  In the short-term, we should at least account for the allocated portion of the reservation."""
"MESOS-4546","Bug","agent",3,"Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect","""Sample Mesos Agent log: https://gist.github.com/brndnmtthws/fb846fa988487250a809  Note, zookeeper has a function to change the list of servers at runtime: https://github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#L1207-L1232  This comes up when using an AWS AutoScalingGroup for managing the set of masters.   The agent when it comes up the first time, resolves the zk:// string. Once all the hosts that were in the original string fail (Each fails, is replaced by a new machine, which has the same DNS name), the agent just keeps spinning in an internal loop, never re-resolving the DNS names.  Two solutions I see are  1. Update the list of servers / re-resolve 2. Have the agent detect it hasn't connected recently, and kill itself (Which will force a re-resolution when the agent starts back up)"""
"MESOS-4542","Bug","allocation|master|test",3,"MasterQuotaTest.AvailableResourcesAfterRescinding is flaky.","""Can be reproduced by running {{GLOG_v=1 GTEST_FILTER=""""MasterQuotaTest.AvailableResourcesAfterRescinding"""" ./bin/mesos-tests.sh --gtest_shuffle --gtest_break_on_failure --gtest_repeat=1000 --verbose}}.  h5. Verbose log from a bad run:   h5. Verbose log from a good run: """
"MESOS-4562","Bug","webui",2,"Mesos UI shows wrong count for ""started"" tasks","""The task started field shows the number of tasks in state """"TASKS_STARTING"""" as opposed to those in """"TASK_RUNNING"""" state."""
"MESOS-4564","Improvement","containerization",2,"Separate Appc protobuf messages to its own file.","""It would be cleaner to keep the Appc protobuf messages separate from other mesos messages."""
"MESOS-4566","Improvement","stout",1,"Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`.","""A few of the critical code paths in {{jsonify}} involve unnecessary temporary string construction and copies (inherited from the {{JSON::*}}). For example, {{strings::trim}} is used to remove trailing 0s from printing {{double}}. We print {{double}} a lot, and therefore constructing a temporary {{std::string}} on printing of every double is extremely costly. This ticket captures the work involved in avoiding them."""
"MESOS-4576","Improvement","stout",2,"Introduce a stout helper for ""which""","""We may want to add a helper to {{stout/os.hpp}} that will natively emulate the functionality of the Linux utility {{which}}.  i.e.   This helper may be useful: * for test filters in {{src/tests/environment.cpp}} * a few tests in {{src/tests/containerizer/port_mapping_tests.cpp}} * the {{sha512}} utility in {{src/common/command_utils.cpp}} * as runtime checks in the {{LogrotateContainerLogger}} * etc."""
"MESOS-4573","Bug","HTTP API",5,"Design doc for scheduler HTTP Stream IDs","""This ticket is for the design of HTTP stream IDs, for use with HTTP schedulers. These IDs allow Mesos to distinguish between different instances of HTTP framework schedulers."""
"MESOS-4590","Task","master|test",2,"Add test case for reservations with same role, different principals","""We don't have a test case that covers $SUBJECT; we probably should."""
"MESOS-4584","Bug","project website",2,"Update Rakefile for mesos site generation","""The stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site."""
"MESOS-4596","Task","containerization",2,"Add common Appc spec utilities.",""" Add common utility functions such as :       - validating image information against actual data in the image directory.       - getting list of dependencies at depth 1 for an image.       - getting image path simple image discovery. """
"MESOS-4611","Bug","libprocess",5,"Passing a lambda to dispatch() always matches the template returning void","""The following idiom does not currently compile:    This seems non-intuitive because the following template exists for dispatch:    However, lambdas cannot be implicitly cast to a corresponding std::function<R()> type. To make this work, you have to explicitly type the lambda before passing it to dispatch.    We should add template support to allow lambdas to be passed to dispatch() without explicit typing.  """
"MESOS-4604","Bug","test",2,"ROOT_DOCKER_DockerHealthyTask is flaky.","""Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:  Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. """
"MESOS-4615","Bug","test",1,"ContainerLoggerTest.DefaultToSandbox is flaky","""Just saw this failure on the ASF CI:  """
"MESOS-4614","Bug","agent|HTTP API|test",3,"SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky","""Just saw this failure on the ASF CI:  """
"MESOS-4622","Documentation","documentation",1,"Update configuration.md with `--cgroups_net_cls_primary_handle` agent flag.","""As part of the net_cls epic, we introduce an agent flag called `--cgroup_net_cls_primary_handle` . We need to update configuration.md with the corresponding help string. """
"MESOS-4619","Bug","documentation",1,"Remove markdown files from doxygen pages","""The doxygen html pages corresponding to doc/* markdown files are redundant and have broken links. They don't serve any reasonable purpose in doxygen site."""
"MESOS-4626","Task","containerization",13,"Support Nvidia GPUs with filesystem isolation enabled in mesos containerizer.","""When filesystem isolation is enabled in the mesos containerizer, containers that use Nvidia GPU resources need access to GPU libraries residing on the host.  We'll need to provide a means for operators to inject the necessary volumes into *all* containers that use """"gpus"""" resources.  See the nvidia-docker project for more details: [nvidia-docker/tools/src/nvidia/volumes.go|https://github.com/NVIDIA/nvidia-docker/blob/fda10b2d27bf5578cc5337c23877f827e4d1ed77/tools/src/nvidia/volumes.go#L50-L103]"""
"MESOS-4625","Task","containerization",5,"Implement Nvidia GPU isolation w/o filesystem isolation enabled.","""The Nvidia GPU isolator will need to use the device cgroup to restrict access to GPU resources, and will need to recover this information after agent failover. For now this will require that the operator specifies the GPU devices via a flag.  To handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using GPU resources, we'll tackle this in a separate ticket."""
"MESOS-4624","Task","agent|master",1,"Add allocation metrics for ""gpus"" resources.","""Allocation metrics are currently hard-coded to include only {{\[""""cpus"""", """"mem"""", """"disk""""\]}} resources. We'll need to add """"gpus"""" to the list to start, possibly following up on the TODO to remove the hard-coding.  See: https://github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#L266-L269 https://github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#L123-L126 """
"MESOS-4623","Task","containerization",3,"Add a stub Nvidia GPU isolator.","""We'll first wire up a skeleton Nvidia GPU isolator, which needs to be guarded by a configure flag due to the dependency on NVML."""
"MESOS-4657","Improvement","containerization",1,"Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles.","""We need to add LOG(INFO) during the prepare phase of `cgroups/net_cls` for debugging management of `net_cls` handles within the isolator. """
"MESOS-4669","Bug","containerization",2,"Add common compression utility","""We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it."""
"MESOS-4667","Improvement","master",3,"Expose persistent volume information in HTTP endpoints","""The per-slave {{reserved_resources}} information returned by {{/state}} does not seem to include information about persistent volumes. This makes it hard for operators to use the {{/destroy-volumes}} endpoint."""
"MESOS-4671","Bug","containerization|HTTP API",1,"Status updates from executor can be forwarded out of order by the Agent.","""Previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received.   However, that seems to be no longer valid due to a recently introduced change in the agent:    This can sometimes lead to status updates being sent out of order depending on the order the {{Future}} is fulfilled from the call to {{status(...)}}."""
"MESOS-4674","Bug","flaky|test",3,"Linux filesystem isolator tests are flaky.","""LinuxFilesystemIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem sometimes fails on CentOS 7 with this kind of output:   LinuxFilesystemIsolatorTest.ROOT_MultipleContainers often has this output:   Whether SSL is configured makes no difference.  This test may also fail on other platforms, but more rarely.  """
"MESOS-4687","Improvement","master",5,"Implement reliable floating point for scalar resources","""Design doc: https://docs.google.com/document/d/14qLxjZsfIpfynbx0USLJR0GELSq8hdZJUWw6kaY_DXc/edit?usp=sharing"""
"MESOS-4684","Bug","containerization",3,"Create base docker image for test suite.","""This should be widely used for unified containerizer testing. Should basically include:  *at least one layer. *repositories.  For each layer: *root file system as a layer tar ball. *docker image json (manifest). *docker version."""
"MESOS-4683","Bug","documentation",2,"Document docker runtime isolator.","""Should include the following information:  *What features are currently supported in docker runtime isolator. *How to use the docker runtime isolator (user manual). *Compare the different semantics v.s. docker containerizer, and explain why."""
"MESOS-4695","Bug","test",1,"SlaveTest.StateEndpoint is flaky","""  Even though this test does {{Clock::pause()}} before starting the agent, there's a possibility that a numified-stringified double to not equal itself, even after rounding to the nearest int."""
"MESOS-4714","Bug","build",2,"""make DESTDIR=<path> install"" broken","""There is a missing '$(DESTDIR)' prefix in the install-data-hook that causes DESTDIR builds to be broken."""
"MESOS-4713","Task","reviewbot",2,"ReviewBot should not fail hard if there are circular dependencies in a review chain","""Instead of failing hard, ReviewBot should post an error to the review that a circular dependency is detected."""
"MESOS-4703","Bug","stout",1,"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)","""Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.  We should make a StoutConfigure.cmake that can be included by any package downstream."""
"MESOS-4702","Documentation","documentation",1,"Document default value of ""offer_timeout""","""There isn't a default value (i.e., offers do not timeout by default), but we should clarify this in {{flags.cpp}} and {{configuration.md}}."""
"MESOS-4723","Improvement","allocation",2,"Add allocator metric for currently satisfied quotas","""We currently expose information on set quotas via dedicated quota endpoints. To diagnose allocator problems one additionally needs information about used quotas."""
"MESOS-4722","Improvement","allocation",1,"Add allocator metric for number of active offer filters","""To diagnose scenarios where frameworks unexpectedly do not receive offers information on currently active filters are needed."""
"MESOS-4721","Improvement","allocation",1,"Expose allocation algorithm latency via a metric.","""The allocation algorithm has grown to become fairly expensive, gaining visibility into its latency enables monitoring and alerting.  Similar allocator timing-related information is already exposed in the log, but should also be exposed via an endpoint."""
"MESOS-4720","Improvement","allocation",2,"Add allocator metrics for total vs offered/allocated resources.","""Exposing the current allocation breakdown as seen by the allocator will allow us to correlated the corresponding metrics in the master with what the allocator sees. We should expose at least allocated or available, and total."""
"MESOS-4731","Improvement","master",3,"Update /frameworks to use jsonify","""This should let us remove the duplicated code in {{http.cpp}} between {{model(Framework)}} and {{json(Full<Framework>)}}."""
"MESOS-4750","Documentation","agent|documentation",2,"Document: Mesos Executor expects all SSL_* environment variables to be set","""I was trying to run Docker containers in a fully SSL-ized Mesos cluster but ran into problems because the executor was failing with a """"Failed to shutdown socket with fd 10: Transport endpoint is not connected"""".  My understanding of why this is happening is because the executor was trying to report its status to Mesos slave over HTTPS, but doesnt have the appropriate certs/env setup inside the executor.  (Thanks to mslackbot/joseph for helping me figure this out on #mesos)  It turns out, the executor expects all SSL_* variables to be set inside `CommandInfo.environment` which gets picked up by the executor to successfully reports its status to the slave.  This part of __executor needing all the SSL_* variables to be set in its environment__ is missing in the Mesos SSL transitioning guide. I request you to please add this vital information to the doc."""
"MESOS-4748","Task","containerization",3,"Add Appc image fetcher tests.","""Mesos now has support for fetching Appc images. Add tests that verifies the new component."""
"MESOS-4747","Bug","test",1,"ContainerLoggerTest.MesosContainerizerRecover cannot be executed in isolation","""Some cleanup of spawned processes is missing in {{ContainerLoggerTest.MesosContainerizerRecover}} so that when the test is run in isolation the global teardown might find lingering processes.   Observered on OS X with clang-trunk and an unoptimized build. """
"MESOS-4768","Bug","test",1,"MasterMaintenanceTest.InverseOffers is flaky","""[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].  Verbose logs from ASF Centos7 build: """
"MESOS-4754","Bug","master",2,"The ""executors"" field is exposed under a backwards incompatible schema.","""In 0.26.0, the master's {{/state}} endpoint generated the following:    In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:    This is a backwards incompatible API change."""
"MESOS-4776","Improvement","libprocess",2,"Libprocess metrics/snapshot endpoint rate limiting should be configurable.","""Currently the {{/metrics/snapshot}} endpoint in libprocess has a [hard-coded|https://github.com/apache/mesos/blob/0.27.1/3rdparty/libprocess/include/process/metrics/metrics.hpp#L52] rate limit of 2 requests per second:    This should be configurable via a libprocess environment variable so that users can control this when initializing libprocess."""
"MESOS-4772","Improvement","security",2,"TaskInfo/ExecutorInfo should include fine-grained ownership/namespacing","""We need a way to assign fine-grained ownership to tasks/executors so that multi-user frameworks can tell Mesos to associate the task with a user identity (rather than just the framework principal+role). Then, when an HTTP user requests to view the task's sandbox contents, or kill the task, or list all tasks, the authorizer can determine whether to allow/deny/filter the request based on finer-grained, user-level ownership. Some systems may want TaskInfo.owner to represent a group rather than an individual user. That's fine as long as the framework sets the field to the group ID in such a way that a group-aware authorizer can interpret it."""
"MESOS-4790","Improvement","master",1,"Revert external linkage of symbols in master/constants.hpp","""src/master/constants.hpp contains:    From commit 232a23b2a2e11f4e905b834aa2a11afe5bf6438a. We should investigate whether this is still necessary on supported compilers; it likely is not."""
"MESOS-4787","Documentation","documentation",2,"HTTP endpoint docs should use shorter paths","""My understanding is that the recommended path for the v1 scheduler API is {{/api/v1/scheduler}}, but the HTTP endpoint [docs|http://mesos.apache.org/documentation/latest/endpoints/] for this endpoint list the path as {{/master/api/v1/scheduler}}; the filename of the doc page is also in the {{master}} subdirectory.  Similarly, we document the master state endpoint as {{/master/state}}, whereas the preferred name is now just {{/state}}, and so on for most of the other endpoints. Unlike we the V1 API, we might want to consider backward compatibility and document both forms -- not sure. But certainly it seems like we should encourage people to use the shorter paths, not the longer ones."""
"MESOS-4786","Documentation","documentation",1,"Example in C++ style guide uses wrong indention for wrapped line","""  Here the second line should be indented by two spaces since it is a wrapped assignment; the corresponding rule is laid out in the preceeding paragraph."""
"MESOS-4785","Documentation","documentation",5,"Reorganize ACL subject/object descriptions.","""The authorization documentation would benefit from a reorganization of the ACL subject/object descriptions. Instead of simple lists of the available subjects and objects, it would be nice to see a table showing which subject and object is used with each action."""
"MESOS-4784","Bug","test",1,"SlaveTest.MetricsSlaveLaunchErrors test relies on implicit blocking behavior hitting the global metrics endpoint","""The test attempts to observe a change in the {{slave/container_launch_errors}} metric, but does not wait for the triggering action to take place. Currently the test passes since hitting the endpoint blocks for some rate limit-related time which provides under many circumstances enough wait time for the action to take place. """
"MESOS-4783","Improvement","test",3,"Disable rate limiting of the global metrics endpoint for mesos-tests execution","""Once we can optionally disable rate limiting in the global metrics endpoint with MESOS-4776 we should disable the rate limiting during the execution of mesos-tests.  * rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting * rate limiting might incur additional wait time which might slown down tests"""
"MESOS-4823","Task","containerization",2,"Implement port forwarding in `network/cni` isolator","""Most docker and appc images wish to expose ports that micro-services are listening on, to the outside world. When containers are running on bridged (or ptp) networking this can be achieved by installing port forwarding rules on the agent (using iptables). This can be done in the `network/cni` isolator.   The reason we would like this functionality to be implemented in the `network/cni` isolator, and not a CNI plugin, is that the specifications currently do not support specifying port forwarding rules. Further, to install these rules the isolator needs two pieces of information, the exposed ports and the IP address associated with the container. Bother are available to the isolator."""
"MESOS-4822","Task","containerization",2,"Add support for local image fetching in Appc provisioner.","""Currently Appc image provisioner supports http(s) fetching. It would be valuable to add support for local file path(URI) based  fetching."""
"MESOS-4821","Task","containerization",1,"Introduce a port field in `ImageManifest` in order to set exposed ports for a container.","""Networking isolators such as `network/cni` need to learn about ports that a container wishes to be exposed to the outside world. This can be achieved by adding a field to the `ImageManifest` protobuf and allowing the `ImageProvisioner` to set these fields to inform the isolator of the ports that the container wishes to be exposed. """
"MESOS-4820","Task","containerization",1,"Need to set `EXPOSED` ports from docker images into `ContainerConfig`","""Most docker images have an `EXPOSE` command associated with them. This tells the container run-time the TCP ports that the micro-service """"wishes"""" to expose to the outside world.   With the `Unified containerizer` project since `MesosContainerizer` is going to natively support docker images it is imperative that the Mesos container run time have a mechanism to expose ports listed in a Docker image. The first step to achieve this is to extract this information from the `Docker` image and set in the `ContainerConfig` . The `ContainerConfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports."""
"MESOS-4818","Task","containerization",3,"Add end to end testing for Appc images.","""Add tests that covers integration test of the Appc provisioner feature with mesos containerizer.  """
"MESOS-4813","Task","containerization",2,"Implement base tests for unified container using local puller.","""Using command line executor to test shell commands with local docker images."""
"MESOS-4810","Bug","docker",3,"ProvisionerDockerPullerTest.ROOT_INTERNET_CURL_ShellCommand fails.",""" """
"MESOS-4807","Bug","libprocess|test",1,"IOTest.BufferedRead writes to the current directory","""libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,  * should the test fail data might be leaked to random locations, * the test cannot be executed from a write-only directory, or * executing the same test in parallel would race on the existence of the created file, and show bogus behavior.  The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture."""
"MESOS-4806","Bug","test",2,"LevelDBStateTests write to the current directory","""All {{LevelDBStateTest}} tests write to the current directory. This is bad for a number of reasons, e.g.,  * should the test fail data might be leaked to random locations, * the test cannot be executed from a write-only directory, or * executing tests from the same suite in parallel (e.g., with {{gtest-parallel}} would race on the existence of the created files, and show bogus behavior.  The tests should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture."""
"MESOS-4801","Improvement","test",1,"Updated `createFrameworkInfo` for hierarchical_allocator_tests.cpp.","""The function of {{createFrameworkInfo}} in hierarchical_allocator_tests.cpp should be updated by enabling caller can set a framework capability to create a framework which can use revocable resources."""
"MESOS-4833","Bug","allocation",5,"Poor allocator performance with labeled resources and/or persistent volumes","""Modifying the {{HierarchicalAllocator_BENCHMARK_Test.ResourceLabels}} benchmark from https://reviews.apache.org/r/43686/ to use distinct labels between different slaves, performance regresses from ~2 seconds to ~3 minutes. The culprit seems to be the way in which the allocator merges together resources; reserved resource labels (or persistent volume IDs) inhibit merging, which causes performance to be much worse."""
"MESOS-4832","Bug","containerization|docker",2,"DockerContainerizerTest.ROOT_DOCKER_RecoverOrphanedPersistentVolumes exits when the /tmp directory is bind-mounted","""If the {{/tmp}} directory (where Mesos tests create temporary directories) is a bind mount, the test suite will exit here:   There appear to be two problems: 1) The docker containerizer should not exit on failure to clean up orphans.  The MesosContainerizer does not do this (see [MESOS-2367]). 2) Unmounting the orphan persistent volume fails for some reason."""
"MESOS-4830","Bug","containerization",1,"Bind docker runtime isolator with docker image provider.","""If image provider is specified as `docker` but docker/runtime is not set, it would be not meaningful, because of no executables. A check should be added to make sure docker runtime isolator is on if using docker as image provider."""
"MESOS-4825","Bug","master",1,"Master's slave reregister logic does not update version field","""The master's logic for reregistering a slave does not update the version field if the slave re-registers with a new version."""
"MESOS-4824","Bug","containerization",2,"""filesystem/linux"" isolator does not unmount orphaned persistent volumes","""A persistent volume can be orphaned when: # A framework registers with checkpointing enabled. # The framework starts a task + a persistent volume. # The agent exits.  The task continues to run. # Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent. # The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.  The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}})    Test implemented here: https://reviews.apache.org/r/44122/"""
"MESOS-4834","Task","containerization",2,"Add 'file' fetcher plugin.","""Add support for """"file"""" based URI fetcher. This could be useful for container image provisioning from local file system."""
"MESOS-4854","Documentation","containerization",1,"Update CHANGELOG with net_cls isolator","""Need to update the CHANGELOG for 0.28 release."""
"MESOS-4850","Task","agent|security",3,"Add authentication to agent endpoints /state and /flags","""The {{/state}} and {{/flags}} endpoints are installed in {{src/slave/slave.cpp}}, and thus are straightforward to make authenticated. Other agent endpoints require a bit more consideration, and are tracked in MESOS-4902.  For more information on agent endpoints, see http://mesos.apache.org/documentation/latest/endpoints/ or search for `route(` in the source code: """
"MESOS-4849","Task","agent|security",2,"Add agent flags for HTTP authentication","""Flags should be added to the agent to: 1. Enable HTTP authentication ({{--authenticate_http}}) 2. Specify credentials ({{--http_credentials}}) 3. Specify HTTP authenticators ({{--authenticators}})"""
"MESOS-4848","Task","agent|security",2,"Agent Authn Research Spike","""Research the master authentication flags to see what changes will be necessary for agent http authentication. Write up a 1-2 page summary/design doc."""
"MESOS-4844","Task","master|security",2,"Add authentication to master endpoints","""Before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when --authenticate_http is enabled, and so that the principal is passed into `route()`."""
"MESOS-4865","Task","containerization",3,"Add GPUs as an explicit resource.","""We will add """"gpus"""" as an explicitly recognized resource in Mesos, akin to cpus, memory, ports, and disk.  In the containerizer, we will verify that the number of GPU resources passed in via the --resources flag matches the list of GPUs passed in via the --nvidia_gpus flag.  In the future we will add autodiscovery so this matching is unnecessary.  However, we will always have to pass """"gpus"""" as a resource to make any GPU available on the system (unlike for cpus and memory, where the default is probed)."""
"MESOS-4864","Task","containerization",3,"Add flag to specify available Nvidia GPUs on an agent's command line.","""In the initial GPU support we will not do auto-discovery of GPUs on an agent.  As such, an operator will need to specify a flag on the command line, listing all of the GPUs available on the system."""
"MESOS-4863","Task","containerization|test",2,"Add Nvidia GPU isolator tests.","""We need to be able to run unit tests that verify GPU isolation, as well as run full blown tests that actually exercise the GPUs.  These tests should only build when the proper configure flags are set for enabling nvidia GPU support."""
"MESOS-4861","Task","containerization",2,"Add configure flags to build with Nvidia GPU support.","""The configure flags can be used to enable Nvidia GPU support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.  They will also be used to conditionally build support for Nvidia GPUs into Mesos."""
"MESOS-4860","Task","containerization",2,"Add a script to install the Nvidia GDK on a host.","""This script can be used to install the Nvidia GDK for Cuda 7.5 on a mesos development machine. The purpose of the Nvidia GDK is to provide all the necessary header files (nvml.h) and library files (libnvidia-ml.so) necessary to build mesos with Nvidia GPU support.  If the machine on which Mesos is being compiled doesn't have any GPUs, then libnvidia-ml.so consists only of stubs, allowing Mesos to build and run, but not actually do anything useful under the hood. This enables us to build a GPU-enabled mesos on a development machine without GPUs and then deploy it to a production machine with GPUs and be reasonably sure it will work."""
"MESOS-4877","Bug","containerization|docker",3,"Mesos containerizer can't handle top level docker image like ""alpine"" (must use ""library/alpine"")","""This can be demonstrated with the {{mesos-execute}} command:  # Docker containerizer with image {{alpine}}: success  # Mesos containerizer with image {{alpine}}: failure  # Mesos containerizer with image {{library/alpine}}: success   In the slave logs:    curl command executed:    Also got the same result with {{ubuntu}} docker image."""
"MESOS-4891","Improvement","agent",8,"Add a '/containers' endpoint to the agent to list all the active containers.","""This endpoint will be similar to /monitor/statistics.json endpoint, but it'll also contain the 'container_status' about the container (see ContainerStatus in mesos.proto). We'll eventually deprecate the /monitor/statistics.json endpoint."""
"MESOS-4889","Task","containerization",5,"Implement runtime isolator tests.","""There different cases in docker runtime isolator. Some special cases should be tested with unique test case, to verify the docker runtime isolator logic is correct."""
"MESOS-4888","Bug","containerization",2,"Default cmd is executed as an incorrect command.","""When mesos containerizer launch a container using a docker image, which only container default Cmd. The executable command is is a incorrect sequence. For example:  If an image default entrypoint is null, cmd is """"sh"""", user defines shell=false, value is none, and arguments as [-c, echo 'hello world']. The executable command is `[sh, -c, echo 'hello world', sh]`, which is incorrect. It should be `[sh, sh, -c, echo 'hello world']` instead.  This problem is only exposed for the case: sh=0, value=0, argv=1, entrypoint=0, cmd=1. """
"MESOS-4886","Improvement","containerization",3,"Support mesos containerizer force_pull_image option.","""Currently for unified containerizer, images that are already cached by metadata manager cannot be updated. User has to delete corresponding images in store if an update is need. We should support `force_pull_image` option for unified containerizer, to provide override option if existed."""
"MESOS-4903","Bug","modules",3,"Allow multiple loads of module manifests","""The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure."""
"MESOS-4902","Improvement","HTTP API",5,"Add authentication to libprocess endpoints","""In addition to the endpoints addressed by MESOS-4850 and MESOS-5152, the following endpoints would also benefit from HTTP authentication: * {{/profiler/*}} * {{/logging/toggle}} * {{/metrics/snapshot}}  Adding HTTP authentication to these endpoints is a bit more complicated because they are defined at the libprocess level.  While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.  We should establish a mechanism for making an endpoint authenticated that allows us to: 1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent 2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess  Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other."""
"MESOS-4912","Bug","containerization",3,"LinuxFilesystemIsolatorTest.ROOT_MultipleContainers fails.","""Observed on our CI: """
"MESOS-4910","Improvement","docker",1,"Deprecate the --docker_stop_timeout agent flag.","""Instead, a combination of {{executor_shutdown_grace_period}} agent flag and optionally task kill policies should be used."""
"MESOS-4922","Bug","containerization",5,"Setup proper /etc/hostname, /etc/hosts and /etc/resolv.conf for containers in network/cni isolator.","""The network/cni isolator needs to properly setup /etc/hostname and /etc/hosts for the container with a hostname (e.g., randomly generated) and the assigned IP returned by CNI plugin. We should consider the following cases: 1) container is using host filesystem 2) container is using a different filesystem 3) custom executor and command executor"""
"MESOS-4941","Improvement","allocation",8,"Support update existing quota.","""We want to support updating an existing quota without the cycle of delete and recreate. This avoids the possible starvation risk of losing the quota between delete and recreate, and also makes the interface friendly.  Design doc: https://docs.google.com/document/d/1c8fJY9_N0W04FtUQ_b_kZM6S0eePU7eYVyfUP14dSys"""
"MESOS-4937","Task","containerization",5,"Investigate container security options for Mesos containerizer","""We should investigate the following to improve the container security for Mesos containerizer and come up with a list of features that we want to support in MVP.  1) Capabilities 2) User namespace 3) Seccomp 4) SELinux 5) AppArmor  We should investigate what other container systems are doing regarding security: 1) [k8s| https://github.com/kubernetes/kubernetes/blob/master/pkg/api/v1/types.go#L2905] 2) [docker|https://docs.docker.com/engine/security/security/] 3) [oci|https://github.com/opencontainers/specs/blob/master/config.md]"""
"MESOS-4932","Task","security",5,"Propose Design for Authorization based filtering for endpoints.","""The design doc can be found here: https://docs.google.com/document/d/1M27S7OTSfJ8afZCklOz00g_wcVrL32i9Lyl6g22GWeY"""
"MESOS-4951","Improvement","agent|libprocess",2,"Enable actors to pass an authentication realm to libprocess","""To prepare for MESOS-4902, the Mesos master and agent need a way to pass the desired authentication realm to libprocess. Since some endpoints (like {{/profiler/*}}) get installed in libprocess, the master/agent should be able to specify during initialization what authentication realm the libprocess-level endpoints will be authenticated under."""
"MESOS-4944","Task","containerization",5,"Improve overlay backend so that it's writable","""Currently, the overlay backend will provision a read-only FS. We can use an empty directory from the container sandbox to act as the upper layer so that it's writable."""
"MESOS-4943","Improvement","containerization",13,"Reduce the size of LinuxRootfs in tests.","""Right now, LinuxRootfs copies files from the host filesystem to construct a chroot-able rootfs. We copy a lot of unnecessary files, making it very large. We can potentially strip a lot files."""
"MESOS-4942","Bug","containerization",2,"Docker runtime isolator tests may cause disk issue.","""Currently slave working directory is used as docker store dir and archive dir, which is problematic. Because slave work dir is exactly `environment->mkdtemp()`, it will get cleaned up until the end of the whole test. But the runtime isolator local puller tests cp the host's rootfs, which size is relatively big. Cleanup has to be done by each test tear down. """
"MESOS-4965","Improvement","storage",8,"Support resizing of an existing persistent volume","""We need a mechanism to update the size of a persistent volume.    The increase case is generally more interesting to us (as long as there still available disk resource on the same disk)."""
"MESOS-4956","Improvement","HTTP API",5,"Add authentication to /files endpoints","""To protect access (authz) to master/agent logs as well as executor sandboxes, we need authentication on the /files endpoints.  Adding HTTP authentication to these endpoints is a bit complicated since they are defined in code that is shared by the master and agent.  While working on MESOS-4850, it became apparent that since our tests use the same instance of libprocess for both master and agent, different default authentication realms must be used for master/agent so that HTTP authentication can be independently enabled/disabled for each.  We should establish a mechanism for making an endpoint authenticated that allows us to: 1) Install an endpoint like {{/files}}, whose code is shared by the master and agent, with different authentication realms for the master and agent 2) Avoid hard-coding a default authentication realm into libprocess, to permit the use of different authentication realms for the master and agent and to keep application-level concerns from leaking into libprocess  Another option would be to use a single default authentication realm and always enable or disable HTTP authentication for *both* the master and agent in tests. However, this wouldn't allow us to test scenarios where HTTP authentication is enabled on one but disabled on the other."""
"MESOS-4985","Bug","containerization",3,"Destroy a container while it's provisioning can lead to leaked provisioned directories.","""Here is the possible sequence of events: 1) containerizer->launch 2) provisioner->provision is called. it is fetching the image 3) executor registration timed out 4) containerizer->destroy is called 5) container->state is still in PREPARING 6) provisioner->destroy is called  So we can be calling provisioner->destory while provisioner->provision hasn't finished yet. provisioner->destroy might just skip since there's no information about the container yet, and later, provisioner will prepare the root filesystem. This root filesystem will not be destroyed as destroy already finishes."""
"MESOS-4984","Bug","test",2,"MasterTest.SlavesEndpointTwoSlaves is flaky","""Observed on Arch Linux with GCC 6, running in a virtualbox VM:  [ RUN      ] MasterTest.SlavesEndpointTwoSlaves /mesos-2/src/tests/master_tests.cpp:1710: Failure Value of: array.get().values.size()   Actual: 1 Expected: 2u Which is: 2 [  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (86 ms)  Seems to fail non-deterministically, perhaps more often when there is concurrent CPU load on the machine."""
"MESOS-4978","Bug","containerization",3,"Update mesos-execute with Appc changes.","""mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier."""
"MESOS-4992","Bug","webui",3,"sandbox uri does not work outisde mesos http server","""The SandBox uri of a framework does not work if i just copy paste it to the browser.  For example the following sandbox uri: http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/frameworks/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009/executors/driver-20160321155016-0001/browse  should redirect to: http://172.17.0.1:5050/#/slaves/50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0/browse?path=%2Ftmp%2Fmesos%2Fslaves%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-S0%2Fframeworks%2F50f87c73-79ef-4f2a-95f0-b2b4062b2de6-0009%2Fexecutors%2Fdriver-20160321155016-0001%2Fruns%2F60533483-31fb-4353-987d-f3393911cc80  yet it fails with the message: """"Failed to find slaves. Navigate to the slave's sandbox via the Mesos UI."""" and redirects to: http://172.17.0.1:5050/#/  It is an issue for me because im working on expanding the mesos spark ui with sandbox uri, The other option is to get the slave info and parse the json file there and get executor paths not so straightforward or elegant though.  Moreover i dont see the runs/container_id in the Mesos Proto Api. I guess this is hidden info, this is the needed piece of info to re-write the uri without redirection. """
"MESOS-5010","Bug","python api",2,"Installation of mesos python package is incomplete","""The installation of mesos python package is incomplete, i.e., the files {{cli.py}}, {{futures.py}}, and {{http.py}} are not installed.    This appears to be first broken with {{d1d70b9}} (MESOS-3969, [Upgraded bundled pip to 7.1.2.|https://reviews.apache.org/r/40630]). Bisecting in {{pip}}-land shows that our install becomes broken for {{pip-6.0.1}} and later (we are using {{pip-7.1.2}}). """
"MESOS-5004","Documentation","documentation",1,"Clarify docs on '/reserve' and '/create-volumes' without authentication","""For both reservations and persistent volume creation, the behavior of the HTTP endpoints differs slightly from that of the framework operations. Due to the implementation of HTTP authentication, it is not possible for a framework/operator to provide a principal when HTTP authentication is disabled. This means that when HTTP authentication is disabled, the endpoint handlers will _always_ receive {{None()}} as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the NONE principal is authorized to do stuff.  The docs should be updated to explain this behavior explicitly."""
"MESOS-5001","Improvement","allocation",1,"Prefix allocator metrics with ""mesos/"" to better support custom allocator metrics.","""There currently exists only a single allocator metric named   In order to support different allocator implementations (the """"mesos"""" allocator being the default one included in the project currently), it would be better to rename the metric so that allocator metrics are prefixed with the allocator implementation name:    This consistent with the approach taken for containerizer metrics, where the mesos containerizer exposes its metrics under a """"mesos/"""" prefix."""
"MESOS-5000","Bug","test",3,"MasterTest.MasterLost is flaky","""The test {{MasterTest.MasterLost}} and {{ExceptionTest.DisallowSchedulerActionsOnAbort}} fail at least half the time under OS X (clang, not optimized, {{30efac7}}), e.g.,   Sometimes also {{FaultToleranceTest.SchedulerFailover}} fails with the same stack trace.  I could trace this to the recent refactoring of the test helpers (MESOS-4633, MESOS-4634),   It appears the lifetimes of some objects are still not ordered correctly. """
"MESOS-5028","Bug","containerization",3,"Copy provisioner cannot replace directory with symlink","""I'm trying to play with the new image provisioner on our custom docker images, but one of layer failed to get copied, possibly due to a dangling symlink.  Error log with Glog_v=1:  {quote} I0324 05:42:48.926678 15067 copy.cpp:127] Copying layer path '/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs' to rootfs '/var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6' E0324 05:42:49.028506 15062 slave.cpp:3773] Container '5f05be6c-c970-4539-aa64-fd0eef2ec7ae' for executor 'test' of framework 75932a89-1514-4011-bafe-beb6a208bb2d-0004 failed to start: Collect failed: Collect failed: Failed to copy layer: cp: cannot overwrite directory /var/lib/mesos/provisioner/containers/5f05be6c-c970-4539-aa64-fd0eef2ec7ae/backends/copy/rootfses/507173f3-e316-48a3-a96e-5fdea9ffe9f6/etc/apt with non-directory {quote}  Content of _/tmp/mesos/store/docker/layers/5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1/rootfs/etc/apt_ points to a non-existing absolute path (cannot provide exact path but it's a result of us trying to mount apt keys into docker container at build time).  I believe what happened is that we executed a script at build time, which contains equivalent of: {quote} rm -rf /etc/apt/* && ln -sf /build-mount-point/ /etc/apt {quote} """
"MESOS-5027","Improvement","master|security|webui",2,"Enable authenticated login in the webui","""The webui hits a number of endpoints to get the data that it displays: {{/state}}, {{/metrics/snapshot}}, {{/files/browse}}, {{/files/read}}, and maybe others? Once authentication is enabled on these endpoints, we need to add a login prompt to the webui so that users can provide credentials."""
"MESOS-5023","Bug","containerization",2,"MesosContainerizerProvisionerTest.DestroyWhileProvisioning is flaky.","""Observed on the Apache Jenkins.  """
"MESOS-5044","Improvement","test",1,"Temporary directories created by environment->mkdtemp cleanup can be problematic.","""Currently in mesos test, we have the temporary directories created by `environment->mkdtemp()` cleaned up until the end of the test suite, which can be problematic. For instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines.   We should have these temp dir created by `environment->mkdtemp` cleaned up during each test teardown. Currently we only clean up the sandbox for each test."""
"MESOS-5041","Task","containerization",8,"Add cgroups unified isolator","""Implement the cgroups unified isolator for Mesos containerizer."""
"MESOS-5058","Bug","allocation",2,"Expose per-role dominant share","""A client's dominant share is crucial measure for how likely it is to receive offers in the future. We should expose it in a dedicated allocator metric.  As currently the {{HierarchicalAllocatorProcess}} does work with generic {{Sorters}} which have no notion of DRF share we need to decide whether and where we would need to limit generality in order to expose the innards of the currently used {{DRFSorter}}. """
"MESOS-5065","Task","containerization|docker",3,"Support docker private registry default docker config.","""For docker private registry with authentication, docker containerizer should support using a default .docker/config.json file (or the old .dockercfg file) locally, which is pre-handled by operators. The default docker config file should be exposed by a new agent flag `--docker_config`. """
"MESOS-5073","Improvement","allocation",1,"Mesos allocator leaks role sorter and quota role sorters.","""The Mesos allocator {{internal::HierarchicalAllocatorProcess}} owns two raw pointer members {{roleSorter}} and {{quotaRoleSorter}}, but fails to properly manage their lifetime; they are e.g., not cleaned up in the allocator process destructor.  Since currently we do not recreate an existing allocator in production code they seem to be unaffected by these leaks; they do affect tests though where we create allocators multiple times."""
"MESOS-5078","Documentation","documentation",5,"Document TaskStatus reasons","""We should document the possible {{reason}} values that can be found in the {{TaskStatus}} message."""
"MESOS-5115","Bug","containerization",2,"Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator.",""" Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.      We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator."""
"MESOS-5114","Bug","stout",2,"Flags::parse does not handle empty string correctly.","""A missing default for quorum size has generated the following master config    This was causing each elected leader to attempt replica recovery.  E.g. {{group.cpp:700] Trying to get '/mesos/log_replicas/0000000012' in ZooKeeper}}  And eventually: {{master.cpp:1458] Recovery failed: Failed to recover registrar: Failed to perform fetch within 1mins}}  Full log on one of the masters https://gist.github.com/clehene/09a9ddfe49b92a5deb4c1b421f63479e  All masters and zk nodes were reachable over the network.  Also once the quorum was configured the master recovery protocol finished gracefully.  """
"MESOS-5113","Bug","containerization",1,"`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag","""If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump: 0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56 56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory. (gdb) bt #0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56 #1  0x00007ffff23280d8 in __GI_abort () at abort.c:89 #2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 """"%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"""", assertion=assertion@entry=0x451f5c """"isSome()"""",     file=file@entry=0x451f65 """"../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"""", line=line@entry=111,     function=function@entry=0x45294a """"const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"""") at assert.c:92 #3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c """"isSome()"""", file=0x451f65 """"../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"""", line=111,     function=0x45294a """"const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"""") at assert.c:101 #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111 Python Exception <class 'IndexError'> list index out of range: #5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331 #6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239 #7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071 #8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471 #9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130 #10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161 #11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82 #12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570 #13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218 #14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,     __args=<unknown type in /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295 #15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353 #16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731 #17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720 #18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)     at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115 #19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6 #20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312 #21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111 (gdb) frame 4 #4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111"""
"MESOS-5112","Task","stout",2,"Introduce `WindowsSocketError`.","""{{WindowsError}} invokes {{::GetLastError}} to retrieve the error code. Windows has a {{::WSAGetLastError}} function which at the interface level, is intended for failed socket operations. We should introduce a {{WindowsSocketError}} which invokes {{::WSAGetLastError}} and use them accordingly."""
"MESOS-5111","Task","libprocess",2,"Update `network::connect` to use the typed error state of `Try`.","""{{network::connect}} function returns a {{Try<int>}} currently and the caller is required to inspect the state of {{errno}} out-of-band. {{network::connect}} should really return something like a {{Try<int, ErrnoError>}}."""
"MESOS-5110","Task","stout",3,"Introduce an additional template parameter to `Try` for typed error.","""Add an additional template parameter {{E}} to the {{Try}} class template.  """
"MESOS-5109","Task","stout",2,"Capture the error code in `ErrnoError` and `WindowsError`.","""The {{ErrnoError}} and {{WindowsError}} classes simply construct the error string via a mechanism such as {{strerror}}. They should also capture the error code, as it is an essential piece of information for such an error type."""
"MESOS-5124","Improvement","cli",3,"TASK_KILLING is not supported by mesos-execute.","""Recently {{TASK_KILLING}} state (MESOS-4547) have been introduced to Mesos. We should add support for this feature to {{mesos-execute}}."""
"MESOS-5130","Task","containerization",1,"Enable `newtork/cni` isolator in `MesosContainerizer` as the default `network` isolator.","""Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator."""
"MESOS-5128","Bug","test",3,"PersistentVolumeTest.AccessPersistentVolume is flaky","""Observed on ASF CI:  """
"MESOS-5127","Bug","containerization",1,"Reset `LIBPROCESS_IP` in `network\cni` isolator.","""Currently the `LIBPROCESS_IP` environment variable was being set to     the Agent IP if the environment variable has not be defined by the     `Framework`. For containers having their own IP address (as with     containers on CNI networks) this becomes a problem since the command     executor tries to bind to the `LIBPROCESS_IP` that does not exist in     its network namespace, and fails. Thus, for containers launched on CNI     networks the `LIBPROCESS_IP` should not be set, or rather is set to     """"0.0.0.0"""", allowing the container to bind to the IP address provided     by the CNI network."""
"MESOS-5144","Task","libprocess",2,"Cleanup memory leaks in libprocess finalize()","""libprocess's {{finalize}} function currently leaks memory for a few different reasons. Cleaning up the {{SocketManager}} will be somewhat involved (MESOS-3910), but the remaining memory leaks should be fairly easy to address."""
"MESOS-5142","Bug","agent|security",2,"Add agent flags for HTTP authorization.","""Flags should be added to the agent to: 1. Enable authorization ({{--authorizers}}) 2. Provide ACLs ({{--acls}})"""
"MESOS-5135","Task","documentation",1,"Update existing documentation to Include references to GPUs as a first class resource.","""Specifically, the documentation in the following files should be udated:  """
"MESOS-5157","Task","webui",1,"Update webui for GPU metrics","""After adding the GPU metrics and updating the resources JSON to include GPU information, the webui should be updated accordingly."""
"MESOS-5153","Bug","agent|security",8,"Sandboxes contents should be protected from unauthorized users","""MESOS-4956 introduced authentication support for the sandboxes. However, authentication can only go as far as to tell whether an user is known to mesos or not. An extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox (browse, read, download, debug)."""
"MESOS-5152","Task","agent|security",2,"Add authentication to agent's /monitor/statistics endpoint","""Operators may want to enforce that only authenticated users (and subsequently only specific authorized users) be able to view per-executor resource usage statistics. Since this endpoint is handled by the ResourceMonitorProcess, I would expect the work necessary to be similar to what was done for /files or /registry endpoint authn."""
"MESOS-5146","Bug","allocation|test",1,"MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.","""Observed on the ASF CI:  """
"MESOS-5160","Task","containerization",1,"Make `network/cni` enabled as the default network isolator for `MesosContainerizer`.","""Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. """
"MESOS-5172","Bug","containerization",3,"Registry puller cannot fetch blobs correctly from http Redirect 3xx urls.","""When the registry puller is pulling a private repository from some private registry (e.g., quay.io), errors may occur when fetching blobs, at which point fetching the manifest of the repo is finished correctly. The error message is `Unexpected HTTP response '400 Bad Request' when trying to download the blob`. This may arise from the logic of fetching blobs, or incorrect format of uri when requesting blobs."""
"MESOS-5171","Task","replicated log",3,"Expose state/state.hpp to public headers","""We want the Modules to be able to use replicated log along with the APIs to communicate with Zookeeper. This change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp)."""
"MESOS-5169","Improvement","security",3,"Introduce new Authorizer Actions for Authorized based filtering of endpoints.","""For authorization based endpoint filtering we need to introduce the authorizer actions outlined via MESOS-4932."""
"MESOS-5167","Task","test",5,"Add tests for `network/cni` isolator","""We need to add tests to verify the functionality of `network/cni` isolator."""
"MESOS-5164","Task","agent|security",5,"Add authorization to agent's /monitor/statistics endpoint.","""Operators may want to enforce that only specific authorized users be able to view per-executor resource usage statistics. For 0.29 MVP, we can make this coarse-grained, and assume that only the operator or a operator-privileged monitoring service will be accessing the endpoint. For a future release, we can consider fine-grained authz that filters statistics like we plan to do for /tasks."""
"MESOS-5187","Bug","containerization",3,"The filesystem/linux isolator does not set the permissions of the host_path.","""The {{filesystem/linux}} isolator is not a drop in replacement for the {{filesystem/shared}} isolator. This should be considered before the latter is deprecated.  We are currently using the {{filesystem/shared}} isolator together with the following slave option. This provides us with a private {{/tmp}} and {{/var/tmp}} folder for each task.    When browsing the Mesos sandbox, one can see the following permissions:   However, when running with the new {{filesystem/linux}} isolator, the permissions are different:   This prevents user code (running as a non-root user) from writing to those folders, i.e. every write attempt fails with permission denied.   *Context*: * We are using Apache Aurora. Aurora is running its custom executor as root but then switches to a non-privileged user before running the actual user code.  * The follow code seems to have enabled our usecase in the existing {{filesystem/shared}} isolator: https://github.com/apache/mesos/blob/4d2b1b793e07a9c90b984ca330a3d7bc9e1404cc/src/slave/containerizer/mesos/isolators/filesystem/shared.cpp#L175-L198 """
"MESOS-5181","Bug","scheduler driver",1,"Master should reject calls from the scheduler driver if the scheduler is not connected.","""When a scheduler registers, the master will create a link from master to scheduler.  If this link breaks, the master will consider the scheduler {{inactive}} and mark it as {{disconnected}}.  This causes a couple problems: 1) Master does not send offers to {{inactive}} schedulers.  But these schedulers might consider themselves """"registered"""" in a one-way network partition scenario. 2) Any calls from the {{inactive}} scheduler is still accepted, which leaves the scheduler in a starved, but semi-functional state.  See the related issue for more context: MESOS-5180  There should be an additional guard for registered, but {{inactive}} schedulers here: https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/master.cpp#L1977  The HTTP API already does this: https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/http.cpp#L459  Since the scheduler driver cannot return a 403, it may be necessary to return a {{Event::ERROR}} and force the scheduler to abort."""
"MESOS-5215","Documentation","documentation",1,"Update the documentation for '/reserve' and '/create-volumes'","""There are a couple issues related to the {{principal}} field in {{DiskInfo}} and {{ReservationInfo}} (see linked JIRAs) that should be better documented. We need to help users understand the purpose of these fields and how they interact with the principal provided in the HTTP authentication header. See linked tickets for background."""
"MESOS-5214","Improvement","master",2,"Populate FrameworkInfo.principal for authenticated frameworks","""If a framework authenticates and then does not provide a {{principal}} in its {{FrameworkInfo}}, we currently allow this and leave {{FrameworkInfo.principal}} unset. Instead, we should populate {{FrameworkInfo.principal}} for them automatically in that case to ensure that the two principals are equal."""
"MESOS-5216","Bug","containerization",5,"Document docker volume driver isolator.","""Should include the followings:  1. What features (driver options) are supported in docker volume driver isolator. 2. How to use docker volume driver isolator.     *related agent flags introduction and usage.     *isolator dependency clarification (e.g., filesystem/linux).     *related driver daemon preprocess.     *volumes pre-specified by users and volume cleanup."""
"MESOS-5228","Task","containerization",3,"Add tests for Capability API.","""Add basic tests for the capability API."""
"MESOS-5232","Task","containerization",1,"Add capability information to ContainerInfo protobuf message.","""To enable support for capability as first class framework entity, we need to add capabilities related information to the ContainerInfo protobuf."""
"MESOS-5237","Bug","stout",2,"The windows version of `os::access` has differing behavior than the POSIX version.","""The POSIX version of {{os::access}} looks like this:    Compare this to the Windows version of {{os::access}} which looks like this following:    As we can see, the case where {{errno}} is set to {{EACCES}} is handled differently between the 2 functions.  We can actually consolidate the 2 functions by simply using the POSIX version. The challenge is that on POSIX, we should use {{::access}} and {{::_access}} on Windows. Note however, that this problem is already solved, as we have an implementation of {{::access}} for Windows in {{3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp}} which simply defers to {{::_access}}.  Thus, I propose to simply consolidate the 2 implementations."""
"MESOS-5239","Bug","containerization",3,"Persistent volume DockerContainerizer support assumes proper mount propagation setup on the host.","""We recently added persistent volume support in DockerContainerizer (MESOS-3413). To understand the problem, we first need to understand how persistent volumes are supported in DockerContainerizer.  To support persistent volumes in DockerContainerizer, we bind mount persistent volumes under a container's sandbox ('container_path' has to be relative for persistent volumes). When the Docker container is launched, since we always add a volume (-v) for the sandbox, the persistent volumes will be bind mounted into the container as well (since Docker does a 'rbind').  The assumption that the above works is that the Docker daemon should see those persistent volume mounts that Mesos mounts on the host mount table. It's not a problem if Docker daemon itself is using the host mount namespace. However, on systemd enabled systems, Docker daemon is running in a separate mount namespace and all mounts in that mount namespace will be marked as slave mounts due to this [patch|https://github.com/docker/docker/commit/eb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a].  So what that means is that: in order for it to work, the parent mount of agent's work_dir should be a shared mount when docker daemon starts. This is typically true on CentOS7, CoreOS as all mounts are shared mounts by default.  However, this causes an issue with the 'filesystem/linux' isolator. To understand why, first I need to show you a typical problem when dealing with shared mounts. Let me explain that using the following commands on a CentOS7 machine:   As you can see above, there're two entries (/run/netns/test) in the mount table (unexpected). This will confuse some systems sometimes. The reason is because when we create a self bind mount (/run/netns -> /run/netns), the mount will be put into the same shared mount peer group (shared:22) as its parent (/run). Then, when you create another mount underneath that (/run/netns/test), that mount operation will be propagated to all mounts in the same peer group (shared:22), resulting an unexpected additional mount being created.  The reason we need to do a self bind mount in Mesos is that sometimes, we need to make sure some mounts are shared so that it does not get copied when a new mount namespace is created. However, on some systems, mounts are private by default (e.g., Ubuntu 14.04). In those cases, since we cannot change the system mounts, we have to do a self bind mount so that we can set mount propagation to shared. For instance, in filesytem/linux isolator, we do a self bind mount on agent's work_dir.  To avoid the self bind mount pitfall mentioned above, in filesystem/linux isolator, after we created the mount, we do a make-slave + make-shared so that the mount is its own shared mount peer group. In that way, any mounts underneath it will not be propagated back.  However, that operation will break the assumption that the persistent volume DockerContainerizer support makes. As a result, we're seeing problem with persistent volumes in DockerContainerizer when filesystem/linux isolator is turned on."""
"MESOS-5256","Task","containerization",3,"Add support for per-containerizer resource enumeration","""Currently the top level containerizer includes a static function for enumerating the resources available on a given agent. Ideally, this functionality should be the responsibility of individual containerizers (and specifically the responsibility of each isolator used to control access to those resources).  Adding support for this will involve making the `Containerizer::resources()` function virtual instead of static and then implementing it on a per-containerizer basis.  We should consider providing a default to make this easier in cases where there is only really one good way of enumerating a given set of resources."""
"MESOS-5253","Bug","containerization",2,"Isolator cleanup should not be invoked if they are not prepared yet.","""If the mesos containerizer destroys a container in PROVISIONING state, isolator cleanup is still called, which is incorrect because there is no isolator prepared yet.   In this case, there no need to clean up any isolator, call provisioner destroy directly."""
"MESOS-5277","Bug","containerization",5,"Need to add REMOVE semantics to the copy backend","""Some Dockerfiles run the `rm` command to remove files from the base image using the """"RUN"""" directive in the Dockerfile. An example can be found here: https://github.com/ngineered/nginx-php-fpm.git  In the final rootfs the removed files should not be present. Presence of these files in the final image can make the container misbehave. For example, the nginx-php-fpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different HTML root. If the default nginx config is still present after the building the image, nginx will start pointing to a different HTML root than the one set in the Dockerfile.   Currently the copy backend cannot handle removal of files from intermediate layers. This can cause issues with docker images built using a Dockerfile similar to the one listed here. Hence, we need to add REMOVE semantics to the copy backend.  """
"MESOS-5275","Task","containerization",5,"Add capabilities support for unified containerizer.","""Add capabilities support for unified containerizer.   Requirements: 1. Use the mesos capabilities API. 2. Frameworks be able to add capability requests for containers. 3. Agents be able to add maximum allowed capabilities for all containers launched.  Design document: https://docs.google.com/document/d/1YiTift8TQla2vq3upQr7K-riQ_pQ-FKOCOsysQJROGc/edit#heading=h.rgfwelqrskmd """
"MESOS-5273","Improvement","documentation",3,"Need support for Authorization information via HELP.","""We should add information about authentication to the help message and thereby endpoint documentation (similarly as MESOS-4934 has done for authentication)."""
"MESOS-5272","Task","containerization",3,"Support docker image labels.","""Docker image labels should be supported in unified containerizer, which can be used for applying custom metadata. Image labels are necessary for mesos features to support docker in unified containerizer (e.g., for mesos GPU device isolator)."""
"MESOS-5286","Improvement","libprocess",5,"Add authorization to libprocess HTTP endpoints","""Now that the libprocess-level HTTP endpoints have had authentication added to them in MESOS-4902, we can add authorization to them as well. As a first step, we can implement a """"coarse-grained"""" approach, in which a principal is granted or denied access to a given endpoint. We will likely need to register an authorizer with libprocess."""
"MESOS-5303","Bug","containerization",3,"Add capabilities support for mesos execute cli.","""Add support for `user` and `capabilities` to execute cli. This will help in testing the `capabilities` feature for unified containerizer."""
"MESOS-5310","Task","containerization",5,"Enable `network/cni` isolator to allow modifications and deletion of CNI config","""Currently the `network/cni` isolator can only load the CNI configs at startup. This makes the CNI networks immutable. From an operational standpoint this can make deployments painful for operators.   To make CNI more flexible the `network/cni` isolator should be able to load configs at run time.   The proposal is to add an endpoint to the `network/cni` isolator, to which when the operator sends a PUT request the `network/cni` isolator will reload  CNI configs. """
"MESOS-5313","Documentation","documentation",1,"Failed to set quota and update weight according to document","""  The right command should be adding {{@}} before the quota json file {{jsonMessageBody}}."""
"MESOS-5317","Improvement","agent|security",2,"Authorize the agent's '/containers' endpoint.","""After the agent's {{/containers}} endpoint is authenticated, we should enabled authorization as well."""
"MESOS-5316","Improvement","agent|security",2,"Authenticate the agent's '/containers' endpoint.","""The {{/containers}} endpoint was recently added to the agent. Authentication should be enabled on this endpoint."""
"MESOS-5333","Bug","HTTP API|libprocess",3,"GET /master/maintenance/schedule/ produces 404.","""Attempts to make a GET request to /master/maintenance/schedule/ result in a 404. However, if I make a GET request to /master/maintenance/schedule (without the trailing /), it works. My current (untested) theory is that this might be related to the fact that there is also a /master/maintenance/schedule/status endpoint (an endpoint built on top of a functioning endpoint), as requests to /help and /help/ (with and without the trailing slash) produce the same functioning result."""
"MESOS-5336","Improvement","master|security",3,"Add authorization to GET /quota.","""We already authorize which http users can set/remove quota for particular roles, but even knowing of the existence of these roles (let alone their quotas) may be sensitive information. We should add authz around GET operations on /quota."""
"MESOS-5335","Improvement","master|security",3,"Add authorization to GET /weights.","""We already authorize which http users can update weights for particular roles, but even knowing of the existence of these roles (let alone their weights) may be sensitive information. We should add authz around GET operations on /weights.  Easy option: GET_ENDPOINT_WITH_PATH /weights - Pro: No new verb - Con: All or nothing  Complex option: GET_WEIGHTS_WITH_ROLE - Pro: Filters contents based on roles the user is authorized to see - Con: More authorize calls (one per role in each /weights request)"""
"MESOS-5350","Improvement","docker|modules",5,"Add asynchronous hook for validating docker containerizer tasks","""It is possible to plug in custom validation logic for the MesosContainerizer via an {{Isolator}} module, but the same is not true of the DockerContainerizer.  Basic logic can be plugged into the DockerContainerizer via {{Hooks}}, but this has some notable differences compared to isolators: * Hooks are synchronous. * Modifications to tasks via Hooks have lower priority compared to the task itself.  i.e. If both the {{TaskInfo}} and {{slaveExecutorEnvironmentDecorator}} define the same environment variable, the {{TaskInfo}} wins. * Hooks have no effect if they fail (short of segfaulting) i.e. The {{slavePreLaunchDockerHook}} has a return type of {{Try<Nothing>}}: https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/include/mesos/hook.hpp#L90 But the effect of returning an {{Error}} is a log message: https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/hook/manager.cpp#L227-L230  We should add a hook to the DockerContainerizer to narrow this gap.  This new hook would: * Be called at roughly the same place as {{slavePreLaunchDockerHook}} https://github.com/apache/mesos/blob/628ccd23501078b04fb21eee85060a6226a80ef8/src/slave/containerizer/docker.cpp#L1022 * Return a {{Future}} and require splitting up {{DockerContainerizer::launch}}. * Prevent a task from launching if it returns a {{Failure}}."""
"MESOS-5348","Improvement","containerization",2,"Enhance the log message when launching docker containerizer.","""Log the launch flag which includes the executor command and other information when launching the docker containerizer."""
"MESOS-5347","Improvement","containerization",2,"Enhance the log message when launching mesos containerizer.","""Log the launch flag which includes the executor command, pre-launch commands and other information when launching the mesos containerizer. """
"MESOS-5345","Task","master",5,"Design doc for TASK_LOST_PENDING","""The TASK_LOST task status describes two different situations: (a) the task was not launched because of an error (e.g., insufficient available resources), or (b) the master lost contact with a running task (e.g., due to a network partition); the master will kill the task when it can (e.g., when the network partition heals), but in the meantime the task may still be running.  This has two problems: 1. Using the same task status for two fairly different situations is confusing. 2. In the partitioned-but-still-running case, frameworks have no easy way to determine when a task has truly terminated.  To address these problems, we propose introducing a new task status, TASK_LOST_PENDING. If a framework opts into this behavior using a new capability, TASK_LOST would mean """"the task is definitely not running"""", whereas TASK_LOST_PENDING would mean """"the task may or may not be running (we've lost contact with the agent), but the master will try to shut it down when possible."""""""
"MESOS-5362","Improvement","security",2,"Add authentication to example frameworks","""Some example frameworks do not have the ability to authenticate with the master. Adding authentication to the example frameworks that don't already have it implemented would allow us to use these frameworks for testing in authenticated/authorized scenarios."""
"MESOS-5372","Improvement","stout",1,"Add random() to os:: namespace ","""The function """"random()"""" is not available in Windows. After this improvement the calls to """"os::random()"""" will result in calls to """"::random()"""" on POSIX and """"::rand()"""" on Windows.  """
"MESOS-5380","Bug","agent",3,"Killing a queued task can cause the corresponding command executor to never terminate.","""We observed this in our testing environment. Sequence of events:  1) A command task is queued since the executor has not registered yet. 2) The framework issues a killTask. 3) Since executor is in REGISTERING state, agent calls `statusUpdate(TASK_KILLED, UPID())` 4) `statusUpdate` now will call `containerizer->status()` before calling `executor->terminateTask(status.task_id(), status);` which will remove the queued task. (Introduced in this patch: https://reviews.apache.org/r/43258). 5) Since the above is async, it's possible that the task is still in queued task when we trying to see if we need to kill unregistered executor in `killTask`: {code}       // TODO(jieyu): Here, we kill the executor if it no longer has       // any task to run and has not yet registered. This is a       // workaround for those single task executors that do not have a       // proper self terminating logic when they haven't received the       // task within a timeout.       if (executor->queuedTasks.empty()) {         CHECK(executor->launchedTasks.empty())             << """" Unregistered executor '"""" << executor->id             << """"' has launched tasks"""";          LOG(WARNING) << """"Killing the unregistered executor """" << *executor                      << """" because it has no tasks"""";          executor->state = Executor::TERMINATING;          containerizer->destroy(executor->containerId);       }     {code}  6) Consequently, the executor will never be terminated by Mesos.  Attaching the relevant agent log: """
"MESOS-5378","Bug","framework|master",3,"Terminating a framework during master failover leads to orphaned tasks","""Repro steps:  1) Setup:   2) Kill all three from (1), in the order they were started.  3) Restart the master and agent.  Do not restart the framework.  Result) * The agent will reconnect to an orphaned task. * The Web UI will report no memory usage * {{curl localhost:5050/metrics/snapshot}} will say:  {{""""master/mem_used"""": 128,}}  Cause)  When a framework registers with the master, it provides a {{failover_timeout}}, in case the framework disconnects.  If the framework disconnects and does not reconnect within this {{failover_timeout}}, the master will kill all tasks belonging to the framework.  However, the master does not persist this {{failover_timeout}} across master failover.  The master will """"forget"""" about a framework if: 1) The master dies before {{failover_timeout}} passes. 2) The framework dies while the master is dead.  When the master comes back up, the agent will re-register.  The agent will report the orphaned task(s).  Because the master failed over, it does not know these tasks are orphans (i.e. it thinks the frameworks might re-register).  Proposed solution) The master should save the {{FrameworkID}} and {{failover_timeout}} in the registry.  Upon recovery, the master should resume the {{failover_timeout}} timers."""
"MESOS-5390","Bug","HTTP API",1,"v1 Executor Protos not included in maven jar","""According to MESOS-4793 the Executor v1 HTTP API was released in Mesos 0.28.0 however the corresponding protos are not included in the maven jar for version 0.28.0 or 0.28.1.  Script to verify """
"MESOS-5389","Bug","containerization",3,"docker containerizer should prefix relative volume.container_path values with the path to the sandbox","""docker containerizer currently requires absolute paths for values of volume.container_path. this is inconsistent with the mesos containerizer which requires relative container_path. it makes for a confusing API. both at the Mesos level as well as at the Marathon level.  ideally the docker containerizer would allow a framework to specify a relative path for volume.container_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it.  /cc [~jieyu]"""
"MESOS-5388","Bug","containerization",5,"MesosContainerizerLaunch flags execute arbitrary commands via shell.","""For example, the docker volume isolator's containerPath is appended (without sanitation) to a command that's executed in this manner. As such, it's possible to inject arbitrary shell commands to be executed by mesos.  https://github.com/apache/mesos/blob/17260204c833c643adf3d8f36ad8a1a606ece809/src/slave/containerizer/mesos/launch.cpp#L206  Perhaps instead of strings these commands could/should be sent as string arrays that could be passed as argv arguments w/o shell interpretation?"""
"MESOS-5404","Improvement","security",3,"Allow `Task` to be authorized.","""As we need to be able to authorize `Tasks` (e.g., for deciding whether to include them in the /state endpoint when applying authorization based filtering) we need to expose it to the authorizer. Secondly we also need to include some additional information (`user` and `Env variables`) in order to provide the authorizer  with meaning information."""
"MESOS-5397","Bug","project website",1,"Slave/Agent Rename Phase 1: Update terms in the website","""The following files need to be updated  site/source/index.html.md """
"MESOS-5425","Improvement","allocation",3,"Consider using IntervalSet for Port range resource math","""Follow-up JIRA for comments raised in MESOS-3051 (see comments there).  We should consider utilizing [{{IntervalSet}}|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/3rdparty/stout/include/stout/interval.hpp] in [Port range resource math|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/src/common/values.cpp#L143]."""
"MESOS-5437","Bug","documentation",1,"AppC  appc_simple_discovery_uri_prefix is lost in configuration.md","""AppC  appc_simple_discovery_uri_prefix is lost in configuration.md"""
"MESOS-5445","Bug","build",2,"Allow libprocess/stout to build without first doing `make` in 3rdparty.","""After the 3rdparty reorg, libprocess/stout are enable to build their dependencies and so one has to do `make` in 3rdpart/ before building libprocess/stout."""
"MESOS-5452","Improvement","containerization",1,"Agent modules should be initialized before all components except firewall.","""On Mesos Agents Anonymous modules should not have any dependencies, by design, on any other Mesos components. This implies that Anonymous modules should be initialized before all other Mesos components other than `Firewall`. The dependency on `Firewall` is primarily to enforce any policies to secure endpoints that might be owned by the Anonymous module."""
"MESOS-5450","Bug","agent",2,"Make the SASL dependency optional.","""Right now there is a hard dependency on SASL, which probably won't work well on Windows (at least) in the near future for our use cases.  In the future, it would be nice to have a pluggable authentication layer."""
"MESOS-5459","Improvement","security",5,"Update RUN_TASK_WITH_USER to use additional metadata","""Currently, the `authorization::Action` `RUN_TASK_WITH_USER` will pass the user as its `Object.value` string, but some authorizers may want to make authorization decisions based on additional task attributes, like role, resources, labels, container type, etc.  We should create a new Action `RUN_TASK` that passes FrameworkInfo and TaskInfo in its Object, and the LocalAuthorizer's RunTaskWithUser ACL can be implemented using the user found in TaskInfo/FrameworkInfo. We may need to leave the old _WITH_USER action around, but it's arguable whether we should call the authorizer once for RUN_TASK and once for RUN_TASK_WITH_USER, or only use the new action and deprecate the old one?"""
"MESOS-5532","Improvement","build",1,"Maven build is too verbose for batch builds","""During a non-interactive (without terminal) Mesos build, maven generates several thousands of log lines when downloading artifacts. This often makes several web-based log viewers unresponsive.  Further, these several thousand line long progress indicator logs don't provide any meaningful information either. From a user's point of view, just knowing that the artifact download succeeded/failed is often enough.  We should be using '--batch-mode' flag to disable these additionals log lines."""
"MESOS-5577","Bug","modules",1,"Modules using replicated log state API require zookeeper headers","""The state API uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during Mesos installation. """
"MESOS-5576","Improvement","leader election|master|replicated log",5,"Masters may drop the first message they send between masters after a network partition","""We observed the following situation in a cluster of five masters: || Time || Master 1 || Master 2 || Master 3 || Master 4 || Master 5 || | 0 | Follower | Follower | Follower | Follower | Leader | | 1 | Follower | Follower | Follower | Follower || Partitioned from cluster by downing this VM's network || | 2 || Elected Leader by ZK | Voting | Voting | Voting | Suicides due to lost leadership | | 3 | Performs consensus | Replies to leader | Replies to leader | Replies to leader | Still down | | 4 | Performs writing | Acks to leader | Acks to leader | Acks to leader | Still down | | 5 | Leader | Follower | Follower | Follower | Still down | | 6 | Leader | Follower | Follower | Follower | Comes back up | | 7 | Leader | Follower | Follower | Follower | Follower | | 8 || Partitioned in the same way as Master 5 | Follower | Follower | Follower | Follower | | 9 | Suicides due to lost leadership || Elected Leader by ZK | Follower | Follower | Follower | | 10 | Still down | Performs consensus | Replies to leader | Replies to leader || Doesn't get the message! || | 11 | Still down | Performs writing | Acks to leader | Acks to leader || Acks to leader || | 12 | Still down | Leader | Follower | Follower | Follower |  Master 2 sends a series of messages to the recently-restarted Master 5.  The first message is dropped, but subsequent messages are not dropped.  This appears to be due to a stale link between the masters.  Before leader election, the replicated log actors create a network watcher, which adds links to masters that join the ZK group: https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/network.hpp#L157-L159  This link does not appear to break (Master 2 -> 5) when Master 5 goes down, perhaps due to how the network partition was induced (in the hypervisor layer, rather than in the VM itself).  When Master 2 tries to send an {{PromiseRequest}} to Master 5, we do not observe the [expected log message|https://github.com/apache/mesos/blob/7a23d0da817be4e8f68d96f524cecf802431033c/src/log/replica.cpp#L493-L494]  Instead, we see a log line in Master 2:   The broken link is removed by the libprocess {{socket_manager}} and the following {{WriteRequest}} from Master 2 to Master 5 succeeds via a new socket."""
"MESOS-5597","Documentation","documentation",5,"Document Mesos ""health check"" feature.","""We don't talk about this feature at all."""
"MESOS-5618","Improvement","replicated log",3,"Added a metric indicating if replicated log for the registrar has recovered or not.","""This gives operator insight about the state of the replicated log for registrar. The operator needs to know when it is safe to move on to another master in the upgrade orchestration pipeline. """
"MESOS-5647","Task","containerization",5,"Expose network statistics for containers on CNI network in the `network/cni` isolator.","""We need to implement the `usage` method in the `network/cni` isolator to expose metrics relating to a containers network traffic.   On receiving a request for getting `usage` for a a given container the `network/cni` isolator could use NETLINK system calls to query the kernel for interface and routing statistics for a given container's network namespace."""
"MESOS-5650","Bug","allocation",5,"UNRESERVE operation causes master to crash.","""{{RESERVE}} operation may cause a master failure:   Possible reasons: * Recent improvements in allocator (b4d746f) * Bug in bookkeeping during the previous {{UNRESERVE}} * Network partition that happened after {{RESERVE}} and before {{UNRESERVE}}"""
"MESOS-5666","Improvement","containerization",2,"Deprecate camel case proto field in isolator ContainerConfig.","""Currently there are extra ExecutorInfo and TaskInfo in isolator ContaienrConfig, because a deprecation cycle is needed to deprecate camel cased proto field names. This JIRA is used for tracking this issue, which should address the TODO in isolator.proto."""
"MESOS-5660","Bug","test",2,"ContainerizerTest.ROOT_CGROUPS_BalloonFramework fails because executor environment isn't inherited","""A recent change forbits the executor to inherit environment variables from the agent's environment. As a regression this break {{ContainerizerTest.ROOT_CGROUPS_BalloonFramework}}."""
"MESOS-5659","Improvement","master",5,"Design doc for TASK_UNREACHABLE","""See MESOS-4049."""
"MESOS-5657","Bug","containerization",3,"Executors should not inherit environment variables from the agent.","""Currently executors are inheriting environment variables form the slave in mesos containerizer. This is problematic, because of two reasons:  1. When we use docker images (such as `mongo`) in unified containerizer, duplicated environment variables inherited from the slave lead to initialization failures, because LANG and/or LC_* environment variables are not set correctly.  2. When we are looking at the environment variables from the executor tasks, there are pages of environment variables listed, which is redundant and dangerous.  Depending on the reasons above, we propose that no longer allow executors to inherit environment variables from the slave. Instead, users should specify all environment variables they need by setting the slave flag `--executor_environment_variables` as a JSON format."""
"MESOS-5675","Improvement","master",3,"Add support for master capabilities","""Right now, frameworks can advertise their capabilities to the master via the {{FrameworkInfo}} they use for registration/re-registration. This allows masters to provide backward compatibility for old frameworks that don't support new functionality.  To allow new frameworks to support backward compatibility with old masters, the inverse concept would be useful: masters would tell frameworks which capabilities are supported by the master, which the frameworks could then use to decide whether to use features only supported by more recent versions of the master.  For now, frameworks can workaround this by looking at the master's version number, but that seems a bit fragile and hacky."""
"MESOS-5674","Bug","containerization|network",3,"Port mapping isolator may fail in 'isolate' method.","""Port mapping isolator may return failure in isolate method, if a symlink to the network namespace handle using that ContainerId already existed. We should overwrite the symlink if it exist.  This affects a couple test failures:   Here is an example failure test log: """
"MESOS-5673","Bug","containerization",3,"Port mapping isolator may cause segfault if it bind mount root does not exist.","""A check is needed for port mapping isolator for its bind mount root. Otherwise, non-existed port-mapping bind mount root may cause segmentation fault for some cases. Here is the test log:  """
"MESOS-5671","Bug","containerization|test",2,"MemoryPressureMesosTest.CGROUPS_ROOT_Statistics is flaky.",""""""
"MESOS-5670","Bug","containerization|test",2,"MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky.",""""""
"MESOS-5669","Bug","containerization|test",3,"CNI isolator should not return failure if /etc/hostname does not exist on host.","""/etc/hostname may not necessarily exist on every system (e.g., CentOS 6). Currently CNI isolator just return a failure if it does not exist on host, because the isolator need to mount it into the container. This is fine for /etc/host and /etc/resolv.conf, but we should make an exception for /etc/hostname, because hostname may still be accessible even if /etc/hostname doesn't exist.  This issue relates to 3 failure tests:     """
"MESOS-5668","Bug","containerization",3,"Add CGROUP namespace to linux ns helper.","""Since linux kernel 4.6, CGROUP namespace is added. we need to support the handle for the cgroup namespace of the process.  This also relates to two test failures on Ubuntu 16:   """
"MESOS-5667","Bug","containerization|test",2,"CniIsolatorTest.ROOT_INTERNET_CURL_LaunchCommandTask fails on CentOS 7.",""""""
"MESOS-5698","Bug","allocation",5,"Quota sorter not updated for resource changes at agent.","""Consider this sequence of events:  1. Slave connects, with 128MB of disk. 2. Master offers resources at slave to framework 3. Framework creates a dynamic reservation for 1MB and a persistent volume of the same size on the slave's resources.   => This invokes {{Master::apply}}, which invokes {{allocator->updateAllocation}}, which invokes {{Sorter::update()}} on the framework sorter and role sorter. If the framework's role has a configured quota, it also invokes {{update}} on the quota role sorter -- in this case, the framework's role has no quota, so the quota role sorter is *not* updated.   => {{DRFSorter::update}} updates the *total* resources at a given slave, among updating other state. New total resources will be 127MB of unreserved disk and 1MB of reserved disk with a volume. Note that the quota role sorter still thinks the slave has 128MB of unreserved disk. 4. The slave is removed from the cluster. {{HierarchicalAllocatorProcess::removeSlave}} invokes:    {{slaves\[slaveId\].total.nonRevocable()}} is 127MB of unreserved disk and 1MB of reserved disk with a volume. When we remove this from the quota role sorter, we're left with total resources on the reserved slave of 1MB of unreserved disk, since that is the result of subtracting <127MB unreserved, 1MB reserved+volume> from <128MB unreserved>.  The implications of this can't be good: at minimum, we're leaking resources for removed slaves in the quota role sorter. We're also introducing an inconsistency between {{total_.resources\[slaveId\]}} and {{total_.scalarQuantities}}, since the latter has already stripped-out volume/reservation information."""
"MESOS-5697","Improvement","containerization|storage",3,"Support file volume in mesos containerizer.","""Currently in mesos containerizer, the host_path volume (to be bind mounted from a host path) specified in ContainerInfo can only be a directory. We should also support the volume type as a file."""
"MESOS-5691","Bug","libprocess",5,"SSL downgrade support will leak sockets in CLOSE_WAIT status","""Repro steps: 1) Start a master:   2) Start an agent with SSL and downgrade enabled:   3) Start a framework that launches lots of executors, one after another:   4) Check FDs, repeatedly   The number of sockets in {{CLOSE_WAIT}} will increase linearly with the number of launched executors."""
"MESOS-5712","Task","documentation|security",1,"Document exactly what is handled by GET_ENDPOINTS_WITH_PATH acl","""Users may expect that the GET_ENDPOINT_WITH_PATH acl can be used with any Mesos endpoint, but that is not (yet) the case. We should clearly document the list of applicable endpoints, in authorization.md and probably even upgrades.md."""
"MESOS-5711","Task","documentation|security",2,"Update AUTHORIZATION strings in endpoint help","""The endpoint help macros support AUTHENTICATION and AUTHORIZATION sections. We added AUTHORIZATION help for some of the newer endpoints, but not the previously authenticated endpoints.  Authorization endpoints needing help string updates: Master::Http::CREATE_VOLUMES_HELP Master::Http::DESTROY_VOLUMES_HELP Master::Http::RESERVE_HELP Master::Http::STATE_HELP Master::Http::STATESUMMARY_HELP Master::Http::TEARDOWN_HELP Master::Http::TASKS_HELP Master::Http::UNRESERVE_HELP Slave::Http::STATE_HELP"""
"MESOS-5709","Task","security",3,"Authorization for /roles","""The /roles endpoint exposes the list of all roles and their weights, as well as the list of all frameworkIds registered with each role. This is a superset of the information exposed on GET /weights, which we already protect. We should protect the data in /roles the same way. - Should we reuse VIEW_FRAMEWORK with role (from /state)? - Should we add a new VIEW_ROLE and adapt GET_WEIGHTS to use it?"""
"MESOS-5708","Task","security",3,"Add authz to /files/debug","""The /files/debug endpoint exposes the attached master/agent log paths and every attached sandbox path, which includes the frameworkId and executorId. Even if sandboxes are protected, we still don't want to expose this information to unauthorized users."""
"MESOS-5707","Task","security",3,"LocalAuthorizer should error if passed a GET_ENDPOINT ACL with an unhandled path","""Since GET_ENDPOINT_WITH_PATH doesn't (yet) work with any arbitrary path, we should a) validate --acls and error if GET_ENDPOINT_WITH_PATH has a path object that doesn't match an endpoint that uses this authz strategy. b) document exactly which endpoints support GET_ENDPOINT_WITH_PATH"""
"MESOS-5706","Task","security",2,"GET_ENDPOINT_WITH_PATH authz doesn't make sense for /flags","""The master or agent flags are exposed in /state as well as /flags, so any user who wants to disable/control access to the flags likely intends to control access to flags no matter what endpoint exposes them. As such, /flags is a poor candidate for GET_ENDPOINT_WITH_PATH authz, since we care more about protecting the flag data than the specific endpoint path. We should remove the GET_ENDPOINT authz from master and agent /flags until we can come up with a better solution, perhaps a first-class VIEW_FLAGS acl."""
"MESOS-5705","Task","master|security",5,"ZK credential is exposed in /flags and /state","""Mesos allows zk credentials to be embedded in the zk url, but exposes these credentials in the /flags and /state endpoint. Even though /state is authorized, it only filters out frameworks/tasks, so the top-level flags are shown to any authenticated user.  """"zk"""": """"zk://dcos_mesos_master:my_secret_password@127.0.0.1:2181/mesos"""",  We need to find some way to hide this data, or even add a first-class VIEW_FLAGS acl that applies to any endpoint that exposes flags."""
"MESOS-5704","Task","master|security",3,"Fine-grained authorization on /frameworks","""Even if ACLs were defined for the actions VIEW_FRAMEWORKS, VIEW_EXECUTORS and VIEW_TASKS, the data these actions were supposed to protect, could still leaked through the master's /frameworks endpoint, since it didn't enable any authorization mechanism."""
"MESOS-5699","Task","documentation",1,"Create new documentation for Mesos networking.","""With introduction of CNI and dockers support docker user-defined networks, there are quite a few options within Mesos for IP-per-container solutions for container networking.   We therefore need to re-write networking documentation for Mesos highlighting all the networking support that Mesos provides for orchestrating containers on IP networks."""
"MESOS-5723","Bug","libprocess",2,"SSL-enabled libprocess will leak incoming links to forks","""Encountered two different buggy behaviors that can be tracked down to the same underlying problem.  Repro #1 (non-crashy): (1) Start a master.  Doesn't matter if SSL is enabled or not. (2) Start an agent, with SSL enabled.  Downgrade support has the same problem.  The master/agent {{link}} to one another. (3) Run a sleep task.  Keep this alive.  If you inspect FDs at this point, you'll notice the task has inherited the {{link}} FD (master -> agent). (4) Restart the agent.  Due to (3), the master's {{link}} stays open. (5) Check master's logs for the agent's re-registration message. (6) Check the agent's logs for re-registration.  The message will not appear.  The master is actually using the old {{link}} which is not connected to the agent.  ----  Repro #2 (crashy): (1) Start a master.  Doesn't matter if SSL is enabled or not. (2) Start an agent, with SSL enabled.  Downgrade support has the same problem. (3) Run ~100 sleep task one after the other, keep them all alive.  Each task links back to the agent.  Due to an FD leak, each task will inherit the incoming links from all other actors... (4) At some point, the agent will run out of FDs and kernel panic.  ----  It appears that the SSL socket {{accept}} call is missing {{os::nonblock}} and {{os::cloexec}} calls: https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L794-L806  For reference, here's {{poll}} socket's {{accept}}: https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/poll_socket.cpp#L53-L75 """
"MESOS-5716","Improvement","containerization|documentation",3,"Document docker private registry with authentication support in Unified Containerizer.","""Add documentation for docker private registry with authentication support in unified containerizer. This is the basic support for docker private registry."""
"MESOS-5727","Bug","containerization",5,"Command executor health check does not work when the task specifies container image.","""Since we launch the task after pivot_root, we no longer has the access to the mesos-health-check binary. The solution is to refactor health check to be a library (libprocess) so that it does not depend on the underlying filesystem.  One note here is that we should strive to keep both the command executor and the task in the same mount namespace so that Mesos CLI tooling does not need to find the mount namespace for the task. It just need to find the corresponding pid for the executor. This statement is *arguable*, see the comment below."""
"MESOS-5745","Bug","test",3,"AuthenticationTest.UnauthenticatedSlave fails with clang++3.8","""With {{clang++-3.8}}, {{make check}} fails with the following message:  """
"MESOS-5740","Improvement","libprocess",3,"Consider adding `relink` functionality to libprocess","""Currently we don't have the {{relink}} functionality in libprocess.  i.e. A way to create a new persistent connection between actors, even if a connection already exists.   This can benefit us in a couple of ways: - The application may have more information on the state of a connection than libprocess does, as libprocess only checks if the connection is alive or not.  For example, a linkee may accept a connection, then fork, pass the connection to a child, and subsequently exit.  As the connection is still active, libprocess may not detect the exit. - Sometimes, the {{ExitedEvent}} might be delayed or might be dropped due to the remote instance being unavailable (e.g., partition, network intermediaries not sending RST's etc).  """
"MESOS-5753","Improvement","containerization",8,"Command executor should use `mesos-containerizer launch` to launch user task.","""Currently, command executor and `mesos-containerizer launch` share a lot of the logic. Command executor should in fact, just use `mesos-containerizer launch` to launch the user task.  Potentially, `mesos-containerizer launch` can be also used by custom executor to launch user tasks."""
"MESOS-5748","Bug","libprocess",2,"Potential segfault in `link` and `send` when linking to a remote process","""There is a race in the SocketManager, between a remote {{link}} and disconnection of the underlying socket.  We potentially segfault here: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1512  {{\*socket}} dereferences the shared pointer underpinning the {{Socket*}} object.  However, the code above this line actually has ownership of the pointer: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1494-L1499  If the socket dies during the link, the {{ignore_recv_data}} may delete the Socket underneath {{link}}: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1399-L1411  ---- The same race exists for {{send}}.  This race was discovered while running a new test in repetition: https://reviews.apache.org/r/49175/  On OSX, I hit the race consistently every 500-800 repetitions: """
"MESOS-5759","Bug","libprocess|test",1,"ProcessRemoteLinkTest.RemoteUseStaleLink and RemoteStaleLinkRelink are flaky","""{{ProcessRemoteLinkTest.RemoteUseStaleLink}} and {{ProcessRemoteLinkTest.RemoteStaleLinkRelink}} are failing occasionally with the error:   There appears to be a race between establishing a socket connection and the test calling {{::shutdown}} on the socket.  Under some circumstances, the {{::shutdown}} may actually result in failing the future in {{SocketManager::link_connect}} error and thereby trigger {{SocketManager::close}}."""
"MESOS-5792","Improvement","build",8,"Add mesos tests to CMake (make check)","""Provide CMakeLists.txt and configuration files to build mesos tests using CMake."""
"MESOS-5782","Improvement","containerization",2,"Renamed 'commands' to 'pre_exec_commands' in ContainerLaunchInfo.","""Currently the 'commands' in isolator.proto ContainerLaunchInfo is somehow confusing. It is a pre-executed command (can be any script or shell command) before launch. We should renamed 'commands' to 'pre_exec_commands' in ContainerLaunchInfo and add comments."""
"MESOS-5804","Bug","allocation|test",3,"ExamplesTest.DynamicReservationFramework is flaky","""Showed up on ASF CI:  https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2466/changes        Logs from a previous good run:  """
"MESOS-5802","Bug","agent",2,"SlaveAuthorizerTest/0.ViewFlags is flaky.",""""""
"MESOS-5824","Improvement","stout",3,"Include disk source information in stringification","""Some frameworks (like kafka_mesos) ignore the Source field when trying to reserve an offered mount or path persistent volume; the resulting error message is bewildering:  {code:none} Task uses more resources cpus(*):4; mem(*):4096;     ports(*):[31000-31000]; disk(kafka, kafka)[kafka_0:data]:960679 than available cpus(*):32; mem(*):256819;  ports(*):[31000-32000]; disk(kafka, kafka)[kafka_0:data]:960679;   disk(*):240169; {code}  The stringification of disk resources should include source information. """
"MESOS-5822","Improvement","build",3,"Add a build script for the Windows CI","""The ASF CI for Mesos runs a script that lives inside the Mesos codebase: https://github.com/apache/mesos/blob/1cbfdc3c1e4b8498a67f8531ab264003c8c19fb1/support/docker_build.sh  ASF Infrastructure have set up a machine that we can use for building Mesos on Windows.  Considering the environment, we will need a separate script to build here."""
"MESOS-5812","Bug","test",3,"MasterAPITest.Subscribe is flaky","""This test seems to be flaky, although on Mac OS X and CentOS 7 the error a bit different.  On Mac OS X:   On CentOS 7 """
"MESOS-5806","Bug","containerization|network",5,"CNI isolator should prepare network related /etc/* files for containers using host mode but specify container images.","""Currently, the CNI isolator will just ignore those containers that want to join the host network (i.e., not specifying NetworkInfo). However, if the container specifies a container image, we need to make sure that it has access to host /etc/* files. We should perform the bind mount for the container. This is also what docker does when a container is running in host mode."""
"MESOS-5825","Improvement","containerization",5,"Support mounting image volume in mesos containerizer.","""Mesos containerizer should be able to support mounting image volume type. Specifically, both image rootfs and default manifest should be reachable inside container's mount namespace."""
"MESOS-5845","Bug","fetcher",3,"The fetcher can access any local file as root","""The Mesos fetcher currently runs as root and does a blind cp+chown of any file:// URI into the task's sandbox, to be owned by the task user. Even if frameworks are restricted from running tasks as root, it seems they can still access root-protected files in this way. We should secure the fetcher so that it has the filesystem permissions of the user its associated task is being run as. One option would be to run the fetcher as the same user that the task will run as."""
"MESOS-5852","Bug","build|cmake",2,"CMake build needs to generate protobufs before building libmesos","""The existing CMake lists place protobufs at the same level as other Mesos sources: https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/CMakeLists.txt#L415  This is incorrect, as protobuf changes need to be regenerated before we can build against them.  Note: in the autotools build, this is done by compiling protobufs into {{libmesos}}, which then builds {{libmesos_no_3rdparty}}: https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/Makefile.am#L1304-L1305"""
"MESOS-5864","Bug","containerization|documentation",2,"Document MESOS_SANDBOX executor env variable.","""And we should document the difference with MESOS_DIRECTORY."""
"MESOS-5879","Bug","agent|containerization",1,"cgroups/net_cls isolator causing agent recovery issues","""We run with 'cgroups/net_cls' in our isolator list, and when we restart any agent process in a cluster running an experimental custom isolator as well, the agents are unable to recover from checkpoint, because net_cls reports that unknown orphan containers have duplicate net_cls handles.  While this is a problem that needs to be solved (probably by fixing our custom isolator), it's also a problem that the net_cls isolator fails recovery just for duplicate handles in cgroups that it is literally about to unconditionally destroy during recovery. Can this be fixed?"""
"MESOS-5878","Bug","test",3,"Strict/RegistrarTest.UpdateQuota/0 is flaky","""Observed on ASF CI (https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2539/consoleFull). Log file is attached. Note that this might have been uncovered due to the recent removal of {{os::sleep}} from {{Clock::settle}}."""
"MESOS-5900","Improvement","libprocess",5,"Support Unix domain socket connections in libprocess","""We should consider allowing two programs on the same host using libprocess to communicate via Unix domain sockets rather than TCP. This has a few advantages:  * Security: remote hosts cannot connect to the Unix socket. Domain sockets also offer additional support for [authentication|https://docs.fedoraproject.org/en-US/Fedora_Security_Team/1/html/Defensive_Coding/sect-Defensive_Coding-Authentication-UNIX_Domain.html]. * Performance: domain sockets are marginally faster than localhost TCP."""
"MESOS-5909","Bug","stout",2,"Stout ""OsTest.User"" test can fail on some systems","""Libc call {{getgrouplist}} doesn't return the {{gid}} list in a sorted manner (in my case, it's returning """"471 100"""") ... whereas {{id -G}} return a sorted list (""""100 471"""" in my case) causing the validation inside the loop to fail.  We should sort both lists before comparing the values."""
"MESOS-5907","Bug","test",1,"ExamplesTest.DiskFullFramework fails on Arch","""This test fails consistently on recent Arch linux, running in a VM."""
"MESOS-5931","Improvement","containerization",8,"Support auto backend in Unified Containerizer.","""Currently in Unified Containerizer, copy backend will be selected by default. This is not ideal, especially for production environment. It would take a long time to prepare an huge container image to copy it from the store to provisioner.  Ideally, we should support `auto backend`, which would automatically/intelligently select the best/optimal backend for image provisioner if user does not specify one from the agent flag.  We should have a logic design first in this ticket, to determine how we want to choose the right backend (e.g., overlayfs or aufs should be preferred if available from the kernel)."""
"MESOS-5930","Bug","master",3,"Orphan tasks can show up as running after they have finished.","""On my cluster I have 111 Orphan Tasks of which some are RUNNING some are FINISHED and some are FAILED. When I open the task details for a FINISHED tasks the following page shows a state of TASK_FINISHED and likewise when I open a FAILED task the details page shows TASK_FAILED.  However when I open the details for the RUNNING tasks they all have a task state of TASK_FINISHED. None of them is in state TASK_RUNNING. """
"MESOS-5927","Bug","containerization",3,"Unable to run ""scratch"" Dockerfiles with Unified Containerizer.","""It is not possible to run Docker containers that are based upon the """"scratch"""" container.  Setup: Mesos 1.0.0 with the following Mesos settings:  {code:none} echo 'docker' | sudo tee /etc/mesos-slave/image_providers echo 'filesystem/linux,docker/runtime' | sudo tee /etc/mesos-slave/isolation   Effect: The container will crash with messages from Mesos reporting it can't mount folder x/y/z. E.g. can't mount /tmp. This means you can't run any container that is not a """"fat"""" container (i.e. one with a full OS). E.g. error:  bq. Failed to enter chroot '/var/lib/mesos/provisioner/containers/fed6add8-0126-40e6-ae81-5859a0c1a2d4/backends/copy/rootfses/4feefc8b-fd5a-4835-95db-165e675f11cd': /tmp in chroot does not existI0729 07:49:56.753474  4362 exec.cpp:413] Executor asked to shutdown  Expected: Run without issues.  Use case: We use scratch based containers with static binaries to keep the image size down. This is a common practice."""
"MESOS-5944","Improvement","agent",1,"Remove `O_SYNC` from StatusUpdateManager logs","""Currently the {{StatusUpdateManager}} uses {{O_SYNC}} to flush status updates to disk.   We don't need to use {{O_SYNC}} because we only read this file if the host did not crash. {{os::write}} success implies the kernel will have flushed our data to the page cache. This is sufficient for the recovery scenarios we use this data for."""
"MESOS-5943","Bug","libprocess|scheduler driver",3,"Incremental http parsing of URLs leads to decoder error","""When requests arrive to the decoder in pieces (e.g. {{mes}} followed by a separate chunk of {{os.apache.org}}) the http parser is not able to handle this case if the split is within the URL component.  This causes the decoder to error out, and can lead to connection invalidation.  The scheduler driver is susceptible to this."""
"MESOS-5958","Bug","build",2,"Reviewbot failing due to python files not being cleaned up after distclean","""This is on ASF CI. https://builds.apache.org/job/mesos-reviewbot/14573/consoleFull  """
"MESOS-5970","Task","libprocess",1,"Remove HTTP_PARSER_VERSION_MAJOR < 2 code in decoder.","""https://reviews.apache.org/r/50683"""
"MESOS-5988","Bug","libprocess",3,"PollSocketImpl can write to a stale fd.","""When tracking down MESOS-5986 with [~greggomann] and [~anandmazumdar]. We were curious why PollSocketImpl avoids the same issue. It seems that PollSocketImpl has a similar race, however in the case of PollSocketImpl we will simply write to a stale file descriptor.  One example is {{PollSocketImpl::send(const char*, size_t)}}:  https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/poll_socket.cpp#L241-L245   If the last reference to the {{Socket}} goes away before the {{socket_send_data}} loop completes, then we will write to a stale fd!  It turns out that we have avoided this issue because in libprocess we happen to keep a reference to the {{Socket}} around when sending:  https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/process.cpp#L1678-L1707   However, this may not be true in all call-sites going forward. Currently, it appears that http::Connection can trigger this bug."""
"MESOS-5986","Bug","libprocess",3,"SSL Socket CHECK can fail after socket receives EOF","""While writing a test for MESOS-3753, I encountered a bug where [this check|https://github.com/apache/mesos/blob/853821cafcca3550b9c7bdaba5262d73869e2ee1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L708] fails at the very end of the test body, while objects in the stack frame are being destroyed. After adding some debug logging output, I produced the following:   The {{in send()17}} line indicates the beginning of {{send()}} for the SSL socket using FD 17. {{in shutdown(): 17}} indicates the beginning of {{shutdown()}} for the same socket, while {{sending on socket: 17}} indicates the execution of the lambda from {{send()}} on the event loop. Since {{shutdown()}} was called in between the call to {{send()}} and the execution of its lambda, it looks like the {{Socket}} was destroyed before the lambda could run. It's unclear why this would happen, since {{send()}}'s lambda captures a shared copy of the socket's {{this}} pointer in order to keep it alive."""
"MESOS-6001","Bug","containerization",3,"Aufs backend cannot support the image with numerous layers.","""This issue was exposed in this unit test `ROOT_CURL_INTERNET_DockerDefaultEntryptRegistryPuller` by manually specifying the `bind` backend. Most likely mounting the aufs with specific options is limited by string length.  """
"MESOS-5995","Bug","HTTP API",1,"Protobuf JSON deserialisation does not accept numbers formated as strings","""Proto2 does not specify JSON mappings but [Proto3|https://developers.google.com/protocol-buffers/docs/proto3#json] does and it recommend to map 64bit numbers as a string. Unfortunately Mesos does not accepts strings in places of uint64 and return 400 Bad  {quote} Request error Failed to convert JSON into Call protobuf: Not expecting a JSON string for field 'value'. {quote} Is this by purpose or is this a bug?"""
"MESOS-6015","Task","containerization",1,"Design for port-mapper CNI plugin","""Create a design doc for port-mapper CNI plugin."""
"MESOS-6014","Epic","containerization",5,"Added port mapping CNI plugin.","""Currently there is no CNI plugin that supports port mapping. Given that the unified containerizer is starting to become the de-facto container run time, having  a CNI plugin that provides port mapping is a must have. This is primarily required for support BRIDGE networking mode, similar to docker bridge networking that users expect to have when using docker containers.   While the most obvious use case is that of using the port-mapper plugin with the bridge plugin, the port-mapping functionality itself is generic and should be usable with any CNI plugin that needs it.  Keeping port-mapping as a CNI plugin gives operators the ability to use the default port-mapper (CNI plugin) that Mesos provides, or use their own plugin."""
"MESOS-6013","Bug","stout",3,"Use readdir instead of readdir_r.","""{{readdir_r}} is deprecated in recent versions of glibc (https://sourceware.org/ml/libc-alpha/2016-02/msg00093.html). As a result, Mesos doesn't build on recent Arch Linux:    Seems like {{readdir_r}} is deprecated; manpage suggests using {{readdir}} instead."""
"MESOS-6023","Task","containerization",8,"Create a binary for the port-mapper plugin","""The CNI port mapper plugin needs to be a separate binary that will be invoked by the `network/cni` isolator as a CNI plugin."""
"MESOS-6022","Task","containerization",3,"unit-test for port-mapper CNI plugin","""Write unit-tests for the port mapper plugin."""
"MESOS-6017","Task","containerization",1,"Introduce `PortMapping` protobuf.","""Currently we have a `PortMapping` message defined for `DockerInfo`. This can be used only by the `DockerContainerizer`. We need to introduce a new Protobuf message in `NetworkInfo` which will allow frameworks to specify port mapping when using CNI with the `MesosContainerizer`."""
"MESOS-6052","Bug","containerization",1,"Unable to launch containers on CNI networks on CoreOS","""CoreOS does not have an `/etc/hosts`. Currently, in the `network/cni` isolator, if we don't see a `/etc/hosts` on the host filesystem we don't bind mount the containers `hosts` file to this target for the `command executor`. On distros such as CoreOS this fails the container launch since the `libprocess` initialization of the `command executor` fails cause it can't resolve its `hostname`.  We should be creating the `/etc/hosts` and `/etc/hostname` files when they are absent on the host filesystem since creating these files should not affect name resolution on the host network namespace, and it will allow the `/etc/hosts` file to be bind mounted correctly and allow name resolution in the containers network namespace as well. """
"MESOS-6067","Task","containerization",8,"Support provisioner to be nested aware for Mesos Pods.","""The provisioner has to be nested aware for sub-container provisioning, as well as recovery and nested container destroy. Better to support multi-level hierarchy. """
"MESOS-6065","Improvement","containerization",5,"Support provisioning image volumes in an isolator.","""Currently the image volumes are provisioned in mesos containerizer. This makes the containerzer logic complicated, and hard to make containerizer launch to be nest aware.  We should implement a 'volume/image' isolator to move these part of logic away from the mesos containerizer."""
"MESOS-6110","Bug","master",3,"Deprecate using health checks without setting the type","""When sending a task launch using the 1.0.x protos and the legacy (non-http) API, tasks with a healthcheck defined are rejected (TASK_ERROR) because the 'type' field is not set.  This field is marked optional in the proto and is not available before 1.1.0, so it should not be required in order to keep the mesos v1 api compatibility promise.  For backwards compatibility temporarily allow the use case when command health check is set without a type."""
"MESOS-6104","Bug","webui",3,"Potential FD double close in libevent's implementation of `sendfile`.","""Repro copied from: https://reviews.apache.org/r/51509/  It is possible to make the master CHECK fail by repeatedly hitting the web UI and reloading the static assets:  1) Paste lots of text (16KB or more) of text into `src/webui/master/static/home.html`.  The more text, the more reliable the repro.  2) Start the master with SSL enabled:   3) Run two instances of this python script repeatedly:   i.e.  """
"MESOS-6115","Bug","stout|test",3,"Source tree contains compiled protobuf source","""Stout's {{protobuf_tests.cpp}} uses checked in, generated protobuf files {{protobuf_tests.pb.h}} and {{protobuf_tests.pb.cc}}.  These files are * not meant to be edited, * might require updates whenever protobuf is updated, and * likely do not follow Mesos coding standards.  We should try to remove them from the source tree."""
"MESOS-6130","Task","containerization",3,"Make the disk usage isolator nesting-aware","""With the addition of task groups, the disk usage isolator must be updated. Since sub-container sandboxes are nested within the parent container's sandbox, the isolator must exclude these folders from its usage calculation when examining the parent container's disk usage."""
"MESOS-6142","Bug","allocation|master",3,"Frameworks may RESERVE for an arbitrary role.","""The master does not validate that resources from a reservation request have the same role the framework is registered with. As a result, frameworks may reserve resources for arbitrary roles.  I've modified the role in [the {{ReserveThenUnreserve}} test|https://github.com/apache/mesos/blob/bca600cf5602ed8227d91af9f73d689da14ad786/src/tests/reservation_tests.cpp#L117] to """"yoyo"""" and observed the following in the test's log: """
"MESOS-6140","Improvement","test",5,"Add a parallel test runner","""In order to allow parallelization of the test execution we should add a parallel test executor to Mesos, and subsequently activate it in the build setup."""
"MESOS-6150","Task","containerization",5,"Introduce the new isolator recover interface for nested container support.","""Currently, the isolator::recover include two parameters: 1. The list of ContainerState, which are the checkpointed conttainers. 2. The hashset of orphans, which are returned from the launcher::recover.  However, to support nested containers in Mesos Pod, this interface is not sufficient. Because unknown nested containers may exist under either the top level alive container or orphan container. We have to include a full list of unknown containers which includes containers from all hierarchy.  We could have added a 3rd parameter to the isolator::recover interface, to guarantee the backward compatibility. However, considering the potential interface changes in the future work and the old orphan hashset should be deprecated, it is the right time to introduce a new protobuf message `ContainerRecoverInfo` for isolator::recover(), which wraps all information for isolators to recover containers."""
"MESOS-6156","Task","containerization",3,"Make the `network/cni` isolator nesting aware","""In pods, child containers share the network and UTS namespace with the parent containers. This implies that during `prepare` and `isolate` the `network/cni` isolator needs to be aware the parent-child relationship between containers to make the following decisions: a) During `prepare` a container should be allocated a new network namespace and UTS namespace only if the container is a top level container. b) During `isolate` the network files (/etc/hosts, /etc/hostname, /etc/resolv.conf) should be created only for top level containers. The network files for child containers will just be symlinks to the parent containers network files."""
"MESOS-6162","Task","containerization",8,"Add support for cgroups blkio subsystem blkio statistics.","""Noted that cgroups blkio subsystem may have performance issue, refer to https://github.com/opencontainers/runc/issues/861"""
"MESOS-6159","Bug","stout",1,"Remove stout's Set type","""stout provides a {{Set}} type which wraps a {{std::set}}. As only addition it provides new constructors,  which simplified creation of a {{Set}} from (up to four) known elements.  C++11 brought {{std::initializer_list}} which can be used to create a {{std::set}} from an arbitrary number of elements, so it appears that it should be possible to retire {{Set}}."""
"MESOS-6208","Bug","containerization",1,"Containers that use the Mesos containerizer but don't want to provision a container image fail to validate.","""Tasks using  features like volumes or CNI in their containers, have to define these in {{TaskInfo.container}}. When these tasks don't want/need to provision a container image, neither {{ContainerInfo.docker}} nor {{ContainerInfo.mesos}} will be set. Nevertheless, the container type in {{ContainerInfo.type}} needs to be set, because it is a required field. In that case, the recently introduced validation rules in {{master/validation.cpp}} ({{validateContainerInfo}} will fail, which isn't expected."""
"MESOS-6216","Bug","security",5,"LibeventSSLSocketImpl::create is not safe to call concurrently with os::getenv","""{{LibeventSSLSocketImpl::create}} is called whenever a potentially ssl-enabled socket is created. It in turn calls {{openssl::initialize}} which calls a function {{reinitialize}} using {{os::setenv}}. Here {{os::setenv}} is used to set up SSL-related libprocess environment variables {{LIBPROCESS_SSL_*}}.  Since {{os::setenv}} is not thread-safe just like the {{::setenv}} it wraps, any calling of functions like {{os::getenv}} (or via {{os::environment}}) concurrently with the first invocation of {{LibeventSSLSocketImpl::create}} performs unsynchronized r/w access to the same data structure in the runtime.  We usually perform most setup of the environment before we start the libprocess runtime with {{process::initialize}} from a {{main}} function, see e.g., {{src/slave/main.cpp}} or {{src/master/main.cpp}} and others. It appears that we should move the setup of libprocess' SSL environment variables to a similar spot."""
"MESOS-6233","Bug","master",2,"Master CHECK fails during recovery while relinking to other masters","""Mesos Version: 1.0.1 OS: CoreOS 1068  """
"MESOS-6234","Bug","libprocess",3,"Potential socket leak during Zookeeper network changes","""There is a potential leak when using the version of {{link}} with {{RemoteConnection::RECONNECT}}.  This was originally implemented to refresh links during master recovery.   The leak occurs here: https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1592-L1597 ^ The comment here is not correct, as that is *not* the last reference to the {{existing}} socket.  At this point, the {{existing}} socket may be a perfectly valid link.  Valid links will all have a reference inside a callback loop created here: https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1503-L1509  -----  We need to stop the callback loop but prevent any resulting {{ExitedEvents}} from being sent due to stopping the callback loop.  This means discarding the callback loop's future after we have called {{swap_implementing_socket}}."""
"MESOS-6246","Bug","libprocess",2,"Libprocess links will not generate an ExitedEvent if the socket creation fails","""Noticed this while inspecting nearby code for potential races.  Normally, when a libprocess actor (the """"linkee"""") links to a remote process, it does the following: 1) Create a socket. 2) Connect to the remote process (asynchronous). 3) Check the connection succeeded.  If (2) or (3) fail, the linkee will receive a {{ExitedEvent}}, which indicates that the link broke.  In case (1) fails, there is no {{ExitedEvent}}: https://github.com/apache/mesos/blob/7c833abbec9c9e4eb51d67f7a8e7a8d0870825f8/3rdparty/libprocess/src/process.cpp#L1558-L1562"""
"MESOS-6263","Bug","containerization",3,"Mesos containerizer should figure out the correct sandbox directory for nested launch.","""Currently the mesos containerizer take the sandbox directory from the agent. Ideally, a nested sandbox dir can be figured out by the containerizer. And there is no need to pass it from the agent. We should remove the `directory` parameter in nested launch interface."""
"MESOS-6280","Improvement","executor",5,"Task group executor should support command health checks.","""Currently, the default (aka pod) executor supports only HTTP and TCP health checks. We should also support command health checks as well."""
"MESOS-6290","Bug","containerization",2,"Support nested containers for logger in Mesos Containerizer.","""Currently, there are two issues in mesos containerizer using logger for nested contaienrs:  1. An empty executorinfo is passed to logger when launching a nested container, it would potentially break some logger modules if any module tries to access the required proto field (e.g., executorId).  2. The logger does not reocver the nested containers yet in MesosContainerizer::recover."""
"MESOS-6302","Bug","containerization",3,"Agent recovery can fail after nested containers are launched","""After launching a nested container which used a Docker image, I restarted the agent which ran that task group and saw the following in the agent logs during recovery:  and the agent continues to restart in this fashion. Attached is the Marathon app definition that I used to launch the task group."""
"MESOS-6305","Improvement","security",3,"Add authorization support for nested container calls","""We need to authorize {LAUNCH, KILL, WAIT}_NESTED_CONTAINER API calls."""
"MESOS-6304","Improvement","executor|modules|security",2,"Add authentication support to the default executor","""The V1 executors should be updated to authenticate with the agent when HTTP executor authentication is enabled. This will be hard-coded into the executor library for the MVP, and it can be refactored into an {{HttpAuthenticatee}} module later. The executor must: * load a JWT from its environment, if present * decorate its requests with an {{Authorization}} header containing the JWT"""
"MESOS-6324","Bug","containerization",1,"CNI should not use `ifconfig` in executors `pre_exec_command`","""Currently the `network/cni` isolator sets up the `pre_exec_command` for executors when a container needs to be launched on a non-host network. The `pre_exec_command` is `ifconfig lo up`. This is done to primarily bring loopback up in the new network namespace.  Setting up the `pre_exec_command` to bring loopback up is problematic since the executors PATH variable is generally very limited (doesn't contain all path that the agents PATH variable has due to security concerns).   Therefore instead of running `ifconfig lo up` in the `pre_exec_command` we should run it in `NetworkCniIsolatorSetup` subcommand, which runs with the same PATH variable as the agent."""
"MESOS-6344","Task","containerization",1,"Allow `network/cni` isolator to take a search path for CNI plugins instead of single directory","""Currently the `network/cni` isolator expects a single directory with the `--network_cni_plugins_dir` . This is very limiting because this forces the operator to put all the CNI plugins in the same directory.   With Mesos port-mapper CNI plugin this would also imply that the operator would have to move this plugin from the Mesos installation directory to a directory specified in the `--network_cni_plugins_dir`.   To simplify the operators experience it would make sense for the `--network_cni_plugins_dir` flag to take in set of directories instead of single directory. The `network/cni` isolator can then search this set of directories to find the CNI plugin."""
"MESOS-6369","Improvement","webui",1,"Add a column for FrameworkID when displaying tasks in the WebUI","""The Mesos Web UI home page shows a list of active/completed/orphan tasks tasks like this: || ID || Name || State || Started || Host || || | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 2 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |  When you start multiple frameworks, the task IDs and names show in the UI may be ambiguous, requiring extra clicks/investigation to disambiguate.    In the above case, to disambiguate between the two tasks with ID {{1}}, the user would need to navigate to each sandbox and check the associated frameworkID in the {{/browse}} view.    We could add a column showing the {{FrameworkID}} next to each task: || Framework || ID || Name || State || Started || Host || || | 179b5436-30ec-45e9-b324-fa5c5a1dd756-0000 | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 179b5436-30ec-45e9-b324-fa5c5a1dd756-0001 | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox | | 179b5436-30ec-45e9-b324-fa5c5a1dd756-0001 | 2 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |  The {{FrameworkID}} s could be links to the associated framework  ----- This involves additions to three tables: https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L152-L157 https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L199-L205 https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L246-L252 """
"MESOS-6371","Improvement","containerization",3,"Remove the 'recover()' interface in 'ContainerLogger'.","""This issue arises from the nested container support in Mesos.  Currently, the container logger interface mainly contains `recover()` and `prepare()` methods. The `prepare` will be called in containerizer::launch() to launch a container, while `recover` will be called in containerizer::recover() to recover containers. Both methods rely on 2 parameters: ExecutorInfo and sandbox directory. The sandbox directory for nested containers can still be passed to the logger. However, because of nested container support, ExecutorInfo is no longer available for nested containers.  In logger prepare, the ExecutorInfo is used for deliver FrameworkID, ExecutorID, and Label for custom metadata. In containerizer launch, we can still pass the ExecutorInfo of a nested container's top level parent to the logger, so that those information will not be lost.  In logger recover, since currently the logger is stateless, and most of the logger modules are doing `noop` in logger::recover(). The recover interface should exist together with `cleanup` method if the logger become stateful in the future. To avoid adding tech debt in containerizer nested container support, we should remove the `recover` in container logger for now (can add it back together with `cleanup` in the future if the container logger become stateful)."""
"MESOS-6388","Bug","master",1,"Report new PARTITION_AWARE task statuses in HTTP endpoints","""At a minimum, the {{/state-summary}} endpoint needs to be updated."""
"MESOS-6386","Bug","containerization",3,"""Reached unreachable statement"" in LinuxCapabilitiesIsolatorTest","""  Observed running the tests as root on CentOS 7.2. Verbose test output attached."""
"MESOS-6411","Documentation","containerization",1,"Add documentation for CNI port-mapper plugin.","""Need to add the CNI port-mapper plugin to the CNI documentation within Mesos."""
"MESOS-6419","Bug","master",8,"The 'master/teardown' endpoint should support tearing down 'unregistered_frameworks'.","""This issue is exposed from [MESOS-6400](https://issues.apache.org/jira/browse/MESOS-6400). When a user is trying to tear down an 'unregistered_framework' from the 'master/teardown' endpoint, a bad request will be returned: `No framework found with specified ID`.  Ideally, we should support tearing down an unregistered framework, since those frameworks may occur due to network partition, then all the orphan tasks still occupy the resources. It would be a nightmare if a user has to wait until the unregistered framework to get those resources back.  This may be the initial implementation: https://github.com/apache/mesos/commit/bb8375975e92ee722befb478ddc3b2541d1ccaa9"""
"MESOS-6432","Bug","allocation",5,"Roles with quota assigned can ""game"" the system to receive excessive resources.","""The current implementation of quota allocation attempts to satisfy each resource quota for a role, but in doing so can far exceed the quota assigned to the role.  For example, if a role has quota for {{\[30,20,10\]}}, it can consume up to: {{\[, , 10\]}} or {{\[, 20, \]}} or {{\[30, , \]}} as only once each resource in the quota vector is satisfied do we stop allocating agent's resources to the role!  As a first step for preventing gaming, we could consider quota satisfied once any of the resources in the vector has quota satisfied. This approach works reasonably well for resources that are required and are present on every agent (cpus, mem, disk). However, it doesn't work well for resources that are optional / only present on some agents (e.g. gpus) (a.k.a. non-ubiquitous / scarce resources). For this we would need to determine which agents have resources that can satisfy the quota prior to performing the allocation."""
"MESOS-6431","Task","containerization",1,"Add support for port-mapping in `mesos-execute`","""Add support to specify port-mappings for a container in mesos-execute."""
"MESOS-6430","Bug","build",2,"The python linter doesn't rebuild the virtual environment before linting when ""pip-requirements.txt"" has changed","""We need to detect if """"pip-requirements.txt"""" changes and rebuild the virtual environment if it has."""
"MESOS-6428","Bug","containerization",3,"Mesos containerizer helper function signalSafeWriteStatus is not AS-Safe","""In {{src/slave/containerizer/mesos/launch.cpp}} a helper function {{signalSafeWriteStatus}} is defined. Its name seems to suggest that this function is safe to call in e.g., signal handlers, and it is used in this file's {{signalHandler}} for exactly that purpose.  Currently this function is not AS-Safe since it e.g., allocates memory via construction of {{string}} instances, and might destructively modify {{errno}}.  We should clean up this function to be in fact AS-Safe."""
"MESOS-6426","Improvement","containerization",8,"Add rlimit support to Mesos containerizer","""Reviews: https://reviews.apache.org/r/53061/ https://reviews.apache.org/r/53062/ https://reviews.apache.org/r/53063/ https://reviews.apache.org/r/53078/"""
"MESOS-6424","Bug","stout",2,"Possible nullptr dereference in flag loading","""Coverity reports the following:   The {{dynamic_cast}} is needed here if the derived {{Flags}} class got intentionally sliced (e.g., to a {{FlagsBase}}). Since the base class of the hierarchy ({{FlagsBase}}) stores the flags they would not be sliced away; the {{dynamic_cast}} here effectively filters out all flags still valid for the {{Flags}} used when the {{Flag}} was {{add}}'ed.  It seems the intention here was to confirm that the {{dynamic_cast}} to {{Flags*}} succeeded like is done e.g., in {{flags.stringify}} and {{flags.validate}} just below.   AFAICT this code has existed since 2013, but was only reported by coverity recently."""
"MESOS-6441","Task","webui",3,"Display reservations in the agent page in the webui.","""We currently do not display the reservations present on an agent in the webui. It would be nice to see this information.  It would also be nice to update the resource statistics tables to make the distinction between unreserved and reserved resources. E.g.  Reserved: Used, Allocated, Available and Total  Unreserved: Used, Allocated, Available and Total"""
"MESOS-6454","Bug","containerization",2,"PosixRLimitsIsolatorTest.TaskExceedingLimit failed on OSX",""""""
"MESOS-6461","Bug","master",2,"Duplicate framework ids in /master/frameworks endpoint 'unregistered_frameworks'.","""This issue was exposed from MESOS-6400. There are duplicate framework ids presented from the /master/frameworks endpoint due to:  https://github.com/apache/mesos/blob/master/src/master/http.cpp#L1338  We should use a `set` or a `hashset` instead of an array, to avoid duplicate ids."""
"MESOS-6504","Bug","containerization",3,"Use 'geteuid()' for the root privileges check.","""Currently, parts of code in Mesos check the root privileges using os::user() to compare to """"root"""", which is not sufficient, since it compares the real user. When people change the mesos binary by 'setuid root', the process may not have the right permission to execute.  We should check the effective user id instead in our code. """
"MESOS-6519","Bug","test",1,"MasterTest.OrphanTasksMultipleAgents","""Observed this on ASF CI.    """
"MESOS-6516","Bug","test",2,"Parallel test running does not respect GTEST_FILTER","""Normally, you can use {{GTEST_FILTER}} to control which tests will be run by {{make check}}. However, this doesn't currently work if Mesos is configured with {{--enable-parallel-test-execution}}."""
"MESOS-6527","Bug","libprocess",2,"Memory leak in the libprocess request decoder.","""The libprocess decoder can leak a {{Request}} object in cases when a client disconnects while the request is in progress. In such cases, the decoder's destructor won't delete the active {{Request}} object that it had allocated on the heap.  https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/decoder.hpp#L271"""
"MESOS-6530","Improvement","stout",3,"Add support for incremental gzip decompression.","""We currently only support compressing and decompressing based on the entire input being available at once. We can add a {{gzip::Decompressor}} to support incremental decompression."""
"MESOS-6528","Task","agent|containerization",3,"Container status of a task in a pod is not correct.","""Currently, the container status is for the top level executor container. This is not ideal. Ideally, we should get the container status for the corresponding nested container and report that with the task status update."""
"MESOS-6551","Task","cli",5,"Add attach/exec commands to the Mesos CLI","""After all of this support has landed, we need to update the Mesos CLI to implement {{attach}} and {{exec}} functionality as outlined in the Design Doc"""
"MESOS-6560","Bug","stout",2,"The default stout stringify always copies its argument","""The default implementation of the template {{stringify}} in stout always copies its argument,   For most types implementing a dedicated {{stringify}} we restrict {{T}} to some {{const}} ref with the exception of the specialization for {{bool}},   Copying by default is bad since it requires {{T}} to be copyable without {{stringify}} actually requiring this. It also likely leads to bad performance. It appears switching to e.g.,  and adjusting the {{bool}} specialization would be a general improvement.  This issue was first detected by Coverity in CID 727974 way back on 2012-09-21."""
"MESOS-6606","Bug","build",2,"Reject optimized builds with libcxx before 3.9","""Recent clang versions optimize more aggressively which leads to runtime errors using valid code, see e.g., MESOS-5745, due to code exposing undefined behavior in libcxx-3.8 and earlier. This was fixed with upstream libcxx-3.9. See https://reviews.llvm.org/D20786 for the patch and https://llvm.org/bugs/show_bug.cgi?id=28469 for the code example extracted from our code base.  We should consider rejecting builds if libcxx-3.8 or older is detected since not all users compiling Mesos might run the test suite. In our decision to reject we could possibly also take the used clang versions into account (which would just ensure we don't run into the known problems from the UB in libcxx)."""
"MESOS-6619","Bug","master",8,"Improve task management for unreachable tasks","""Scenario:  # Framework starts non-partition-aware task T on agent A # Agent A is partitioned. Task T is marked as a """"completed task"""" in the {{Framework}} struct of the master, as part of {{Framework::removeTask}}. # Agent A re-registers with the master. The tasks running on A are re-added to their respective frameworks on the master as running tasks. # In {{Master::\_reregisterSlave}}, the master sends a {{ShutdownFrameworkMessage}} for all non-partition-aware frameworks running on the agent. The master then does {{removeTask}} for each task managed by one of these frameworks, which results in calling {{Framework::removeTask}}, which adds _another_ task to {{completed_tasks}}. Note that {{completed_tasks}} does not attempt to detect/suppress duplicates, so this results in two elements in the {{completed_tasks}} collection.  Similar problems occur when a partition-aware task is running on a partitioned agent that re-registers: the result is a task in the {{tasks}} list _and_ a task in the {{completed_tasks}} list.  Possible fixes/changes:  * Adding a task to the {{completed_tasks}} list when an agent becomes partitioned is debatable; certainly for partition-aware tasks, the task is not """"completed"""". We might consider adding an """"{{unreachable_tasks}}"""" list to the HTTP endpoints. * Regardless of whether we continue to use {{completed_tasks}} or add a new collection, we should ensure the consistency of that data structure after agent re-registration."""
"MESOS-6618","Bug","test",3,"Some tests use hardcoded port numbers.","""DockerContainerizerTest.ROOT_DOCKER_NoTransitionFromKillingToRunning and many HealthCheckTests use hardcoded port numbers. This can create false failures if these tests are run in parallel on the same machine.  It appears instead we should use random port numbers."""
"MESOS-6626","Improvement","stout",3,"Support `foreachpair` for LinkedHashMap","""{{LinkedHashMap}} does not support iteration via {{foreachpair}}; it should."""
"MESOS-6622","Bug","flaky|test",2,"NvidiaGpuTest.ROOT_INTERNET_CURL_CGROUPS_NVIDIA_GPU_NvidiaDockerImage is flaky","""This test occasionally times out after one minute:      The test itself has a future that waits for 2 minutes for the executor to start up."""
"MESOS-6621","Bug","libprocess",3,"SSL downgrade path will CHECK-fail when using both temporary and persistent sockets","""The code path for downgrading sockets from SSL to non-SSL includes this code:  https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2311-L2321  It is possible for libprocess to hold both temporary and persistent sockets to the same address.  This can happen when a message is first sent ({{ProcessBase::send}}), and then a link is established ({{ProcessBase::link}}).  When the target of the message/link is a non-SSL socket, both temporary and persistent sockets go through the downgrade path.  If a temporary socket is present while a persistent socket is being created, the above code will remap both temporary and persistent sockets to the same address (it should only remap the persistent socket).  This leads to some CHECK failures if those sockets are used or closed later: *  https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L1942 *  https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2044"""
"MESOS-6631","Task","master",3,"Disallow frameworks from modifying FrameworkInfo.roles.","""In """"phase 1"""" of the multi-role framework support, we want to preserve the existing behavior of single-role framework support in that we disallow frameworks from modifying their role.  With multi-role framework support, we will initially disallow frameworks from modifying the roles field. Note that in the case that the master has failed over but the framework hasn't re-registered yet, we will use the framework info from the agents to disallow changes to the roles field. We will treat {{FrameworkInfo.roles}} as a set rather than a list, so ordering does not matter for equality.  One difference between {{role}} and {{roles}} is that for {{role}} modification, we ignore it. But, with {{roles}} modification, since this is a new feature, we can disallow it by rejecting the framework subscription.  Later, in phase 2, we will allow frameworks to modify their roles, see MESOS-6627."""
"MESOS-6630","Task","allocation",3,"Add some benchmark test for quota allocation","""Comparing to non-quota allocation, current quota allocation involves a separate allocation stage and additional tracking such as headroom and role consumed quota. Thus quota allocation performance could be drastically different (probably slower) than non-quota allocation. A dedicated benchmark for quota allocation is necessary."""
"MESOS-6646","Bug","libprocess",1,"StreamingRequestDecoder incompletely initializes its http_parser_settings","""Coverity reports in CID1394703 at {{3rdparty/libprocess/src/decoder.hpp:767}}:   It seems like {{StreamingRequestDecoder}} should properly initialize its member {{settings}}, e.g., with {{http_parser_settings_init}}."""
"MESOS-6656","Bug","agent|containerization",0,"Nested containers can become unkillable","""An incident occurred recently in a cluster running a build of Mesos based on commit {{757319357471227c0a1e906076eae8f9aa2fdbd6}} from master. A task group of five tasks was launched via Marathon. After the tasks were launched, one of the containers quickly exited and was successfully destroyed. A couple minutes later, the task group was killed manually via Marathon, and the agent can then be seen repeatedly attempting to kill the tasks for hours. No calls to {{WAIT_NESTED_CONTAINER}} are visible in the agent logs, and the executor logs do not indicate at any point that the nested containers were launched successfully.  Agent logs:   Executor log:   Meanwhile, the tasks show up as {{STAGING}} in the Mesos web UI, and their sandboxes are empty."""
"MESOS-6654","Bug","containerization",3,"Duplicate image layer ids may make the backend failed to mount rootfs.","""Some images (e.g., 'mesosphere/inky') may contain duplicate layer ids in manifest, which may cause some backends unable to mount the rootfs (e.g., 'aufs' backend). We should make sure that each layer path returned in 'ImageInfo' is unique.  Here is an example manifest from 'mesosphere/inky':   These two layer ids are totally identical:   It would make the backend (e.g., aufs) failed to mount the rootfs due to invalid arguments.   We should make sure the vector of layer paths that is passed to the backend contains only unique layer path."""
"MESOS-6653","Bug","containerization",3,"Overlayfs backend may fail to mount the rootfs if both container image and image volume are specified.","""Depending on MESOS-6000, we use symlink to shorten the overlayfs mounting arguments. However, if more than one image need to be provisioned (e.g., a container image is specified while image volumes are specified for the same container), the symlink .../backends/overlay/links would fail to be created since it exists already.  Here is a simple log when we hard code overlayfs as our default backend:   We should differenciate the links for different provisioned images."""
"MESOS-6672","Bug","stout",1,"Class DynamicLibrary's default copy constructor can lead to inconsistent state","""The class {{DynamicLibrary}} provides a RAII wrapper around a low-level handle to a loaded library. Currently it supports copy- and move-construction which would lead to two libraries holding handles to the same library. This can e.g., lead to libraries being unloaded while other wrappers still hold handles."""
"MESOS-6720","Bug","cmake",2,"Check that `PreferredToolArchitecture` is set to `x64` on Windows before building","""If this variable is not set before we build, it will cause the linker to occasionally hang forever, due to a MSVC toolchain bug in the linker.  We should make this easy on developers and check for them. If the variable is not set, we should display an error message explaining."""
"MESOS-6719","Improvement","master",2,"Unify ""active"" and ""state""/""connected"" fields in Master::Framework","""Rather than tracking whether a framework is """"active"""" separately from whether it is """"connected"""", we should consider using a single """"state"""" variable to track the current state of the framework (connected-and-active, connected-and-inactive, disconnected, etc.)"""
"MESOS-6713","Bug","agent",3,"Port `slave_recovery_tests.cpp`","""https://reviews.apache.org/r/65408/"""
"MESOS-6750","Bug","webui",1,"Metrics on the Agent view of the Mesos web UI flickers between empty and non-empty states","""When viewing a specific agent on the Mesos WebUI, the metrics panel on the left side of the UI will alternate between having values and being empty.  This is due to two different callbacks that run: * This one sets the metrics into the {{$scope.state}} variable: https://github.com/apache/mesos/blob/1.1.x/src/webui/master/static/js/controllers.js#L564-L577 * This one blows away the {{$scope.state}} in favor of a new one: https://github.com/apache/mesos/blob/1.1.x/src/webui/master/static/js/controllers.js#L521  The metrics callback should simply assign to a different variable."""
"MESOS-6749","Task","agent|master",3,"Update master and agent endpoints to expose FrameworkInfo.roles.","""With the addition of the FrameworkInfo.roles field, all of the endpoints that expose the framework information need to be updated to expose this additional field.  It should be the case that for the v1-style operator calls, the new field will be automatically visible thanks to the direct mapping from protobuf (we should verify this).  We can track the updates to metrics separately."""
"MESOS-6745","Bug","test",2,"MesosContainerizer/DefaultExecutorTest.KillTask/0 is flaky","""This repros consistently for me (< 20 test iterations), using {{master}} as of {{ab79d58c9df0ffb8ad35f6662541e7a5c3ea4a80}}. Test log:  """
"MESOS-6744","Bug","test",2,"DefaultExecutorTest.KillTaskGroupOnTaskFailure is flaky","""This repros consistently for me (~10 test iterations or fewer). Test log:  """
"MESOS-6743","Bug","docker",5,"Docker executor hangs forever if `docker stop` fails.","""If {{docker stop}} finishes with an error status, the executor should catch this and react instead of indefinitely waiting for {{reaped}} to return.  An interesting question is _how_ to react. Here are possible solutions.  1. Retry {{docker stop}}. In this case it is unclear how many times to retry and what to do if {{docker stop}} continues to fail.  2. Unmark task as {{killed}}. This will allow frameworks to retry the kill. However, in this case it is unclear what status updates we should send: {{TASK_KILLING}} for every kill retry? an extra update when we failed to kill a task? or set a specific reason in {{TASK_KILLING}}?  3. Clean up and exit. In this case we should make sure the task container is killed or notify the framework and the operator that the container may still be running."""
"MESOS-6758","Improvement","containerization",5,"Support 'Basic' auth docker private registry on Unified Containerizer.","""Currently, the Unified Containerizer only supports the private docker registry with 'Bearer' authorization (token is needed from the auth server). We should support the 'Basic' auth registry as well."""
"MESOS-6785","Bug","master",3,"CHECK failure on duplicate task IDs","""The master crashes with a CHECK failure in the following scenario:  # Framework launches task X on agent A1. The framework may or may not be partition-aware; let's assume it is not partition-aware. # A1 becomes partitioned from the master. # Framework launches task X on agent A2. # Master fails over. # Agents A1 and A2 both re-register with the master. Because the master has failed over, the task on A1 is _not_ terminated (""""non-strict registry semantics"""").  This results in two running tasks with the same ID, which causes a master {{CHECK}} failure among other badness:  """
"MESOS-6784","Bug","agent",1,"IOSwitchboardTest.KillSwitchboardContainerDestroyed is flaky",""" """
"MESOS-6795","Bug","libprocess",2,"Listening socket might get closed while the accept is still in flight.","""This might result in the SocketImpl::accept to invoke network::accept on a wrong fd (or a closed fd). We discovered this while triaging a weird behavior in test (https://issues.apache.org/jira/browse/MESOS-6759)."""
"MESOS-6793","Bug","test",2,"CniIsolatorTest.ROOT_EnvironmentLibprocessIP fails on systems using dash as sh","""On systems using {{dash}} as default shell (e.g., Debian, Ubuntu) {{CniIsolatorTest.ROOT_EnvironmentLibprocessIP}} often fails with   """
"MESOS-6790","Bug","webui",1,"Wrong task started time in webui","""Reported by [~janisz] {quote} Hi  When task has enabled Mesos healthcheck start time in UI can show wrong time. This happens because UI assumes that first status is task started [0]. This is not always true because Mesos keeps only recent tasks statuses [1] so when healthcheck updates tasks status it can override task start time displayed in webui.  Best Tomek  [0] https://github.com/apache/mesos/blob/master/src/webui/master/static/js/controllers.js#L140 [1] https://github.com/apache/mesos/blob/f2adc8a95afda943f6a10e771aad64300da19047/src/common/protobuf_utils.cpp#L263-L265 {quote}"""
"MESOS-6789","Bug","libprocess",2,"SSL socket's 'shutdown()' method is broken","""We recently uncovered two issues with the {{LibeventSSLSocketImpl::shutdown}} method: * The introduction of a shutdown method parameter with [this commit|https://reviews.apache.org/r/54113/] means that the implementation's method is no longer overriding the default implementation. In addition to fixing the implementation method's signature, we should add the {{override}} specifier to all of our socket implementations' methods to ensure that this doesn't happen in the future. * The {{LibeventSSLSocketImpl::shutdown}} function does not actually shutdown the SSL socket. The proper function to shutdown an SSL socket is {{SSL_shutdown}}, which is called in the implementation's destructor. We should move this into {{shutdown()}} so that by the time that method returns, the socket has actually been shutdown."""
"MESOS-6806","Documentation","containerization",1,"Update the addition, deletion and modification logic of CNI configuration files.","""We need update the CNI documentation to highlight that we can add/delete and modify CNI networks on the fly without the need for agent restart."""
"MESOS-6805","Bug","master",2,"Check unreachable task cache for task ID collisions on launch","""As discussed in MESOS-6785, it is possible to crash the master by launching a task that reuses the ID of an unreachable/partitioned task. A complete solution to this problem will be quite involved, but an incremental improvement is easy: when we see a task launch operation, reject the launch attempt if the task ID collides with an ID in the per-framework {{unreachableTasks}} cache. This doesn't catch all situations in which IDs are reused, but it is better than nothing."""
"MESOS-6802","Bug","libprocess",3,"SSL socket can lose bytes in the case of EOF","""During recent work on SSL-enabled tests in libprocess (MESOS-5966), we discovered a bug in {{LibeventSSLSocketImpl}}, wherein the socket can either fail to receive an EOF, or lose data when an EOF is received.  The {{LibeventSSLSocketImpl::event_callback(short events)}} method immediately sets any pending {{RecvRequest}}'s promise to zero upon receipt of an EOF. However, at the time the promise is set, there may actually be data waiting to be read by libevent. Upon receipt of an EOF, we should attempt to read the socket's bufferevent first to ensure that we aren't losing any data previously received by the socket."""
"MESOS-6815","Bug","stout",5,"Enable glog stack traces when we call things like `ABORT` on Windows","""Currently in the Windows builds, if we call `ABORT` (etc.) we will simply bail out, with no stack traces.  This is highly undesirable. Stack traces are important for operating clusters in production. We should work to enable this behavior, including possibly working with glog to add this support if they currently they do not natively support it."""
"MESOS-6811","Bug","test",3,"IOSwitchboardServerTest.SendHeartbeat and IOSwitchboardServerTest.ReceiveHeartbeat broken on OS X","""The tests IOSwitchboardServerTest.SendHeartbeat and IOSwitchboardServerTest.ReceiveHeartbeat are broken on OS X.   The issue is caused by the way the socket paths are constructed in the tests,   The lengths of the components are  * sandbox path: 55 characters (including directory delimiters), * {{mesos-io-switchboard}}: 20 characters, * UUID: 36 characters  which amounts to a total of 113 non-zero characters.  Since the socket is already created in the test's sandbox and only a single socket is created in the test, it appears that it might be possible to strip e.g., the UUID from the path to make the path fit."""
"MESOS-6822","Bug","network",2,"CNI reports confusing error message for failed interface setup.","""Saw this today:    which is produced by this code: https://github.com/apache/mesos/blob/1e72605e9892eb4e518442ab9c1fe2a1a1696748/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L1854-L1859    Note that ssh'ing into the machine confirmed that {{ifconfig}} is available in {{PATH}}.    Full log: http://pastebin.com/hVdNz6yk"""
"MESOS-6820","Bug","test",1,"FaultToleranceTest.FrameworkReregister is flaky.","""I just saw {{FaultToleranceTest.FrameworkReregister}} fail in internal CI on a Debian 8 system. Running the test in repetition on my OS X machine I was able to reproduce the issue on OS X as well.  """
"MESOS-6826","Bug","stout",3,"OsTest.User fails on recent Arch Linux.","""  Appeared relatively recently (last two weeks). Cause appears to be that {{getpwnam\_r}} now returns {{EINVAL}} for an invalid input, which {{os::getuid()}} and {{os::getgid()}} are not prepared to handle."""
"MESOS-6837","Bug","master",3,"FaultToleranceTest.FrameworkReregister is flaky","""Observed on internal CI:    Looks like another instance of MESOS-4695."""
"MESOS-6840","Task","allocation|test",5,"Tests for quota capacity heuristic.","""We need more tests to ensure capacity heuristic works as expected."""
"MESOS-6868","Bug","agent",3,"Transition Windows away from `os::killtree`.","""Windows does not have as robust a notion of a process hierarchy as Unix, and thus functions like `os::killtree` will always have critical limitations and semantic mismatches between Unix and Windows.  We should transition away from this function when we can, and replace it with something similar to how we kill a cgroup."""
"MESOS-6860","Bug","test",5,"Some tests use CHECK instead of ASSERT","""Some tests check preconditions with {{CHECK}} instead of e.g., {{ASSERT_TRUE}}. When such a check fails it leads to a undesirable complete abort of the test run, potentially dumping core. We should make sure tests check preconditions in a proper way, e.g., with {{ASSERT_TRUE}}."""
"MESOS-6874","Bug","containerization",5,"Agent silently ignores FS isolation when protobuf is malformed","""cc [~vinodkone]    I accidentally set my Mesos ContainerInfo to include a DockerInfo instead of a MesosInfo:        I would have expected a validation error before or during containerization, but instead, the agent silently decided to ignore filesystem isolation altogether, and launch my executor on the host filesystem. """
"MESOS-6892","Bug","stout",5,"Reconsider process creation primitives on Windows","""Windows does not have the same notions of process hierarchies as Unix, and so killing groups of processes requires us to make sure all processes are contained in a job object, which acts something like a cgroup. This is particularly important when we decide to kill a task, as there is no way to reliably do this unless all the processes you'd like to kill are in the job object.  This causes us a number of issues; it is a big reason we needed to fork the command executor, and it is the reason tasks are currently unkillable in the default executor.  As we clean this issue up, we need to think carefully about the process governance semantics of Mesos, and how we can map them to a reliable, simple Windows implementation."""
"MESOS-6886","Task","security",3,"Add authorization tests for debug API handlers","""Should test authz of all 3 debug calls."""
"MESOS-6894","Task","containerization",5,"Checkpoint 'ContainerConfig' in Mesos Containerizer.","""This information can be used ford image GC in Mesos Containerizer, as well as other purposes."""
"MESOS-6900","Bug","test",2,"Add test for framework upgrading to multi-role capability.","""Frameworks can upgrade to multi-role capability as long as the framework's role remains the same.  We consider the framework roles unchanged if  * a framework previously didn't specify a {{role}} now has {{roles=()}}, or * a framework which previously had {{role=A}} and now has {{roles=(A)}}."""
"MESOS-6938","Bug","libprocess|test",3,"Libprocess reinitialization is flaky, can segfault.","""This was observed on ASF CI. Based on the placement of the stacktrace, the segfault seems to occur during libprocess reinitialization, when {{process::initialize}} is called: """
"MESOS-6934","Improvement","containerization",8,"Support pulling Docker images with V2 Schema 2 image manifest","""MESOS-3505 added support for pulling Docker images by their digest to the Mesos Containerizer provisioner. However currently it only works with images that were pushed with Docker 1.9 and older or with Registry 2.2.1 and older. Newer versions use Schema 2 manifests by default. Because of CAS constraints the registry does not convert those manifests on-the-fly to Schema 1 when they are being pulled by digest.    Compatibility details are documented here: https://docs.docker.com/registry/compatibility/  Image Manifest V2, Schema 2 is documented here: https://docs.docker.com/registry/spec/manifest-v2-2/"""
"MESOS-6951","Bug","containerization",3,"Docker containerizer: mangled environment when env value contains LF byte.","""Consider this Marathon app definition:    The JSON-encoded newline in the value of the {{TESTVAR}} environment variable leads to a corrupted task environment. What follows is a subset of the resulting task environment (as printed via {{env}}, i.e. in key=value notation):    That is, the trailing part of the intended value ended up being interpreted as variable name, and only the leading part of the intended value was used as actual value for {{TESTVAR}}.  Common application scenarios that would badly break with that involve pretty-printed JSON documents or YAML documents passed along via the environment.  Following the code and information flow led to the conclusion that Docker's {{--env-file}} command line interface is the weak point in the flow. It is currently used in Mesos' Docker containerizer for passing the environment to the container:    (Ref: [code|https://github.com/apache/mesos/blob/c0aee8cc10b1d1f4b2db5ff12b771372fdd5b1f3/src/docker/docker.cpp#L584])   Docker's {{--env-file}} argument behavior is documented via  {quote} The --env-file flag takes a filename as an argument and expects each line to be in the VAR=VAL format, {quote} (Ref: https://docs.docker.com/engine/reference/commandline/run/)  That is, Docker identifies individual environment variable key/value pair definitions based on newline bytes in that file which explains the observed environment variable value fragmentation. Notably, Docker does not provide a mechanism for escaping newline bytes in the values specified in this environment file.  I think it is important to understand that Docker's {{--env-file}} mechanism is ill-posed in the sense that it is not capable of transmitting the whole range of environment variable values allowed by POSIX. That's what the Single UNIX Specification, Version 3 has to say about environment variable values:  {quote} the value shall be composed of characters from the portable character set (except NUL and as indicated below).  {quote} (Ref: http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap08.html)  About """"The portable character set"""": http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap06.html#tagtcjh_3  It includes (among others) the LF byte. Understandably, the current Docker {{--env-file}} behavior will not change, so this is not an issue that can be deferred to Docker: https://github.com/docker/docker/issues/12997  Notably, the {{--env-file}} method for communicating environment variables to Docker containers was just recently introduced to Mesos as of https://issues.apache.org/jira/browse/MESOS-6566, for not leaking secrets through the process listing. Previously, we specified env key/value pairs on the command line which leaked secrets to the process list and probably also did not support the full range of valid environment variable values.  We need a solution that 1) does not leak sensitive values (i.e. is compliant with MESOS-6566). 2) allows for passing arbitrary environment variable values.  It seems that Docker's {{--env}} method can be used for that. It can be used to define _just the names of the environment variables_ to-be-passed-along, in which case the docker binary will read the corresponding values from its own environment, which we can clearly prepare appropriately when we invoke the corresponding child process. This method would still leak environment variable _names_ to the process listing, but (especially if documented) this should be fine."""
"MESOS-6950","Bug","containerization",2,"Launching two tasks with the same Docker image simultaneously may cause a staging dir never cleaned up","""If user launches two tasks with the same Docker image simultaneously (e.g., run {{mesos-executor}} twice with the same Docker image), there will be a staging directory which is for the second task never cleaned up, like this: """
"MESOS-6949","Bug","test",2,"SchedulerTest.MasterFailover is flaky","""This was observed in a CentOS 7 VM, with libevent and SSL enabled:      Find attached the entire log from a failed run."""
"MESOS-6961","Bug","executor",1,"Executors don't use glog for logging.","""Built-in Mesos executors use {{cout}}/{{cerr}} for logging. This is not only inconsistent with the rest of the codebase, it also complicates debugging, since, e.g., a stack trace is not printed on an abort. Having timestamps will be also a huge plus.  Consider migrating logging in all built-in executors to glog.  There have been reported issues related to glog internal state races when a process that has glog initialized {{fork-exec}}s another process that also initialize glog. We should investigate how this issue is related to this ticket, cc [~tillt], [~vinodkone], [~bmahler]."""
"MESOS-6959","Task","cmake",3,"Separate the mesos-containerizer binary into a static binary, which only depends on stout","""The {{mesos-containerizer}} binary currently has [three commands|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/main.cpp#L46-L48]:  * [MesosContainerizerLaunch|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/launch.cpp] * [MesosContainerizerMount|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/mount.cpp] * [NetworkCniIsolatorSetup|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L1776-L1997]  These commands are all heavily dependent on stout, and have no need to be linked to libprocess.  In fact, adding an erroneous call to {{process::initialize}} (either explicitly, or by accidentally using a libprocess method) will break {{mesos-containerizer}} can cause several Mesos containerizer tests to fail.  (The tasks fail to launch, saying {{Failed to synchronize with agent (it's probably exited)}}).  Because this binary only depends on stout, we can separate it from the other source files and make this a static binary. """
"MESOS-6989","Bug","docker",1,"Docker executor segfaults in ~MesosExecutorDriver()","""With the current Mesos master state (commit 42e515bc5c175a318e914d34473016feda4db6ff), the Docker executor segfaults during shutdown.   Steps to reproduce:  1) Start master:  (note that building it at 13:37 is not part of the repro)  2) Start agent:   3) Run {{mesos-execute}} with the Docker containerizer:   Relevant agent output that shows the executor segfault:   The complete task stderr:     """
"MESOS-7014","Task","security",3,"Add implicit executor authorization to local authorizer","""The local authorizer should be updated to perform implicit authorization of executor actions. When executors authenticate using a default executor secret, the authorizer will receive an authorization {{Subject}} which contains claims, but no principal. In this case, implicit authorization should be performed. Implicit authorization rules should enforce that an executor can perform actions on itself; i.e., subscribe as itself, send messages as itself, launch nested containers within itself."""
"MESOS-7013","Task","modules|security",2,"Update the authorizer interface for executor authentication","""The authorizer interface must be updated to accommodate changes introduced by the implementation of executor authentication: * The {{authorization::Subject}} message must be extended to include the {{claims}} from a {{Principal}} * The local authorizer must be updated to accommodate this interface change"""
"MESOS-7012","Task","agent|executor|security",2,"Add authorization actions for V1 executor calls","""Authorization actions should be added for the V1 executor calls: * Subscribe * Update * Message"""
"MESOS-7011","Task","agent|security",1,"Add an '--executor_secret_key' flag to the agent","""A new {{\-\-executor_secret_key}} flag should be added to the agent to allow the operator to specify a secret file to be loaded into the default executor JWT authenticator and SecretGenerator modules. This secret will be used to generate default executor secrets when {{\-\-generate_executor_secrets}} is set, and will be used to verify those secrets when {{\-\-authenticate_http_executors}} is set."""
"MESOS-7009","Task","security",1,"Add a 'secret' field to the 'Environment' message","""A new field of type {{Secret}} should be added to the {{Environment}} message to enable the inclusion of secrets in executor and task environments."""
"MESOS-7008","Bug","allocation|master",3,"Quota not recovered from registry in empty cluster.","""When a quota was set and the master is restarted, removal of the quota reliably leads to a {{CHECK}} failure for me.  Start a master:   Set a quota. This creates an implicit role.   Restart the master process using the same {{work_dir}} and attempt to delete the quota after the master is started. The {{DELETE}} succeeds with an {{OK}}.   After handling the request, the master hits a {{CHECK}} failure and is aborted. """
"MESOS-7007","Bug","agent",3,"filesystem/shared and --default_container_info broken since 1.1","""I face this issue, that prevent me to upgrade to 1.1.0 (and the change was consequently introduced in this version):  I'm using default_container_info to mount a /tmp volume in the container's mount namespace from its current sandbox, meaning that each container have a dedicated /tmp, thanks to the {{filesystem/shared}} isolator.  I noticed through our automation pipeline that integration tests were failing and found that this is because /tmp (the one from the host!) contents is trashed each time a container is created.  Here is my setup:  * {{--isolation='cgroups/cpu,cgroups/mem,namespaces/pid,*disk/du,filesystem/shared,filesystem/linux*,docker/runtime'}} * {{--default_container_info='\{""""type"""":""""MESOS"""",""""volumes"""":\[\{""""host_path"""":""""tmp"""",""""container_path"""":""""/tmp"""",""""mode"""":""""RW""""\}\]\}'}}  I discovered this issue in the early days of 1.1 (end of Nov, spoke with someone on Slack), but had unfortunately no time to dig into the symptoms a bit more.  I found nothing interesting even using GLOGv=3.  Maybe it's a bad usage of isolators that trigger this issue ? If it's the case, then at least a documentation update should be done.  Let me know if more information is needed."""
"MESOS-7005","Documentation","documentation",3,"Add executor authentication documentation","""Documentation should be added regarding executor authentication. This will include updating: 1) the configuration docs to include new agent flags 2) the authentication documentation 3) the authorization documentation 4) the upgrade documentation 5) the CHANGELOG"""
"MESOS-7004","Task","modules|security",5,"Enable multiple HTTP authenticator modules","""To accommodate executor authentication, we will add support for the loading of multiple authenticator modules. The {{--http_authenticators}} flag is already set up for this, but we must relax the constraint in Mesos which enforces just a single authenticator.  In order to load multiple authenticators for a realm, a new Mesos-level authenticator, the {{CombinedAuthenticator}}, will be added. This class will call multiple authenticators and combine their results if necessary."""
"MESOS-7003","Task","executor|security",5,"Introduce a 'Principal' type","""We will introduce a new type to represent the identity of an authenticated entity in Mesos: the {{Principal}}. To accomplish this, the following should be done: * Add the new {{Principal}} type * Update the {{AuthenticationResult}} to use {{Principal}} * Update all authenticated endpoint handlers to handle this new type * Update the default authenticator modules to use the new type"""
"MESOS-7001","Task","modules|security",5,"Implement a JWT authenticator","""A JSON web token (JWT) authenticator module should be added to authenticate executors which use default credentials generated by the agent. This module will be loaded as an HTTP authenticator by default when {{--authenticate_http_executors}} is set, unless HTTP authenticators are specified explicitly."""
"MESOS-7000","Task","agent|modules|security",5,"Implement a JWT SecretGenerator","""The default {{SecretGenerator}} for the generation of default executor credentials will be a module which generates JSON web tokens. This module will be loaded by default when executor secret generation is enabled."""
