"issuekey","type","components","storypoint","title","description_text"
"MESOS-6999","Task","agent|security",5,"Add agent support for generating and passing executor secrets","""The agent must generate and pass executor secrets to all executors using the V1 API. For MVP, the agent will have this behavior by default when compiled with SSL support. To accomplish this, the agent must: * load the default {{SecretGenerator}} module * call the secret generator when launching an executor * pass the generated secret into the executor's environment"""
"MESOS-6998","Task","agent|security",5,"Add authentication support to agent's '/v1/executor' endpoint","""The new agent flag {{--authenticate_http_executors}} must be added. When set, it will require that requests received on the {{/v1/executor}} endpoint be authenticated, and the default JWT authenticator will be loaded. Note that this will require the addition of a new authentication realm for that endpoint."""
"MESOS-6997","Task","executor|modules|security",2,"Add the SecretGenerator module interface","""A new {{SecretGenerator}} module interface will be added to permit the agent to generate default executor credentials."""
"MESOS-6996","Task","security",2,"Add a 'Secret' protobuf message","""A {{Secret}} protobuf message should be added to serve as a generic message for sending credentials and other secrets throughout Mesos."""
"MESOS-7026","Task","agent|HTTP API|master",5,"Update authorization / authorization-filtering to handle hierarchical roles.","""Authorization and endpoint filtering will need to be updated in order to allow the authorization to be performed in a hierarchical manner (e.g. a user can see all beneath /eng/* vs. a user can see all beneath /eng/frontend/*)."""
"MESOS-7022","Bug","master",3,"Update framework authorization to support multiple roles","""Currently the master assumes that a framework is only in a single role, see {{Master::authorizeFramework}}. This code should be updated to support frameworks with multiple roles. In particular it should get authorization of the framework's principal to register in each of the framework's roles."""
"MESOS-7028","Bug","libprocess|test",5,"NetSocketTest.EOFBeforeRecv is flaky.","""This was observed on ASF CI:  """
"MESOS-7042","Bug","containerization",2,"Send SIGKILL after SIGTERM to IOSwitchboard after container termination.","""This is follow up for MESOS-6664"""
"MESOS-7047","Task","agent",2,"Update agent for hierarchical roles.","""Agents use the role name in the file system path for persistent volumes: a persistent volume is written to {{work_dir/volumes/roles/<role-name>/<persistence-id>}}. When using hierarchical roles, {{role-name}} might contain slashes. It seems like there are three options here:  # When converting the role name into the file system path, escape any slashes that appear. # Hash the role name before using it in the file system path. # Create a directory hierarchy that corresponds to the nesting in the role name. So a volume for role {{a/b/c/d}} would be stored in {{roles/a/b/c/d/<persistence-id>}}.  If we adopt #3, we'd probably also want to cleanup the filesystem when a volume is removed."""
"MESOS-7051","Improvement","libprocess",2,"Introduce a new http::Headers abstraction.","""Introduce a new http::Headers abstraction to replace the previous hashmap 'Headers'. The benefit is that it can be embedded with other header classes (e.g., WWW-Authenticate) to parse a header content, as well as doing validation."""
"MESOS-7069","Bug","containerization",2,"The linux filesystem isolator should set mode and ownership for host volumes.","""If the host path is a relative path, the linux filesystem isolator should set the mode and ownership for this host volume since it allows non-root user to write to the volume. Note that this is the case of sharing the host fileysystem (without rootfs)."""
"MESOS-7076","Bug","build|libprocess|test",8,"libprocess tests fail when using libevent 2.1.8","""Running {{libprocess-tests}} on Mesos compiled with {{--enable-libevent --enable-ssl}} on an operating system using libevent 2.1.8, SSL related tests fail like      Tests failing are    """
"MESOS-7099","Bug","allocation",5,"Quota can be exceeded due to coarse-grained offer technique.","""The current implementation of quota allocation allocates the entire available resources on an agent when trying to satisfy the quota. What this means is that quota can be exceeded by the size of an agent.    This is especially problematic for large machines, consider a 48 core, 512 GB memory server where a role is given 4 cores and 4GB of memory. Given our current approach, we will send an offer for the entire 48 cores and 512 GB of memory!    This ticket is to perform fine grained offers when the allocation will exceed the quota."""
"MESOS-7102","Bug","agent",2,"Crash when sending a SIGUSR1 signal to the agent.","""Looks like sending a {{SIGUSR1}} to the agent crashes it. This is a regression and used to work fine in the 1.1 release. Note that the agent does unregisters with the master and the crash happens after that.  Steps to reproduce: - Start the agent. - Send it a {{SIGUSR1}} signal.  The agent should crash with a stack trace similar to this: """
"MESOS-7124","Improvement","libprocess|stout",3,"Replace monadic type get() functions with operator*","""In MESOS-2757 we introduced {{T* operator->}} for {{Option}}, {{Future}} and {{Try}}. This provided a convenient short-hand for existing member functions {{T& get}} providing identical functionality.    To finalize the work of MESOS-2757 we should replace the existing {{T& get()}} member functions with functions {{T& operator*}}.    This is desirable as having both {{operator->}} and {{get}} in the code base at the same time lures developers into using the old-style {{get}} instead of {{operator->}} where it is not needed, e.g.,    instead of      We still require the functionality of {{get}} to directly access the contained value, but the current API unnecessarily conflates two (at least from a usage perspective) unrelated aspects; in these instances, we should use an {{operator*}} instead,      Using {{operator*}} in these instances makes it much less likely that users would use it in instances when they wanted to call functions of the wrapped value, i.e.,    appears more natural than          Note that this proposed change is in line with the interface of {{std::optional}}. Also, {{std::shared_ptr}}'s {{get}} is a useful function and implements an unrelated interface: it surfaces the wrapped pointer as opposed to its {{operator*}} which dereferences the wrapped pointer. Similarly, our current {{get}} also produce values, and are unrelated to {{std::shared_ptr}}'s {{get}}."""
"MESOS-7130","Bug","executor",2,"port_mapping isolator: executor hangs when running on EC2","""Hi, I'm experiencing a weird issue: I'm using a CI to do testing on infrastructure automation. I recently activated the {{network/port_mapping}} isolator.  I'm able to make the changes work and pass the test for bare-metal servers and virtualbox VMs using this configuration.  But when I try on EC2 (on which my CI pipeline rely) it systematically fails to run any container.  It appears that the sandbox is created and the port_mapping isolator seems to be OK according to the logs in stdout and stderr and the {tc} output :   Then the executor never come back in REGISTERED state and hang indefinitely.  {GLOG_v=3} doesn't help here.  My skills in this area are limited, but trying to load the symbols and attach a gdb to the mesos-executor process, I'm able to print this stack:   I concluded that the underlying shell script launched by the isolator or the task itself is just .. blocked. But I don't understand why.  Here is a process tree to show that I've no task running but the executor is:   If someone has a clue about the issue I could experience on EC2, I would be interested to talk..."""
"MESOS-7154","Documentation","documentation",2,"Document provisioner auto backend support.","""Document the provisioner auto backend semantic in container-image.md"""
"MESOS-7153","Bug","libprocess",3,"The new http::Headers abstraction may break some modules.","""In the favor of the new http::Headers abstraction, the headers class was changed from a hashmap to a class. However, this change may potentially break some modules since functionalities like constructor using initializer or calling methods from undered_map. We should have the new class derived from the hashmap instead. """
"MESOS-7160","Bug","test",3,"Parsing of perf version segfaults","""Parsing the perf version [fails with a segfault in ASF CI|https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-eu2)/3294/], """
"MESOS-7168","Bug","containerization",3,"Agent should validate that the nested container ID does not exceed certain length.","""This is related to MESOS-691.  Since nested container ID is generated by the executor, the agent should verify that the length of it does not exceed certain length."""
"MESOS-7193","Bug","libprocess|test",5,"Use of `GTEST_IS_THREADSAFE` in asserts is problematic.","""Some test cases in libprocess use {{ASSERT_TRUE(GTEST_IS_THREADSAFE)}}. This is a misuse of that define, [the documentation in GTest says|https://github.com/google/googletest/blob/master/googletest/include/gtest/internal/gtest-port.h#L155-L163]:  Currently, the use of {{GTEST_IS_THREADSAFE}} works fine in the assert, because it is defined to be {{1}}. But newer upstream versions of GTest use a more complicated define, that can yield to be undefined, causing compilation errors."""
"MESOS-7200","Improvement","master",5,"Add support for hierarchical roles to the local authorizer","""We should update the local authorizer so that role values for role-based actions matching whole role subhierarchies are understood, e.g., given roles {{a/b/c}}, {{a/b/d}} and {{a/e}} it should be possible to specify a role value {{a/b/%}} matching actions on roles {{a/b/c}} and {{a/b/d}}, or a value {{a/%}} matching actions on all above roles."""
"MESOS-7197","Bug","allocation",3,"Requesting tiny amount of CPU crashes master.","""If a task is submitted with a tiny CPU request e.g. 0.0004, then when it completes the master crashes due to a CHECK failure:   I can reproduce this with the following command:   If I replace 0.0004 with 0.001 the issue no longer occurs."""
"MESOS-7210","Bug","docker",3,"HTTP health check doesn't work when mesos runs with --docker_mesos_image","""When running mesos-slave with option """"docker_mesos_image"""" like:   from the container that was started with option """"pid: host"""" like:   and example marathon job, that use MESOS_HTTP checks like:   I see the errors like:   Looks like option docker_mesos_image makes, that newly started mesos job is not using """"pid host"""" option same as mother container was started, but has his own PID namespace (so it doesn't matter if mother container was started with """"pid host"""" or not it will never be able to find PID)"""
"MESOS-7208","Bug","containerization",3,"Persistent volume ownership is set to root when task is running with non-root user","""I’m running docker container in universal containerizer, mesos 1.1.0. switch_user=true, isolator=filesystem/linux,docker/runtime.  Container is launched with marathon, “user”:”someappuser”. I’d want to use persistent volume, but it’s exposed to container with root user permissions even if root folder is created with someppuser ownership (looks like mesos do chown to this folder).   here logs for my container:   """
"MESOS-7220","Bug","stout",1,"'EXPECT_SOME' and other asserts don't work with 'Try's that have a custom error state.","""MESOS-5110 introduced an additional template parameter for {{Try}} to support custom error types. Using these values with {{EXPECT_SOME}} doesn't work, i.e.    won't compile. The other assertions in {{stout/gtest.hpp}} are likely affected as well."""
"MESOS-7226","Improvement","cmake",5,"Introduce precompiled headers (on Windows)","""Precompiled headers (PCHs) exist on both Windows and Linux. For Linux, you can refer to https://gcc.gnu.org/onlinedocs/gcc/Precompiled-Headers.html. Straight from the GNU CC documentation: """"The time the compiler takes to process these header files over and over again can account for nearly all of the time required to build the project.""""  PCHs are only being proposed for the CMake system.  In theory, we can introduce this change with only a few, non-intrusive code changes.  The feature will primarily be a CMake change.  See: https://github.com/sakra/cotire"""
"MESOS-7246","Documentation","documentation",1,"Add documentation for AGENT_ADDED/AGENT_REMOVED events.","""We need to add documentation to the existing Mesos Operator API docs for the newly added {{AGENT_ADDED}}/{{AGENT_REMOVED}} events. The protobuf definition for the events can be found here: https://github.com/apache/mesos/blob/master/include/mesos/v1/master/master.proto"""
"MESOS-7258","Improvement","master|scheduler api",13,"Provide scheduler calls to subscribe to additional roles and unsubscribe from roles.","""The current support for schedulers to subscribe to additional roles or unsubscribe from some of their roles requires that the scheduler obtain a new subscription with the master which invalidates the event stream.    A more lightweight mechanism would be to provide calls for the scheduler to subscribe to additional roles or unsubscribe from some roles such that the existing event stream remains open and offers to the new roles arrive on the existing event stream. E.g.    SUBSCRIBE_TO_ROLE   UNSUBSCRIBE_FROM_ROLE    One open question pertains to the terminology here, whether we would want to avoid using """"subscribe"""" in this context. An alternative would be:    UPDATE_FRAMEWORK_INFO    Which provides a generic mechanism for a framework to perform framework info updates without obtaining a new event stream.    In addition, it would be easier to use if it returned 200 on success and an error response if invalid, etc. Rather than returning 202.    *NOTE*: Not specific to this issue, but we need to figure out how to allow the framework to not leak reservations, e.g. MESOS-7651."""
"MESOS-7252","Bug","framework",2,"Need to fix resource check in long-lived framework","""The multi-role changes in Mesos changed the implementation of `Resources::contains`.  This results in the search for a given resource to be performed only for unallocated resources. For allocated resources the search is actually performed only for a given role.   Due to this change the resource check in both the long-lived framework are failing leading to these frameworks not launching any tasks.   The fix would be to unallocate all resources in a given offer and than do the `contains` check."""
"MESOS-7251","Bug","containerization",2,"Support pulling images from AliCloud private registry.","""The image puller via curl doesn't work when I'm specifying the image name as: registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75 400 BAD REQUEST  But the docker pulls it successfully  bq. docker pull registry.cn-hangzhou.aliyuncs.com/kaiyu/pytorch-cuda75"""
"MESOS-7265","Bug","agent|executor",3,"Containerizer startup may cause sensitive data to leak into sandbox logs.","""The task sandbox logging does show the callup for the containerizer launch with all of its flags. This is not safe when assuming that we may not want to leak sensitive data into the sandbox logging.  Example: """
"MESOS-7264","Bug","agent",1,"Possibly duplicate environment variables should not leak values to the sandbox.","""When looking into MESOS-7263, the following also came up.    Within the contents of `stdout`:   There seems no obvious need to warn the user as the value is identical."""
"MESOS-7263","Bug","agent|executor",3,"User supplied task environment variables cause warnings in sandbox stdout.","""The default executor causes task/command environment variables to get duplicated internally, causing warnings in the resulting sandbox {{stdout}}.    Result in {{stdout}} of the sandbox:  """
"MESOS-7272","Bug","containerization|docker",2,"Unified containerizer does not support docker registry version < 2.3.","""in file `src/uri/fetchers/docker.cpp`  ```     Option<string> contentType = response.headers.get(""""Content-Type"""");           if (contentType.isSome() &&               !strings::startsWith(                   contentType.get(),                   """"application/vnd.docker.distribution.manifest.v1"""")) {             return Failure(                 """"Unsupported manifest MIME type: """" + contentType.get());           }   ```  Docker fetcher check the contentType strictly, while docker registry with version < 2.3 returns manifests with contentType `application/json`, that leading failure like `E0321 13:27:27.572402 40370 slave.cpp:4650] Container 'xxx' for executor 'xxx' of framework xxx failed to start: Unsupported manifest MIME type: application/json; charset=utf-8`."""
"MESOS-7270","Bug","test",1,"Java V1 Framwork Test failed on macOS","""macOS's scheduler make ExamplesTest.JavaV1Framework terminate before the scheduler driver stopped, which results in an exception and a test failure. This failure is not seen in an Linux environment yet but there's a possibility that it would also happen."""
"MESOS-7280","Bug","containerization|docker",2,"Unified containerizer provisions docker image error with COPY backend","""Error occurs on some specific docker images with COPY backend, both 1.0.2 and 1.2.0. It works well with OVERLAY backend on 1.2.0.  {quote} I0321 09:36:07.308830 27613 paths.cpp:528] Trying to chown '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7' to user 'root' I0321 09:36:07.319628 27613 slave.cpp:5703] Launching executor ct:Transcoding_Test_114489497_1490060156172:3 of framework 20151223-150303-2677017098-5050-30032-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/data/mesos/slaves/55f6df5e-2812-40a0-baf5-ce96f20677d3-S102/frameworks/20151223-150303-2677017098-5050-30032-0000/executors/ct:Transcoding_Test_114489497_1490060156172:3/runs/7e518538-7b56-4b14-a3c9-bee43c669bd7' I0321 09:36:07.321436 27615 containerizer.cpp:781] Starting container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework '20151223-150303-2677017098-5050-30032-0000' I0321 09:36:37.902195 27600 provisioner.cpp:294] Provisioning image rootfs '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7 *E0321 09:36:58.707718 27606 slave.cpp:4000] Container '7e518538-7b56-4b14-a3c9-bee43c669bd7' for executor 'ct:Transcoding_Test_114489497_1490060156172:3' of framework 20151223-150303-2677017098-5050-30032-0000 failed to start: Collect failed: Failed to copy layer: cp: cannot create regular file ‘/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9/usr/bin/python’: Text file busy* I0321 09:36:58.707991 27608 containerizer.cpp:1622] Destroying container '7e518538-7b56-4b14-a3c9-bee43c669bd7' I0321 09:36:58.708468 27607 provisioner.cpp:434] Destroying container rootfs at '/data/mesos/provisioner/containers/7e518538-7b56-4b14-a3c9-bee43c669bd7/backends/copy/rootfses/8d2f7fe8-71ff-4317-a33c-a436241a93d9' for container 7e518538-7b56-4b14-a3c9-bee43c669bd7 {quote}  Docker image is a private one, so that i have to try to reproduce this bug with some sample Dockerfile as possible."""
"MESOS-7306","Improvement","containerization",8,"Support mount propagation for host volumes.","""Currently, all mounts in a container are marked as 'slave' by default. However, for some cases, we may want mounts under certain directory in a container to be propagate back to the root mount namespace. This is useful for the case where we want the mounts to survive container failures.    See more documentation about mount propagation in:  https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt    Given mount propagation is very hard for users to understand, probably worth limiting this to just host volumes because we only see use case for that at the moment.    Some relevant discussion can be found here:  https://github.com/kubernetes/community/blob/master/contributors/design-proposals/propagation.md"""
"MESOS-7305","Task","containerization",8,"Adjust the recover logic of MesosContainerizer to allow standalone containers.","""The current recovery logic in MesosContainerizer assumes that all top level containers are tied to some Mesos executors. Adding standalone containers will invalid this assumption. The recovery logic must be changed to adapt to that."""
"MESOS-7304","Task","containerization|fetcher",3,"Fetcher should not depend on SlaveID.","""Currently, various Fetcher interfaces depends on SlaveID, which is an unnecessary coupling. For instance:   Looks like the only reason we need a SlaveID is because we need to calculate the fetcher cache directory based on that. We should calculate the fetcher cache directory in the caller and pass that directory to Fetcher."""
"MESOS-7316","Bug","HTTP API",1,"Upgrading Mesos to 1.2.0 results in some information missing from the `/flags` endpoint.","""From OSS Mesos Slack: I recently tried upgrading one of our Mesos clusters from 1.1.0 to 1.2.0. After doing this, it looks like the {{zk}} field on the {{/master/flags}} endpoint is no longer present.   This looks related to the recent {{Flags}} refactoring that was done which resulted in some flags no longer being populated since they were not part of {{master::Flags}} in {{src/master/flags.hpp}}."""
"MESOS-7314","Task","master",5,"Add offer operations for converting disk resources","""One should be able to convert {{RAW}} and {{BLOCK}} disk resources into a different types by applying operations to them. The offer operations and the related validation and resource handling needs to be implemented."""
"MESOS-7329","Task","master|security",3,"Authorize offer operations for converting disk resources","""All offer operations are authorized, hence authorization logic has to be added to new offer operations as well."""
"MESOS-7349","Documentation","documentation",3,"Document Mesos ""check"" feature.","""This should include framework authors recommendations about how and when to use general checks as well as comparison with health checks."""
"MESOS-7347","Task","master",8,"Prototype resource offer operation handling in the master","""Prototype the following workflow in the master, in accordance with the resource provider design; * Handle accept calls including resource provider related offer operations ({{CREATE_VOLUME}}, ...) * Implement internal bookkeeping of the disk resources these operations will be applied on * Implement resource bookkeeping for resource providers in the master * Send resource provider operations to resource providers"""
"MESOS-7364","Improvement","build",3,"Upgrade vendored GMock / GTest","""We currently vendor gmock 1.7.0. The latest upstream version of gmock is 1.8.0, which fixes at least one annoying warning (MESOS-6539)."""
"MESOS-7361","Improvement","agent",3,"Command checks via agent pollute agent logs.","""Command checks via agent leverage debug container API of the agent to start checks. Each such invocation triggers a bunch of logs on the agent, because the API was not originally designed with periodic invocations in mind. We should find a way to avoid excessive logging on the agent."""
"MESOS-7355","Improvement","agent|containerization",1,"Set MESOS_SANDBOX in debug containers.","""Currently {{MESOS_SANDBOX}} is not set for debug containers, see [https://github.com/apache/mesos/blob/7f04cf886fc2ed59414bf0056a2f351959a2d1f8/src/slave/containerizer/mesos/containerizer.cpp#L1392-L1407]. The most reasonable value seems to be task's sandbox."""
"MESOS-7367","Bug","test",1,"MasterAPITest.GetRoles is flaky on machines with non-C locale.","""{{MasterAPITest.GetRoles}} test sets role weight to a real number using {{.}} as a decimal mark. This however is not correct on machines with non-standard locale, because weight parsing code relies on locale: [https://github.com/apache/mesos/blob/7f04cf886fc2ed59414bf0056a2f351959a2d1f8/src/master/master.cpp#L727-L750]. This leads to test failures: [https://pastebin.com/sQR2Tr2Q].  There are several solutions here.  h4. 1. Change parsing code to be locale-agnostic. This seems to be the most robust solution. However, the {{--weights}} flag is deprecated and will probably be removed soon, together with the parsing code.   h4. 2. Fix call sites in our tests to ensure decimal mark is locale dependent. This seems like a reasonable solution, but I'd argue we can do even better.  h4. 3. Use locale-agnostic format for doubles in tests. Instead of saying {{""""2.5""""}} we can say {{""""25e-1""""}} which is locale agnostic."""
"MESOS-7374","Bug","containerization",3,"Running DOCKER images in Mesos Container Runtime without `linux/filesystem` isolation enabled renders host unusable","""If I run the pod below (using Marathon 1.4.2) against a mesos agent that has the flags (also below), then the overlay filesystem replaces the system root mount, effectively rendering the host unusable until reboot.  flags:  - {{--containerizers mesos,docker}} - {{--image_providers APPC,DOCKER}} - {{--isolation cgroups/cpu,cgroups/mem,docker/runtime}}  pod definition for Marathon:   Mesos should probably check for this and avoid replacing the system root mount point at startup or launch time."""
"MESOS-7377","Task","executor|security",2,"Add authentication to the checker and health checker libraries","""The health checker library in {{src/checks/health_checker.cpp}} must be updated to authenticate with the agent's HTTP operator API."""
"MESOS-7416","Task","HTTP API|master",5,"Filter results of `/master/slaves` and the v1 call GET_AGENTS","""The results returned by both the endpoint {{/master/slaves}} and the API v1 {{GET_AGENTS}} return full information about the agent state which probably need to be filtered for certain uses, particularly in a multi-tenancy scenario.  The kind of leaked data includes specific role names and their specific allocations."""
"MESOS-7415","Task","c++ api|HTTP API|master",3,"Add authorization to master's operator maintenance API in v0 and v1","""None of the maintenance primitives in either API v0 or API v1 have any kind of authorization, which allows any user with valid credentials to do things such as shutting down a machine, schedule time off on an agent, modify maintenance schedule, etc.  The authorization support needs to be added to the v0 endpoints:  * {{/master/machine/up}} * {{/master/machine/down}} * {{/master/maintenance/schedule}} * {{/master/maintenance/status}}  as well as to the v1 calls:  * {{GET_MAINTENANCE_STATUS}} * {{GET_MAINTENANCE_SCHEDULE}} * {{UPDATE_MAINTENANCE_SCHEDULE}} * {{START_MAINTENANCE}} * {{STOP_MAINTENANCE}}"""
"MESOS-7414","Task","HTTP API|master",5,"Enable authorization for master's logging API calls: GET_LOGGING_LEVEL  and SET_LOGGING_LEVEL","""The Operator API calls {{GET_LOGGING_LEVEL}}  and {{SET_LOGGING_LEVEL}} lack authorization so any recognized user will be able to change the logging level of a given master.  The v0 endpoint {{/logging/toggle}} has authorization through the {{GET_ENDPOINT_WITH_PATH}} action. We need to decide whether it should also use additional authorization.  Note that there are already actions defined for authorization of these actions as they were already implemented in the agent."""
"MESOS-7433","Task","agent|containerization",3,"Set working directory in DEBUG containers.","""Currently working directory is not set for DEBUG containers. The most reasonable value seems to be parent's working directory."""
"MESOS-7431","Bug","containerization",5,"Registry puller cannot fetch manifests from Google GCR: 403 Forbidden.","""When the registry puller is pulling a repository from Google's GCE Container Registry, a '403 Forbidden' error occurs instead of 401 when fetching manifests."""
"MESOS-7438","Bug","test",2,"Double free or corruption when using parallel test runner","""I observed the following when using the parallel test runner:    Not sure how reproducible this is, appears to occur in the authentication path of the agent."""
"MESOS-7449","Task","containerization",5,"Refactor containerizers to not depend on TaskInfo or ExecutorInfo","""The Containerizer interfaces should be refactored so that they do not depend on {{TaskInfo}} or {{ExecutorInfo}}, as a standalone container will have neither.  Currently, the {{launch}} interface depends on those fields.  Instead, we should consistently use {{ContainerInfo}} and {{CommandInfo}} in Containerizer and isolators."""
"MESOS-7457","Bug","allocation",2,"HierarchicalAllocatorTest.NestedRoleQuota is flaky",""" """
"MESOS-7474","Bug","fetcher",5,"Mesos fetcher cache doesn't retry when missed.","""Mesos Fetcher doesn't retry when a cache is missed. It needs to have the ability to pull from source when it fails.     421 15:52:53.022902 32751 fetcher.cpp:498] Fetcher Info: {""""cache_directory"""":""""\/tmp\/mesos\/fetch\/slaves\/<slaveid>"""",""""items"""":[\{""""action"""":""""RETRIEVE_FROM_CACHE"""",""""cache_filename"""":""""<file.name.tar.gz>)"""",""""uri"""":\{""""cache"""":true,""""executable"""":false,""""extract"""":true,""""value"""":""""https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>""""}}],""""sandbox_directory"""":""""\/var\/lib\/mesos\/slave\/slaves\/<slaveid>\/frameworks\<frameworkid>\/executors\/name\/runs\/<id>""""}   I0421 15:52:53.024926 32751 fetcher.cpp:409] Fetching URI '""""https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>""""   I0421 15:52:53.024942 32751 fetcher.cpp:306] Fetching from cache   I0421 15:52:53.024958 32751 fetcher.cpp:84] Extracting with command: tar -C """"\/var\/lib\/mesos\/slave\/slaves\/<slaveid>\/frameworks\<frameworkid>\/executors\/name\/runs\/<id>' -xf '/tmp/mesos/fetch/slaves/f3feeab8-a2fe-4ac1-afeb-ec7bd4ce7b0d-S29/c1-docker-hub.tar.gz'   tar: /""""https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>"""": Cannot open: No such file or directory   tar: Error is not recoverable: exiting now   Failed to fetch '""""https:\/\/<file.path.s3.amazonaws.com>\/<file.name.tar.gz>""""': Failed to extract: command tar -C '""""\/var\/lib\/mesos\/slave\/slaves\/<slaveid>\/frameworks\<frameworkid>\/executors\/name\/runs\/<id>' -xf '/tmp/mesos/fetch/slaves/<file.name.tar.gz>""""' exited with status: 512  """
"MESOS-7471","Bug","provisioner",2,"Provisioner recover should not always assume 'rootfses' dir exists.","""The mesos agent would restart due to many reasons (e.g., disk full). Always assume the provisioner 'rootfses' dir exists would block the agent to recover.    This issue may occur due to the race between removing the provisioner container dir and the agent restarts:   In provisioner recover, when listing the container rootfses, it is possible that the 'rootfses' dir does not exist. Because a possible race between the provisioner destroy and the agent restart. For instance, while the provisioner is destroying the container dir the agent restarts. Due to os::rmdir() is recursive by traversing the FTS tree, it is possible that 'rootfses' dir is removed but the others (e.g., scratch dir) are not.  Currently, we are returning an error if the 'rootfses' dir does not exist, which blocks the agent from recovery. We should skip it if 'rootfses' does not exist."""
"MESOS-7488","Task","agent",5,"Add `--ip6` and `--ip6_discovery_command` flag to Mesos agent","""As a first step to support IPv6 containers on Mesos, we need to provide {{--ip6}} and {{--ip6_discovery_command}} flags to the agent so that the operator can specify an IPv6 address for the {{libprocess}} actor on the agent. In this ticket we will not aim to add IPv6 communication support for Mesos but will aim to use the IPv6 address provided by the operator to fill in the v6 address for any containers running on the host network in a dual stack environment."""
"MESOS-7502","Bug","test",1,"Build error on Windows when using ""int"" for a file descriptor","""There is a build error for mesos-tests in src/tests/check_tests.cpp on Windows associated with the use of an """"int"""" file descriptor:  C:\mesos\mesos\src\tests\check_tests.cpp(1890): error C2440: 'initializing': cannot convert from 'Try<std::array<os::WindowsFD,2>,Error>' to 'Try<std::array<int,2>,Error>' [C:\mesos\mesos\build\src\tests\mesos-tests.vcxproj]"""
"MESOS-7506","Bug","containerization",8,"Multiple tests leave orphan containers.","""I've observed a number of flaky tests that leave orphan containers upon cleanup. A typical log looks like this:      All currently affected tests:  """
"MESOS-7504","Bug","containerization",3,"Parent's mount namespace cannot be determined when launching a nested container.","""I've observed this failure twice in different Linux environments. Here is an example of such failure:  """
"MESOS-7542","Improvement","agent|executor",3,"Add executor reconnection retry logic to the agent","""Currently, the agent sends a single {{ReconnectExecutorMessage}} to PID-based executors during recovery. It would be more robust to have the agent retry these messages until {{executor_reregister_timeout}} has elapsed."""
"MESOS-7540","Improvement","agent",1,"Add an agent flag for executor re-registration timeout.","""Currently, the executor re-register timeout is hard-coded at 2 seconds. It would be beneficial to allow operators to specify this value."""
"MESOS-7546","Bug","containerization",3,"WAIT_NESTED_CONTAINER sometimes returns 404","""{{WAIT_NESTED_CONTAINER}} sometimes returns 404s even though the nested container has already exited and the parent task/executor is still running.  This happens when an agent uses more than one containerizer (e.g.,  {{docker,mesos}}, {{WAIT_NESTED_CONTAINER}} and the exit status of the nested container has already been checkpointed.  The root cause of this is a bug in the {{ComposingContainerizer}} in the following lines: https://github.com/apache/mesos/blob/1c7ffbeb505b3f5ab759202195f0b946a20cb803/src/slave/containerizer/composing.cpp#L620-L628  """
"MESOS-7561","Task","storage",2,"Add storage resource provider specific information in ResourceProviderInfo.","""For storage resource provider, there will be some specific configuration information. For instance, the most important one is the `ContainerConfig` of the CSI Plugin container.    That config information will be sent to the corresponding agent that will use the resources provided by the resource provider. For storage resource provider particularly, the agent needs to launch the CSI Node Plugin to mount the volumes.    Comparing to adding first class storage resource provider information, an alternative is to add a generic labels field in ResourceProviderInfo and let resource provider itself figure out the format of the labels. However, I believe a first class solution is better and more clear."""
"MESOS-7558","Task","master",3,"Add resource provider validation","""Similar to how it's done during agent registration/re-registration, the informations provided by a resource provider need to get validation during certain operation (e.g. re-registration, while applying offer operations, ...). Some of these validations only cover the provided informations (e.g. are the resources in {{ResourceProviderInfo}} only of type {{disk}}), others take the current cluster state into account (e.g. do the resources that a task wants to use exist on the resource provider)."""
"MESOS-7555","Task","master",5,"Add resource provider IDs to the registry","""To support resource provider re-registration following a master fail-over, the IDs of registered resource providers need to be kept in the registry. An operation to commit those IDs using the registrar needs to be added as well."""
"MESOS-7564","Bug","agent|executor",5,"Introduce a heartbeat mechanism for v1 HTTP executor <-> agent communication.","""Currently, we do not have heartbeats for executor <-> agent communication. This is especially problematic in scenarios when IPFilters are enabled since the default conntrack keep alive timeout is 5 days. When that timeout elapses, the executor doesn't get notified via a socket disconnection when the agent process restarts. The executor would then get killed if it doesn't re-register when the agent recovery process is completed.    Enabling application level heartbeats or TCP KeepAlive's can be a possible way for fixing this issue.    We should also update executor API documentation to explain the new behavior."""
"MESOS-7578","Task","containerization",5,"Write a proposal to make the I/O Switchboards optional","""Right now DEBUG containers can only be started using the LaunchNestedContainerSession API call. They will enter its parent’s namespaces, inherit environment variables, stream its I/O, and Mesos will tie their life-cycle to the lifetime of the HTTP connection.  Streaming the I/O of a container requires an I/O Switchboard and adds some overhead and complexity:  - Mesos will launch an extra process, called an I/O Switchboard for each nested container. These process aren’t free, they take some time to create/destroy and consume resources. - I/O Switchboards are managed by a complex isolator. - /O Swichboards introduce new race conditions, and have been a source of deadlocks in the past.   Some use cases require some of the features provided by DEBUG containers, but don’t need the functionality provided by the I/O switchboard. For instance, the Default Executor uses DEBUG containers to perform (health)checks, but it doesn’t need to stream anything to/from the container. """
"MESOS-7581","Bug","build",1,"Specifying an unbundled dependency can cause build to pick up wrong Boost version","""Specifying an unbundled dependency can cause the build to pick up a wrong Boost version. Assuming we have e.g., both protobuf and Boost installed in {{PREFIX}}, configuring with {{--with-protobuf=PREFIX}} causes the build to pick up the Boost version from {{PREFIX}} instead of using the bundled one.  This appears to be due to how we specify Boost include paths. Boost paths are added with {{-isystem}} to suppress warnings; the protobuf include path, on the other hand, would be added with {{-I}}. GCC and for compatibility clang first search all paths specified with {{-I}} left-to-right before looking at paths given with {{-isystem}}, see [the GCC documenation|https://gcc.gnu.org/onlinedocs/gcc/Directory-Options.html]."""
"MESOS-7594","Task","master",5,"Implement 'apply' for resource provider related operations","""Resource providers provide new offer operations ({{CREATE_BLOCK}}, {{DESTROY_BLOCK}}, {{CREATE_VOLUME}}, {{DESTROY_VOLUME}}). These operations can be applied by frameworks when they accept on offer. Handling of these operations has to be added to the master's {{accept}} call. I.e. the corresponding resource provider needs be extracted from the offer's resources and a {{resource_provider::Event::OPERATION}} has to be sent to the resource provider. The resource provider will answer with a {{resource_provider::Call::Update}} which needs to be handled as well."""
"MESOS-7604","Bug","test",1,"SlaveTest.ExecutorReregistrationTimeoutFlag aborts on Windows",""""""
"MESOS-7630","Improvement","agent|master",5,"Add simple filtering to unversioned operator API","""Add filtering for the following endpoints: - {{/frameworks}} - {{/slaves}} - {{/tasks}} - {{/containers}}  We should investigate whether we should use RESTful style or query string to filter the specific resource. We should also figure out whether it's necessary to filter a list of resources."""
"MESOS-7627","Bug","executor",3,"Mesos slave stucks","""*Description of the problem*  Sometimes all containers on mesos-slave becomes unhealthy without any reason. Then Mesos try to kill them without success. As result old containers are still running in unhealthy state and new containers have not started. You can see what happens on host machine and in docker container mesos-slave.  We have been seen this problem several times on month on different hosts and different clusters. Restart of mesos-slave solves the problem, but it is not solution.  {quote} Mesos 1.1 Marathon 1.3.6 Docker version 17.03.0-ce {quote}  stderr of container:   stdout of container   n container with mesos-slave   Container with mesos-slave   Container with mesos-slave   host-machine   Mesos slave logs    I ask you to take a look on this problem before mesos-slave is not restared and I can collect some additional information."""
"MESOS-7643","Improvement","containerization",2,"The order of isolators provided in '--isolation' flag is not preserved and instead sorted alphabetically","""According to documentation and comments in code the order of the entries in the --isolation flag should specify the ordering of the isolators. Specifically, the `create` and `prepare` calls for each isolator should run serially in the order in which they appear in the --isolation flag, while the `cleanup` call should be serialized in reverse order (with exception of filesystem isolator which is always first).  But in fact, the isolators provided in '--isolation' flag are sorted alphabetically.  That happens in [this line of code|https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/containerizer.cpp#L377]. In this line use of 'set<string>' is done (apparently instead of list or vector) and set is a sorted container."""
"MESOS-7652","Bug","containerization",3,"Docker image with universal containerizer does not work if WORKDIR is missing in the rootfs.","""hello, used the following docker image recently  quay.io/spinnaker/front50:master https://quay.io/repository/spinnaker/front50  Here the link to the Dockerfile https://github.com/spinnaker/front50/blob/master/Dockerfile  and here the source {color:blue}FROM java:8  MAINTAINER delivery-engineering@netflix.com  COPY . workdir/  WORKDIR workdir  RUN GRADLE_USER_HOME=cache ./gradlew buildDeb -x test && \   dpkg -i ./front50-web/build/distributions/*.deb && \   cd .. && \   rm -rf workdir  CMD [""""/opt/front50/bin/front50""""]{color}   The image works fine with the docker containerizer, but the universal containerizer shows the following in stderr.  """"Failed to chdir into current working directory '/workdir': No such file or directory""""  The problem comes from the fact that the Dockerfile creates a workdir but then later removes the created dir as part of a RUN. The docker containerizer has no problem with it if you do  docker run -ti --rm quay.io/spinnaker/front50:master bash  you get into the working dir, but the universal containerizer fails with the error.  thanks for your help, Michael"""
"MESOS-7663","Documentation","documentation",2,"Update the documentation to reflect the addition of reservation refinement.","""There are a few things we need to be sure to document:    * What reservation refinement is.  * The new """"format"""" for Resource, when using the RESERVATION_REFINEMENT capability.  * The filtering of resources if a framework is not RESERVATION_REFINEMENT capable.  * The current limitations that only a single reservation can be pushed / popped within a single RESERVE / UNRESERVE operation."""
"MESOS-7662","Bug","documentation",2,"Documentation regarding TASK_LOST is misleading","""Our protos describe {{TASK_LOST}} as a terminal state [\[1\]|https://github.com/apache/mesos/blob/fb54d469dcadf762e9c3f8a2fed78ed7b306120a/include/mesos/mesos.proto#L1722] [\[2\]|https://github.com/apache/mesos/blob/fb54d469dcadf762e9c3f8a2fed78ed7b306120a/include/mesos/mesos.proto#L64-L73].  A task might go from {{TASK_LOST}} to {{TASK_RUNNING}} or another state if Mesos is not using a strict register, so the documentation is misleading.  Marathon used to assume that {{TASK_LOST}} was a terminal past and that resulted in production pain for some users.  We should update the documentation to make the life of frameworks developers a bit better =)."""
"MESOS-7661","Bug","libprocess",3,"Libprocess timers with long durations trigger immediately","""{{process::delay()}} will schedule a method to be run right ahead when called with a veeeery long {{Duration}}.  This happens because [{{Timeout}} tries to add two long durations|https://github.com/apache/mesos/blob/13cae29e7832d8bb879c68847ad0df449d227f17/3rdparty/libprocess/include/process/timeout.hpp#L33-L38], leading to an [integer overflow in {{Duration}}|https://github.com/apache/mesos/blob/13cae29e7832d8bb879c68847ad0df449d227f17/3rdparty/stout/include/stout/duration.hpp#L116].  I'd expect libprocess to either:    1. Never run the method.   2. Schedule it in the longest possible {{Duration}}.  {{Duration::operator+=()}} should probably also handle integer overflows differently. If an addition leads to an integer overflow, it might make more sense to return {{Duration::max()}} than a negative duration."""
"MESOS-7660","Bug","allocation",3,"HierarchicalAllocator uses the default filter instead of a very long one","""If a framework accepts/refuses an offer using a very long filter, [the {{HierarchicalAllocator}} will use the default {{Filter}} instead|https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/hierarchical.cpp#L1046-L1052]. Meaning that it will filter the resources for only 5 seconds.  This can happen when a framework sets {{Filter::refuse_seconds}} to a number of seconds [larger than what fits in {{Duration}}|https://github.com/apache/mesos/blob/13cae29e7832d8bb879c68847ad0df449d227f17/3rdparty/stout/include/stout/duration.hpp#L401-L405].  The following [tests are flaky|https://issues.apache.org/jira/browse/MESOS-7514] because of this: {{ReservationTest.ReserveShareWithinRole}} and {{ReservationTest.PreventUnreservingAlienResources}}."""
"MESOS-7675","Improvement","agent",8,"Isolate network ports.","""If a task uses network ports, there is no isolator that can enforce that it only listens on the ports that it has resources for. Implement a ports isolator that can limit tasks to listen only on allocated TCP ports.  Roughly, the algorithm for this follows what standard tools like {{lsof}} and {{ss}} do.  * Find all the listening TCP sockets (using netlink) * Index the sockets by their node (from the netlink information) * Find all the open sockets on the system (by scanning {{/proc/\*/fd/\*}} links) * For each open socket, check whether its node (given in the link target) in the set of listen sockets that we scanned * If the socket is a listening socket and the corresponding PID is in the task, send a resource limitation for the task  Matching pids to tasks depends on using cgroup isolation, otherwise we would have to build a full process tree, which would be nice to avoid.  Scanning all the open sockets can be avoided by using the {{net_cls}} isolator with kernel + libnl3 patches to publish the socket classid when we find the listening socket.  Design Doc: https://docs.google.com/document/d/1BGmANq8IW-H4-YVUlpdf6qZFTZnDe-OKAY_e7uNp7LA Kernel Patch: http://marc.info/?l=linux-kernel&m=150293015025396&w=2"""
"MESOS-7691","Improvement","containerization",8,"Support local enabled cgroups subsystems automatically.","""Currently, each cgroup subsystem needs to be turned on as an isolator, e.g., """"cgroups/blkio"""". Ideally, mesos should be able to detect all local enabled cgroup subsystems and turn them on automatically (or we call it auto cgroups)."""
"MESOS-7699","Bug","build",8,"""stdlib.h: No such file or directory"" when building with GCC 6 (Debian stable freshly released)","""Hi,    It seems the issue comes from a workaround added a while ago:  https://reviews.apache.org/r/40326/  https://reviews.apache.org/r/40327/    When building with external libraries it turns out creating build commands line with -isystem /usr/include which is clearly stated as being wrong, according to GCC guys:  https://gcc.gnu.org/bugzilla/show_bug.cgi?id=70129    I'll do some testing by reverting all -isystem to -I and I'll let it know if it gets built.    Regards, Adam.      """
"MESOS-7695","Improvement","HTTP API",3,"Add heartbeats to master stream API","""Just like master uses heartbeats for scheduler API to keep the connection alive, it should do the same for the streaming API."""
"MESOS-7713","Task","libprocess",3,"Optimize number of copies made in dispatch/defer mechanism","""Profiling agents reregistration for a large cluster shows, that many CPU cycles are spent on copying protobuf objects. This is partially due to copies made by a code like this:  {{param}} could be copied 8-10 times before it reaches {{method}}. Specifically, {{reregisterSlave}} accepts vectors of rather complex objects, which are passed to {{defer}}. Currently there are some places in {{defer}}, {{dispatch}} and {{Future}} code, which could use {{std::move}} and {{std::forward}} to evade some of the copies."""
"MESOS-7709","Task","containerization",5,"Add --default_container_dns flag to the agent.","""Mesos support both CNI (through `network/cni` isolator) and CNM (through docker) specification. Both these specifications allow for DNS entries for containers to be set on a per-container, and per-network basis.   Currently, the behavior of the agent is to use the DNS nameservers set in /etc/resolv.conf when the CNI or CNM plugin that is used to attached the container to the CNI/CNM network doesnt' explicitly set the DNS for the container. This is a bit inflexible especially when we have a mix of v4 and v6 networks.   The operator should be able to specify DNS nameservers for the networks he installs either the override the ones provided by the plugin or as defaults when the plugins are not going to specify DNS name servers.  In order to achieve the above goal we need to introduce a `\--dns` flag to the agent. The `\--dns` flag should support a JSON (or a JSON file) with the following schema: """
"MESOS-7728","Bug","java api",3,"Java HTTP adapter crashes JVM when leading master disconnects.","""When a Java scheduler using HTTP v0-v1 adapter loses the leading Mesos master, {{V0ToV1AdapterProcess::disconnected()}} is invoked, which in turn invokes Java scheduler [code via JNI|https://github.com/apache/mesos/blob/87c38b9e2bc5b1030a071ddf0aab69db70d64781/src/java/jni/org_apache_mesos_v1_scheduler_V0Mesos.cpp#L446]. This call uses the wrong object, {{jmesos}} instead of {{jscheduler}}, which crashes JVM:  """
"MESOS-7742","Bug","agent",5,"Race conditions in IOSwitchboard: listening on unix socket and premature closing of the connection.","""Observed this on ASF CI and internal Mesosphere CI. Affected tests:      This issue comes at least in three different flavours. Take {{AgentAPIStreamingTest.AttachInputToNestedContainerSession}} as an example.  h5. Flavour 1      h5. Flavour 2      h5. Flavour 3  """
"MESOS-7757","Task","master",5,"Update master to handle updates to agent total resources","""With MESOS-7755 we update the allocator interface to support updating the total resources on an agent. These allocator invocations are driven by the master when it receives an update the an agent's total resources.  We could transport the updates from agents to the master either as update to {{UpdateSlaveMessage}}, e.g., by adding a {{repeated Resource total}} field; in order to distinguish updates to {{oversubscribed}} to updates to {{total}} we would need to introduce an additional tag field (an empty list of {{Resource}} has the same representation as an absent list of {{Resource}}). Alternatively we could introduce a new message transporting just the update to {{total}}; it should be possible to reuse such a message for external resource providers which we will likely add at a later point."""
"MESOS-7755","Task","allocation",3,"Update allocator to support updating agent total resources","""Agents encapsulate resource providers making their resources appear to the master as agent resources. In order to permit updates to the resources of a local resource provider (e.g., available disk expanded physically by adding another driver, resource provider resources disappeared since resource provider disappeared), we need to allow agents to change their total resources.  Expected semantics for the hierarchical allocator would be that {{total}} can shrink independently of the current {{allocated}}; should {{allocated}} exceed {{total}} no allocations can be made until {{allocated < total}}."""
"MESOS-7761","Bug","project website",1,"Website ruby deps do not bundle on macOS","""When trying to bundle the ruby dependencies of the website on macOS-10.12.5 I get    It seems eventmachine-1.0.3 has known and fixed issues on macOS-10.10.1 already. I suspect there might be a similar issue for the macOS-10.12.5 I am using."""
"MESOS-7758","Bug","build|stout",2,"Stout doesn't build standalone.","""Stout doesn't build in a standalone configuration:    Note that the build expects {{3rdparty/googletest-release-1.8.0/googlemock-build-stamp}}, but {{googletest}} hasn't been staged yet:  """
"MESOS-7762","Bug","stout",1,"net::IP::Network not building on Windows","""Building master (well, 2c1be9ced) is currently broken on Windows. Repro:    (Build instructions here: https://github.com/apache/mesos/blob/master/docs/windows.md)  Get a bunch of compilation errors:  """
"MESOS-7770","Bug","containerization",3,"Persistent volume might not be mounted if there is a sandbox volume whose source is the same as the target of the persistent volume.","""This issue is only for Mesos Containerizer.  If the source of a sandbox volume is a relative path, we'll create the directory in the sandbox in Isolator::prepare method: https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L480-L485  And then, we'll try to mount persistent volumes. However, because of this TODO in the code: https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/isolators/filesystem/linux.cpp#L726-L739  We'll skip mounting the persistent volume. That will cause a silent failure.  This is important because the workaround we suggest folks to solve MESOS-4016 is to use an additional sandbox volume."""
"MESOS-7769","Bug","libprocess",1,"libprocess initializes to bind to random port if --ip is not specified","""When running current [HEAD|https://github.com/apache/mesos/commit/c90bea80486c089e933bef64aca341e4cfaaef25],  {noformat:title=without --ip} ./mesos-master.sh --work_dir=/tmp/mesos-test1 ... I0707 14:14:05.927870  5820 master.cpp:438] Master db2a2d26-a9a9-4e6f-9909-b9eca47a2862 (<host>) started on <addr>:36839   It would be great this is caught by tests/CI."""
"MESOS-7772","Bug","agent",1,"Copy-n-paste error in slave/main.cpp","""Coverity diagnosed a copy-n-paste error in {{slave/main.cpp}} (https://scan5.coverity.com/reports.htm#v10074/p10429/fileInstanceId=120155401&defectInstanceId=33592186&mergedDefectId=1414687+1+Comment),    We check the incorrect IP for some value here (check on {{ip6}}, but use of {{ip}}), and it seems extremely likely we intended to use {{flags.ip6}}."""
"MESOS-7777","Bug","docker",3,"Agent failed to recover due to mount namespace leakage in Docker 1.12/1.13","""Docker changed its default mount propagation to """"shared"""" since 1.12 to enable persistent volume plugins. However, Docker has a known issue (https://github.com/moby/moby/issues/25718) that it sometimes leaks its mount namespace to other processes, which could make Mesos agents fail to remove Docker containers during recovery. The following shows the logs of such a faliure:  """
"MESOS-7790","Task","allocation",8,"Design hierarchical quota allocation.","""When quota is assigned in the role hierarchy (see MESOS-6375), it's possible for there to be """"undelegated"""" quota for a role. For example:        Here, the """"eng"""" role has 60 of its 90 cpus of quota delegated to its children, and 30 cpus remain undelegated. We need to design how to allocate these 30 cpus undelegated cpus. Are they allocated entirely to the """"eng"""" role? Are they allocated to the """"eng"""" role tree? If so, how do we determine how much is allocated to each role in the """"eng"""" tree (i.e. """"eng"""", """"eng/ads"""", """"eng/build"""")."""
"MESOS-7792","Improvement","libprocess",5,"Add support for ECDH ciphers","""[Elliptic curve ciphers|https://wiki.openssl.org/index.php/Elliptic_Curve_Cryptography] are a family of ciphers supported by OpenSSL. They allow to have smaller keys, but require an extra configuration parameter, the actual curve to be used, which can't be done through libprocess as it is."""
"MESOS-7791","Bug","libprocess",5,"subprocess' childMain using ABORT when encountering user errors","""In {{process/posix/subprocess.hpp}}'s {{childMain}} we exit with {{ABORT}} when there was a user error,    We here abort instead of simply {{_exit}}'ing and letting the user know that we couldn't deal with the given arguments.  Abort can potentially dump core, and since this abort is before the {{execvpe}}, the process image can potentially be large (e.g., >300 MB) which could quickly fill up a lot of disk space."""
"MESOS-7796","Bug","containerization|fetcher|network",3,"LIBPROCESS_IP isn't passed on to the fetcher","""{{LIBPROCESS_IP}} is not passed on to the fetcher.  The fetcher program uses libprocess, which depending on the DNS configuration might fail during initialization:  """
"MESOS-7803","Bug","stout",2,"fs::list drops path components on Windows","""fs::list(/foo/bar/*.txt) returns a.txt, b.txt, not /foo/bar/a.txt, /foo/bar/b.txt    This breaks a ZooKeeper test on Windows."""
"MESOS-7806","Task","stout",1,"Add copy assignment operator to `net::IP::Network`","""Currently, we can't extend the class `net::IP::Network` with out adding a copy assignment operator in the derived class, due to the use of `std::unique_ptr` in the base class. Hence, need to introduce a copy assignment operator into the base class."""
"MESOS-7805","Bug","documentation",1,"mesos-execute has incorrect example TaskInfo in help string","""{{mesos-execute}} documents that a task can be defined via JSON as   If one actually uses that example task definition one gets   Removing the resource role field allows the task to execute."""
"MESOS-7816","Task","storage",3,"Add HTTP connection handling to the resource provider driver","""The {{resource_provider::Driver}} is responsible for establishing a connection with an agent/master resource provider API and provide calls to the API, receive events from the API. This is done using HTTP and should be implemented similar to how it's done for schedulers and executors (see {{src/executor/executor.cpp, src/scheduler/scheduler.cpp}})."""
"MESOS-7814","Improvement","framework",3,"Improve the test frameworks.","""These improvements include three main points: * Adding a {{name}} flag to certain frameworks to distinguish between instances. * Cleaning up the code style of the frameworks. * For frameworks with custom executors, such as balloon framework, adding a {{executor_extra_uris}} flag containing URIs that will be passed to the {{command_info}} of the executor."""
"MESOS-7819","Improvement","libprocess",5,"Libprocess internal state is not monitored by metrics.","""Libprocess does not expose its internal state via metrics. Active sockets, number of HTTP proxies, number of running actors, number of pending messages for all active sockets, etc — may be of interest when monitoring and debugging Mesos clusters."""
"MESOS-7830","Bug","containerization",3,"Sandbox_path volume does not have ownership set correctly.","""This issue was exposed when using sandbox_path volume to support shared volume for nested containers under one task group. Here is a scenario:  The agent process runs as 'root' user, while the framework user is set as 'nobody'. No matter the commandinfo user is set or not, any non-root user cannot access the sandbox_path volume (e.g., a PARENT sandbox_path volume is not writable from a nested container). This is because the source path at the parent sandbox level is created by the agent process (aka root in this case).   While the operator is responsible for guaranteeing a nested container should have permission to write to its sandbox path volume at its parent's sandbox, we should guarantee the source path created at parent's sandbox should be set as the same ownership as this sandbox's ownership."""
"MESOS-7837","Improvement","agent",3,"Propagate resource updates from local resource providers to master","""When a resource provider registers with a resource provider manager, the manager should sent a message to its subscribers informing them on the changed resources.  For the first iteration where we add agent-specific, local resource providers, the agent would be subscribed to the manager. It should be changed to handle such a resource update by informing the master about its changed resources. In order to support master failovers, we should make sure to similarly inform the master on agent reregistration."""
"MESOS-7840","Improvement","cli",3,"Add Mesos CLI command to list active tasks","""We need to add a command to list all the tasks running in a Mesos cluster by checking the endpoint {{/tasks}} and reporting the results."""
"MESOS-7853","Task","containerization",5,"Support shared PID namespace.","""Currently, with the 'namespaces/pid' isolator enabled, each container will have its own pid namespace. This does not meet the need for some scenarios. For example, under the same executor container, one task wants to reach out to another task which need to share the same pid namespace.  We should support container pid namespace to be configurable. Users can choose one container to share its parent's pid namespace or not.  User facing API:   A new agent flag: --disallow_top_level_pid_ns_sharing (defaults to be: false) this is a security concern from operator's perspective. While some of the nested containers share the pid namespace from their parents, the top level containers always not share the pid ns from the agent."""
"MESOS-7851","Bug","master",3,"Master stores old resource format in the registry","""We intend for the master to store all internal resource representations in the new, post-reservation-refinement format. However, [when persisting registered agents to the registrar|https://github.com/apache/mesos/blob/498a000ac1bb8f51dc871f22aea265424a407a17/src/master/master.cpp#L5861-L5876], the master does not convert the resources; agents provide resources in the pre-reservation-refinement format, and these resources are stored as-is. This means that after recovery, any agents in the master's {{slaves.recovered}} map will have {{SlaveInfo.resources}} in the pre-reservation-refinement format.    We should update the master to convert these resources before persisting them to the registry."""
"MESOS-7849","Bug","agent|containerization",3,"The rlimits and linux/capabilities isolators should support nested containers","""The rlimits and linix/capabilities isolators don't support nesting. That means that the rlimits or capabilities set for tasks that launched by the DefaultExecutor are silently ignored."""
"MESOS-7858","Bug","containerization",5,"Launching a nested container with namespace/pid isolation, with glibc < 2.25, may deadlock the LinuxLauncher and MesosContainerizer","""This bug in glibc (fixed in glibc 2.25) will sometimes cause a child process of a {{fork}} to {{assert}} incorrectly, if the parent enters a new pid namespace before forking:  https://sourceware.org/bugzilla/show_bug.cgi?id=15392 https://sourceware.org/bugzilla/show_bug.cgi?id=21386  The LinuxLauncher code happens to do this when launching nested containers: * The MesosContainerizer process launches a subprocess, with a customized {{ns::clone}} function as an argument.  The thread then basically waits for the launch to succeed and return a child PID: https://github.com/apache/mesos/blob/1.3.x/src/slave/containerizer/mesos/linux_launcher.cpp#L495 * A separate thread in the Mesos agent forks and then waits for the grandchild to report a PID: https://github.com/apache/mesos/blob/1.3.x/src/linux/ns.hpp#L453 * The child of the fork first enters the namespaces (including a pid namespace) and then forks a grandchild.  The child then calls {{waitpid}} on the grandchild: https://github.com/apache/mesos/blob/1.3.x/src/linux/ns.hpp#L555 * Due to the glibc bug, the grandchild sometimes never returns from the {{fork}} here: https://github.com/apache/mesos/blob/1.3.x/src/linux/ns.hpp#L540  According to the glibc bug, we can work around this by: {quote} The obvious solution is just to use clone() after setns() and never use fork() - and one can certainly patch both programs to do so. Nevertheless it would be nice to see if fork() also worked after setns(), especially since there is no inherent reason for it not to. {quote}"""
"MESOS-7863","Bug","agent",5,"Agent may drop pending kill task status updates.","""Currently there is an assumption that when a pending task is killed, the framework will still be stored in the agent. However, this assumption can be violated in two cases:  # Another pending task was killed and we removed the framework in 'Slave::run' thinking it was idle, because pending tasks were empty (we remove from pending tasks when processing the kill). (MESOS-7783 is an example instance of this). # The last executor terminated without tasks to send terminal updates for, or the last terminated executor received its last acknowledgement. At this point, we remove the framework thinking there were no pending tasks if the task was killed (removed from pending)."""
"MESOS-7865","Bug","agent",3,"Agent may process a kill task and still launch the task.","""Based on the investigation of MESOS-7744, the agent has a race in which """"queued"""" tasks can still be launched after the agent has processed a kill task for them. This race was introduced when {{Slave::statusUpdate}} was made asynchronous:  (1) {{Slave::__run}} completes, task is now within {{Executor::queuedTasks}} (2) {{Slave::killTask}} locates the executor based on the task ID residing in queuedTasks, calls {{Slave::statusUpdate()}} with {{TASK_KILLED}} (3) {{Slave::___run}} assumes that killed tasks have been removed from {{Executor::queuedTasks}}, but this now occurs asynchronously in {{Slave::_statusUpdate}}. So, the executor still sees the queued task and delivers it and adds the task to {{Executor::launchedTasks}}. (3) {{Slave::_statusUpdate}} runs, removes the task from {{Executor::launchedTasks}} and adds it to {{Executor::terminatedTasks}}."""
"MESOS-7872","Bug","scheduler driver",5,"Scheduler hang when registration fails.","""I'm finding that if framework registration fails, the mesos driver client will hang indefinitely with the following output:   I'd have expected one or both of the following: - SchedulerDriver.run() should have exited with a failed Proto.Status of some form - Scheduler.error() should have been invoked when the """"Got error"""" occurred  Steps to reproduce: - Launch a scheduler instance, have it register with a known-bad framework info. In this case a role containing slashes was used - Observe that the scheduler continues in a TASK_RUNNING state despite the failed registration. From all appearances it looks like the Scheduler implementation isn't invoked at all  I'd guess that because this failure happens before framework registration, there's some error handling that isn't fully initialized at this point."""
"MESOS-7871","Bug","agent",2,"Agent fails assertion during request to '/state'","""While processing requests to {{/state}}, the Mesos agent calls {{Framework::allocatedResources()}}, which in turn calls {{Slave::getExecutorInfo()}} on executors associated with the framework's pending tasks.  In the case of tasks launched as part of task groups, this leads to the failure of the assertion [here|https://github.com/apache/mesos/blob/a31dd52ab71d2a529b55cd9111ec54acf7550ded/src/slave/slave.cpp#L4983-L4985]. This means that the check will fail if the agent processes a request to {{/state}} at a time when it has pending tasks launched as part of a task group.  This assertion should be removed since this helper function is now used with task groups."""
"MESOS-7877","Improvement","test",2,"Audit test code for undefined behavior in accessing container elements","""We do not always make sure we never access elements from empty containers, e.g., we use patterns like the following   While the intention here is to diagnose an empty {{offers}}, the code still exhibits undefined behavior in the element access if {{offers}} was indeed empty (compilers might aggressively exploit undefined behavior to e.g., remove """"impossible"""" code). Instead one should prevent accessing any elements of an empty container, e.g.,   We should audit and fix existing test code for such incorrect checks and variations involving e.g., {{EXPECT_NE}}."""
"MESOS-7883","Bug","allocation",1,"Quota heuristic check not accounting for mount volumes","""This may be expected but came as a surprise to us. We are unable to create a quota bigger than the root disk space on slaves.    Given two clusters with the same number of slaves and root disk size, but one that also has mount volumes, is what the disk resources look like:            In {{fin-fang-foom}}, I was able to create a quota for {{143490mb}} which is the total of available disk resources, root in this case, as reported by Mesos. For {{hydra}}, I am only able to create a quota for {{143489mb}}. This is equivalent to the total of root disks available in {{hydra}} rather than the total available disks reported by Mesos resources which is {{254084mb}}.    With a modified Mesos that adds logging to {{quota_handler}}, we can see that only the {{disk(*)}} number increases in {{nonStaticClusterResources}} after every iteration. The final iteration is {{disk(*):143489}} which is the maximum quota I was able to create on {{hydra}}. We expected that quota heuristic check would also include resources such as {{disk(*)[MOUNT:/dcos/volume2]:7373}}      """
"MESOS-7892","Task","agent",3,"Filter results of `/state` on agent by role.","""The results returned by {{/state}} include data about resource reservations per each role, which should be filtered for certain users, particularly in a multi-tenancy scenario.  The kind of leaked data includes specific role names and their specific reservations."""
"MESOS-7917","Bug","docker",3,"Docker statistics not reported on Windows.","""On Windows, the JSON information provided by the agent at the /container API does not contain the expected {{statistics}} object for Docker containers on Windows. This breaks the dcos-metrics tool, required for DC/OS integration on Windows."""
"MESOS-7916","Improvement","executor",5,"Improve the test coverage of the DefaultExecutor.","""We should write tests for the {{DefaultExecutor}} to cover the following common scenarios:  # -Start a task that uses a GPU, and make sure that it is made available to the task.- # -Launch a Docker task with a health check.- # -Launch two tasks and verify that they can access a volume owned by the Executor via {{sandbox_path}} volumes.- # -Launch two tasks, each one in its own task group, and verify that they can access a volume owned by the Executor via {{sandbox_path}} volumes.- # -Launch a task that uses an env secret, make sure that it is accessible.- # -Launch a task using a URI and make sure that the artifact is accessible.- # -Launch a task using a Docker image + URIs, make sure that the fetched artifact is accessible.- # Launch one task and ensure that (health) checks can read from a persistent volume. # -Ensure that the executor's env is NOT inherited by the nested tasks.-"""
"MESOS-7923","Bug","network",1,"Make args optional in mesos port mapper plugin","""Current implementation of the mesos-port-mapper plugin fails if the args field is absent in the cni config which makes it very specific to mesos. Instead, if args could be optional then this plugin could be used in a more generic environment. """
"MESOS-7922","Bug","agent|master",2,"Fix communication between old masters and new agents.","""For re-registration, agents currently send the resources in tasks and executors to the master in the """"post-reservation-refinement"""" format, which is incompatible for pre-1.4 masters. We should change the agent such that it always downgrades the resources to the """"pre-reservation-refinement"""" format, and the master unconditionally upgrade the resources to """"post-reservation-refinement"""" format."""
"MESOS-7921","Bug","libprocess",8,"ProcessManager::resume sometimes crashes accessing EventQueue.","""The following segfault is found on [ASF|https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)/4159/] in {{MesosContainerizerSlaveRecoveryTest.ResourceStatistics}} but it's flaky and shows up in other tests and environments (with or without --enable-lock-free-event-queue) as well.    {noformat: title=Configuration}  ./bootstrap '&&' ./configure --verbose '&&' make -j6 distcheck      A builds@mesos.apache.org query shows many such instances: https://lists.apache.org/list.html?builds@mesos.apache.org:lte=1M:process%3A%3AEventQueue%3A%3AConsumer%3A%3Aempty"""
"MESOS-7924","Improvement","webui",5,"Add a javascript linter to the webui.","""As far as I can tell, javascript linters (e.g. ESLint) help catch some functional errors as well, for example, we've made some """"strict"""" mistakes a few times that ESLint can catch: MESOS-6624, MESOS-7912."""
"MESOS-7934","Bug","libprocess",5,"OOM due to LibeventSSLSocket send incorrectly returning 0 after shutdown.","""LibeventSSLSocket can return 0 from send incorrectly, which leads the caller to send the data twice!  See here: https://github.com/apache/mesos/blob/1.3.1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L396-L398  In some particular cases, it's possible that the caller keeps getting back 0 and loops infinitely, blowing up the memory and OOMing the process.  One example is when a send occurs after a shutdown:  """
"MESOS-7947","Improvement","executor",8,"Add GC capability to nested containers","""We should extend the existing API or add a new API for nested containers for an executor to tell the Mesos agent that a nested container is no longer needed and can be scheduled for GC."""
"MESOS-7946","Bug","executor|test",2,"DefaultExecutorTest.SigkillExecutor test fails on Windows","""  The above commit introduced the test {{MesosContainerizer/DefaultExecutorTest.SigkillExecutor}} which fails on Windows. At a rough glance, if this is dependent on the {{SIGKILL}} signal, it may not be applicable on Windows and just needs to be disabled."""
"MESOS-7945","Bug","master|test",2,"MasterAPITest.EventAuthorizationFiltering is flaky.","""    The above commit introduced the test {{MasterAPITest.EventAuthorizationFiltering}} which is flaky."""
"MESOS-7951","Improvement","agent|executor|HTTP API",8,"Design Doc for Extended KillPolicy","""After introducing the {{KillPolicy}} in MESOS-4909, some interactions with framework developers have led to the suggestion of a couple possible improvements to this interface. Namely,  * Allowing the framework to specify a command to be run to initiate termination, rather than a signal to be sent, would allow some developers to avoid wrapping their application in a signal handler. This is useful because a signal handler wrapper modifies the application's process tree, which may make introspection and debugging more difficult in the case of well-known services with standard debugging procedures.  * In the case of terminations which do begin with a signal, it would be useful to allow the framework to specify the signal to be sent, rather than assuming SIGTERM. PostgreSQL, for example, permits several shutdown types, each initiated with a [different signal|https://www.postgresql.org/docs/9.3/static/server-shutdown.html]."""
"MESOS-7964","Bug","agent",2,"Heavy-duty GC makes the agent unresponsive","""An agent is observed to performe heavy-duty GC every half an hour:   Each GC activity took 5+ minutes. During the period, the agent became unresponsive, the health check timed out, and no endpoint responded as well. When a disk-usage GC is trigged, around 300 pruning actors would be generated (https://github.com/apache/mesos/blob/master/src/slave/gc.cpp#L229). My hypothesis is that these actors would used all of the worker threads, and some of them took a long time to finish (possibly due to many files to delete, or too many fs operations at once, etc)."""
"MESOS-7972","Bug","test",1,"SlaveTest.HTTPSchedulerSlaveRestart test is flaky.","""Saw this on ASF CI when testing 1.4.0-rc5    """
"MESOS-7970","Improvement","libprocess",2,"Adding process::Executor::execute()","""It would be easier to use {{process::Executor}} if we can add an {{execute()}} interface that runs a function asynchronously and returns a {{Future}}, so we do the following: """
"MESOS-7966","Bug","master",5,"check for maintenance on agent causes fatal error","""We interact with the maintenance API frequently to orchestrate gracefully draining agents of tasks without impacting service availability.    Occasionally we seem to trigger a fatal error in Mesos when interacting with the api. This happens relatively frequently, and impacts us when downstream frameworks (marathon) react badly to leader elections.    Here is the log line that we see when the master dies:        It's quite possibly we're using the maintenance API in the wrong way. We're happy to provide any other logs you need - please let me know what would be useful for debugging.    Thanks."""
"MESOS-7978","Bug","webui",1,"Lint javascript files to enable linting.","""To enable the linting of our javascript codebase, the javascript files should first be linted so that new commits will not have to include fixes for current issues."""
"MESOS-7980","Bug","build|stout",2,"Stout fails to compile with libc >= 2.26.","""Glibc 2.26 removes """"xlocale.h"""" [1] which makes stout fail to compile. Stout should be using 'locale.h' instead.   [1]: https://sourceware.org/glibc/wiki/Release/2.26#Removal_of_.27xlocale.h.27"""
"MESOS-8003","Bug","test",2,"PersistentVolumeEndpointsTest.SlavesEndpointFullResources is flaky.","""Observed on internal CI:    Full log attached."""
"MESOS-8001","Bug","test",2,"PersistentVolumeEndpointsTest.NoAuthentication is flaky.","""Observed a failure on internal CI:    Full log attached."""
"MESOS-7998","Bug","test",2,"PersistentVolumeEndpointsTest.UnreserveVolumeResources is flaky.","""Observed a failure on the internal CI:    Full log attached."""
"MESOS-8015","Improvement","HTTP API|modules|scheduler api|security",2,"Design a scheduler (V1) HTTP API authenticatee mechanism.","""Provide a design proposal for a scheduler HTTP API authenticatee module."""
"MESOS-8013","Task","containerization",3,"Add test for blkio statistics","""In [MESOS-6162|https://issues.apache.org/jira/browse/MESOS-6162], we have added the support for cgroups blkio statistics. In this ticket, we'd like to add a test to verify the cgroups blkio statistics can be correctly retrieved via Mesos containerizer's {{usage()}} method."""
"MESOS-8012","Improvement","cli",3,"Support Znode paths for masters in the new CLI","""Right now the new Mesos CLI only works in single master mode with a single master IP and port. We should add support for finding the mesos leader in HA mode by hitting a set of zk instances similar to how {{mesos-resolve}} works."""
"MESOS-8021","Improvement","HTTP API|modules|scheduler api|security",3,"Update HTTP scheduler library to allow for modularized authenticatee.","""Allow the scheduler library to load an HTTP authenticatee module providing custom mechanisms for authentication."""
"MESOS-8017","Improvement","HTTP API|modules|security",2,"Introduce a basic HTTP authenticatee.","""Refactor the hardcoded basic HTTP authentication code from within the scheduler library into the (modularized) interface provided by MESOS-8016"""
"MESOS-8016","Improvement","HTTP API|modules|scheduler api|security",2,"Introduce modularized HTTP authenticatee.","""Define the implementation of a modularized interface for the scheduler library authentication, providing the means of an authenticatee. This interface will allow consumers of HTTP APIs to use replaceable authentication mechanisms via a defined interface."""
"MESOS-8032","Task","storage",5,"Launch CSI plugins in storage local resource provider.","""Launching a CSI plugin requires the following steps:  1. Verify the configuration.  2. Prepare a directory in the work directory of the resource provider where the socket file should be placed, and construct the path of the socket file.  3. If the socket file already exists and the plugin is already running, we should not launch another plugin instance.  4. Otherwise, launch a standalone container to run the plugin and connect to it through the socket file."""
"MESOS-8031","Task","agent",3,"SLRP Configuration","""A typical SLRP configuration could look like the following:  The {{csi_plugins}} field lists the configurations to launch standalone containers for CSI plugins. The plugins are specified through a map, then we use the {{controller_plugin_name}} and {{node_plugin_name}} fields to refer to the corresponding plugin. With this design, we can support both headless and split-component deployment for CSI."""
"MESOS-8030","Epic","agent",13,"A resource provider for supporting local storage through CSI","""The Storage Local Resource Provider (SLRP) is a resource provider component in Mesos to manage persistent local storage on agents. SLRP should support the following MVP functions: * Registering to the RP manager (P0) * Reporting available disk resources through a CSI controller plugin. (P0) * Processing resource converting operations (CREATE_BLOCK, CREATE_VOLUME, DESTROY_BLOCK, DESTROY_VOLUME) issued by frameworks to convert RAW disk resources to mount or block volumes through a CSI controller plugin (P0) * Publish/unpublish a disk resource through CSI controller/node plugins for a task (P0) * Support storage profiles through modules (P1) * Tracking and checkpointing resources and reservations (P1) """
"MESOS-8048","Bug","test",2,"ReservationEndpointsTest.GoodReserveAndUnreserveACL is flaky.","""As just observed on our internal CI;    Error Message      Log:  """
"MESOS-8046","Bug","test",2,"MasterTestPrePostReservationRefinement.ReserveAndUnreserveResourcesV1 is flaky.","""As seen on our internal CI.    Error Message        Log:  """
"MESOS-8052","Bug","stout",2,"""protoc"" not found when running ""make -j4 check"" directly in stout","""If we run {{make -j4 check}} without running {{make}} first, we will get the following error message: """
"MESOS-8051","Bug","agent|executor",2,"Killing TASK_GROUP fail to kill some tasks","""When starting following pod definition via marathon:        mesos will successfully kill all {{ct2}} containers but fail to kill all/some of the {{ct1}} containers. I've attached both master and agent logs. The interesting part starts after marathon issues 6 kills:        All {{.ct1}} tasks fail eventually (~30s) where {{.ct2}} are successfully killed."""
"MESOS-8058","Bug","agent",2,"Agent and master can race when updating agent state.","""In {{2af9a5b07dc80151154264e974d03f56a1c25838}} we introduce the use of {{UpdateSlaveMessage}} for the agent to inform the master about its current total resources. Currently we trigger this message only on agent registration and reregistration.    This can race with operations applied in the master and communicated via {{CheckpointResourcesMessage}}.    Example:    1. Agent ({{cpus:4(\*)}} registers.  2. Master is triggered to apply an operation to the agent's resources, e.g., a reservation: {{cpus:4(\*) -> cpus:4(A)}}. The master applies the operation to its current view of the agent's resources and sends the agent a {{CheckpointResourcesMessage}} so the agent can persist the result.  3. The agent sends the master an {{UpdateSlaveMessage}}, e.g., {{cpus:4(\*)}} since it hasn't received the {{CheckpointResourcesMessage}} yet.  4. The master processes the {{UpdateSlaveMessage}} and updates its view of the agent's resources to be {{cpus:4(\*)}}.  5. The agent processes the {{CheckpointResourcesMessage}} and updates its view of its resources to be {{cpus:4(A)}}.  6. The agent and the master have an inconsistent view of the agent's resources."""
"MESOS-8075","Task","libprocess",5,"Add ReadWriteLock to libprocess.","""We want to add a new {{ReadWriteLock}} similar to {{Mutex}}, which can provide better concurrecy protection for mutual exclusive actions, but allow high concurrency for actions which can be performed at the same time.    One use case is image garbage collection: the new API {{provisioner::pruneImages}} needs to be mutually exclusive from {{provisioner::provision}}, but multiple {{{provisioner::provision}} can concurrently run safely."""
"MESOS-8082","Bug","test",5,"updateAvailable races with a periodic allocation and leads to flaky tests.","""When an operator requests a resource modification (reserve resources, create a persitent volume and so on), a corresponding endpoint handler can request allocator state modification twice: recover resources from rescinded offers and for update applied operation. These operations should happen atomically, i.e., no other allocator change can happen in-between. This is however not the case: a periodic allocation can kick in. Solutions to this race might be: moving offer management to the allocator, coupling operations in the allocator, pausing allocator.    While this race does not necessarily lead to bugs in production—as long as operators and tooling can handle failures and retry—, it makes some tests using resource modification flaky, because in tests we do not plan for failures and retries."""
"MESOS-8079","Task","provisioner",5,"Checkpoint and recover layers used to provision rootfs in provisioner","""This information will be necessary for {{provisioner}} to determine all layers of active containers, which we need to retain when image gc happens."""
"MESOS-8078","Improvement","HTTP API",2,"Some fields went missing with no replacement in api/v1.","""Hi friends,     These fields are available via the state.json but went missing in the v1 of the API:  -leader_info- -> available via GET_MASTER which should always return leading master info  start_time  elected_time    As we're showing them on the Overview page of the DC/OS UI, yet would like not be using state.json, it would be great to have them somewhere in V1."""
"MESOS-8076","Bug","test",2,"PersistentVolumeTest.SharedPersistentVolumeRescindOnDestroy is flaky.","""I'm observing {{ROOT_MountDiskResource/PersistentVolumeTest.SharedPersistentVolumeRescindOnDestroy/0}} being flaky on our internal CI. From what I see in the logs, when {{framework1}} accepts an offer, creates volumes, launches a task, and kills it right after, the executor might manage to register in-between and hence an unexpected {{TASK_RUNNING}} status update is sent. To fix this, one approach is to explicitly wait for {{TASK_RUNNING}} before attempting to kill the task."""
"MESOS-8095","Bug","test",2,"ResourceProviderRegistrarTest.AgentRegistrar is flaky.","""Observed it in internal CI. Test log attached."""
"MESOS-8093","Bug","scheduler driver|test",2,"Some tests miss subscribed event because expectation is set after event fires.","""Tests    all have the same problem. They initiate a scheduler subscribe call in reaction to {{connected}} event. However, an expectation for {{subscribed}} event is created _afterwards_, which might lead to an uninteresting mock function call for {{subscribed}} followed by a failure to wait for {{subscribed}}, see attached log excerpt for more details. Problematic code is here: https://github.com/apache/mesos/blob/1c51c98638bb9ea0e8ec6a3f284b33d6c1a4e8ef/src/tests/containerizer/runtime_isolator_tests.cpp#L593-L615    A possible solution is to await for {{subscribed}} only, without {{connected}}, setting the expectation before a connection is attempted, see https://github.com/apache/mesos/blob/1c51c98638bb9ea0e8ec6a3f284b33d6c1a4e8ef/src/tests/default_executor_tests.cpp#L139-L159."""
"MESOS-8100","Task","agent",5,"Authorize standalone container calls from local resource providers.","""We need to add authorization for a local resource provider to call the standalone container API to prevent the provider from manipulating arbitrary containers. We can use the same JWT-based authN/authZ mechanism for executors, where the agent will create a auth token for each local resource provider instance:  """
"MESOS-8097","Task","agent",2,"Add filesystem layout for local resource providers.","""We need to add a checkpoint directory for local resource providers. The checkpoints should be tied to the slave ID, otherwise resources with the same ID appearing on different agents (due to agent failover and registering with a new ID) may confuse frameworks."""
"MESOS-8106","Improvement","containerization",3,"Docker fetcher plugin unsupported scheme failure message is not accurate.","""https://github.com/apache/mesos/blob/1.4.0/src/uri/fetchers/docker.cpp#L843    This failure message is not accurate. For such a case, if the user/operator give a wrong credential to communicate to a BASIC auth based docker private registry. The authentication failed but the log is still saying: """"Unsupported auth-scheme: BASIC""""    """
"MESOS-8102","Task","test",5,"Add a test CSI plugin for storage local resource provider.","""We need a dummy CSI plugin for testing storage local resoure providers. The test CSI plugin would just create subdirectories under its working directories to mimic the behavior of creating volumes, then bind-mount those volumes to mimic publish."""
"MESOS-8101","Task","storage",5,"Import resources from CSI plugins in storage local resource provider.","""The following lists the steps to import resources from a CSI plugin:  1. Launch the node plugin      1.1 GetSupportedVersions      1.2 GetPluginInfo      1.3 ProbeNode      1.4 GetNodeCapabilities  2. Launch the controller plugin      2.1 GetSuportedVersions      2.2 GetPluginInfo      2.3 GetControllerCapabilities  3. GetCapacity  4. ListVolumes  5. Report to the resource provider through UPDATE_TOTAL_RESOURCES"""
"MESOS-8108","Task","storage",3,"Process offer operations in storage local resource provider","""The storage local resource provider receives offer operations for reservations and resource conversions, and invoke proper CSI calls to implement these operations."""
"MESOS-8121","Bug","containerization",3,"Unified Containerizer Auto backend should check xfs ftype for overlayfs backend.","""when using xfs as the backing filesystem in unified containerizer, the `ftype` has to be equal to 1 if we are using the overlay fs backend. we should add the detection in auto backend logic because some OS (like centos 7.2) has xfs ftype=0 by default.    https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/"""
"MESOS-8119","Bug","test",3,"ROOT_DOCKER_DockerHealthyTask segfaults in debian 8.","""This test consistently cannot recover the agent on two debian 8 builds: with SSL and CMake based. The error is always the same (full logs attached):  """
"MESOS-8123","Bug","test",1,"GPU tests are failing due to TASK_STARTING.","""For instance: NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_VerifyDeviceAccess    """
"MESOS-8128","Bug","stout",3,"Make os::pipe file descriptors O_CLOEXEC.","""File descriptors from {{os::pipe}} will be inherited across exec. On Linux we can use [pipe2|http://man7.org/linux/man-pages/man2/pipe.2.html] to atomically make the pipe {{O_CLOEXEC}}."""
"MESOS-8125","Bug","docker",2,"Agent should properly handle recovering an executor when its pid is reused","""Here's how to reproduce this issue:    # Start a task using the Docker containerizer (the same will probably happen with the command executor).  # Stop the corresponding Mesos agent while the task is running.  # Change the executor's checkpointed forked pid, which is located in the meta directory, e.g., {{/var/lib/mesos/slave/meta/slaves/latest/frameworks/19faf6e0-3917-48ab-8b8e-97ec4f9ed41e-0001/executors/foo.13faee90-b5f0-11e7-8032-e607d2b4348c/runs/latest/pids/forked.pid}}. I used pid 2, which is normally used by {{kthreadd}}.  # Reboot the host"""
"MESOS-8130","Task","agent|master",2,"Add placeholder handlers for offer operation feedback","""In order to sketch out the flow of messages necessary to facilitate offer operation feedback, we should add some empty placeholder handlers to the master and agent as detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#]."""
"MESOS-8141","Task","storage",3,"Add filesystem layout for storage resource providers.","""We need directories for placing mount points and checkpoint CSI volume state for storage resource providers. Unlike resource checkpoints, CSI volume states should persist across agents since otherwise the CSI plugin might not work properly."""
"MESOS-8143","Task","storage",3,"Publish and unpublish storage local resources through CSI plugins.","""Storage local resource provider needs to call the following CSI API to publish CSI volumes for tasks to use:  1. ControllerPublishVolume (optional)  2. NodePublishVolume    Although we don't need to unpublish CSI volumes after tasks are completed, we still needs to unpublish them for DESTROY_VOLUME or DESTROY_BLOCK:  1. NodeUnpublishVolume  2. ControllerUnpublishVolume (optional)"""
"MESOS-8171","Bug","c++ api|java api|scheduler driver",2,"Using a failoverTimeout of 0 with Mesos native scheduler client can result in infinite subscribe loop","""Over the past year, the Marathon team has been plagued with an issue that hits our CI builds periodically in which the scheduler driver enters a tight loop, sending 10,000s of SUBSCRIBE calls to the master per second. I turned on debug logging for the client and the server, and it pointed to an issue with the {{doReliableRegistration}} method in sched.cpp. Here's the logs:        In Marathon, when we are running our tests, we set the failoverTimeout to 0 in order to cause the Mesos master to immediately forget about a framework when it disconnects.    On line 860 of sched.cpp, the retry-delay is set to 1/10th the failoverTimeout, which provides the best explanation for why the value is 0:        Reading through the code, it seems that once this value is 0, it will always be zero, since backoff is multiplicative (0 * 2 == 0), and the failover_timeout / 10 limit is applied each time.    To make matters worse, failoverTimeout of {{0}} is the default:        I've confirmed that when using 1.4.0 of the Mesos client java jar, this default is used if failoverTimeout is not set:    """
"MESOS-8179","Bug","scheduler driver",1,"Scheduler library has incorrect assumptions about connections.","""Scheduler library assumes that a connection cannot be interrupted between continuations, for example {{send()}} and {{_send()}}: [https://github.com/apache/mesos/blob/509a1ab3226bbec7c369f431656f4ec692da00ba/src/scheduler/scheduler.cpp#L553]. This is not true, {{detected()}} can fire in-between, leading to disconnection:      The bug has been introduced in https://reviews.apache.org/r/62594"""
"MESOS-8183","Task","agent",3,"Add a container daemon to monitor a long-running standalone container.","""The `ContanierDaemon` class is responsible to monitor if a long-running service running in a standalone container is ready to serve, and restart the service container if not. It does not manage the lifecycle of the contanier it monitors, so the container persists across `ContainerDaemon`s."""
"MESOS-8207","Task","agent|master|storage",2,"Reconcile offer operations between resource providers, agents, and master","""We need to implement reconciliation of pending or unacknowledged offer operations between resource providers and agent, and agents and master. """
"MESOS-8211","Task","agent",5,"Handle agent local resources in offer operation handler","""The master will send {{ApplyOfferOperationMessage}} instead of {{CheckpointResourcesMessage}} when an agent has the 'RESOURCE_PROVIDER' capability set. The agent handler for the message needs to be updated to support operations on agent resources."""
"MESOS-8224","Task","release",1,"mesos.interface 1.4.0 cannot be installed with pip","""This breaks some framework development tooling.    WIth latest pip:      This works fine for previous releases:      But it does not for 1.4.0:      Verbose output shows that pip skips the 1.4.0 distribution:  """
"MESOS-8222","Task","agent|master",3,"Add resource versions to RunTaskMessage","""To support speculative application of certain offer operations we have added resource versions to offer operation messages. This permits checking compatibility of master and agent state before applying operations.    Launch operations are not modelled with offer operation messages, but instead with {{RunTaskMessage}}. In order to provide the same consistency guarantees we need to add resource versions to {{RunTaskMessage}} as well. Otherwise we would only rely on resource containment checks in the agent to catch inconsistencies; these can be unreliable as there is no guarantee that the matched agent resource is unique (e.g., with two {{RESERVE}} operations on similar resorces triggered on the same agent and one of these failing, the other succeeding, we would end up potentially sending one framework a success status and the other a failed one, but would not do anything the make sure the speculative operation application matches the resources belonging to the sent offer operation status update)."""
"MESOS-8221","Improvement","agent",5,"Use protobuf reflection to simplify downgrading of resources.","""We currently have a {{downgradeResources}} function which is called on every  {{repeated Resource}} field in every message that we checkpoint. We should leverage  protobuf reflection to automatically downgrade any instances of {{Resource}} within any  protobuf message."""
"MESOS-8219","Task","master",1,"Validate that any offer operation is only applied on resources from a single provider","""Offer operations can only be applied to resources from one single resource provider. A number of places in the implementation assume that the provider ID obtained from any {Resource} in an offer operation is equivalent to the one from any other resource. We should update the master to validate that invariant and reject malformed operations."""
"MESOS-8240","Improvement","cli",8,"Add an option to build the new CLI and run unit tests.","""An update of the discarded [https://reviews.apache.org/r/52543/]    Also needs to be available for CMake."""
"MESOS-8245","Bug","test",3,"SlaveRecoveryTest/0.ReconnectExecutor is flaky.","""Observed it today in our CI. Logs attached."""
"MESOS-8249","Bug","containerization|provisioner",13,"Support image prune in mesos containerizer and provisioner.","""Implement image prune in containerizer and the provisioner, by using mark and sweep to garbage collect unused layers."""
"MESOS-8263","Bug","flaky",2,"ResourceProviderManagerHttpApiTest.ConvertResources is flaky","""From a ASF CI run:      """
"MESOS-8258","Bug","test",2,"Mesos.DockerContainerizerTest.ROOT_DOCKER_SlaveRecoveryTaskContainer is flaky.","""  Full log attached."""
"MESOS-8267","Bug","test",2,"NestedMesosContainerizerTest.ROOT_CGROUPS_RecoverLauncherOrphans is flaky.","""  Full log attached."""
"MESOS-8270","Task","agent",3,"Add an agent endpoint to list all active resource providers","""Operators/Frameworks might need information about all resource providers currently running on an agent. An API endpoint should provide that information and include resource provider name and type."""
"MESOS-8280","Bug","image-gc|provisioner",3,"Mesos Containerizer GC should set 'layers' after checkpointing layer ids in provisioner.","""    Please neglect the debugging logs like '111111'. To reproduce this issue, just continuously trigger image gc. The log above was from a scenario that we launch two nested containers. One sleeps 1 second, another sleep forever.    This is related to this patch: https://github.com/apache/mesos/commit/e273efe6976434858edb85bbcf367a02e963a467#diff-a3593ed0ebd2b205775f7f04d9b5afe7    The root cause is that we did not set the 'layers' after we checkpoint the layer ids in provisioner. The log below is the prove:  """
"MESOS-8293","Bug","allocation",3,"Reservation may not be allocated when the role has no quota.","""Reservations that belong to a role that has no quota may not be allocated even when the reserved resources are allocatable to the role.    This is because in the current implementation the reserved resources may be counted towards the headroom left for unallocated quota limit in the second stage allocation.    https://github.com/apache/mesos/blob/c844db9ac7c0cef59be87438c6781bfb71adcc42/src/master/allocator/mesos/hierarchical.cpp#L1764-L1767    Roles with quota do not have this issue because currently their reservations are taken care of in the first stage."""
"MESOS-8289","Bug","test",3,"ReservationTest.MasterFailover is flaky when run with `RESOURCE_PROVIDER` capability.","""On a system under load, {{ResourceProviderCapability/ReservationTest.MasterFailover/1}} can fail. {{GLOG_v=2}} of the failure:   """
"MESOS-8288","Bug","test",3,"SlaveTest.IgnoreV0ExecutorIfItReregistersWithoutReconnect is flaky.","""  Full log attached."""
"MESOS-8295","Improvement","containerization|image-gc",2,"Add excluded image parameter to containerizer::pruneImages() interface.","""Add excluded image parameter to containerizer::pruneImages() interface."""
"MESOS-8294","Improvement","containerization|image-gc|provisioner",8,"Support container image basic auto gc.","""Add heuristic logic in the agent for basic auto image gc support.    Please see this section for the new interface design:  https://docs.google.com/document/d/1TSn7HOFLWpF3TLRVe4XyLpv6B__A1tk-tU16B1ZbsCI/edit#heading=h.iepp3ce9i22i  """
"MESOS-8297","Bug","executor",5,"Built-in driver-based executors ignore kill task if the task has not been launched.","""If docker executor receives a kill task request and the task has never been launch, the request is ignored. We now know that: the executor has never received the registration confirmation, hence has ignored the launch task request, hence the task has never started. And this is how the executor enters an idle state, waiting for registration and ignoring kill task requests."""
"MESOS-8310","Documentation","containerization|image-gc|provisioner",2,"Document container image garbage collection.","""Document container image garbage collection."""
"MESOS-8314","Task","agent|HTTP API|master",3,"Add authorization to display of resource provider information in API calls and endpoints","""The {{GET_RESOURCE_PROVIDERS}} call is used to list all resource providers known to a Mesos agent. We akso display resource provider infos for the master's {{GET_AGENTS}} call. These call needs to be authorized."""
"MESOS-8325","Bug","agent|containerization",2,"Mesos containerizer does not properly handle old running containers","""We were testing an upgrade scenario recently and encountered the following assertion failure:      Looking into {{Slave::_launch}}, indeed we find an unguarded access to the parent container's {{ContainerConfig}} [here|https://github.com/apache/mesos/blob/c320ab3b2dc4a16de7e060b9e15e9865a73389b0/src/slave/containerizer/mesos/containerizer.cpp#L1716].    We recently [added checkpointing|https://github.com/apache/mesos/commit/03a2a4dfa47b1d47c5eb23e81f5ef8213e46d545] of {{ContainerConfig}} to the Mesos containerizer. It seems that we are not appropriately handling upgrades, when there may be old containers running for which we do not expect to recover a {{ContainerConfig}}."""
"MESOS-8327","Task","containerization",13,"Add container-specific CGroup FS mounts under /sys/fs/cgroup/* to Mesos containers","""Containers launched with Unified Containerizer do not include container-specific CGroup FS mounts under {{/sys/fs/cgroup}}, which are created by default by Docker (usually readonly for unprivileged containers). Let's honor the same convention for Mesos containers.    For example, this is needed by Uber's [{{automaxprocs}}|https://github.com/uber-go/automaxprocs] patch for Go programs, which amends {{GOMAXPROCS}} per CPU quota and requires access to the CPU cgroup subsystem."""
"MESOS-8339","Bug","allocation",3,"Quota headroom may be insufficiently held when role has more reservation than quota.","""If a role has more reservation than its quota, the current quota headroom calculation is insufficient in guaranteeing quota allocation.    Consider, role `A` with 100 (units of resource, same below) reservation and 10 quota and role `B` with no reservation and 90 quota. Let's say there is no allocation yet. The existing allocator would calculate that the required headroom is 100. And since unallocated quota role reserved resource is also 100, no additional resources would be held back for the headroom.    While role `A` would have no problem getting its quota satisfied. Role `B` may have difficulty getting any resources because the """"headroom"""" can only be allocated to `A`.    The solution is to calculate per-role headroom before aggregating the quantity. And unallocated reservations should not count towards quota headroom. In the above case. The headroom for role `A` should be zero, the headroom for role `B` should be 90. Thus the aggregated headroom will be `90`."""
"MESOS-8344","Improvement","HTTP API",2,"Improve JSON v1 operator API performance.","""According to some user reports, a simple comparison of the v1 operator API (using the """"GET_TASKS"""" call) and the v0 /tasks HTTP endpoint shows that the v1 API suffers from an inefficient implementation:    {noformat: title=Curl Timing}  Operator HTTP API (GET_TASKS): 0.02s user 0.08s system 1% cpu 9.883 total  Old /tasks API: /tasks: 0.00s user 0.00s system 1% cpu 0.222 total  {noformat}    Looking over the implementation, it suffers from the same issues we originally had with the JSON endpoints:    * Excessive copying up the """"tree"""" of state building calls.  * Building up the state object as opposed to directly serializing it."""
"MESOS-8350","Bug","master",2,"Resource provider-capable agents not correctly synchronizing checkpointed agent resources on reregistration","""For resource provider-capable agents the master does not re-send checkpointed resources on agent reregistration; instead the checkpointed resources sent as part of the {{ReregisterSlaveMessage}} should be used.    This is not what happens in reality. If e.g., checkpointing of an offer operation fails and the agent fails over the checkpointed resources would, as expected, not be reflected in the agent, but would still be assumed in the master.    A workaround is to fail over the master which would lead to the newly elected master bootstrapping agent state from {{ReregisterSlaveMessage}}."""
"MESOS-8352","Bug","allocation",3,"Resources may get over allocated to some roles while fail to meet the quota of other roles.","""In the quota role allocation stage, if a role gets some resources on an agent to meet its quota, it will also get all other resources on the same agent that it does not have quota for. This may starve roles behind it that have quotas set for those resources.    To fix that, we need to track quota headroom in the quota role allocation stage. In that stage, if a role has no quota set for a scalar resource, it will get that resource only when two conditions are both met:    - It got some other resources on the same agent to meet its quota; And    - After allocating those resources, quota headroom is still above the required amount."""
"MESOS-8361","Improvement","framework",3,"Example frameworks to support launching mesos-local.","""The scheduler driver and library support implicit launching of mesos-local for a convenient test setup. Some of our example frameworks account for this in supporting implicit ACL rendering and more.     We should unify the experience by documenting this behaviour and adding it to all example frameworks."""
"MESOS-8365","Improvement","image-gc",3,"Create AuthN support for prune images API","""We want to make sure there is a way to configure AuthZ for new API added in MESOS-8360."""
"MESOS-8388","Task","master",2,"Show LRP resources in master and agent endpoints.","""Currently, only resource provider info is shown. We should also show the resources provided by resource providers."""
"MESOS-8373","Task","agent|master",3,"Test reconciliation after operation is dropped en route to agent","""Since new code paths were added to handle operations on resources in 1.5, we should test that such operations are reconciled correctly after an operation is dropped on the way from the master to the agent."""
"MESOS-8391","Bug","agent|containerization|executor",3,"Mesos agent doesn't notice that a pod task exits or crashes after the agent restart","""h4. (1) Agent doesn't detect that a pod task exits/crashes    # Create a Marathon pod with two containers which just do {{sleep 10000}}.  # Restart the Mesos agent on the node the pod got launched.  # Kill one of the pod tasks    *Expected result*: The Mesos agent detects that one of the tasks got killed, and forwards {{TASK_FAILED}} status to Marathon.    *Actual result*: The Mesos agent does nothing, and the Mesos master thinks that both tasks are running just fine. Marathon doesn't take any action because it doesn't receive any update from Mesos.    h4. (2) After the agent restart, it detects that the task crashed, forwards the correct status update, but the other task stays in {{TASK_KILLING}} state forever    # Perform steps in (1).  # Restart the Mesos agent    *Expected result*: The Mesos agent detects that one of the tasks got crashed, forwards the corresponding status update, and kills the other task too.    *Actual result*: The Mesos agent detects that one of the tasks got crashed, forwards the corresponding status update, but the other task stays in `TASK_KILLING` state forever.    Please note, that after another agent restart, the other tasks gets finally killed and the correct status updates get propagated all the way to Marathon."""
"MESOS-8390","Improvement","master",3,"Notion of ""transitioning"" agents in the master is now inaccurate.","""While [~xujyan] and I were discussing https://reviews.apache.org/r/57535/ we found a recent change that made the concept of """"in transition"""" agents confusing. See my comment here: https://reviews.apache.org/r/52083/#review170066    Given the new semantics described in the summary of https://reviews.apache.org/r/52083, the need for a separate method {{transitioning}} no longer exists because now it just wraps around a single variable {{unrecovered}} and gives it an alias which is less intuitive (because when reading the word transitioning one would think it has a more general meaning)."""
"MESOS-8389","Improvement","master",5,"Notion of ""removable"" task in master code is inaccurate.","""In the past, the notion of a """"removable"""" task meant: the task is terminal and acknowledged. It appears now that a removable task is defined purely by its state (terminal or unreachable) but not whether the terminal update is acknowledged.    As a result, the code that is calling this function ({{isRemovable}}) ends up being unintuitive. One example of a confusing piece of code is within {{updateTask}}. Here, we have logic which says, if the task is removable, recover the resources *but don't remove it*. This seems more intuitive if directly described as: """"if the task is no longer consuming resources, then (e.g. transitioned to terminal or unreachable) then recover the resources"""".    If one looks up the documentation of {{isRemovable}}, it says """"When a task becomes removable, it is erased from the master's primary task data structures"""", but that isn't accurate since this function doesn't say whether the terminal task has been acknowledged, which is required for a task to be removable.    I think an easy improvement here would be to move this notion of removable towards something like {{isTerminalOrUnreachable}}. We could also think about how to name this concept more generally, like {{canReleaseResources}} to describe whether the task's resources are considered allocated.    If we do introduce a notion of {{isRemovable}}, it seems it should be saying whether the task could be removed from the master, which includes checking that terminal tasks have been acknowledged."""
"MESOS-8403","Improvement","agent|storage",3,"Add agent HTTP API operator call to mark local resource providers as gone","""It is currently not possible to mark local resource providers as gone (e.g., after agent reconfiguration). As resource providers registered at earlier times could still be cached in a number of places, e.g., the agent or the master, the only way to e.g., prevent this cache from growing too large is to fail over caching components (to e.g., prevent an agent cache to update a fresh master cache during reconciliation).    Showing unavailable and known to be gone resource providers in various endpoints is likely also confusing to users.    We should add an operator call to mark resource providers as gone. While the entity managing resource provider subscription state is the resource provider manager, it still seems to make sense to add this operator call to the agent API as currently only local resource providers are supported. The agent would then forward the call to the resource provider manager which would transition its state for the affected resource provider, e.g., setting its state to {{GONE}} and removing it from the list of known resource providers, and then send out an update to its subscribers."""
"MESOS-8402","Improvement","storage",8,"Resource provider manager should persist resource provider information","""Currently, the resource provider manager used to abstract away resource provider subscription and state does not persist resource provider information. It has no notion of e.g., disconnected or forcibly removed resource providers. This makes it hard to implement a number of features, e.g.,    * removal of a resource provider and make it possible to garbage collect its cached state (e.g., in the resource provider manager, agent, or master), or  * controlling resource provider resubscription, e.g., by observing and enforcing resubscription timeouts.    We should extend the resource provider manager to persist the state of each resource provider (e.g., {{CONNECTED}}, {{DISCONNECTED}}, its resources and other attributes). This information should also be exposed in resource provider reconciliation, and be reflected in master or agent endpoints."""
"MESOS-8416","Bug","containerization",5,"CHECK failure if trying to recover nested containers but the framework checkpointing is not enabled.","""    If the framework does not enable the checkpointing. It means there is no slave state checkpointed. But containers are still checkpointed at the runtime dir, which mean recovering a nested container would cause the CHECK failure due to its parent's sandbox dir is unknown."""
"MESOS-8411","Bug","agent",5,"Killing a queued task can lead to the command executor never terminating.","""If a task is killed while the executor is re-registering, we will remove it from queued tasks and shut down the executor if all the its initial tasks could not be delivered. However, there is a case (within {{Slave::___run}}) where we leave the executor running, the race is:    # Command-executor task launched.  # Command executor sends registration message. Agent tells containerizer to update the resources before it sends the tasks to the executor.  # Kill arrives, and we synchronously remove the task from queued tasks.  # Containerizer finishes updating the resources, and in {{Slave::___run}} the killed task is ignored.  # Command executor stays running!    Executors could have a timeout to handle this case, but it's not clear that all executors will implement this correctly. It would be better to have a defensive policy that will shut down an executor if all of its initial batch of tasks were killed prior to delivery.    In order to implement this, one approach discussed with [~vinodkone] is to look at the running + terminated but unacked + completed tasks, and if empty, shut the executor down in the {{Slave::___run}} path. This will require us to check that the completed task cache size is set to at least 1, and this also assumes that the completed tasks are not cleared based on time or during agent recovery."""
"MESOS-8419","Bug","agent",1,"RP manager incorrectly setting framework ID leads to CHECK failure","""The resource provider manager [unconditionally sets the framework ID|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/manager.cpp#L637] when forwarding operation status updates to the agent. This is incorrect, for example, when the resource provider [generates OPERATION_DROPPED updates during reconciliation|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/storage/provider.cpp#L1653-L1657], and leads to protobuf errors in this case since the framework ID's required {{value}} field is left unset."""
"MESOS-8417","Bug","libprocess",1,"Mesos can get ""stuck"" when a Process throws an exception.","""When a {{Process}} throws an exception, we log it, terminate the throwing {{Process}}, and continue to run. However, currently there exists no known user-level code that I'm aware of that handles the unexpected termination due to an uncaught exception.    Generally, this means that when an exception is thrown (e.g. a bad call to {{std::map::at}}), the {{Process}} terminates with a log message but things get """"stuck"""" and the user has to debug what is wrong / kill the process.    Libprocess would likely need to provide some primitives to better support handling unexpected termination of a {{Process}} in order for us to provide a strategy where we continue running.    In the short term, it would be prudent to abort libprocess if any {{Process}} throws an exception so that users can observe the issue and we can get it fixed."""
"MESOS-8424","Task","master",3,"Test that operations are correctly reported following a master failover","""As the master keeps track of operations running on a resource provider, it needs to be updated on these operations when agents reregister after a master failover. E.g., an operation that has finished during the failover should be reported as finished by the master after the agent on which the resource provider is running has reregistered."""
"MESOS-8422","Bug","master",5,"Master's UpdateSlave handler not correctly updating terminated operations","""I created a test that verifies that operation status updates are resent to the master after being dropped en route to it (MESOS-8420).    The test does the following:    # Creates a volume from a RAW disk resource.  # Drops the first `UpdateOperationStatusMessage` message from the agent to the master, so that it isn't acknowledged by the master.  # Restarts the agent.  # Verifies that the agent resends the operation status update.    The good news are that the agent is resending the operation status update, the bad news are that it triggers a CHECK failure that crashes the master.    Here are the relevant sections of the log produced by the test:        We can see that once the SLRP reregisters with the agent, the following happens:    # The agent will send an {{UpdateSlave}} message to the master including the converted resources and the {{CREATE_VOLUME}} operation with the status {{OPERATION_FINISHED}}.  # The master will update the agent's resources, including the volume created by the operation.  # The agent will resend the operation status update.  # The master will try to apply the operation and crash, because it already updated the agent's resources on step #2."""
"MESOS-8442","Task","documentation",1,"Source tree contains generated endpoint documentation","""Even though we generate documentation automatically in CI, the source tree still contains checked in, generated endpoint documentation in {{docs/endpoints}}.    We should remove these source files from the tree. We need to make sure to    * not break automatic website generation with {{support/mesos-website/build.sh}},  * not break the local website generation workflow with {{site/mesos-website-dev.sh}}, and  * not break local website generation workflow with {{rake}} via {{site/Rakefile}}."""
"MESOS-8444","Bug","agent",2,"GC failure causes agent miss to detach virtual paths for the executor's sandbox","""I launched a task via {{mesos-execute}} which just did a {{sleep 10}}, when the task finished, {{Slave::removeExecutor()}} and {{Slave::removeFramework()}} were called and they will try to gc 3 directories:  # /<slave-work-dir>/slaves/<slaveID>/frameworks/<frameworkID>/executors/<executorID>/runs/<containerID>  # /<slave-work-dir>/slaves/<slaveID>/frameworks/<frameworkID>/executors/<executorID>  # /<slave-work-dir>/slaves/<slaveID>/frameworks/<frameworkID>    For 1 and 2, the code to gc them is like this:      So here {{then()}} is used which means we will only do the detach when the gc succeeds. But the problem is the order of 1, 2 and 3 deleted by gc can not be guaranteed, from my test, 3 will be deleted first for most of times. Since 3 is the parent directory of 1 and 2, so the gc for 1 and 2 will fail:    So we will NOT do the detach for 1 and 2 which is a leak."""
"MESOS-8446","Bug","agent",1,"Agent miss to detach `virtualLatestPath` for the executor's sandbox during recovery","""In {{Framework::recoverExecutor()}}, we attach {{executor->directory}} to 3 virtual paths:  (1) /agent_workdir/frameworks/FID/executors/EID/runs/CID  (2) /agent_workdir/frameworks/FID/executors/EID/runs/latest  (3) /frameworks/FID/executors/EID/runs/latest  But in this method, when we find the executor completes, we only do detach for (1) and (2) but not (3). We should do detach for (3) too as what we do in {{Slave::removeExecutor}}, otherwise, it will be a leak."""
"MESOS-8447","Bug","build",1,"Incomplete output of apply-reviews.py --dry-run","""The script {{support/apply-reviews.py}} has a flag {{--dry-run}} which should dump the commands which would be performed. This flag is useful to e.g., reorder patch chains or to manually resolve intermediate conflicts while still being able to pull a full chain.    The output looks like this    Trying to replay that dry run leads to an error since the commands to create the commit message files are not printed.    We should add these commands to the output."""
"MESOS-8456","Improvement","allocation",8,"Allocator should allow roles to burst above guarantees but below limits.","""Currently, allocator only allocates resources for quota roles up to their guarantee in the first allocation stage. The allocator should continue allocating resources to these roles in the second stage below their quota limit. In other words, allocator should allow roles to burst above their guarantee but below the limit."""
"MESOS-8455","Improvement","agent|master",3,"Avoid unnecessary copying of protobuf in the v1 API.","""Now that we have move support for protobufs, we can avoid the unnecessary copying of protobuf in the v1 API to improve the performance."""
"MESOS-8454","Improvement","webui",3,"Add a download link for master and agent logs in WebUI","""Just like task sandboxes, it would be great for us to provide a download link for mesos and agent logs in the WebUI. Right now the the log link opens up the pailer, which is not really convenient to do `grep` and such while debugging."""
"MESOS-8453","Bug","test",3,"ExecutorAuthorizationTest.RunTaskGroup segfaults.","""  Full log attached."""
"MESOS-8463","Bug","allocation|test",3,"Test MasterAllocatorTest/1.SingleFramework is flaky","""Observed in our internal CI on a ubuntu-16 setup in a plain autotools build,  """
"MESOS-8470","Bug","allocation",2,"CHECK failure in DRFSorter due to invalid framework id.","""A framework registering with a custom {{FrameworkID}} containing slashes such as {{/foo/bar}} will trigger a CHECK failure at https://github.com/apache/mesos/blob/177a2221496a2caa5ad25e71c9982ca3eed02fd4/src/master/allocator/sorter/drf/sorter.cpp#L167:    The sorter should be defensive with any {{FrameworkID}} containing slashes."""
"MESOS-8474","Bug","storage|test",2,"Test StorageLocalResourceProviderTest.ROOT_ConvertPreExistingVolume is flaky","""Observed on our internal CI on ubuntu16.04 with SSL and GRPC enabled,  """
"MESOS-8473","Task","agent|master",3,"Authorize `GET_OPERATIONS` calls.","""The {{GET_OPERATIONS}} call lists all known operations on a master or agent. Authorization has to be added to this call."""
"MESOS-8480","Bug","containerization",2,"Mesos returns high resource usage when killing a Docker task.","""The way we get resource statistics for Docker tasks is through getting the cgroup subsystem path through {{/proc/<pid>/cgroup}} first (taking the {{cpuacct}} subsystem as an example):    Then read {{/sys/fs/cgroup/cpuacct//docker/66fbe67b64ad3a86c6e080e18578bc9e540e55ee0bdcae09c2e131a4264a3a3b/cpuacct.stat}} to get the statistics:    However, when a Docker container is being teared down, it seems that Docker or the operation system will first move the process to the root cgroup before actually killing it, making {{/proc/<pid>/docker}} look like the following:    This makes a racy call to [{{cgroup::internal::cgroup()}}|https://github.com/apache/mesos/blob/master/src/linux/cgroups.cpp#L1935] return a single '/', which in turn makes [{{DockerContainerizerProcess::cgroupsStatistics()}}|https://github.com/apache/mesos/blob/master/src/slave/containerizer/docker.cpp#L1991] read {{/sys/fs/cgroup/cpuacct///cpuacct.stat}}, which contains the statistics for the root cgroup:    This can be reproduced by [^test.cpp] with the following command:  """
"MESOS-8477","Bug","build",1,"Make clean fails without Python artifacts.","""Make clean may fail if there are no Python artifacts created by previous builds.             Triggered by [https://github.com/apache/mesos/blob/62d392704c499e06da0323e50dfd016cdac06f33/src/Makefile.am#L2218-L2219]"""
"MESOS-8485","Bug","test",3,"MasterTest.RegistryGcByCount is flaky","""Observed this while testing Mesos 1.5.0-rc1 in ASF CI.         """
"MESOS-8484","Bug","stout",1,"stout test NumifyTest.HexNumberTest fails. ","""The current Mesos master shows the following on my machine:        This problem disappears for me when reverting the latest boost upgrade."""
"MESOS-8482","Bug","test",1,"Signed/Unsigned comparisons in tests","""Many tests in mesos currently have comparisons between signed and unsigned integers, eg    or comparisons between values of different enums, e.g. TaskState and v1::TaskState:    Usually, the compiler would catch these and emit a warning, but these are currently silenced because gtest headers are included using the {{-isystem}} command line flag."""
"MESOS-8492","Task","storage",3,"Checkpoint profiles in storage local resource provider.","""SLRP should be able to handle missing profiles from an arbitrary disk profile module, and probably need to checkpoint them for recovery."""
"MESOS-8489","Bug","containerization",8,"LinuxCapabilitiesIsolatorFlagsTest.ROOT_IsolatorFlags is flaky","""Observed this on internal Mesosphere CI.    h2. Steps to reproduce   # Add {{::sleep(1);}} before [removing|https://github.com/apache/mesos/blob/e91ce42ed56c5ab65220fbba740a8a50c7f835ae/src/linux/cgroups.cpp#L483] """"test"""" cgroup   # recompile   # run `GLOG_v=2 sudo GLOG_v=2 ./src/mesos-tests --gtest_filter=LinuxCapabilitiesIsolatorFlagsTest.ROOT_IsolatorFlags --gtest_break_on_failure --gtest_repeat=10 --verbose`    h2. Race description    While recovery is in progress for [the first slave|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/tests/containerizer/linux_capabilities_isolator_tests.cpp#L733], calling [`StartSlave()`|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/tests/containerizer/linux_capabilities_isolator_tests.cpp#L738] leads to calling [`slave::Containerizer::create()`|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/tests/cluster.cpp#L431] to create a containerizer. An attempt to create a mesos c'zer, leads to calling [`cgroups::prepare`|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/slave/containerizer/mesos/linux_launcher.cpp#L124]. Finally, we get to the point, where we try to create a [""""test"""" container|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/linux/cgroups.cpp#L476]. So, the recovery process for the second slave [might detect|https://github.com/apache/mesos/blob/ce0905fcb31a10ade0962a89235fa90b01edf01a/src/slave/containerizer/mesos/linux_launcher.cpp#L268-L301] this """"test"""" container as an orphaned container.    Thus, there is the race between recovery process for the first slave and an attempt to create a c'zer for the second agent."""
"MESOS-8488","Improvement","containerization",2,"Docker bug can cause unkillable tasks.","""Due to an [issue on the Moby project|https://github.com/moby/moby/issues/33820], it's possible for Docker versions 1.13 and later to fail to catch a container exit, so that the {{docker run}} command which was used to launch the container will never return. This can lead to the Docker executor becoming stuck in a state where it believes the container is still running and cannot be killed.    We should update the Docker executor to ensure that containers stuck in such a state cannot cause unkillable Docker executors/tasks.    One way to do this would be a timeout, after which the Docker executor will commit suicide if a kill task attempt has not succeeded. However, if we do this we should also ensure that in the case that the container was actually still running, either the Docker daemon or the DockerContainerizer would clean up the container when it does exit.    Another option might be for the Docker executor to directly {{wait()}} on the container's Linux PID, in order to notice when the container exits."""
"MESOS-8487","Task","HTTP API",13,"Design API changes for supporting quota limits.","""Per MESOS-8068, the introduction of a quota limit requires introducing this in the API. We should send out the proposed changes more broadly in the interest of being more rigorous about API changes.    [Design doc|https://docs.google.com/document/d/13vG5uH4YVwM79ErBPYAZfnqYFOBbUy2Lym0_9iAQ5Uk/edit#]"""
"MESOS-8486","Task","webui",1,"Webui should display role limits.","""With the addition of quota limits (see MESOS-8068), the UI should be updated to display the per role limit information. Specifically, the 'Roles' tab needs to be updated."""
"MESOS-8497","Bug","containerization",2,"Docker parameter `name` does not work with Docker Containerizer.","""When deploying a marathon app with Docker Containerizer (need to check Mesos Containerizer) and the parameter name set, Mesos is not able to recognize/control/kill the started container.    Steps to reproduce    # Deploy the below marathon app definition   #  Watch task being stuck in staging and mesos not being able to kill it/communicate with it   ##   {quote}e.g., Agent Logs: W0126 18:38:50.000000  4988 slave.cpp:6750] Failed to get resource statistics for executor ‘instana-agent.1a1f8d22-02c8-11e8-b607-923c3c523109’ of framework 41f1b534-5f9d-4b5e-bb74-a0e387d5739f-0001: Failed to run ‘docker -H unix:///var/run/docker.sock inspect mesos-1c6f894d-9a3e-408c-8146-47ebab2f28be’: exited with status 1; stderr=’Error: No such image, container or task: mesos-1c6f894d-9a3e-408c-8146-47ebab2f28be{quote}   # Check on node and see container running, but not being recognized by mesos    """
"MESOS-8530","Bug","executor",5,"Default executor tasks can get stuck in KILLING state","""The default executor will transition a task to {{TASK_KILLING}} and mark its container as being killed before issuing the {{KILL_NESTED_CONTAINER}} call.    If the kill call fails, the task will get stuck in {{TASK_KILLING}}, and the executor won't allow retrying the kill.  """
"MESOS-8537","Bug","executor",3,"Default executor doesn't wait for status updates to be ack'd before shutting down","""The default executor doesn't wait for pending status updates to be acknowledged before shutting down, instead it sleeps for one second and then terminates:        The event handler should exit if upon receiving a {{Event::ACKNOWLEDGED}} the executor is shutting down, no tasks are running anymore, and all pending status updates have been acknowledged."""
"MESOS-8536","Bug","master",2,"Pending offer operations on resource provider resources not properly accounted for in allocator","""The master currently does not accumulate the resources used by offer operations on master failover. While we create a datastructure to hold this information, we missed updating it.      Here {{usedByOperations}} is not updated.    This leads to problems when the operation becomes terminal and we try to recover the used resources which might not be known to the framework sorter inside the hierarchical allocator."""
"MESOS-8546","Bug","test",2,"PythonFramework test fails with cache write failure.","""After some recent changes, the  {{ExamplesTest.PythonFramework}} fails on centos and ubuntu rather frequently (but not always).    The symptom always is like this (taken from an ASF CI run):      """
"MESOS-8545","Bug","agent",8,"AgentAPIStreamingTest.AttachInputToNestedContainerSession is flaky.",""""""
"MESOS-8548","Bug","storage",2,"Test StorageLocalResourceProviderTest.ROOT_Metrics is flaky","""The SLRP Metrics test is flaky because the agent might got two {{SlaveRegisteredMessage}}s due to its retry logic for registration, and thus it would send two {{UpdateSlaveMessage}}s. As a result, the futures waiting for these messages will be ready before the plugin is actually launched. This will lead to a race between the SIGKILL and LAUNCH_CONTAINER in the test, and if the kill happens before SLRP gets connected to the plugin, SLRP will wait for 1 minutes before giving up, which is too long for the test to wait for a second launch."""
"MESOS-8550","Bug","leader election|master",2,"Bug in `Master::detected()` leads to coredump in `MasterZooKeeperTest.MasterInfoAddress`.","""  This failure is most likely caused by calling [leader->has_domain()|https://github.com/apache/mesos/blob/994213739b1afc473bbd9d15ded7c3fd26eaa924/src/master/master.cpp#L2159] on empty `leader`, from logs:  """
"MESOS-8554","Task","HTTP API",3,"Enhance V1 scheduler send API to receive sync response from Master","""Current scheduler HTTP API doesn't provide a way for the scheduler to get a synchronous response back from the Master. A synchronous API means the scheduler wouldn't have to wait on the event stream to check the status of operations that require master-only validation/approval/rejection."""
"MESOS-8567","Bug","storage",3,"Test UriDiskProfileTest.FetchFromHTTP is flaky.","""The {{UriDiskProfileTest.FetchFromHTTP}} test is flaky on Debian 9:      I also run it in repetition and got the following error log (although the test itself is passed):  """
"MESOS-8569","Task","stout",2,"Allow newline characters when decoding base64 strings in stout.","""Current implementation of `stout::base64::decode` errors out on encountering a newline character (""""\n"""" or """"\r\n"""") which is correct wrt [RFC4668#section-3.3|https://tools.ietf.org/html/rfc4648#section-3.3]. However, most implementations insert a newline to delimit encoded string and ignore (instead of erroring out) the newline character while decoding the string. Since stout facilities are used by third-party modules to encode/decode base64 data, it is desirable to allow decoding of newline-delimited data."""
"MESOS-8576","Improvement","containerization|docker",3,"Improve discard handling of 'Docker::inspect()'","""In the call path of {{Docker::inspect()}}, each continuation currently checks if {{promise->future().hasDiscard()}}, where the {{promise}} is associated with the output of the {{docker inspect}} call. However, if the call to {{docker inspect}} becomes hung indefinitely, then continuations are never invoked, and a subsequent discard of the returned {{Future}} will have no effect. We should add proper {{onDiscard}} handling to that {{Future}} so that appropriate cleanup is performed in such cases."""
"MESOS-8574","Improvement","docker|executor",5,"Docker executor makes no progress when 'docker inspect' hangs","""In the Docker executor, many calls later in the executor's lifecycle are gated on an initial {{docker inspect}} call returning: https://github.com/apache/mesos/blob/bc6b61bca37752689cffa40a14c53ad89f24e8fc/src/docker/executor.cpp#L223    If that first call to {{docker inspect}} never returns, the executor becomes stuck in a state where it makes no progress and cannot be killed.    It's tempting for the executor to simply commit suicide after a timeout, but we must be careful of the case in which the executor's Docker container is actually running successfully, but the Docker daemon is unresponsive. In such a case, we do not want to send TASK_FAILED or TASK_KILLED if the task's container is running successfully."""
"MESOS-8591","Improvement","docker|test",3,"Add infra to test a hung Docker daemon","""We should add infrastructure to our tests which enables us to test the behavior of the Docker executor and containerizer in the presence of a hung Docker daemon.    One possible first-order solution is to build a simple binary which never returns. We could initialize the agent/executor with this binary instead of the Docker CLI in order to simulate a Docker daemon which hangs on every call."""
"MESOS-8592","Bug","storage",1,"Avoid failure for invalid profile in `UriDiskProfileAdaptor`","""We should be defensive and not fail the profile module when the user provides an invalid profile in the profile matrix."""
"MESOS-8594","Bug","master",1,"Mesos master stack overflow in libprocess socket send loop.","""Mesos master crashes under load. Attached are some infos from the `lldb`:    To quote [~abudnik]  {quote}it’s the stack overflow bug in libprocess due to the way `internal::send()` and `internal::_send()` are implemented in `process.cpp`  {quote}"""
"MESOS-8598","Bug","storage",1,"Allow empty resource provider selector in `UriDiskProfileAdaptor`.","""Currently in {{UriDiskProfileAdaptor}}, it is invalid for a profile to have a resource provider selector with 0 resource providers. However, one can put non-existent provider types and names into the selector to achieve the same effect, and this is semantically inconsistent. We should allow an empty list of resource providers directly."""
"MESOS-8601","Bug","master",3,"Master crashes during slave reregistration after failover.","""The following happened after a master failover.  During slave reregistration, new tasks were added and the new leading master notified all of its subscribers, and triggered the following check failure:      This was because the master tried to get the framework info when sending the notification: https://github.com/apache/mesos/blob/1.5.x/src/master/master.cpp#L11190    But it added the framework after that:  https://github.com/apache/mesos/blob/1.5.x/src/master/master.cpp#L6963"""
"MESOS-8605","Bug","docker",3,"Terminal task status update will not send if 'docker inspect' is hung","""When the agent processes a terminal status update for a task, it calls {{containerizer->update()}} on the container before it forwards the update: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/slave.cpp#L5509-L5514    In the Docker containerizer, {{update()}} calls {{Docker::inspect()}}, which means that if the inspect call hangs, the terminal update will not be sent: https://github.com/apache/mesos/blob/9635d4a2d12fc77935c3d5d166469258634c6b7e/src/slave/containerizer/docker.cpp#L1714"""
"MESOS-8604","Bug","allocation",2,"Quota headroom tracking may be incorrect in the presence of hierarchical reservation.","""When calculating the global quota headroom, we subtract all unallocated reservations by doing        We only traverse roles with reservation. In the presence of hierarchal reservation, this is problematic. Consider a child role (e.g. """"a/b"""") with no reservations, it can still get reserved resources if its ancestor has reservations (e.g. """"a"""" has reservations). However, allocated reserved resources of role “a/b” will be ignored given the above code.    The consequence is that availableHeadroom will be underestimated because allocated reservations are underestimated. This would lead to excessive resources set aside for quota headroom."""
"MESOS-8626","Bug","allocation",3,"The 'allocatable' check in the allocator is problematic with multi-role frameworks","""The [allocatable|https://github.com/apache/mesos/blob/1.5.x/src/master/allocator/mesos/hierarchical.cpp#L2471-L2479] check in the allocator (shown below) was originally introduced to help alleviate the situation where a framework receives only disk, but not cpu/memory, thus cannot launch a task.        When we introduce multi-role capability to the frameworks, this check makes less sense now. For instance, consider the following case:  1) There is a single agent and a single framework in the cluster  2) The agent has cpu/memory reserved to role A, and disk reserved to B  3) The framework subscribes to both role A and role B  4) The framework expects that it'll receive an offer containing the resources on the agent  5) However, the framework receives no disk resources due to the following [code|https://github.com/apache/mesos/blob/1.5.x/src/master/allocator/mesos/hierarchical.cpp#L2078-L2100]. This is counter intuitive.        Two comments:  1) If `allocatable` check is still necessary (see MESOS-7398)?  2) If we want to keep `allocatable` check for the original purpose, we should do that based on framework not role, given that a framework can subscribe to multiple roles now?    Some related JIRAs:  MESOS-1688  MESOS-7398  """
"MESOS-8640","Improvement","containerization",2,"Validate `DockerInfo` exists when container's type is `DOCKER`","""Currently when framework launches a task whose ContainerInfo's type is DOCKER (i.e., Docker containerizer will be used to launch the container), we do not validate if the `DockerInfo` exists in the ContainerInfo, so such task will be sent from master to agent, and will eventually fail due to pulling image with empty name.    Actually we have a validation in [this code|https://github.com/apache/mesos/blob/1.5.0/src/docker/docker.cpp#L605:L607], but it is too late (i.e., when Docker executor tries to create the Docker container), we should do the validation much earlier, e.g., in master."""
"MESOS-8647","Bug","agent|test",2,"Enable resource provider agent capability by default","""In 1.5.0 we introduced a resource provider agent capability which e.g., enables a modified operation protocol. We should enable this capability by default.         If tests explicitly depend on the agent being fully operational, they should be adjusted for the modified protocol. It is e.g., not enough to wait for a {{dispatch}} to the agent's recovery method, but instead one should wait for a dedicated {{UpdateSlaveMessage}} from the agent."""
"MESOS-8657","Task","storage",3,"Build CSI proto in CMake.","""We should be able to build CSI proto with CMake."""
"MESOS-8653","Task","storage",2,"Make the CSI client to support CSI v0.2.","""CSI v0.2 is incompatible with v0.1, thus we need to modify the CSI client to support the new CSI API. We may consider supporting both v0.1 and v0.2 in Mesos 1.6, or just deprecating v0.1."""
"MESOS-8650","Task","storage",2,"Bump CSI bundle to v0.2.","""Upgrade CSI spec bundle in {{3rdparty/}}."""
"MESOS-8715","Improvement","storage",2,"Consider removing conditional inclusion in the public header `csi/spec.hpp`.","""Currently we conditionally include {{csi.grpc.pb.h}} in {{csi/spec.hpp}} based on the configuration config {{ENABLE_GRPC}}, which is not ideal since this makes the public header depends on an some-what internal configuration flag. We could consider one of the following approaches to remove such dependency:    1. Generate a blank {{csi.grpc.pb.h}} when gRPC is not enabled.  2. Split {{csi/spec.hpp}} into {{csi/messages.hpp}} and {{csi/services.hpp}}, and do the conditional inclusion of {{csi/services.hpp}} in the implementation files.  3. Only include {{csi.pb.h}} in {{csi/spec.hpp}} since Mesos is only publicly dependent on the proto messages. Have a {{src/csi/services.hpp}} to include {{csi.grpc.pb.h}}.  4. Remove this wrapper header file and directly include {{csi.pb.h}} and {{csi.grpc.pb.h}}."""
"MESOS-8711","Bug","test",1,"SlaveTest.ChangeDomain is disabled.","""This test has been disabled in https://github.com/apache/mesos/commit/c0468b240842d4aaf04249cb0a58c59c43d1850d. We should either fix or remove it."""
"MESOS-8702","Improvement","containerization",2,"Replace the manual parsing in Mesos code with the native protobuf map support","""In MESOS-7656, we have updated the JSON <=> protobuf message conversion in stout for map support which means we can use the native protobuf map feature now in Mesos code. So we should replace the manual parsing for the following fields with the native protobuf map.    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/docker/v1.proto#L65:L68]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L33:L36]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L61:L64]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L88:L91]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L107:L110]    [https://github.com/apache/mesos/blob/1.5.0/include/mesos/oci/spec.proto#L151:L154]    Please note, for [Appc image manifest|https://github.com/apache/mesos/blob/1.5.0/include/mesos/appc/spec.proto#L43], we also have a field {{repeated Label labels = 4}}, but we should not replace it with the native protobuf map support, because according to the [Appc image spec|https://github.com/appc/spec/blob/master/spec/aci.md#image-manifest-schema], this field is not a map, instead it is a list of objects.    And in {{mesos.proto}}, we also have a couple protobuf messages which have field like {{optional Labels labels = 10}}, .e.g. {{TaskInfo.labels}}, I would not suggest to replace them with native protobuf map since that would be an API changes which may break framework's code."""
"MESOS-8719","Bug","storage",2,"Mesos configured with `--enable-grpc` doesn't compile on non-Linux builds","""Commit {{59cca968e04dee069e0df2663733b6d6f55af0da}} added {{examples/test_csi_plugin.cpp}} to non-Linux builds that are configured using the {{--enable-grpc}} flag. As {{examples/test_csi_plugin.cpp}} includes {{fs/linux.hpp}}, it can only compile on Linux and needs to be disabled for non-Linux builds."""
"MESOS-8717","Task","storage",5,"Support CSI v0.2 in SLRP.","""SLRP needs to be modified to talk to plugins using CSI v0.2."""
"MESOS-8728","Improvement","agent|master",2,"Don't print full usage for invocation errors","""The current usage string for mesos-master comes in at 399 lines, and for mesos-agent at 685 lines.         Printing such a wall of text will overflow most terminal windows, making it necessary to scroll up to see the actual error when invoking mesos with an incorrect command line."""
"MESOS-8741","Bug","libprocess",1,"`Add` to sequence will not run if it races with sequence destruction","""Adding item to sequence is realized by dispatching `add()` to the sequence actor. However, this could race with the sequence actor destruction.:    After the dispatch but before the dispatched `add()` message gets processed by the sequence actor, if the sequence gets destroyed, a terminate message will be injected to the *head* of the message queue. This would result in the destruction of the sequence without the `add()` call ever gets processed. User would end up with a pending future and the future's `onDiscarded' would not be triggered during the sequence destruction.    The solution is to set the `inject` flag to `false` so that the terminating message is enqueued to the end of the sequence actor message queue. All `add()` messages that happen before the destruction will be processed before the terminating message."""
"MESOS-8735","Task","agent|master|storage",5,"Implement recovery for resource provider manager registrar","""In order to properly persist and recover resource provider information in the resource provider manager we should   # Include a registrar in the manager, and   # Implement missing recovery functionality in the registrar so it can return a recovered registry."""
"MESOS-8733","Bug","test",1,"OversubscriptionTest.ForwardUpdateSlaveMessage is flaky","""Observed this failure in CI,  """
"MESOS-8732","Task","containerization",8,"Use composing containerizer in some agent tests.","""If we assign """"docker,mesos"""" to the `containerizers` flag for an agent, then `ComposingContainerizer` will be used for many tests that do not specify `containerizers` flag. That's the goal of this task.    I tried to do that by adding [`flags.containerizers = """"docker,mesos"""";`|https://github.com/apache/mesos/blob/master/src/tests/mesos.cpp#L273], but it turned out that some tests are started to hang due to a paused clocks, while docker c'zer and docker library use libprocess clocks."""
"MESOS-8748","Task","security",3,"Create ACL for grow and shrink volume","""As follow up work of MESOS-4965, we should make sure new operations are properly protected in ACL and authorizer."""
"MESOS-8769","Bug","containerization",1,"Agent crashes when CNI config not defined","""I was deploying an application through marathon in an integration test that looked like this:   * Mesos container (UCR)   * container network   * some network name specified    Given network name did not exist, I did not even passed CNI config to the agent.    After Mesos tried to deploy my task, the agent crashed because of missing CNI config.    """
"MESOS-8774","Task","agent",8,"Authenticate and authorize calls to the resource provider manager's API","""The resource provider manager is exposed via an agent endpoint against which resource providers subscribe or perform other actions. We should authenticate and authorize any interactions there.    Since currently local resource providers run on agents who manages their lifetime it seems natural to extend the framework used for executor authentication to resource providers as well. The agent would then generate a secret token whenever a new resource provider is started and inject it into the resource providers it launches. Resource providers in turn would use this token when interacting with the manager API."""
"MESOS-8777","Task","storage",5,"Support `STAGE_UNSTAGE_VOLUME` CSI capability in SLRP","""CSI v0.2 introduces a new `STAGE_UNSTAGE_VOLUME` node capability. If a plugin has this capability, SLRP needs to call `NodeStageVolume` before publishing a volume, and call `NodeUnstageVolume` after unpublishing a volume."""
"MESOS-8786","Bug","containerization",5,"CgroupIsolatorProcess accesses subsystem processes directly.","""The {{CgroupsIsolatorProcess}} interacts with the different cgroups subsystems via {{Processes}} dealing with a dedicated subsystem each. Each {{Process}} is held by {{CgroupsIsolatorProcess}} directly and e.g., no intermediate wrapper class is involved performing {{dispatch}} to an underlying process.    Since no wrapper around these {{Subsystem}} processes is used, a user needs to make sure to only {{dispatch}} to the process himself, he should e.g., never directly invoke functions on the {{Process}} or else inconsistencies or races can arise inside the {{Subsystem}} process; if e.g., a {{Subsystem}} dispatches to itself, {{CgroupsIsolatorProcess}} might concurrently invoke {{Subsystem}} functions.      {{CgroupsIsolatorProcess}} does not always {{dispatch}} to these process, but invokes them directly. We should fix this by either introducing wrappers around the {{Subsystem}} wrappers, or by explicitly fixing {{CgroupsIsolatorProcess}} to always use {{dispatch}} to interact with its subsystems. While the first approach seems cleaner and more future-proof, the latter might be less effort _now_."""
"MESOS-8784","Bug","master",3,"OPERATION_DROPPED operation status updates should include the operation/framework IDs","""The agent should include the operation/framework IDs in operation status updates sent in response to a reconciliation request from the master. These status updates have the operation status: {{OPERATION_DROPPED}}."""
"MESOS-8783","Bug","master",3,"Transition pending operations to OPERATION_UNREACHABLE when an agent is removed.","""Pending operations on an agent should be transitioned to `OPERATION_UNREACHABLE` when an agent is marked unreachable. We should also make sure that we pro-actively send operation status updates for these operations when the agent becomes unreachable.    We should also make sure that we send new operation updates if/when the agent reconnects - perhaps this is already accomplished with the existing operation update logic in the agent?"""
"MESOS-8782","Bug","master",3,"Transition operations to OPERATION_GONE_BY_OPERATOR when marking an agent gone.","""The master should transition operations to the state {{OPERATION_GONE_BY_OPERATOR}} when an agent is marked gone, sending an operation status update to the frameworks that created them.    We should also remove them from {{Master::frameworks}}."""
"MESOS-8781","Bug","master",3,"Mesos master shouldn't silently drop operations","""We should make sure that all call places of {{void Master::drop(Framework*, const Offer::Operation&, const string&)}} send a status update if an operation ID was specified. OR we should make sure that they do NOT send one, and make that method send one."""
"MESOS-8789","Improvement","agent|HTTP API|master",3,"/roles and webui roles table should display distinct offered and allocated resources.","""The role endpoints currently show accumulated values for resources (allocated), containing offered resources. For gaining an overview showing our allocated resources separately from the offered resources could improve the signal quality, depending on the use case.    This also affects the UI display, for example the """"Roles"""" tab.  """
"MESOS-8799","Task","HTTP API|master",2,"Master should show dynamic resources in state endpoint","""The master currently only shows static agent resources, i.e., resources defined in the agent's {{SlaveInfo}} in its state endpoint. We should fix this code to show the dynamic resources so that at least resource provider resources are shown. We might need to filter out oversubscribed resources for backward-compatibility."""
"MESOS-8794","Task","containerization",13,"Support docker image tarball hdfs based fetching.","""Support docker image tarball hdfs based fetching."""
"MESOS-8813","Task","containerization",8,"Support multiple tasks with different users can access a persistent volume.","""See [design doc|https://docs.google.com/document/d/1QyeDDX4Zr9E-0jKMoPTzsGE-v4KWwjmnCR0l8V4Tq2U/edit#heading=h.f4x59l41lxwx] for why we need to do this."""
"MESOS-8810","Task","containerization",13,"Grant non-root task user the permissions to access the SANDBOX_PATH volume of PARENT type","""See [design doc|https://docs.google.com/document/d/1QyeDDX4Zr9E-0jKMoPTzsGE-v4KWwjmnCR0l8V4Tq2U/edit#heading=h.s6f8rmu65g2p] for why we need to do this."""
"MESOS-8809","Task","stout",3,"Add functions for manipulating POSIX ACLs into stout","""We need to add functions for setting/getting POSIX ACLs into stout so that we can leverage these functions to grant volume permissions to the specific task user.    This will introduce a new dependency {{libacl-devel}} when building Mesos."""
"MESOS-8819","Task","java api",1,"mesos.pom file hardcodes developers","""Currently {{src/java/mesos.pom.in}} hardcodes developers. The information there duplicates {{docs/comitters.md}} and is currently likely outdated and will get out of sync again in the future.    It seems we should either automatically populate this field during the release process or drop this field without replacement. We already point to the dev mailing list which can be used to reach Mesos developers."""
"MESOS-8818","Bug","containerization",2,"VolumeSandboxPathIsolatorTest.SharedParentTypeVolume fails on macOS","""This test fails on macOS with:      Likely a regression introduced in commit {{189efed864ca2455674b0790d6be4a73c820afd6}} which removed {{volume/sandbox_path}} for POSIX."""
"MESOS-8839","Bug","agent|storage|test",3,"Resource provider manager registrar recovery can race with agent on agent state leading to hard failures","""When running in the agent the resource provider manager persists its state into the agent's state. The agent uses a LevelDB state which protects against concurrent access. The way we modelled LevelDB an {{fetch}} when a lock is present leads to a failed {{Future}} result. When the resource provider manager encounters a failed recovery it emits a fatal error, e.g.,    We should not fail hard for such recoverable failure scenarios."""
