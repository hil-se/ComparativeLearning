"issuekey","type","components","storypoint","title","description_text"
"SERVER-16612","Task","Storage",5,"Implicitly zeroed files in WiredTiger","""There is a problem with the implicit zeroing of files by the kernel on certain platforms - see SERVER-15369 for more details. A workaround was put into place for this issue for mmapv1 files to explicitly zero .ns files.    The purpose of this ticket is to determine:  * in what areas will WiredTiger have similar vulnerabilities to this issue?  * what will be the customer impact of this issue to WiredTiger?  * beyond advising customers to avoid using WiredTiger on platforms with the issue, can we reasonably work around the problem by explicitly zeroing all files rather than relying on the kernel's implicit zeroing?"""
"SERVER-17014","Bug","Indexing",0,"foreground index build blocks database reads and writes","""Some discussion is at https://groups.google.com/forum/#!topic/mongodb-dev/_1IrogzovEQ. When I create an index with background:false then many (all?) operations in the db are blocked even for engines like WiredTiger that don't require a per-db writer lock. The URL above shows thread stacks where background jobs (TTLMonitor, ClientCursorMonitor) get blocked on a per-db lock by a background:false index create. I assume bad things can happen when TTL enforcement doesn't run for too long.  This creates other problems as """"show collections"""", db.$foo.getIndexes() and queries from other collections in the same database will be blocked for the duration of the index create.   While background:true is the workaround background index creation can take more time."""
"SERVER-18840","Improvement","Testing Infrastructure",1,"resmoke should indicate status of test in abbreviated log output during run, before logging everything at the end","""When resmoke is running a batch of tests and logging to buildlogger, after each test finishes it prints a line like:        It would be helpful to indicate here if the test passed/failed, so if a suite is running in evergreen but hasn't finished yet, i can eyeball if the suite is going to fail by looking at the logs.      """
"SERVER-19895","Improvement","Testing Infrastructure",2,"resmoke failures should self-document","""When resmoke fails, it should print out steps to help the user debug the failure. E.g. when resmoke detects that it's run in Evergreen, it should print out the places that the user should look for symptoms.    Original description:  At shutdown time, a fixture (Replication or Sharding) checks the return status of all the process shutdown procedures.  If any of them have returned 'false' (which means that the process returned a non-0 exit status), it fails the test suite associated with that fixture.  It would be helpful if the fixture wrote a message to the log stating which process caused the suite failure.  Currently, the only way to diagnose this is to scour the logs looking for the exit status of each process; since we are looking for a line that is *not* """"exited with code 0."""", this is not a simple search to undertake."""
"SERVER-20056","Improvement","WiredTiger",3,"Log a startup warning if wiredTigerCacheSizeGB is > 80% of RAM","""Currently you can set wiredTigerCacheSizeGB to be 100% of available memory, which will almost certainly lead to problems.    Perhaps a startup check here to confirm that wiredTigerCacheSizeGB is < 80% of available Memory.  """
"SERVER-20960","New Feature","Indexing",0,"Default index build option support in config/runtime","""Add support for index build preferences in the config file and/or mongod runtime parameters.    Specifically to control preferences for background or foreground index build options.    *EDIT*: in SERVER-24041 the following request was added:  {quote}  It would be great if I can block foreground index creation by configuration.  {quote}"""
"SERVER-21861","Improvement","Shell",3,"Better Timestamp object comparison in mongo shell","""It would be great if the <, >, <=, >= operators on Timestamp objects would work as expected. The bsonWoCompare can be used for the mean time even though it will end up comparing the member functions as well as long as 't' fields are compared first over the 'i' fields."""
"SERVER-24918","Task","Performance",2,"perf.yml is not using Evergreen modules","""The control file for the performance project in Evergreen 'perf.yml' is not using the Evergreen module support. We should move to use modules to make testing changes in mongo-perf and dsi easier.    Module for DSI and for mongo-perf repos.Â """
"SERVER-25548","Improvement","Testing Infrastructure",2,"resmoke should not continue after a shell crash","""Currently, resmoke stops the test if a mongo server has crashed in {{job.py}}. We should have it do the same for a shell crash.    For suites that use the shell to spawn servers, a shell crash would cause the servers to not be terminated. Subsequent tests will then run on servers from previous tests, causing either test failures or hangs.    Having these additional failures defeat the purpose of running with continueOnFailure."""
"SERVER-26319","Improvement","Testing Infrastructure",1,"Deleting a test results in ""ValueError: Unrecognized jstest""","""When a file is included or [excluded explicitly by name|https://github.com/mongodb/mongo/blob/r3.4.0-rc1/buildscripts/resmokelib/selector.py#L237] (i.e. not by glob pattern or tag), then an error is raised if the test isn't found. This is done to ensure that the blacklist for a suite is updated when a test is renamed or deleted. We should consider improving [the """"ValueError: Unrecognized jstest"""" error message|https://github.com/mongodb/mongo/blob/r3.4.0-rc1/buildscripts/resmokelib/selector.py#L295] to be clearer about potentially needing to update an entry in the blacklist.    h6. Original description    In master, deleting the test loglong.js causes a failure:  https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_sharded_collections_jscore_passthrough_c172cc49e735b6e48f58662e5588961216d3cff0_16_09_25_23_17_00    In this patch, I moved loglong.js to a new location and it causes the compile to fail:  https://evergreen.mongodb.com/version/57e577943ff12239ba00ab4f"""
"SERVER-26625","Improvement","Indexing",2,"Make collStats command consistent on background index reporting","""The collStats command includes indexes that are undergoing background index build in the 'indexSizes' section. It does not include them in 'nindexes' or 'indexDetails'. We should make reporting consistent, and make clear which indexes are undergoing background build (if any)."""
"SERVER-26867","Improvement","Testing Infrastructure",2,"Timeout-related assert.soon failures should trigger the hang analyzer","""When an test fails due to a timeout, it would be nice if that would trigger the hang analyzer before tearing down the processes. This would let us see what the server is doing that is preventing it from doing what we expect.    -Timeout-related failures include:-   * -assert.soon()-   * -wtimeout expiring-   * -$maxTimeMs expiring-    Update: Per discussion below we will be limiting this to {{assert.soon}}, {{assert.soonNoExcept}}, {{assert.retry}}, and {{assert.assert.retryNoExcept}}."""
"SERVER-26953","Improvement","Diagnostics|WiredTiger",0,"Track bytes read into cache per query","""When slow queries are logged we include information about documents and index entries scanned to help diagnose performance issues. However that information doesn't tell us whether the documents or index entries scanned were in cache or not, and if they are not in cache they can have a much more significant performance impact. Reporting bytes read into cache for each query logged would help diagnose the cause of performance issues related to cache pressure.    This might be accomplished by tracking bytes read into cache per cursor or per session in WT, and computing the difference between the value of this counter before and after each query. Query performance impact would need to be evaluated."""
"SERVER-26988","Bug","WiredTiger",5,"Secondary delay causes large drop in insert rate on the primary due to cache full condition","""With a heavy insert load and a secondary that is delayed the cache on the primary fills to 100% and operation rates drop. Here's a run showing behavior on the primary with the secondary delayed due to lag, but a similar effect is seen if the secondary is intentionally delayed using slaveDelay.    !lagging.png|width=100%!    * from D-E the cache is 95% full and insert rate drops considerably, possibly due to application threads doing evictions?  * F-G and H-I seem to be seem to be related to checkpoints, possibly also in combination with the full cache?  * the rate of pages walked for eviction is generally very high, about 6k times the rate of pages actually evicted, suggesting that the issue is difficulty finding pages to evict to keep the cache at target levels    The high rate of pages walked for eviction suggests a connection to SERVER-22831, which also showed that symptom in connection with a full cache; however the above run was on 3.2.5-rc1 where SERVER-22831 was fixed, so it seems there is a different issue here.    The above test involved  * 3.2.5-rc1  * 2-node replica set  * 25 GB cache  * 100 GB oplog  * 5 threads inserting 800 byte documents into 5 separate collections      """
"SERVER-28940","Improvement","Testing Infrastructure",5,"Make resmoke fixture setup/teardown their own testcases.","""The fixture setup and teardown logs only go to logkeeper, but don't show up on the Evergreen sidebar. So the assertions can't be extracted without figuring out the logkeeper URL from the task log.    Since everything else that logs to logkeeper (i.e. test and hooks) is a testcase that shows up on the sidebar, the fixture events should get their own spots as well."""
"SERVER-29999","Improvement","Testing Infrastructure",5,"Implement FSM workload scheduler for concurrency_simultaneous task","""We'll want to port [the {{scheduleWorkloads()}} function|https://github.com/mongodb/mongo/blob/r3.7.7/jstests/concurrency/fsm_libs/runner.js#L148-L206] from JavaScript to Python so that resmoke.py is in control over the groups of FSM workloads that are run together. The behavior around the number of subsets an individual FSM workload can be a part of should be identical to what it is for the {{concurrency_simultaneous.yml}} test suite today.    Additionally, specifying {{\-\-suites=concurrency_simultaneous jstests/concurrency/fsm_workloads/workloadA.js jstests/concurrency/fsm_workloads/workloadB.js}} should run those two FSM workloads together rather than in sequence. That is to say, if a list of files is omitted then {{numSubsets}} groups of FSM workloads should be run and if a list of files is present then exactly 1 group of FSM workloads should be run. The latter aims to serve an engineer who wishes to reproduce a particular failure by running the same group of FSM workloads together."""
"SERVER-30204","Task","Testing Infrastructure",3,"Create resmoke.py hook that drops all databases without restarting the cluster","""-Add support for including background/perpetual workloads.- Background/perpetual workloads should be done as background threads in resmoke.py and don't need any special handling other than ensuring adequate documentation in our internal wiki.    Also, add a new hook to drop all DBs and collections after every FSM test. When running in """"the same"""" DB Or """"same collection"""" FSM modes, pass the DB or collection that are not dropped to the new cleanup hook. CleanupOption of not dropping certain DBs will be taken into account as needed.    The new hook will be used in place of CleanEveryN to avoid the overhead of spinning up a large cluster multiple times."""
"SERVER-31535","Task","Build|Packaging",3,"Platform Support: remove Ubuntu 12.04 builds","""Ubuntu 12.04 is EOL, so we should drop support for it in 3.6."""
"SERVER-31570","Improvement","Replication|Testing Infrastructure",3,"Adjust mongobridge port allocations for easier debugging","""Mongobridge assigns ports sequentially so you have to use modular arithmetic to figure out which bridges are associated with which mongods. If we assigned (bridge port) = (mongod port) + 10000, it would be very easy to map them to each other. """
"SERVER-32223","Improvement","Testing Infrastructure",2,"Add burn-in tests for configurations in other variants in addition to other suites","""Many tests accidentally forget to mark """"requires_persistence"""" or other things that are only tested on non-required builders. Similarly, mmap is now run on its own builder. Incorporating some of these """"variant"""" flags into burn-in tests on the required builders could avoid easy test failures."""
"SERVER-33000","Task","Build",8,"Platform Support: add Ubuntu 18.04","""This can go to the backlog for now, but Ubuntu 18.04 will be available soon, so we should be ready to add this platform as soon as there are images out there."""
"SERVER-32443","Task","Replication",1,"Create a sys-perf task for running linkbench automatically against a replica set","""Once the MongoDB linkbench implementation is relatively stable and is utilizing the new transactions API, we should work on integrating an automated benchmark into sys-perf. It would probably be good to run this benchmark against both a 1-node and 3-node replica set, at least."""
"SERVER-32437","New Feature","Packaging",8,"Platform Support: add Amazon Linux 2","""On December 13th, Amazon released Amazon Linux 2. The installation of our current package for AMZL fails:        I guess the missing dependencies could be manually added but still, Amazon Linux 2 is not officially supported at the moment"""
"SERVER-32642","Improvement","Testing Infrastructure",2,"Return raw command response in the validate JS hook","""Modify {{CollectionValidator.validateCollections}} to return the raw command response to make the class more flexible."""
"SERVER-32825","Improvement","Storage",3,"Add the infrastructure for upgrade/downgrade of V2Unique indexes","""Addition of new index format allows use of both V2 and V2Unique format. Initially we would add a gating variable to select the unique index format. Switch to FCV as the deciding factor for which format should be used.     * Setting FCV=4.2 will cause all unique indexes to be updated to V2Unique. Do not update the content of the indexes.  * Having FCV=4.0 would create older V2 format indexes"""
"SERVER-32883","Task","Replication|Testing Infrastructure",8,"Enhanced FSM testing for reading from secondaries","""1. Change the {{secondary_reads_passthrough.yml}} test suite which was added as part of SERVER-34384 to use the """"forceSyncSourceCandidate"""" failpoint as a server parameter to force secondary #2 to sync from secondary #1.    2. Add a new version of the {{concurrency_replication.yml}} test suite that uses a 5-node replica set with each secondary syncing in succession of each other (i.e. a linear chain), writeConcern=\{w: 1\}, readConcern=\{level: """"local"""", afterClusterTime: ...\}, and readPreference=\{mode: """"secondary""""\}. We'll also likely want to make a wrapper around a {{Mongo}} connection object to the primary and to a specific secondary so that an individual worker thread talks to a particular secondary all the time rather than some secondaries potentially never being read from.    {quote}  I think there's some additional complexity here because we want FSM worker thread to do reads from different secondary. (We'll probably pin it to a particular secondary similar to how we """"round-robin"""" when using multiple mongos processes.) It seems like we'll want to have a Mongo connection object implemented in JavaScript that for commands which are present in [this list|https://github.com/mongodb/mongo/blob/6841ce738419923002958acc760e150769b6f615/jstests/libs/override_methods/set_read_preference_secondary.js#L10-L23] are routed via a direct connection to the secondary and commands not present in that list are routed via a direct connection to the primary. I think the existing """"connection cache"""" in the concurrency framework makes it relatively straightforward to have direct connections to other nodes in the cluster.  {quote}    In creating this wrapper around two separate {{Mongo}} connection objects, we may also want to change how SERVER-34383 was implemented to construct a wrapper around a secondary's connection from the connection cache instead of creating a replica set connection for the worker thread.    h6. Original description    As part of SERVER-32606 it turned out that our testing of tailing the oplog on secondaries, including the case of chained replication, is light, while the code paths for secondary reads have gotten quite different now from reads on primaries.    We should have a passthrough test where we test these behaviors. This is related to SERVER-32606, but was too big a task to do as part of that ticket."""
"SERVER-33002","Task","Build",8,"Platform Support: add MacOS 10.13 (High Sierra)","""1. Add image and distro to evergreen  2. Run test build  3. Communicate availability to Storage  4. Once functional, we want to put this in rotation.   5. Review performance and open subsequent investigation ticket if makespan is substantially different"""
"SERVER-32999","Task","Build",3,"Platform Support: remove Debian 7","""[Debian 7 is going EOL soon|https://wiki.debian.org/LTS]. Opening a ticket to deprecate and ultimately remove this platform."""
"SERVER-32997","Bug","Storage",13,"Mobile SE: Design and implement multi-reader or single-writer concurrency","""SERVER-32675 resolved Mobile SE's some of the major issues with concurrency. I still see a few tests hitting either write conflicts or DB locked. These tests need to be investigated and a fix made accordingly. This ticket will track that effort."""
"SERVER-33149","Bug","Indexing",3,"createIndexes fails to report an error when index is not created with the specified name","""The createIndexes command fails to report an error when an index is not created with the specified name because an index already exists with the same keys but with a *different* name."""
"SERVER-33146","Bug","Packaging",3,"mongod.service does not source system-wide environment variables as stated in the documentation","""Following the [Kerberos tutorial|https://docs.mongodb.com/manual/tutorial/control-access-to-mongodb-with-kerberos-authentication/#krb5-ktname] I am unable to configure the keytab environment variable for the rpm installed mongod.    The [service file|https://github.com/mongodb/mongo/blob/master/rpm/mongod.service] does not source /etc/sysconfig/mongod    As a workaround I modified the service file to include        """
"SERVER-33342","Task","Build",3,"Turn on shared scons cache for shipping builders.","""enterprise-linux-64-amazon-ami  ubuntu1204  ubuntu1404  ubuntu1604  amazon  rhel62  rhel70  enterprise-ubuntu1204-64  enterprise-ubuntu1404-64  enterprise-ubuntu1604-64  enterprise-suse12-64  suse12  enterprise-suse11-64  suse11  enterprise-debian71-64  enterprise-debian81-64  debian71  debian81"""
"SERVER-33340","Task","Build",8,"Turn on shared cache for non-shipping (non-push) builders","""We originally planned to test for two weeks - but we may be able to shorten this since we began testing with: SERVER-33278    linux-64-repeated-execution  linux-64-duroff  linux-64-lsm  enterprise-rhel-62-64-bit-inmem  linux-64-ephemeralForTest  ubuntu1404-rockdb  ubuntu1604-debug-asan  ubuntu1604-asan  enterprise-rhel-62-64-bit-coverage"""
"SERVER-33427","Improvement","Testing Infrastructure",2,"improve detectability of test failing because ShardingTest/ReplSetTest not shut down","""{quote}  > Max Hirschhorn Kevin Albertson, I noticed that failing to shut down a ShardingTest/ReplSetTest doesn't cause the test to log a """"failed to load"""" line or a javascript stack trace (which makes sense, since which line would you error on?).    As an outcome of SERVER-25777, the mongo shell could already exit with a non-zero return code without printing a """"failed to load"""" message.    > The line that _is_ logged (""""a call to MongoRunner.stopMongod(), ReplSetTest#stopSet(), or ShardingTest#stop() may be missing from the test"""") also isn't/can't be logged at LogSeverity::Error, since it's not logged by a server process (and which makes the log line contain """" E """", which is another thing I typically look for when a test fails without """"failed to load"""").  >   > It took some confusion and additional scrolling through the logs for me to realize why my new test was reporting failure when it seemed like the test ran to completion successfully. Just a thought, in case there's something that can be done to make this failure easier to detect.    Esha Maharishi, I think your confusion is understandable. The goal of the message was to make it more obvious to the user what the remediation ought to be. Since that message isn't being surfaced clearly enough, we should change the logic in the mongo shell so that it is.    I don't see a reason that the mongo shell must use {{cout}} for logging the """"exiting with a failure due to unterminated processes"""" message, so we could replace it with a call to {{severe()}} instead (and prefix the log message with 'F'). Do you think that would be sufficient for your purposes? Would you mind filing a new SERVER ticket for this improvement request?    > For example, even just moving the """"a call to MongoRunner.stopMongod(), ReplSetTest#stopSet(), or ShardingTest#stop() may be missing from the test"""" just before/after the """"Summary: 1 test(s) ran in 35.86 seconds (0 succeeded, 0 were skipped, 1 failed, 0 errored)"""" could help.    Those messages are logged by two different processes (the mongo shell with the former and resmoke.py with the latter) so that isn't really something we'd consider. A related feature in resmoke.py would be to have special handling around certain exit codes from known processes. This case in the mongo shell would be one, but a memory leak detected by ASan/LSan would be another.  {quote}    See comment thread on SERVER-25640; one good idea from that thread is to make the mongo shell log an error message at a more severe log level."""
"SERVER-33470","Task","Testing Infrastructure",1,"Log archival message, even if successful, in hook_test_archival.py","""The archival message is only logged if there is an [error submitting files for archive|https://github.com/mongodb/mongo/blob/e3f361769cd13ba88aa24c1c0a71c76b187f64dd/buildscripts/resmokelib/testing/hook_test_archival.py#L115-L116].    We should have a logger.info message even on success, as there could be files that were skipped during the tar process."""
"SERVER-33641","Improvement","Replication|Testing Infrastructure",3,"Call checkOplogs when checkReplicatedDataHashes fails","""We should do the following to improve the relevance of diagnostics we have in the face of data inconsistency issues:    # Update {{ReplSetTest#stopSet()}} to call {{ReplSetTest#checkOplogs()}} [in addition to {{ReplSetTest#checkReplicatedDataHashes()}}|https://github.com/mongodb/mongo/blob/r3.7.7/src/mongo/shell/replsettest.js#L2174-L2189]. Care should be taken to ensure that tests do not run significantly longer because they need to verify a large oplog when shutting down the replica set.  # Update the {{PeriodicKillSecondaries}} hook to run the {{CheckReplOplogs}} hook [in addition to the {{CheckReplDBHash}} and {{ValidateCollections}} hooks|https://github.com/mongodb/mongo/blob/r3.7.7/buildscripts/resmokelib/testing/hooks/periodic_kill_secondaries.py#L137-L147].    h6. Original description    We now save all of the data files, but it would be great if the test could check the oplogs automatically and note any differences."""
"SERVER-33848","Task","Performance",2,"Update compile flags for sys-perf and performance projects","""MongoDB Community Server has had SSL support since 2.6 or 3.0, yet we weren't compiling with SSL support in sys-perf tests. Julian will fix that as part of his work.    We should review our compile code to make sure it reflects what is actually shipped to users. (Of course, we may have debug symbols and other differences, if they are intentional.)"""
"SERVER-33651","Improvement","Storage",1,"Mobile SE: Use full synchronous mode for SQLite writes","""SQLite allows some startup configuration options. The defaults should work for most of our use cases, but might still need some fine tuning. This ticket is to study the available options, and come up with any non default that might better suit us.    Also, at the conclusion of the ticket, update the design doc to specify these setting we come up with."""
"SERVER-33695","Improvement","Testing Infrastructure",2,"Include the loop name in the before and after recovery files in powertest.py","""Powercycle rsyncs the data files before and after a recovery runs (mongod started after power cycle event). We should name the resulting directory to also include the loop number, i.e., {{beforerecovery-1}}. We can still use rsync, and then rename the directory."""
"SERVER-33740","Task","Storage|Testing Infrastructure",2,"Add Evergreen task for running powercycle against mobile storage engine","""We should create a {{powercycle_mobile}} Evergreen task that performs powercycle testing while running against the mobile storage engine. It should be as straightforward as copy [the definition for the {{powercycle}} task|https://github.com/mongodb/mongo/blob/789f74a3837c0daf799be2b8296f339977c551b8/etc/evergreen.yml#L3862-L3879] and specifying {{\-\-storageEngine=mobile}} in the {{mongod_extra_options}} parameter to the """"run powercycle test"""" function, although some care would need to be taken to disable the FSM clients if we do this ticket before resolving SERVER-32993.    {code:yaml}  - name: powercycle_mobile    exec_timeout_secs: 7200 # 2 hour timeout for the task overall    depends_on:    - name: compile    commands:    - func: """"do setup""""    - func: """"set up remote credentials""""      vars:        <<: *powercycle_remote_credentials    - func: """"set up EC2 instance""""      vars:        <<: *powercycle_ec2_instance    - command: expansions.update      <<: *powercycle_expansions    - func: """"run powercycle test""""      vars:        <<: *powercycle_test        mongod_extra_options: --mongodOptions=\""""--setParameter enableTestCommands=1 --storageEngine mobile\""""  {code}"""
"SERVER-33787","Task","Build|Packaging",2,"Platform Support: remove Debian 7 builds","""It will be EOLed in May."""
"SERVER-33817","Task","Testing Infrastructure",3,"Powercycle test using kill mongod","""Create a new powercycle task which has a {{crashOption}} to kill the monogd instead of crashing the remote host."""
"SERVER-33853","Improvement","Testing Infrastructure",1,"Define a new test tag to temporarily disable a test","""When engineers need to temporarily disable a JavaScript test, they have to update the YAML file for each suite the test runs under and explicitly blacklist it.    We should define a new tag (e.g. """"temporarily_disabled"""") that can be added to a test to quickly prevent it from running in all suites.    The new tag exclusion could be specified in all the suites configuration files (and the tag would be no different than any other tag) or could be implemented in resmoke.    """
"SERVER-33936","Bug","Packaging",3,"3.6 nightly builds not available for download","""At https://www.mongodb.com/download-center#development there's a menu item for 3.6 nightly but the download link doesn't work. Also the [all binaries page|https://www.mongodb.org/dl/osx?_ga=2.203632720.1133726881.1521122429-282575863.1477067354&_gac=1.144900480.1520974309.EAIaIQobChMI_47T6Zbq2QIVRx6GCh2T0QmgEAAYASAAEgJkJPD_BwE] has links for 3.2-latest and 3.4-latest builds but no 3.6-latest."""
"SERVER-33926","Bug","Packaging",13,"Unattended installation fails when deselecting Compass","""h3. Summary  * Unattended installation documentation does not explicitly mention that Compass will be installed when using ADDLOCAL=""""all"""".  * Unattended installation does not appear to work unless Compass is selected.    h3. Details     The Install on Windows page of the mongodb manual provides a section describing [unattended installations|https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/#unattended-installation].    When using ADDLOCAL=""""all"""", the installer downloads and installs Compass. I do not want Compass, so I uninstalled both Compass and mongo CE before re-installing using ADDLOCAL=""""Server,Client"""". This causes the installer to fail and roll back the entire installation.    """
"SERVER-33978","Improvement","Testing Infrastructure",1,"References to sudo in evergreen.yml should use ${set_sudo}","""The {{generate compile expansions}} and {{umount shared scons directory}} functions reference sudo:      It should use the following form:    """
"SERVER-34144","Improvement","Testing Infrastructure",1,"Powercycle output improvements","""Two requests. I apologize if these should be separate tickets:  # Please log exactly what database and collection is being queried against for the canary checks. Also include the exact query and the criteria being used to verify the document is in the correct state (I think canary documents are insert only so the verification is an existence check).  # Please rename the powercycle replset name to `powercycle`. It is currently misspelled as `powercyle`. Bringing up a node as a replica set member that can be queried requires an exact string match on the `--replSet` name. It's easy to read the replset name as `powercycle` to only later learn it's misspelled."""
"SERVER-34075","Task","Replication|Testing Infrastructure",3,"powercycle_replication* must run replication recovery to observe canary documents","""SERVER-29213 will break the powercycle_replication tests ability to query for the canary document after a crash. As such, that patch is temporarily disabling them.    Specifically, after SERVER-29213, bringing a node up in standalone may result in stale data relative to what the node has accepted. The node has not lost the data, but simply, replication recovery needs to be done for the data to be queryable.    The powercycle tests bring a node back up to check for the canary document in standalone mode and the node is brought up on a different port than is used when running as a replica set member.    We suspect SERVER-34070 will make it easier to make the required changes to re-enable the powercycle_replication* tests. What's problematic is that running replication recovery requires starting the node up with the {{\-\-replSet}} option. However, a node running with {{\-\-replSet}} on a different port than in the replset config will not come up as a PRIMARY nor SECONDARY and thus not service reads."""
"SERVER-34155","Task","Replication|Testing Infrastructure",2,"Add clean shutdowns to kill_secondaries and kill_primaries passthroughs","""Clean shutdowns leave the server in a different state then unclean shutdowns with respect to recover to a stable timestamp and are interesting by themselves. We do not have a lot of coverage around clean shutdowns and replication."""
"SERVER-34150","Task","Replication|Testing Infrastructure",5,"Create a passthrough that does clean shutdowns","""Recoverable rollback does work specifically to make fastcount correct across clean shutdown. A passthrough that does clean shutdowns on primaries and secondaries could catch some bugs here and around general data consistency."""
"SERVER-34198","Task","Build",3,"Update content-type for gzip files","""The content-type for the gzip files we make available for download is {{application/x-gzip}}. This causes problems with some browsers that don't understand this content type.    According to [RFC6648|https://tools.ietf.org/html/rfc6648] the {{x-}} types are deprecated, and [RFC6713|https://tools.ietf.org/html/rfc6713] says the right type for gzip files is {{application/gzip}}.    The work in this ticket has two parts:  * Update evergreen.yml to use the right content type for new files  * Update the existing files to have the right content type"""
"SERVER-34242","Task","Testing Infrastructure",5,"Enable causal consistency in concurrency_replication suite","""Either enable causal consistency in concurrency_replication, or create a separate suite with causal consistency enabled.     Causal consistency would allow us to run multi_statement_transaction_simple.js with varying writeConcerns, since we could use causal consistency to ensure that the worker threads read the writes performed during setup."""
"SERVER-34241","Task","Testing Infrastructure",1,"Remove the skipValidationNamespaces for config.transactions when WT-3998 is fixed","""Remove skipValidationNamespaces for config.transactions for the ValidateCollections hook in replica_sets_kill_primary_jscore_passthrough.yml"""
"SERVER-34258","Bug","Testing Infrastructure",5,"Error from mount_drives.sh on Windows","""The {{setfacl: No such file or directory}} error is observed when running mount_drives.sh on a windows remote instance:    """
"SERVER-34306","Improvement","Testing Infrastructure",1,"validate_collections.js hook should report node that failed validation","""When reporting a failure, it can be hard to trace back which node the validation failed on. E.g:  """
"SERVER-34298","Bug","Testing Infrastructure",1,"PeriodicKillSecondaries will still run after_suite following an after_test failure","""After the PeriodicKillSecondaries hook runs it resets its {{_start_time}} variable but when the underlying test fails, this step is bypassed causing the following {{after_suite}} that checks the variable to run."""
"SERVER-34371","Improvement","Testing Infrastructure",2,"Stop ignoring errors when the test fixture fails to delete data files","""The standalone test fixture [attempts to delete data files|https://github.com/mongodb/mongo/blob/73cf755e6e4cf5e0e3f43e0d98954c583ed00060/buildscripts/resmokelib/testing/fixtures/standalone.py#L53] before starting a node. We ignore errors when deleting data files, so we don't know if the deletion was successful. An example of when a deletion could fail is on Windows when another process is keeping the file open. When we fail to delete data files, tests can fail because they expect to start up with clean data files. We should add logging to understand when we fail to delete data files."""
"SERVER-34374","Bug","Testing Infrastructure",2,"resmoke.py uses bytestrings for representing pathnames, leading to silently failing to clear the dbpath on Windows","""https://bugs.python.org/issue24672 describes an issue in Python where {{shutil.rmtree()}} fails to delete files with non-ASCII pathnames when a bytestring (i.e. a {{str}} instance in Python 2). [The ntpath.py module in Python preserves type of its argument|https://github.com/python/cpython/blob/6a336f6484a13c01516b6bfc3b767075cc2cb4f7/Lib/ntpath.py#L398-L401] so it sufficient to use a {{unicode}} instance instead in order to have Python call the W-suffixed Win32 APIs that return Unicode strings.    I've verified on a Windows spawn host that the following patch to config.py addresses this issue. The change to parser.py is to just do the same if someone were to specify {{\-\-dbpathPrefix}} when trying to reproduce a failure outside of Evergreen.    {code:diff}  diff --git a/buildscripts/resmokelib/config.py b/buildscripts/resmokelib/config.py  index 66753c389d..2f13c2df96 100644  --- a/buildscripts/resmokelib/config.py  +++ b/buildscripts/resmokelib/config.py  @@ -34,7 +34,7 @@ DEFAULT_BENCHMARK_MIN_TIME = datetime.timedelta(seconds=5)     # Default root directory for where resmoke.py puts directories containing data files of mongod's it   # starts, as well as those started by individual tests.  -DEFAULT_DBPATH_PREFIX = os.path.normpath(""""/data/db"""")  +DEFAULT_DBPATH_PREFIX = os.path.normpath(u""""/data/db"""")     # Names below correspond to how they are specified via the command line or in the options YAML file.   DEFAULTS = {  diff --git a/buildscripts/resmokelib/parser.py b/buildscripts/resmokelib/parser.py  index d9f40da3e9..1353f899fd 100644  --- a/buildscripts/resmokelib/parser.py  +++ b/buildscripts/resmokelib/parser.py  @@ -352,7 +352,7 @@ def update_config_vars(values):  # pylint: disable=too-many-statements       _config.ARCHIVE_LIMIT_TESTS = config.pop(""""archive_limit_tests"""")       _config.BASE_PORT = int(config.pop(""""base_port""""))       _config.BUILDLOGGER_URL = config.pop(""""buildlogger_url"""")  -    _config.DBPATH_PREFIX = _expand_user(config.pop(""""dbpath_prefix""""))  +    _config.DBPATH_PREFIX = unicode(_expand_user(config.pop(""""dbpath_prefix"""")))       _config.DBTEST_EXECUTABLE = _expand_user(config.pop(""""dbtest_executable""""))       _config.DRY_RUN = config.pop(""""dry_run"""")       _config.EXCLUDE_WITH_ANY_TAGS = _tags_from_list(config.pop(""""exclude_with_any_tags""""))  {code}    However, I'm not sure if more special handling on Linux platforms is necessary as the changes from https://github.com/pypa/setuptools/commit/5ad13718686bee04a93b4e86929c1bb170f14a52 suggest we shouldn't use Unicode string literals if {{sys.getfilesystemencoding() == 'ascii'}}. We currently set the {{LANG=C}} environment variable on all of Ubuntu 16.04 builders (SERVER-31717, SERVER-33184) so it isn't clear why we'd even be able to create files with non-ASCII pathnames. CC [~mark.benvenuto]    """
"SERVER-34380","Improvement","Performance",2,"system_perf.yml: Remove the compile_proxy task","""In SERVER-33513 I added a """"compile_proxy"""" task in system_perf.yml as a layer of indirection. This was a workaround due to not being able to use {{depends_on}} on a variant level. That has now been implemented in EVG-2923. We should therefore use that instead.    Note: While the compile_proxy task is only used for master, the end result of this ticket should also be backported to stable branches, so that system_perf.yml is as consistent as possible across the branches."""
"SERVER-34405","Task","Performance",1,"Add sys-perf move_chunk_waiting task for WT. ","""Master branch only"""
"SERVER-34420","Improvement","Testing Infrastructure",2,"Set the idle event in the stepdown thread even if the thread exits in stepdown.py","""In [stepdown.py|https://github.com/mongodb/mongo/blob/c6af07af7922c58293e86992ff9ef0a9ad77d398/buildscripts/resmokelib/testing/hooks/stepdown.py#L157], if the stepdown thread exits before setting the idle event, we endlessly keep waiting to pause the stepdown thread.         This could be fixed in [stepdown.py|https://github.com/mongodb/mongo/blob/c6af07af7922c58293e86992ff9ef0a9ad77d398/buildscripts/resmokelib/testing/hooks/stepdown.py#L181-L185]:  """
"SERVER-34451","Improvement","Packaging",5,"MongoDB installation on Windows error: setup wizard ended prematurely","""There are few instances where users are failing to install MongoDB on Windows (e.g. win10) with a setup error message {{setup wizard ended prematurely}}    The general solution seems to just uncheck MongoDB Compass installation. This is likely to be caused by one or more of:   * Firewall/Antivirus blocking access to {{https://compass.mongodb.com/api/v2/download/latest}}   * Server has no access to the Internet  * PowerShell execution policy [Set ExecutionPolicy|https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.security/set-executionpolicy?view=powershell-6] preventing the script to be executed.         """
"SERVER-34456","Improvement","Storage",5,"Add comprehensive testing for KeyString length decoding","""Unique index key format would be changed to enable PIT reads from secondary. As a result, unique indexes could have both old and new format keys after an upgrade. Reading keys from mixed format index requires distinguishing old and new format keys.   Index keys are stored as KeyString objects. To read keys correctly from mixed format indexes, a function was written to decode the KeyString and calculate the size of key.    This ticket aims to add comprehensive test for this new KeyString length decoding function."""
"SERVER-34488","Bug","Testing Infrastructure",3,"hang_analyzer.py fails because ptrace protection is not disabled",""""""
"SERVER-34486","Task","Testing Infrastructure",1,"Set transactionLifetimeLimitSeconds=1 in the fuzzer suites that run with replication enabled","""{code:yaml}  mongod_options:    set_parameters:      transactionLifetimeLimitSeconds: 1  {code}    should be added to the following test suites in order to avoid having the fuzzer trigger spurious Evergreen timeouts when it goes to wait for itself to be able to take a non-intent lock after having started a transaction.    * {{jstestfuzz_interrupt_replication.yml}}  * {{jstestfuzz_replication.yml}}  * {{jstestfuzz_replication_initsync.yml}}  * {{jstestfuzz_replication_session.yml}}  * {{jstestfuzz_sharded_causal_consistency.yml}} (uses replica set shards)  * {{jstestfuzz_sharded_continuous_stepdown.yml}} (uses replica set shards)"""
"SERVER-34497","Task","Replication|Testing Infrastructure",1,"Remove CheckPrimary hook","""It's unnecessary due to SERVER-31670."""
"SERVER-34555","Task","Testing Infrastructure",5,"Migrate concurrency_sharded_with_stepdowns{,_and_balancer}.yml test suites to run directly via resmoke.py","""The changes from SERVER-19630 make it so FSM workloads run as individual test cases in the {{concurrency_sharded_causal_consistency\{,_and_balancer\}.yml}} and {{concurrency_sharded_replication\{,_and_balancer\}.yml}} test suites. The {{concurrency_sharded_with_stepdowns\{,_and_balancer\}.yml}} test suites weren't migrated to the new-style because there are parts of setting up the environment to run the FSM workloads under that aren't prepared to have the primary of the CSRS or replica set shard stepped down. Rather than trying to get the all the retry logic correct (e.g. [by handling the {{ManualInterventionRequired}} when attempting to shard the collection|https://github.com/mongodb/mongo/blob/53c378f137bc4f577f6c92f71f47ede70ec93456/jstests/libs/override_methods/mongos_manual_intervention_actions.js]), we should instead delay when resmoke.py's {{StepdownThread}} actually runs after the FSM workload has started.    A sketch of the interactions between [the {{_StepdownThread}} class|https://github.com/mongodb/mongo/blob/14d03a79f55d69ccdd27bb4a08906a4be5eb4a8e/buildscripts/resmokelib/testing/hooks/stepdown.py#L98] and {{resmoke_runner.js}} via the filesystem is described in the appropriate place of the {{runWorkloads()}} function below.    {code:diff}  diff --git a/jstests/concurrency/fsm_libs/resmoke_runner.js b/jstests/concurrency/fsm_libs/resmoke_runner.js  index d94fd4e31c..af0afca2bb 100644  --- a/jstests/concurrency/fsm_libs/resmoke_runner.js  +++ b/jstests/concurrency/fsm_libs/resmoke_runner.js  @@ -104,6 +104,15 @@                   cleanup.push(workload);               });    +            // After the $config.setup() function has been called, it is safe for the stepdown  +            // thread to start running. The main thread won't attempt to interact with the cluster  +            // until all of the spawned worker threads have finished.  +            //  +            // TODO: Call writeFile('./stepdown_permitted', '') function to indicate that the  +            // stepdown thread can run. It is unnecessary for the stepdown thread to indicate that  +            // it is going to start running because it will eventually after the worker threads have  +            // started.  +               // Since the worker threads may be running with causal consistency enabled, we set the               // initial clusterTime and initial operationTime for the sessions they'll create so that               // they are guaranteed to observe the effects of the workload's $config.setup() function  @@ -128,17 +137,34 @@               }                 try {  -                // Start this set of worker threads.  -                threadMgr.spawnAll(cluster, executionOptions);  -                // Allow 20% of the threads to fail. This allows the workloads to run on  -                // underpowered test hosts.  -                threadMgr.checkFailed(0.2);  +                try {  +                    // Start this set of worker threads.  +                    threadMgr.spawnAll(cluster, executionOptions);  +                    // Allow 20% of the threads to fail. This allows the workloads to run on  +                    // underpowered test hosts.  +                    threadMgr.checkFailed(0.2);  +                } finally {  +                    // Threads must be joined before destruction, so do this even in the presence of  +                    // exceptions.  +                    errors.push(...threadMgr.joinAll().map(  +                        e => new WorkloadFailure(  +                            e.err, e.stack, e.tid, 'Foreground ' + e.workloads.join(' '))));  +                }               } finally {  -                // Threads must be joined before destruction, so do this even in the presence of  -                // exceptions.  -                errors.push(...threadMgr.joinAll().map(  -                    e => new WorkloadFailure(  -                        e.err, e.stack, e.tid, 'Foreground ' + e.workloads.join(' '))));  +                // Until we are guaranteed that the stepdown thread isn't running, it isn't safe for  +                // the $config.teardown() function to be called. We should signal to resmoke.py that  +                // the stepdown thread should stop running and wait for the stepdown thread to  +                // signal that it has stopped.  +                //  +                // TODO: Call removeFile('./stepdown_permitted') so the next time the stepdown  +                // thread checks to see if it should keep running that it instead stops stepping  +                // down the cluster and creates a file named """"./stepdown_off"""".  +                //  +                // TODO: Call the ls() function inside of an assert.soon() / assert.soonNoExcept()  +                // and wait for the """"./stepdown_off"""" file to be created. assert.soonNoExcept()  +                // should probably be used so that an I/O-related error from attempting to list the  +                // contents of the directory while the file is being created doesn't lead to a  +                // JavaScript exception that causes the test to fail.               }           } finally {               // Call each workload's teardown function. After all teardowns have completed check if  {code}"""
"SERVER-34548","Bug","Testing Infrastructure",3,"Make FSM workloads able to be run via burn_in_tests.py (with --repeat=2)","""Individual FSM workloads are not designed to clean up after themselves - rather, they expect the runners to take care of that. This can be problematic when you create or modify a workload, as that is picked up by burn_in_tests, which runs that workload several times without evident cleanup between runs. As a result, that test can conflict with itself (e.g. trying to create a database that already exists by the second run)."""
"SERVER-34539","Task","MapReduce|Sharding",2,"Re-enable sharded mapReduce concurrency testing and only use a single mongos","""Concurrent sharded mapReduce testing was disabled as part of SERVER-20057. However, it appears the bug in this ticket only occurs when there are multiple mongoses. I believe this testing should be re-enabled, but only use one mongos, since there may be other concurrent sharded mapReduce related issues, such as SERVER-33538, that can be found from this test coverage."""
"SERVER-34579","Task","Storage",5,"Do not populate indexDetails for mobile storage engine","""This causes apitest_dbcollection.js to fail on mongoe. Reproduce by:            """
"SERVER-34567","Task","Testing Infrastructure",1,"Remove the ""build new tools"" step from the compile benchmark task","""We only specify {{\-\-use-new-tools}} to SCons in the {{$\{task_compile_flags\}}} expansions that need to, so building new tools in the compile_benchmarks task is unnecessary."""
"SERVER-34598","Improvement","Replication",5,"Add millisecond-granularity wallclock times for the various metrics in replSetGetStatus's optimes subdocument","""The response to {{replSetGetStatus}} includes a subdocument named {{optimes}}, which contains the OpTime for various important oplog events, including {{lastCommittedOpTime}}, {{readConcernMajorityOpTime}}, {{appliedOpTime}} and {{durableOpTime}}. As of MongoDB 3.6, the actual oplog entries corresponding to these OpTimes have a wall clock time with milliseconds resolution recorded in them. We should extend {{replSetGetStatus}} to report the wall clock times corresponding to these optimes, so that we can (usually) get millisecond-granularity measurements of replication lag and back-to-back majority read-modify-write latencies.    The work for this ticket is split into SERVER-40080, SERVER-40078, and SERVER-40353. SERVER-34598 is an umbrella ticket with no work items."""
"SERVER-34593","Improvement","Testing Infrastructure",3,"resmoke.py should be able to run multiple instances of a single test in parallel","""If you try to run a single Javascript test with resmoke.py using a combination of the {{--repeat=N}} flag and the {{-j=M}} flag, it will still run the test sequentially. e.g.         Ideally it could parallelize repeated execution of a single test. For example, if {{--repeat=100}} and {{-j=10}}, it would run 10 instances of the test in parallel, that would each execute 10 times.    This could be very helpful for quickly trying to reproduce a particular test failure locally."""
"SERVER-34587","Task","Packaging",1,"Update signing key to 4.0","""We're using 3.8 key right now, should switch to 4.0"""
"SERVER-34624","Task","Build|Testing Infrastructure",1,"Remove C++ 14 builder from 3.4","""Remove the C++14 builder, which happens to be the only DEBUG builder run on the """"test""""-sized VMs, from the 3.4 branch to avoid spurious timeouts.    *Original Description*  Reduce number of jobs for all DEBUG builders using the strategy described in SERVER-29355"""
"SERVER-34614","Task","Testing Infrastructure",2,"parallelTester should use a different connection for each new test","""Because each test uses the same connection, tests can share a set of authenticated users, and can interfere with the state of getLastError.    Each new test should get its own connection."""
"SERVER-34654","Task","Replication",3,"Write test for a transaction that writes to a collection that is created concurrently","""We want to verify that transactions interact with collection creation operations correctly. To verify this, we should test the following case:    # A transaction writes to a collection C that doesn't exist. In a different session, collection C is then created. Verify that the transaction can then write to collection C and commit.  # A transactions writes to a collection C that exists. A client in a different session that tries to create C should fail.    *Note: when writing these test cases, we should verify any behavior that differs from what is described above and make sure it matches the desired/expected behavior."""
"SERVER-34652","Task","Replication",3,"Write tests for transactions that write to a collection that is concurrently dropped","""We want to test the interaction between transactions and collection drops. We should test the following cases:    # A transaction writes to a collection on one session. On a separate session, that a drop is attempted on that collection. The collection drop should block until either the transaction commits or maxTimeMS expires. We should also test this for dropDatabase.  # Create collections A and B. Start a transaction T by reading from a collection A. In a separate session, drop collection B. Then try to write to collection B in transaction T. Verify that this write fails, since the collection was dropped."""
"SERVER-34647","Task","Replication",3,"Write test for transaction that opens multiple cursors","""We want to write a test that exercises a transaction that opens multiple cursors and reads data from them. We should make sure that the results returned from multiple cursors inside a transaction all return data from the same snapshot. We should also test this for multiple cursors on the same collection and multiple cursors that span different collections. Additionally, we should verify that killing multiple cursors that open inside a transaction behaves correctly."""
"SERVER-34680","Bug","Testing Infrastructure",3,"jsCore_mobile task fails if bypass compile is triggered","""The jsCore_mobile task can fail if bypass compile is triggered because the mongoe binary is not generated during compile.     For an example, see:    https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_required_mobile_jsCore_mobile_patch_00f32ac53c595f098ea200ab7b9d7278be4a5193_5ae0cc70c9ec44641fc7335c_18_04_25_18_44_14  """
"SERVER-34706","Task","Replication",3,"Write unit test to verify transactions oplog entries are created correctly","""We should verify that transactions oplog entries are created in the proper format. This can likely be tested in {{op_observer_impl_test.cpp}}. We should also be able to remove any logic from JS tests that explicitly check oplog entry formats once this unit test is added."""
"SERVER-34704","Task","Replication",1,"Write test for transactions on collections that are renamed concurrently","""We want to test the interaction of transactions and collection renames. We should test cases where a transactions writes to a collection A that is concurrently renamed to B. We should verify the renameCollection blocks until the transaction commits or until maxTimeMS expires. We should test this when A and B are in the same database and when they are in different databases."""
"SERVER-34703","Task","Replication",1,"Write test for transactions with concurrent index drops and creates","""We want to test the interaction of transaction writes and index creates and drops. We should test this for a transaction that writes to some documents covered by an index that is created concurrently, and similarly an index that is dropped concurrently. We should verify that the drop/createIndex blocks until the transaction commits or until maxTimeMS expires.    We may consider just doing this and SERVER-34704 as part of the same test."""
"SERVER-34738","Improvement","Testing Infrastructure",2,"mongo_lock.py graph should display lock type for LockManager locks","""When a thread is waiting on a lock in the lock graph, it would be useful to know what lock mode it is waiting on."""
"SERVER-34711","Improvement","Testing Infrastructure",3,"Enable burn_in_tests to understand Evergreen task selectors","""After adopting task selector/tag approach to address SERVER-33647, the only task failed in the patch build is {{burn_in_tests}}. It looks the {{buildscripts/ciconfig/evergreen.py}} script doesn't understand task selectors.    Link to the patch build: https://evergreen.mongodb.com/version/5ae1639dc9ec44641fd52543      cc: [~max.hirschhorn]"""
"SERVER-34779","Task","Testing Infrastructure",8,"Check the dbhash periodically in a new version of the replica_sets_jscore_passthrough.yml test suite","""We should create another version of {{jstests/libs/override_methods/run_check_repl_dbhash.js}} and possibly of {{ReplSetTest#checkReplicatedDataHashes()}} that *doesn't* require (1) flushing background indexes with collMod operations, (2) fsync+locking the primary, and (3) call {{ReplSetTest#awaitReplication()}}. A background thread inside of resmoke.py should then run the """"dbhash"""" command periodically via the hook file and cause the test to be marked as a failure if a data inconsistency is detected."""
"SERVER-34778","New Feature","Storage|Testing Infrastructure",3,"Add support for specifying atClusterTime to the dbhash command","""This makes it possible to detect transient data inconsistency failures (e.g. related to timestamping differences between the primary and secondary of a replica set) that have been resolved by the time we've finished waiting for all operations to have replicated. This requires changing the """"dbhash"""" command to [call {{getMinimumVisibleSnapshot()}}, etc. as {{AutoGetCollectionForRead}} does currently|https://github.com/mongodb/mongo/blob/r3.7.7/src/mongo/db/db_raii.cpp#L135-L167]."""
"SERVER-34793","Task","Testing Infrastructure",1,"Add call to BF suggestion server on failed task completion","""Add a call to the BF suggestion server task registration API during the post phase for tasks that have failed tests.    The API call must not alter the task execution result."""
"SERVER-34788","Improvement","Testing Infrastructure",1,"Improve error message when assert.commandWorked/Failed gets an unexpected type","""When {{assert.commandWorked()}} or {{assert.commandFailed()}} is passed a non-object, we throw:  {code:js}      function _assertCommandWorked(res, msg, {ignoreWriteErrors, ignoreWriteConcernErrors}) {          _validateAssertionMessage(msg);            if (typeof res !== """"object"""") {              doassert(""""unknown response given to commandWorked"""");          }    [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.654+0000 2018-04-29T18:58:39.653+0000 E QUERY    [js] Error: unknown response given to commandWorked :  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.654+0000 doassert@src/mongo/shell/assert.js:18:14  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.654+0000 _assertCommandWorked@src/mongo/shell/assert.js:485:13  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.654+0000 assert.commandWorked@src/mongo/shell/assert.js:594:16  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.654+0000 CollectionValidator/this.validateNodes/<@jstests/hooks/validate_collections.js:128:17  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.655+0000 CollectionValidator/this.validateNodes@jstests/hooks/validate_collections.js:127:13  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.655+0000 @jstests/hooks/run_validate_collections.js:36:5  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.655+0000 @jstests/hooks/run_validate_collections.js:5:2  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.655+0000 failed to load: jstests/hooks/run_validate_collections.js  [ValidateCollections:job0:b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041:ValidateCollections] 2018-04-29T18:58:39.658+0000 Full collection validation after running 'b23b-mdb_793e-ent_7007-qa_a6ce-1525027831965-041' failed  {code}    We could, at the very least, include what the type of {{res}} was in the assertion error message."""
"SERVER-34826","Task","Replication",3,"Write targeted FSM workload for read repeatability in transactions ","""We should write an FSM workload that verifies _read repeatability_ of transactions. This workload can presumably have each thread be in either aÂ _Read_ orÂ _Update_Â state, where the _Read_ state executes multiples reads sequentially, expecting to see the same result set for each read. The _Update_ state could update some random subset of documents in a collection. This test would be good at verifying repeatability under higher concurrency and load than our targeted tests. Eventually we may also add a repeatability test that runs against all our existing FSM workloads, but in lieu of that, this could be a valuable targeted workload to exercise a key property of transactions under snapshot isolation."""
"SERVER-34867","Improvement","Replication|Testing Infrastructure",1,"Run powercycle tests with `storage.recovery` logging set to 2","""There are test failures where debugging would be aided by having recovery logging turned on. E.g: [the stable timestamp a node takes checkpoints at|https://github.com/mongodb/mongo/blob/c0d6b410b15227051ca96dc54f8d6c1df77630cf/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L242-L243]."""
"SERVER-34865","Bug","Testing Infrastructure",2,"Test archival fails when temporary files are removed","""The following error occurred when archiving a failed test:    The code doe not handle the case where a temporary file is in a directory list and then subsequently deleted before it is examined:    The {{OSError}} should handle this case."""
"SERVER-35042","New Feature","Build|Storage",2,"Mobile builders should not be enterprise","""The mobile builders are currently enterprise builds for historical reasons, but they don't need to be. They also probably shouldn't be, since we plan to add some additional build variants in the near future for which we don't want to require the enterprise code to build."""
"SERVER-35036","Task","Testing Infrastructure",3,"Remove database and collection cleanup from $config.teardown functions","""As mentioned in [this comment|https://jira.mongodb.org/browse/SERVER-34548?focusedCommentId=1868113&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1868113] of SERVER-34548, having FSM workloads drop their (unique) database or collection in the {{$config.teardown()}} function undermines our data consistency checks as the contents will be deleted before the resmoke.py hooks run. We should leave it to the {{CleanupConcurrencyWorkloads}} hook from SERVER-30204 to drop all the databases and collections after the data consistency checks run."""
"SERVER-35100","Improvement","Testing Infrastructure",1,"Do not log a Python stack trace when a hook dynamic test fails","""Currently, when a dynamic test (run in a hook) fails, the exception that was thrown gets logged.    The stack trace in the logs is unrelated to the cause of the failure and only adds noise. It should be removed. The log statement is [here|https://github.com/mongodb/mongo/blob/6ab1592260c9b21d802aa65a11d268c0a97b11a7/buildscripts/resmokelib/testing/hooks/interface.py#L79]."""
"SERVER-35071","Task","Performance",3,"Split MMAPv1 tasks into separate variants in sys-perf","""* Split mmapv1 tasks to a separate variant  * Remove _WT and _MMAPv1 from task names  * Use anchors to collapse task lists   * Schedule mmap variants to run every 7 days  * BUILD ticket to copy history correctly  * backports"""
"SERVER-35165","Task","Testing Infrastructure",1,"Disable and re-enable update_test_lifecycle Evergreen task on the 4.0 branch","""{{git log \-\-since=28.days \-\-pretty=format:%H}} returns commits from prior to when we created the 4.0 branch and therefore prior to when we created the {{mongodb\-mongo\-v4.0}} Evergreen project."""
"SERVER-35160","Improvement","Shell|Testing Infrastructure",2,"ScopedThreads should automatically inherit TestData from their parent thread","""For a test that uses ScopedThread to pass in the auth passthrough tests, the test author needs to remember to manually pass TestData into the spawned thread so it can inherit the proper auth credentials.  startParallelShell automatically copies the TestData to the new shell, ScopedThread should behave the same."""
"SERVER-35154","Improvement","Shell|Testing Infrastructure",5,"Exceptions that escape a ScopedThread should fail the test","""If you start a ScopedThread in a test and it throws an exception, that exception is swallowed and does not error the test, even if the main test thread calls join() and returnData() on the ScopedThread.  This can lead to subtle problems with tests, and issues where tests are broken and no longer testing what they are supposed to, but no one notices."""
"SERVER-35197","Task","Testing Infrastructure",1,"Change CleanEveryN to CleanupConcurrencyWorkloads in concurrency_replication_causal_consistency suite","""Given the fact that CleanUpConcurrencyWorkloads hook caused a bunch of test failures in my [patch|https://evergreen.mongodb.com/version/5b05b067e3c3314cc0af1572], I am going to switch back to CleanEveryN and put a TODO comment with this ticket number there."""
"SERVER-35195","Improvement","Testing Infrastructure",1,"Remove Python linting rule requiring docstring for magic functions","""Add an exception for rule d105 in pydocstyles and remove redundant magic method docstrings.    docstrings for magic functions don't provide much value. They're mostly useful for describing parameters, which is not checked by d105.  """
"SERVER-35203","Improvement","Testing Infrastructure",1,"Unittests accept --logLevel","""It might be nice to add an optional {{--logLevel=<int>}} parameter to unittest_main, so that when locally debugging unittest failures it's possible to ramp up any debugging log output from the main code that's called by the tests."""
"SERVER-35250","Improvement","Testing Infrastructure",3,"save dbtest debug symbols in debug_symbols tar","""I would like to be able to symbolize stack traces generated by dbtest in BF's."""
"SERVER-35233","Bug","Testing Infrastructure",1,"Powercycle remote collection validation does not skip views","""The remote collection validation can fail if a view exists."""
"SERVER-35263","Task","Replication|Testing Infrastructure",2,"Add FSM workloads for testing atomicity and isolation of updates inside a transaction across multiple collections and databases","""Extend [the {{multi_statement_transaction_atomicity_isolation.js}} FSM workload|https://github.com/mongodb/mongo/blob/r4.1.0/jstests/concurrency/fsm_workloads/multi_statement_transaction_atomicity_isolation.js] from SERVER-34293 to support running the updates and consistency checks against collections or databases specified via {{$config.data}}. The {{multi_statement_transaction_atomicity_isolation.js}} FSM workload should continue to only run against the {{db[collName]}} collection provided by the concurrency framework."""
"SERVER-35262","Task","Testing Infrastructure",3,"Add concurrency_simultaneous_replication.yml test suite","""This would increase the variety of concurrent operations we exercise against a replica set. The existing {{concurrency_simultaneous.yml}} test suite is limited in that we cannot run FSM workloads which use transactions. The {{concurrency_simultaneous_replication}} Evergreen task should be added to all build variants we currently run the {{concurrency_simultaneous}} Evergreen task against."""
"SERVER-35261","Task","Testing Infrastructure",2,"Add CheckReplDBHashInBackground hook to concurrency_replication.yml test suite","""Apply the following patch and run it several times in Evergreen to see if there are any failures before turning it on.    {code:diff}  diff --git a/buildscripts/resmokeconfig/suites/concurrency_replication.yml b/buildscripts/resmokeconfig/suites/concurrency_replication.yml  index 7ae625b..05dfa72 100644  --- a/buildscripts/resmokeconfig/suites/concurrency_replication.yml  +++ b/buildscripts/resmokeconfig/suites/concurrency_replication.yml  @@ -16,6 +16,7 @@ selector:   executor:     archive:       hooks:  +      - CheckReplDBHashInBackground         - CheckReplDBHash         - ValidateCollections       tests: true  @@ -26,7 +27,7 @@ executor:     # The CheckReplDBHash hook waits until all operations have replicated to and have been applied     # on the secondaries, so we run the ValidateCollections hook after it to ensure we're     # validating the entire contents of the collection.  -  #  +  - class: CheckReplDBHashInBackground     # TODO SERVER-26466: Add CheckReplOplogs hook to the concurrency suite.     - class: CheckReplDBHash     - class: ValidateCollections  {code}    *Note*: Unlike SERVER-34555, there shouldn't need to be any additional synchronization as the {{CheckReplDBHashInBackground}} hook is safe to run while the {{$config.setup()}} and {{$config.teardown()}} functions are being run."""
"SERVER-35313","Bug","Testing Infrastructure",2,"CleanupConcurrencyWorkloads resmoke hook needs to handle the balancer","""If the balancer is enabled, then the CleanupConcurrencyWorkloads hook should stop it before cleaning up the DBs and collections, and then restart it when finished."""
"SERVER-35383","Task","Testing Infrastructure",2,"Increase electionTimeoutMillis for the ContinuousStepdown hook used in stepdown suites","""The {{electionTimeoutMillis}} parameter for the {{ContinuousStepdown}} hook, used in the concurrency stepdown suites, is set to 5000. We should increase this per the captured discussion:    {quote}  > > On 2018/05/30 22:09:12, maxh wrote:  > > > [note] As mentioned in SERVER-34666, I don't think we should shorten the  > > > election timeout as it can lead to an election happening that isn't  > initiated  > > by  > > > the StepdownThread due to heartbeats being delayed. I'm okay with keeping  it  > > > as-is for now because it is consistent with the replica set configuration  > the  > > > JavaScript version would have used; however, I'd like for there to be a  > > > follow-up SERVER ticket to change it.  > > >   > > >  > >  >  https://jira.mongodb.org/browse/SERVER-34666?focusedCommentId=1873407&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1873407  > >   > > For the followup ticket, do we just want to remove this value and use the  > > default, or set it to a higher timeout?  >   > I'm not sure - I'd like to get some input from Judah on it. I'm currently  > wondering if we really need to avoid setting the election timeout to 24 hours  > when all_nodes_electable=true. We're going to use the replSetStepUp command in  > the Python version of the StepdownThread to cause one of the secondaries to  run  > for election anyway. If for some reason the replSetStepUp command fails, then  > the former primary will try and step back up after 10 seconds on its own  anyway.  >   >  https://github.com/mongodb/mongo/blob/r4.1.0/buildscripts/resmokelib/testing/fixtures/replicaset.py#L149-L154    If you only want elections to come from the StepdownThread, then I'd recommend  setting the election timeout to 24 hours. The replSetStepUp command should still  work, and if it fails for some reason, then no other node will try to run for  election. There's no real difference between the default 10 seconds and the  current 5 seconds except for the amount of flakiness you'd expect (not the  existence of flakiness that we're trying to remove completely).  {quote}"""
"SERVER-35398","Improvement","Storage",2,"Mobile SE: Remove code for capped collection","""Remove the capped collection code after SERVER-33605 gets checked in"""
"SERVER-35473","Improvement","Storage",5,"Mobile SE: Fix writeConflictRetry loop with map-reduce jstests","""With fixes for SERVER-32997 I still see issues with map-reduce jstests in {{concurrency/fsm_workloads/}}. {{concurrency}} and {{concurrency_simultaneous}} test suites stay disabled for mobile SE waiting on fixing map-reduce (and validate). This ticket tracks the work needed to fix map-reduce concurrency issues."""
"SERVER-35506","Improvement","Testing Infrastructure",2,"The Powercycle wait_for_mongod_shutdown function should ensure the mongod process is no longer running","""The {{wait_for_mongod_shutdown}} function waits until the {{mongod.lock}} file is deleted. It should just check that mongod process is no longer running."""
"SERVER-36019","New Feature","Testing Infrastructure",5,"Create script to collect resource utilization of Android application","""The script should assume that the application is already running on the device and shouldn't concern itself with how the application was started (i.e. it'll be the responsibility of some other part of this mobile testing framework which deals with that). The script should run continuously and collect the resource utilization at some configurable frequency until the it is signaled to exit. It should define functions like {{start()}} and {{stop()}} so that the script can also be used as a library and controlled programmatically.    * CPU consumption: {{python systrace.py}}  ** Reference: https://developer.android.com/studio/command-line/systrace    * Memory consumption: {{adb shell dumpsys meminfo}}  ** Refence: https://developer.android.com/studio/command-line/dumpsys#meminfo    * Battery consumption: {{adb shell dumpsys batterystats}}  ** Reference: https://developer.android.com/studio/command-line/dumpsys#battery"""
"SERVER-35537","New Feature","Performance|Testing Infrastructure",8,"Create version of benchRun() which can be used with embedded","""This makes it possible use our existing https://github.com/mongodb/mongo-perf tests as performance tests for embedded. We can likely get away with creating separate executables for each mongo-perf test case that uses {{ServiceEntryPointEmbedded}}, {{embedded::initialize()}}, and {{DBDirectClient}} (instead of [{{DBClientConnection}}|https://github.com/mongodb/mongo/blob/5a1bdde940b6a91e1133b64ee5365ce595b23e3a/src/mongo/shell/bench.cpp#L1363]) to perform operations.    *Note*: We'll also need to remove or split out the """"check"""" function as it requires linking in the JavaScript engine."""
"SERVER-35559","Task","Testing Infrastructure",2,"Update transaction retry functions to not call abort after commit","""After the changes in SERVER-35094 to disallow calling {{abortTransaction()}} after {{commitTransaction()}} The {{withTxnAndAutoRetry}} helper function and retry logic in the background dbhash hook will need to be changed to not call {{abortTransaction()}} if the failure error comes from {{commitTransaction()}}"""
"SERVER-35588","Bug","Testing Infrastructure",2,"powertest.py should call replSetReconfigure command only after successful replSetGetConfig","""replSetReconfig is not safe to retry when there's an AutoReconnect error.  If it succeeded before the network was disconnected, it will likely fail the retry.    I believe it is safe to retry the whole else-clause starting with replSetGetConfig    https://github.com/mongodb/mongo/blob/1d89d2c88bcb39045701b87612b866ae2eb49378/pytests/powertest.py#L1440    In this case, if the previous reconfig succeeded, the """"if"""" at line 1457 will prevent attempting to reconfig again.  """
"SERVER-35737","Bug","Build",2,"install_compass fails on MacOS ","""Using MacOSÂ 10.13.3 and Python 2.7.3.  When trying to run install_compass after downloading the 4.0.0-rc7 tarball:    Â     I checked the end point it's trying to hit (defined on line 156 of install_compass):   [https://compass.mongodb.com/api/v2/download/latest/compass-community/stable/osx]    It's displaying this message:      Can we handle end point errors more gracefully?"""
"SERVER-35800","Improvement","Testing Infrastructure",2,"resmoke.py should retry getting a build_id and test_id from logkeeper","""The changes from SERVER-35472 made it so that resmoke.py would exit if it couldn't communicate with logkeeper. This has lead to setup failures in Evergreen that are caused by the logkeeper application server not responding with a build_id or test_id quickly enough. It might be that retrying would succeed that we should make 10 attempts and fail if we still don't get a build_id or test_id.    *Note*: Retrying the request to get a build_id or new test_id is safe as [it simply inserts a new document|https://github.com/evergreen-ci/logkeeper/blob/e83432bd04ba111c72907af7f3fa50a52ea531b6/views.go#L219]. The only quirk is that the """"Job logs"""" tab may show extra entries in the case that resmoke.py never received a response from the logkeeper application server but the database still eventually did the work."""
"SERVER-35852","Improvement","Testing Infrastructure",2,"Convert backup_restore.js blacklist to use a YAML based list for transaction tests","""Having the test understand existing blacklists will reduce the likelihood of a test not being added to the blacklist for {{backup_restore.js}}."""
"SERVER-36010","Improvement","Diagnostics|Logging",2,"Change log messages for Windows stacktraces to use error() or severe() rather than log()","""This would make it more obvious to our users and potentially easier for automated tools to detect that these messages are process-fatal.    * https://github.com/mongodb/mongo/blob/026f69dbf4f98e91b499bde5cb4ce73c332e9549/src/mongo/util/exception_filter_win32.cpp#L137-L138  * https://github.com/mongodb/mongo/blob/026f69dbf4f98e91b499bde5cb4ce73c332e9549/src/mongo/util/exception_filter_win32.cpp#L160  * https://github.com/mongodb/mongo/blob/026f69dbf4f98e91b499bde5cb4ce73c332e9549/src/mongo/util/exception_filter_win32.cpp#L163  * https://github.com/mongodb/mongo/blob/026f69dbf4f98e91b499bde5cb4ce73c332e9549/src/mongo/util/exception_filter_win32.cpp#L174"""
"SERVER-36043","Bug","Packaging",1,"systemd unit for mongod starts before multi.user target","""The {{mongod}} service unit is configured to be part of the multi-user.target, but the unit configuration also has the following parameter:    {{After=network.target}}    Basically, this makes {{mongod}} start just after the network is up and not during multi-user (one of the latest targets to trigger). This causes problems on those servers where there are network storage services or authentication services (like AD) still pending to start. {{mongod}} would not be able to start if it depends on them.    Removing the {{After}} parameter would make systemd to start {{mongod}} as part of the correct target. If an {{After}} is needed for some reason, {{After=multi-user.target}} would avoid most of the problems with dependencies between services.    Affects all native packages (RPM and DEB) for systemd based Linux distros."""
"SERVER-36078","Improvement","Testing Infrastructure",2,"Integrate adb resource monitor into mongoebench test suite for Android","""This involves creating a new hook that calls the \{{start()}} and \{{stop()}} methods before and after each test, respectively. The generated files from the resource monitor should be packaged into subdirectories based on the (test case, execution) pair which ran."""
"SERVER-36077","New Feature","Testing Infrastructure",3,"Create new resmoke.py test suite for running mongoebench on an Android device","""This should build on top of the work from SERVER-36076 to run {{mongoebench}} as a statically-linked binary on an Android device. The {{mongoebench}} and JSON config file can likely be copied over the to device and then run using some combination of the {{adb push}} and {{adb shell}} commands.    *Note*: Integrating the {{buildscripts/mobile/adb_monitor.py}} utility into this test suite should happen as part of SERVER-36078."""
"SERVER-36076","New Feature","Performance|Testing Infrastructure",5,"Create new resmoke.py test suite for running mongoebench on a desktop","""It should run {{mongoebench}} with the various JSON config files that live in the {{src/third_party/mongo-perf/mongoebench/}} directory that have been vendored into the source tree as part of the changes from SERVER-36069.    This involves creating a new {{buildscripts/resmokelib/testing/testcases/mongoebench_test.py}} test case that executes {{mongoebench}} with the appropriate arguments. For example, the value for the {{\-\-benchmarkMinTimeSecs}} command line option should be forwarded as the {{\-\-time}} command line option to {{mongoebench}}.    This also involves creating a new hook similar to [the {{CombineBenchmarkResults}} hook|https://github.com/mongodb/mongo/blob/r4.0.0/buildscripts/resmokelib/testing/hooks/combine_benchmark_results.py] that parses the JSON stats file specified as the {{\-\-output}} command line option (from SERVER-36073) to {{mongoebench}}. The new hook should accumulate benchmark results of all the test cases we run as part of the test suite and serialize them as a JSON file (taking its name from the {{\\--perfReportFile}} command line option) that can be used for the {{json.send}} Evergreen command to display the performance results. The test case should also handle the {{\-\-benchmarkRepetitions}} command line option (in Python, as there is no equivalent option to forward to {{mongoebench}}) and accumulate the benchmark results of multiple executions.    We may find it beneficial to define separate test suites that each run a subset of the test cases [similar to what is done in the performance Evergreen project when these test cases are run with benchrun.py|https://evergreen.mongodb.com/build/performance_linux_wt_standalone_80c7c825a44cf99b17e81f4233445c7ab1927706_18_07_11_01_45_09] to avoid having an Evergreen task run for a long time."""
"SERVER-36073","Task","Testing Infrastructure",1,"Save stats from BenchRunner::finish() to a JSON file in mongoebench","""We can add a new command line {{\-\-output}} as a path for where to save the benchRun stats."""
"SERVER-36069","Task","Testing Infrastructure",3,"Vendor mongoebench-compatible JSON config files from mongodb/mongo-perf into src/third_party","""The JSON config files should live in a directory called {{src/third_party/mongo-perf/mongoebench/}}. It should be possible to rerun the vendoring script and automatically update them to pick up on new and modified mongo-perf test cases. We'll likely want to filter out test cases which rely on capped collections or server-side JavaScript."""
"SERVER-36067","Task","Testing Infrastructure",2,"Upload artifacts from running install-mobile-test target in Evergreen to S3","""We currently upload the artifacts from running the {{install\-mobile\-dev}} target in Evergreen to S3 as        We should do something similar for the {{install\-mobile\-test}} target so that the {{mongoebench}} binary from SERVER-35537 can be run on an Android device. The following S3 paths were proposed by [~acm]:    """
"SERVER-36090","Bug","Build|Packaging",8,"install_compass fails on MacOS due to SSL version","""    The link for the compass download is behind the load balancer, which has disabled older TLS versions.    There are a couple of possible solutions we should consider."""
"SERVER-36129","Bug","Testing Infrastructure",3,"Concurrency stepdown suites should wait for replication of workload setups before starting stepdown thread","""The concurrency stepdown suites [wait until after setup has been called for each workload before starting the stepdown thread|https://github.com/mongodb/mongo/blob/a291ec89affd9e849ac62ad55a736bfb940a0bb6/jstests/concurrency/fsm_libs/resmoke_runner.js#L101-L111] because the setup methods don't run with overriden majority read/write concern. The effects of each setup are not guaranteed to be majority committed at this point though, so an immediate stepdown can still roll back some of the setup, like the creation of the TTL index in [indexed_insert_ttl.js|https://github.com/mongodb/mongo/blob/a291ec89affd9e849ac62ad55a736bfb940a0bb6/jstests/concurrency/fsm_workloads/indexed_insert_ttl.js#L30-L31].    A fix for this would be waiting for replication on all shards and the config server before starting the stepdown thread."""
"SERVER-36169","Bug","Testing Infrastructure",1,"Resmoke: bare raise outside except in the stepdown hook","""The stepdown hook code contains three misplaced bare raise statement, outside an except block: [here|https://github.com/mongodb/mongo/blob/99d3436094d31de348edfac9fe0e40e60b28391e/buildscripts/resmokelib/testing/hooks/stepdown.py#L409],Â [here|https://github.com/mongodb/mongo/blob/99d3436094d31de348edfac9fe0e40e60b28391e/buildscripts/resmokelib/testing/hooks/stepdown.py#L423] and [here|https://github.com/mongodb/mongo/blob/99d3436094d31de348edfac9fe0e40e60b28391e/buildscripts/resmokelib/testing/hooks/stepdown.py#L442]."""
"SERVER-36162","Bug","Testing Infrastructure",5,"Powercycle - ensure internal crash command has been executed on the remote host","""It's possible that due to an ssh connection error, the remote command to internally crash a server will never run. The {{powertest.py}} script expects that the crash command will fail, as the ssh connection will be terminated. However, it should examine the output of the crash command to determine it it was actually run on the remote host.    Here's a case where the remote command failed to execute:    """
"SERVER-36233","Bug","Testing Infrastructure",2,"Prohibit running the ""profile"" command from secondary read override test suites.","""Running the {{profile}} command can cause system.profile to be created. Since system.profile is an unreplicated collection, there is no point in testing it in secondary read override test suites. It can also cause other tests to pick up the unreplicated collection and fail.    We should prevent the {{profile}} command from being run in the {{set_read_preference_secondary.js}} override file."""
"SERVER-36230","Bug","Testing Infrastructure",2,"Waits-for graph no longer being generated by hang_analyzer.py script","""In BF-9986, we saw many threads waiting on DB locks, but the hang analyzer cannot generate the graph for us. The lock manager [dumps the information|https://logkeeper.mongodb.org/lobster/build/6321d9a6e900f89055816cb177b89f73/test/5b4f50b3f84ae847db0364c1#bookmarks=0%2C178236%2C180602&f=10ReplicaSetFixture%3Ajob0%3Aprimary] though.    """
"SERVER-36409","Bug","Packaging",1,"Install v3.2 from yum repository errors out on RHEL 7Server","""The yum repositories for version 3.2 has an issue that prevents install on RHEL 7Server:           """
"SERVER-36451","Bug","Testing Infrastructure",2,"ContinuousStepdown with killing nodes can hang due to not being able to start the primary","""The replica_sets_kill_primary_jscore_passthrough tests occasionally timeout due waiting for a primary to be selected.     The tests increase the election timeout to 24 hours to have control over which node is the leader. However, this can lead to a situation where the leader has been killed and both secondaries were unable to take over due to having stale oplogs. When the server is brought back up and attempts to stepup, there is a chance it has not yet heard back heartbeats from the other nodes in the cluster and assumes they are down. This means the stepup fails and another election is not attempted causing the test to eventually timeout.    A possible solution, in the event of a failure would be to retry the stepup after some delay. This would allow the secondaries more time to respond to the heart beat request."""
"SERVER-36431","Bug","Testing Infrastructure",1,"Powercycle should check for existence of a process before accessing it's attributes","""It's possible then when iterating over a list of processes that the process could finish before it's accessed:      An additional check should be added  """
"SERVER-36507","Bug","Storage",1,"Downgrading WT from FCV 4.2 -> 4.0 requires an ""acquiesced"" system","""The 4.0 -> 3.6 FCV downgrade path in storage would acquiesce the system by [closing/re-opening the WT connection|https://github.com/mongodb/mongo/blob/aa0062e8aaa5a8273bb33a2afaf8c9cdf5fbede7/src/mongo/db/storage/wiredtiger/wiredtiger_kv_engine.cpp#L665-L670]. That was originally done to facilitate changing table logging settings.    4.2 development [optimistically removed the close/open along with the table logging changes|https://github.com/mongodb/mongo/commit/013b82bf5f58bd7de8ae2f4d28d24f82afa22e64#diff-8fd4ad8935bb2bf3f91bb01f4785c544L659] under the assumption that changing the file compatibility on reconfigure (which remains in the upgrade/downgrade path) did not need to be acquiesced to the same degree."""
"SERVER-36530","New Feature","Testing Infrastructure",1,"Run the agg expression fuzzer in Evergreen","""Add Evergreen tasks for the new agg expression fuzzer."""
"SERVER-36622","Improvement","Packaging",3,"Package tests fail for newer Ubuntu","""Our package tests expect a failure for Ubuntu 18.04 for the install_compass script since it was previously hard coded to only work on 14.04 and 16.04. It has been changed to work on >= 14.04 so our package tests should expect a success on newer Ubuntu's like 18.04"""
"SERVER-36615","Task","Packaging",8,"Add linux repo package testing steps to the server projects","""* Add chef recipes to configure repos and install mongod  * Add Inspec test and kitchen configuration  * Add logic to push task to test and rebuild repos  """
"SERVER-36698","Task","Testing Infrastructure",3,"Add suite for agg expr fuzzer optimized vs unoptimized","""Acceptance Criteria:  * Optimized agg expression fuzzer is running and green in Evergreen"""
"SERVER-36751","Task","Testing Infrastructure",1,"Prevent concurrent dropDatabase commands in the concurrency_simultaneous_replication suite","""*Problem*   In the {{concurrency_simultaneous_replication}} test suite, we run 10 operations in parallel on the same database, there's a small chance (e.g. 5% for some workloads) that an operation could be a {{dropDatabase}}. For slower build variants, a single {{dropDatabase}} command can take multiple minutes to finish if there is heavy activity from other workloads that are happening in parallel.    Our tests will [retry an operation for up to 10 minutes|https://github.com/mongodb/mongo/blob/17686781044525b9c3fbdf06ca326c8f4fb383ba/jstests/libs/override_methods/implicitly_retry_on_database_drop_pending.js#L148] if {{DatabaseDropPending}} errors are encountered. After seeing the error, [A {{getLastError}} command is used|https://github.com/mongodb/mongo/blob/17686781044525b9c3fbdf06ca326c8f4fb383ba/jstests/libs/override_methods/implicitly_retry_on_database_drop_pending.js#L25] to wait for the {{dropDatabase}} command to be committed.    There is variability in the order that {{getLastError}} returns from different workload clients, which may cause certain workload clients to always be stuck behind other clients that are doing more {{dropDatabase}} commands. When this happens, the client will receive another {{DatabaseDropPending}} error. But the client is [unable to distinguish|https://github.com/mongodb/mongo/blob/17686781044525b9c3fbdf06ca326c8f4fb383ba/jstests/libs/override_methods/implicitly_retry_on_database_drop_pending.js#L141] whether the error is caused by the same dropDatabase command or a new one, causing the new wait to continue eat into the 10 minute timeout. There is a small probability that this cycle will happen for a handful of times in a row, which when combined with slow multi-minute {{dropDatabase}} commands, will exceed the 10 minute timeout.    *Solution*   The solution is to avoid retrying {{dropDatabase}} commands when it returns a {{DatabaseDropPending}} error. This will cause the workload to transition to a new state and continue to do so until the new state is no longer a {{dropDatabase}} call. Then it will wait on the ongoing {{dropDatabase}} call.    When the database is finally dropped, it's guaranteed that none of the clients waiting on it would be another drop database, so they should all be able to proceed. There might be edge cases where one client is able to execute multiple commands and one of those commands is another {{dropDatabase}}, but the likelihood of this happening 5 times in a row should be much smaller if not negligible.    From a correctness perspective, this change will make some {{dropDatabase}}Â implicitly into no-ops, which should not cause loss of test coverage, as databases can't be dropped in parallel in the first place. The tests that run parallel dropDatabases also all randomized tests and don't expect these operations to all succeed when there are parallel clients operating on the same database.    -We should also write a dedicated regression test that does a high number of collection DDL operations while dropping and creating databases to simulate the timeout failures we've seen, the changes from this ticket should prevent the test from failing.-    The -new test and the- changes to not retry {{dropDatabase}} should be limited to affect only the {{concurrency_simultaneous_replication}} suite, as we have not seen this failure elsewhere so far."""
"SERVER-36757","Improvement","Testing Infrastructure",2,"Generate and extract mongoebench-compatible JSON config files to consistent locations","""The script from SERVER-36069 writes the mongoebench-compatible JSON config files to a {{src/third_party/mongo-perf/mongoebench/}} directory. The """"fetch benchmark embedded files"""" added to Evergreen as part of SERVER-36076 extracts the mongoebench-compatible JSON config files to a top-level {{benchrun_embedded/}} directory. The {{benchrun_embedded*.yml}} test suites similiarly run the mongoebench-compatible JSON config files from a top-level {{benchrun_embedded/}} directory.    We should have these directories be consistent with each other so that resmoke.py can be used to run the tests, regardless of whether the mongoebench-compatible JSON config files were generated locally or downloaded from S3. We should also add an entry for the directory to a {{.gitignore}} file because we made a decision to not include the mongoebench-compatible JSON config files in the source tree."""
"SERVER-36756","Improvement","Testing Infrastructure",1,"Log the githash of the 10gen/jstestfuzz repository when the fuzzer's self-tests fail","""(This came up when looking at a BF ticket with [~sviatlana.zuiko].)    There is currently no information about the version of the 10gen/jstestfuzz repository we are running [when it fails|https://github.com/mongodb/mongo/blob/d0b0d782a14e9c0ac5724e35fb0bc2e20abcca67/etc/evergreen.yml#L1699-L1712] and the Build Baron ends up going off of the timestamp of when the Evergreen task ran. The following is the information that [we log about the 10gen/jepsen repository|https://github.com/mongodb/mongo/blob/d0b0d782a14e9c0ac5724e35fb0bc2e20abcca67/etc/evergreen.yml#L1575-L1577].    """
"SERVER-36780","Bug","Packaging",1,"Debian packages for MongoDB 4.0.1 missing","""MongoDB 4.0.1 was released [6 August 2018|https://docs.mongodb.com/manual/release-notes/4.0/#aug-6-2018]Â and the installation documentation refers to version 4.0.1 under [Install a specific release of MongoDB|https://docs.mongodb.com/manual/tutorial/install-mongodb-on-debian/#install-a-specific-release-of-mongodb], but this version does not appear to exist in the APT repository:    {{$ apt-cache policy mongodb-org}}   {{mongodb-org:}}   {{Â  Installed: (none)}}   {{Â  Candidate: 4.0.0}}   {{Â  Version table:}}   {{Â  Â  Â 4.0.0 500}}   {{Â  Â  Â  Â  500 [http://repo.mongodb.org/apt/debian] stretch/mongodb-org/4.0/main amd64 Packages}}    This appears to apply to both Debian Stretch and Debian Jessie. Looking at the APT release file for [Stretch|http://repo.mongodb.org/apt/debian/dists/stretch/mongodb-org/4.0/Release] and [Jessie|http://repo.mongodb.org/apt/debian/dists/jessie/mongodb-org/4.0/Release], they do not appear to have been updated since the end of June.    Â """
"SERVER-36812","Improvement","Testing Infrastructure",2,"Log obvious details when resmoke observes killed processes","""In BF-10349, the shell crashed due to segfault, but the shell didn't print out stack trace on exit. Resmoke logged the test exited with -11. However there are 10 mongo shells, it's not clear which one crashed. It's also not clear that's the shell who crashed. We have core dumps in this case, which have sufficient stack trace for debugging. It will be great if the error message can indicate that core dump is available and which process the developer should look into.        Resmoke may also start mongods, I'm not sure if their exit error messages are clear. It would be great it's obvious who observed the crash and the error message from resmoke is consistent with that from the shell (e.g. {{ReplSetTest}}).  """
"SERVER-36819","Bug","Testing Infrastructure",1,"Enterprise RHEL 7.1 PPC64LE builder attempts to run concurrency_simultaneous_replication on rhel72-zseries-build distro","""This leads to system failures in Evergreen because we're attempting to run binaries compiled for PowerPC on a zSeries platform. This has come up not too long ago in SERVER-35416 that it might be audit the other {{concurrency*}} tasks which were added recently.            https://github.com/mongodb/mongo/blob/2bed54b084995f2c2dd048b6a70b6fd678e1ac30/etc/evergreen.yml#L11568-L11570"""
"SERVER-36817","Bug","Testing Infrastructure",1,"replSetFreeze command run by stepdown thread may fail when server is already primary","""As part of the changes to address SERVER-35383 and based on [this comment|https://jira.mongodb.org/browse/SERVER-35124?focusedCommentId=1916761&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-1916761] from SERVER-35124, the stepdown thread in resmoke.py runs the {{\{replSetFreeze: 0\}}} command to make the former primary electable in the next round of stepdowns. Since the primary is only stepped down [for 10 seconds (by default)|https://github.com/mongodb/mongo/blob/2bed54b084995f2c2dd048b6a70b6fd678e1ac30/buildscripts/resmokelib/testing/hooks/stepdown.py#L28], it is possible for enough time to have passed for the primary to have tried to step back up on its own before the {{\{replSetFreeze: 0\}}} command is run.    We either need to handle the {{OperationFailure: cannot freeze node when primary or running for election. state: Primary}} exception or prevent it from occurring."""
"SERVER-36816","Improvement","Testing Infrastructure",2,"Avoid reloading the view catalog on the primary and secondaries during the dbhash check","""The changes from SERVER-25640 made it so the {{listCollections}} command is run with a {{$in}} query containing the names of the collections returned by the {{dbHash}} command.    * [against the primary|https://github.com/mongodb/mongo/blob/2bed54b084995f2c2dd048b6a70b6fd678e1ac30/src/mongo/shell/replsettest.js#L1855-L1858]  * [against the secondary|https://github.com/mongodb/mongo/blob/2bed54b084995f2c2dd048b6a70b6fd678e1ac30/src/mongo/shell/replsettest.js#L1907-L1910]    The query leads to the view catalog being reloaded because [a very special filter must be used|https://github.com/mongodb/mongo/blob/2bed54b084995f2c2dd048b6a70b6fd678e1ac30/src/mongo/db/commands/list_collections_filter.cpp#L37-L42] to prevent that behavior. There is logic in the {{checkDBHashesForReplSet()}} function that's only enabled for the fuzzer test suites to skip [checking the dbhash when reloading the view catalog fails due to an invalid view definition|https://github.com/mongodb/mongo/blob/2bed54b084995f2c2dd048b6a70b6fd678e1ac30/src/mongo/shell/replsettest.js#L1860-L1865]; however, it seems more worthwhile to avoid reloading the view catalog as we've found that an {{InvalidNamespace}} error response may be returned for certain patterns involving null bytes.    We should instead use the very special filter to prevent the view catalog from being reloaded on the server during the {{listCollections}} command and do the actual filtering on the client-side.    """
"SERVER-36897","Bug","Testing Infrastructure",2,"OplogReader.hasNext can return false -> true, confusing `checkOplogs`","""{{`ReplSetTest.checkOplog`}} will establish a reverse table scanning cursor on the oplog from each node, then """"BFS"""" to compare the oplog for consistency. It will first get the latest entry in each oplog and perform a comparison, then advance all of the cursors and compare again. It allows the oplogs to have a different number of entries, so long as they match on the latest entries.    To do so, {{checkOplog}} relies on [{{OplogReader.hasNext}}|https://github.com/mongodb/mongo/blob/2145028db135b539c51713acad6952ef36e646cf/src/mongo/shell/replsettest.js#L2105] to always return false after it has done so for the first time. However, if the call that executes the query receives a {{CappedPositionLost}} (thus [not instantiating the shell's internal {{_cursor}}|https://github.com/mongodb/mongo/blob/2145028db135b539c51713acad6952ef36e646cf/src/mongo/shell/query.js#L112], a follow-up {{OplogReader.hasNext}} can return true, having re-issued the find and receiving a batch from the oplog starting at the latest entry."""
"SERVER-36960","Bug","Testing Infrastructure",2,"Stepdown thread should handle AutoReconnect exceptions when executing replSetStepUp","""The stepdown thread can terminate when issuing a {{replSetStepUp}} command, as  {{stepdown.py}} does not handle {{AutoReconnect}} [exceptions|https://github.com/mongodb/mongo/blob/2704d7a89e64167fcff7356ada111b313146474e/buildscripts/resmokelib/testing/hooks/stepdown.py#L319-L330]."""
"SERVER-36980","Improvement","Testing Infrastructure",1,"Remove old aggregation fuzzer from evergreen","""The aggregation fuzzer is being revamped. We should remove the previous aggregation fuzzer from evergreen."""
"SERVER-36976","Improvement","Testing Infrastructure",2,"Run new agg-fuzzer in evergreen","""The aggregation fuzzer evergreen task should be replaced with the new agg-fuzzer.Â """
"SERVER-37074","Bug","Testing Infrastructure",2,"Validation hook should continue downgrading if a downgrade was interrupted","""SERVER-36718 made the changes so that we can use {{forceValidationWithFeatureCompatibilityVersion}} to upgrade the servers before validating the collections. But BF-10462 is a case when a downgrade in the test was interrupted and there is no way for the validation hook to start the upgrade in the middle of a downgrade. Therefore, the validation hook should first downgrade and then upgrade in this case."""
"SERVER-37101","New Feature","Testing Infrastructure",3,"Add optimization mode aggregation (pipeline) fuzzer to evergreen","""Add run the aggregation (pipeline) fuzzer in optimization mode in evergreen.    This was previously attempted and the change can be seen here: https://evergreen.mongodb.com/version/5b9923622fbabe77fd9660b1"""
"SERVER-37120","Task","Performance",1,"Turn off linux-replSet-initialsync-logkeeper Build Variant for 3.4 and 3.6 branches","""These tests no longer run as the mongod versions are no longer compatible with FCV version in the snapshot."""
"SERVER-37143","Task","Testing Infrastructure",2,"Retry on Interrupted errors in the background DB hash hook","""Some tests kill all active sessions, which can include the session used by the background dbhash hook. This can cause the dbhash command to return an {{Interrupted}} error.    The dbhash hook should handle it in a similar way to transient transaction errors."""
"SERVER-37228","Bug","Testing Infrastructure",1,"Escape double quotes in hang analyzer's waitsfor graph","""Double quotes are not always escaped right now. See [this failure|https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_replica_sets_jscore_passthrough_patch_6818230171cb12727892802c608ba9247815ef06_5ba32958e3c331286f2645a2_18_09_20_05_00_33##%257B%2522compare%2522%253A%255B%257B%2522hash%2522%253A%25226818230171cb12727892802c608ba9247815ef06%2522%257D%252C%257B%2522hash%2522%253A%25226818230171cb12727892802c608ba9247815ef06%2522%257D%255D%257D] for an example.  """
"SERVER-37272","Task","Storage",5,"Disable hybrid index builds on FCV 4.0","""Because of the interaction hybrid index builds have with prepared transactions (see SERVER-38588), hybrid index builds are dependent on the two-phase behavior of simultaneous index builds, which will only be enabled in FCV 4.2. For this reason, hybrid index builds should be disabled unless a node is in FCV 4.2."""
"SERVER-37270","Task","Storage",20,"Remove foreground index build functionality","""This work will cause the background:true index option to be ignored, thus making all index builds run through the new hybrid index build path."""
"SERVER-37301","Task","Build",3,"Add Ubuntu 18.04 zSeries build variant","""Add community and enterprise builds for Ubuntu 18.04 zSeries"""
"SERVER-37289","Task","Testing Infrastructure",1,"Use authenticated client to run the refreshLogicalSessionCacheNow command in resmoke sharded cluster fixture","""To be able to use PyMongo 3.6+ with resmoke, we need to use an authenticated client to run the {{refreshLogicalSessionCacheNow}}. Since PyMongo 3.6+ will use implicit sessions, if the command is run unauthenticated then we hit SERVER-34820 and fail with a """"there are no users authenticated"""" message."""
"SERVER-37359","Improvement","Testing Infrastructure",3,"Update the test lifecycle script to use the new Evergreen test stats endpoint","""The [lifecycle_test_failures.py|https://github.com/mongodb/mongo/blob/82b62cf1e513657a0c35d757cf37eab0231ebc9b/buildscripts/lifecycle_test_failures.py] script should be updated to use the new Evergreen API endpoint for test execution statistics when it becomes available."""
"SERVER-37387","Bug","Testing Infrastructure",1,"mongo_lock.py graph displays lock request type for LockManager locks","""Changes for SERVER-34738 now display the lock requester's mode. We need to display the lock holder's mode as well.    We should change the following    to      Other references to {{lock_request[""""mode""""]}} should be modified as well, such that the mode is properly associated to holder."""
"SERVER-37373","Bug","Testing Infrastructure",2,"Fully qualified files in suite YML do not run in Burn_in tests on Windows","""The burn_in task does not run a fully specified file in the suite YML on Windows"""
"SERVER-37428","Bug","Performance",2,"Sys-perf: linux builds using enterprise bits","""Note: this only affects internal testing systems.Â     The system_perf.yml file support for enterprise modules is doing the wrong thing for non-enterprise builds. There are three compile tasks in one variant, but they all get the enterprise module. The fix seems to be making the enterprise build more like the wtdevelop build, by checking out the enterprise module to a directory outside the mongo source directory, and actively copying it in for the enterprise build as is done in """"use wiredtiger develop"""" function.Â """
"SERVER-37478","Improvement","Testing Infrastructure",1,"Run agg fuzzer more on Linux 64","""Run the fuzzer more on Linux 64 and use it for any fuzzer patch builds. This should allow us to catch more bugs found by new fuzzer improvements before they slip through into mainline evergreen."""
"SERVER-37467","Bug","Testing Infrastructure",1,"Have collect_resource_info.py recover from transient errors.","""Its output is useful! It's a missed opportunity when the output file doesn't contain all the data it could.    """
"SERVER-37490","Task","Testing Infrastructure",1,"Increase the ConnectTimeout for powercycle","""We should increase the {{ConnectTimeout}} used in powercycle tests from 10 to 30 seconds."""
"SERVER-37555","Bug","Catalog",3,"An abort of collMod need not refreshEntry for an index on rollback","""When a collMod operation aborts its WriteUnitOfWork, it need not call refreshEntry on rollback. An index catalog change via collMod already invoke refreshEntry. The refreshEntry itself should restore its state on rollback as itÂ registers an IndexRemoveChange to do that."""
"SERVER-37599","Improvement","Shell|Testing Infrastructure",1,"Log exit code of shell-spawned processes","""We should print {{res.exitCode}} [here|https://github.com/mongodb/mongo/blob/c6bceb292246721c5a0950e84d6b71ee1bc04bdf/src/mongo/shell/servers.js#L1254] when the shell fails to spawn a process to assist with debugging."""
"SERVER-37645","Task","Storage",5,"Add parsing for new index build fields in index catalog entries ","""Probably should upgrade the parsing to an IDL while we're at it. *_Update: or not, because that actually sounds like a potential black hole given how much we pass around BSON elsewhere in the index building layer._*    // Defaults to false in-memory if absent  runTwoPhaseIndexBuild: <bool>,    // Defaults to """"scanning"""" if absent. Can only be set to """"scanning"""", """"verifying"""" or """"committing""""  buildPhase: <string>,    // Defaults to 1 if absent.  versionOfBuild: <integer>,    // No default if absent. Should have a bool function to say whether it is present.  buildConstraintViolationsFile: <string>,    newWritesInterceptorTable: <string> (pick a field name that seems suitable for this)    Check in with the design before finalizing."""
"SERVER-37644","Task","Storage",5,"Make the createIndexes command join already in-progress index builds","""Depends on SERVER-37643 to move all index builds behind the index build interface established in SERVER-37636.    The createIndexes command should check whether the index(es) is already being built and wait upon it if so. A new waiting function must be added to the index build interface.    An appropriate error message should be returned if: commitQuorum does not match that of the in-progress index build; the indexes and specs do not match identically those in a single index builder.    Note that there can be multiple indexes with the same [key pattern but different collations|https://github.com/mongodb/mongo/blob/9f363b489585124afa1e26412e19f6728763e1ad/src/mongo/db/catalog/index_catalog_impl.cpp#L749-L768]Â (SERVER-24239)"""
"SERVER-37643","Task","Storage",20,"add createIndexes command logic to the index build interface","""The index builder interface established is established in SERVER-37636.    This ticket will add a Threadpool and move all the instances of MultiIndexBlock (index builder class) that are all over the place behind the interface and running on the Threadpool.    We should be able to register index builds via the interface and then wait upon a condition variable to hear back on the Status result.    Keep in mind SERVER-37644, which is to make index builds joinable via the createIndexes command. The condition variable setup must be such that we can have multiple waiters who can all hear back about the same result. Maybe an interface internal helper function to get something to wait upon for a Status result."""
"SERVER-37639","Task","Storage",13,"Add checkIfCommitQuorumIsSatisfied() to the replication interface to check whether a given commit quorum is satisfied by a given set of commit ready members.","""A simultaneous index build will have a commitQuorum setting, set by the user via createIndexes, which will dictate how many members of the replica set must be ready for commit before the primary will commit the index. Each index build will track which members are ready and must check whether the commitQuorum is satisfied.     commitQuorum is the same type and takes the same settings as writeConcern.w: an integer number reflecting a number of replica set members; majority; or a replica tag set.    The function should take a list of host:port pairs, which we are using to uniquely identify replica set members, along with the commitQuorum. It should return whether or not quorum is satisfied, leveraging the writeConcern checking machinery if we can, and/or the topology coordinator's member config; and probably error if the quorum can never be satisfied."""
"SERVER-37678","Task","Testing Infrastructure",2,"Update linter to enforce SSPL in header files","""SERVER-37651 changed the license from AGPL to SSPL; would be nice if the linter enforced the new license in new files automatically."""
"SERVER-37668","Task","Testing Infrastructure",1,"Disable the aggregation fuzzer on Windows 2008R2 DEBUG","""It is failing due to issues such as SERVER-37429 and others that still need to be investigated."""
"SERVER-37664","New Feature","Testing Infrastructure",5,"Add support for doing resmoke.py process management through jasper","""https://github.com/mongodb/jasper is a library for doing process management through commands over a socket. Having process management available as a service means (1) we can consolidate the various implementations we have through the {{subprocess}} / {{subprocess32}} Python packages and the mongo shell's {{shell_utils_launcher.cpp}} C++ code, and (2) we can allow tests to interact with the cluster in a potentially destructive way. #2 enables tools such as genny to be able to run performance workloads that measure the latency of operations after restarting a mongod or mongos process.    MAKE-497 exposed jasper through the {{curator}} binary."""
"SERVER-37663","New Feature","Performance|Testing Infrastructure",3,"Add support for running genny via resmoke.py locally","""To aid the local development experience of writing new performance workloads, we should add a new """"test kind"""" for running genny tests through resmoke.py. The latency and other metrics collected won't be particularly interesting because all of the processes will be running on the same machine, but we'll be able to ensure the mechanics of the new performance workloads are sound before submitting them to Evergreen and running on a distributed cluster.    We should have resmoke.py YAML suite file configurations for  * stand-alone mongod {{MongoDFixture}}  * 1-node replica set ({{ReplicaSetFixture}} with {{num_nodes=1}})  * 3-node replica set ({{ReplicaSetFixture}} with {{num_nodes=3}} and {{all_nodes_electable=true}})  * sharded cluster (({{ShardedClusterFixture}} with {{configsvr_options.num_nodes=3}}, {{num_mongos=3}}, {{num_shards=3}}, and {{num_rs_nodes_per_shard=3}})    in order to match the configurations the {{genny_workloads}} Evergreen task runs in as part of the dsi Evergreen project.    *Note*: There isn't a need to wire up genny's output format and resmoke.py's {{\-\-perfReportFile}} because this mode is only intended for local development and not for running in Evergreen."""
"SERVER-37694","Bug","Storage",1,"Coverity analysis defect 105088: Redundant test","""Test always evaluates the same  Defect 105088 (STATIC_C)   Checker DEADCODE (subcategory redundant_test)   File:  {{/src/mongo/db/storage/biggie/store.h}}   Function {{mongo::biggie::RadixStore<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>::_merge3Helper(mongo::biggie::RadixStore<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>::Node *, const mongo::biggie::RadixStore<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>::Node *, const mongo::biggie::RadixStore<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>::Node *, std::vector<mongo::biggie::RadixStore<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>::Node *, std::allocator<mongo::biggie::RadixStore<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char>>>::Node *>> &, std::vector<unsigned char, std::allocator<unsigned char>>&)}}     /src/mongo/db/storage/biggie/store.h, line: 1337     {color:red}At condition """"baseNode"""", the value of """"baseNode"""" cannot be """"NULL"""".{color}  {code:first-line=1337}                     } else if (baseNode && (!otherNode || (otherNode && baseNode != otherNode))) {      /src/mongo/db/storage/biggie/store.h, line: 1337     {color:red}The condition """"baseNode"""" must be true.{color}  {code:first-line=1337}                     } else if (baseNode && (!otherNode || (otherNode && baseNode != otherNode))) { {code}  """
"SERVER-37789","Improvement","Testing Infrastructure",1,"Add --genny flag to resmoke.py","""I'd like to be able to specify a genny executable without modifying a committed file. People shouldn't need to use a locally compiled version of genny and they shouldn't need to stash between git branch operations."""
"SERVER-37778","Task","Build",3,"Platform Support: Add Community & Enterprise Ubuntu 18.04  (zSeries)","""Platform Support: Add Community & Enterprise Ubuntu 18.04  (zSeries)"""
"SERVER-37772","Task","Build",3,"Platform Support: Add Community & Enterprise RHEL 8 x64","""Platform Support: Add Community & Enterprise RHEL 8 x64  """
"SERVER-37771","Task","Build",3,"Platform Support: Add Enterprise Ubuntu 18.04 PPCLE","""Platform Support: Add Enterprise Ubuntu 18.04 PPCLE"""
"SERVER-37770","Task","Build",5,"Platform Support: Add Community and Enterprise Ubuntu 18.04 ARM64","""Platform Support: Add Community Ubuntu 18.04 ARM64"""
"SERVER-37769","Task","Build",3,"Platform Support: Add Community & Enterprise SLES 15 x64","""Platform Support: Add Community & Enterprise SLES 15 x64"""
"SERVER-37768","Task","Build",3,"Platform Support: Add Community & Enterprise Debian 10 x64","""Platform Support: Add Community & Enterprise Debian 10 x64  """
"SERVER-37767","Task","Build",2,"Platform Support: Remove Debian 8 x64","""Platform Support: Remove Debian 8 x64  - Only in 4.2, not in previous releases: weâll apply the latest/latest-1 policy for new releases  - For existing MongoDB releases weâll follow the vendor  """
"SERVER-37926","Bug","Storage",1,"Excess allocated memory associated with cursors due to WT ""modify"" operations","""In SERVER-37795 we observed excess allocated memory associated with cursors due to WT """"modify"""" operations in 3.6.8. This should not occur in 4.0 because improved cursor caching should limit the number of cursors, but we still see excess allocated memory:    !comparison.png|width=100%!    The excess memory in 4.0.3 is more than double that of 3.6.8. Note that the rate of updates is somewhat higher in 4.0.3 than 3.6.8 (probably SERVER-36221), but that doesn't appear to be enough larger to account for the >2x larger excess memory allocated in 4.0.3.    Possibly related: """"cached cursor count"""" seems to grow indefinitely, although we only have that information for the oplog collection and not globally.    Notes:  * Verified that the memory is associated with cursors by disabling cursor caching in 3.6.8 and the excess memory disappeared.  * Uesed the heap profiler to confirmed that the excess allocated memory is coming from the same allocation sites in 4.0.3 as 3.6.8.    Repro:      """
"SERVER-37940","Task","Storage",0,"Establish a code path for FCV 4.0 and FCV 4.2 index builds","""Index builds started in FCV 4.0 will use continue to use the v4.0 binary index build oplog entry """"createIndexes"""". Index builds started in FCV 4.2 will use the new oplog entries for simultaneous index builds.    However, that's a farther out goal. The current need is to establish two code paths, unaffected by FCV for the moment, that can be used to incrementally add the new replication logic for simultaneous index builds."""
"SERVER-38116","Improvement","Testing Infrastructure",3,"Update fuzzer tasks to use generate.tasks","""Update the fuzzer tasks in 'etc/evergreen.yml' to use generate.tasks and be split up dynamically. The number of tasks generated and the number of files generated in each task should be configurable."""
"SERVER-38115","Improvement","Testing Infrastructure",3,"Consolidate setting of resmoke.py --job to a python script","""Create """"buildscripts/evergreen_resmoke_job_count.py"""" that will determine the number of jobs to pass to resmoke.py. This should be similar to the existing """"buildscripts/evergreen_task_timeout.py"""" script. The """"run tests"""" function in evergreen.yml should be updated to use the new script."""
"SERVER-38114","Improvement","Testing Infrastructure",2,"Update evergreen.yml to use generate tasks for already split tasks","""Update evergreen.yml to use the """"generate resmoke sub-suites"""" function for tasks that have previously been split. The names of the tasks updated should be appended with a """"_gen"""" so that the display task name can use the original name."""
"SERVER-38113","Improvement","Testing Infrastructure",2,"Generate evergreen config for generated sub-suites","""Update """"generate_resmoke_suite.py"""" to generate an evergreen configuration file that describes the sub-suites to run. This should also generate the display task configuration for all the sub-suites."""
"SERVER-38112","Improvement","Testing Infrastructure",2,"Add ""generate resmoke sub-suites"" in etc/evergreen.yml","""In order to dynamically generate tasks for sub-suites, evergreen needs a function, """"generate resmoke sub-suites"""" that calls """"generate_resmoke_suite.py"""" (which should be changed to generate_resmoke_sub_suites.py) with the appropriate arguments and then calls generate.tasks on the file that was created.    This new function should be usable by any current task that is running resmoke tests."""
"SERVER-38111","Improvement","Testing Infrastructure",2,"Implement max_sub_suites in generate_resmoke_suite.py","""The 'generate_resmoke_suite.py' file will divide all the tests in a suite into sub-suite based on their runtime and a target max runtime. There may be cases where we want to limit the number of sub-suites it should create. This work is to add an optional command line argument that allows the caller to specify the max number of sub-suites and implement logic to limit the number of sub-suites when that option is specified.     Note: If the limit is hit, it is acceptable to random assign the remaining tests into sub-suites (i.e. we don't need to optimally pack the extra suites based on runtime)."""
"SERVER-38110","Improvement","Testing Infrastructure",2,"Generate resmoke config YAML for a sub-suite","""When dividing a suite into sub-suites, the 'generate_resmoke_suite.py' file needs to create a resmoke yaml configuration file for each sub-suite that is being created. To generate this file, the original suite yaml should be read, the selector section should be updated to include the chosen test files for the sub-suite, and the new configuration should be written to disk."""
"SERVER-38109","Improvement","Testing Infrastructure",2,"Update generate_resmoke_suite.py to use cached history endpoint","""In PM-1158, we started to cache evergreen test history data in order to provide faster access. The generate_resmoke_suite.py should be updated to use the new endpoint to get the last 2 weeks of data instead of the 'test_history' endpoint that it is currently using."""
"SERVER-38140","Task","Storage",8,"Only acquire a Database IX lock for replicated collection drops","""Temporary collections are dropped on step up and state transitions cannot take locks that conflict with prepared transactions."""
"SERVER-38182","Bug","Testing Infrastructure",2,"run_check_repl_dbhash_background aborts all transactions after an error even if they've already successfully committed ","""The dbHash command is run inside of a transaction in run_check_repl_dbhash_background.js on each node in the test's replica set. When committing the transactions, we will [iterate over each session (one per node) and call {{commitTransaction}}|https://github.com/mongodb/mongo/blob/410656e971aff8f491a87337a17d04bd866389ba/jstests/hooks/run_check_repl_dbhash_background.js#L236-L245]. If any of these calls throws an error, we [catch the error and iterate over each session to try to abort each transaction|https://github.com/mongodb/mongo/blob/410656e971aff8f491a87337a17d04bd866389ba/jstests/hooks/run_check_repl_dbhash_background.js#L251-L257]. But if any transaction was already successfully committed, trying to abort it errors with:      We should only try to abort transactions that are still active."""
"SERVER-38211","Improvement","Storage",8,"Optimise duplicate check algorithm for timestamp safe unique index","""Timestamp safe unique index for 4.2 incorporates a duplicate key check/prevention algorithm. Older Unique index used to prevent duplicate keys in a different way. We put this algorithm to be timestamp safe.    Linkbench (LOAD_NODE_BULK) shows a performance penalty due to this check. This ticket tracks the effort in optimising the algorithm and get the performance back."""
"SERVER-38225","Bug","Storage",8,"Implement constant time fast count for biggie SE","""Currently the numRecords RecordStore method takes time proportional to the number of documents, making test runs extremely slow."""
"SERVER-38305","Improvement","Build|Performance",1,"Move all sys-perf compile tasks over to Amazon Linux 2","""As part of the {{etc/system_perf.yml}} project configuration, an enterprise version of the server is built on Amazon Linux 2. Now that EVG-727 / EVG-5379 has been addressed, we can go ahead and make all sys-perf compile tasks include the enterprise module everywhere. The outcome of this ticket should involve having only two build variants:    # Compile on Amazon Linux 2  # WT Develop Compile on Amazon Linux 2    *Note*: We're currently running all of the sys-perf workloads on Amazon Linux 2 AMIs."""
"SERVER-38292","Bug","WiredTiger",0,"mongodb crash with Got signal: 11 (Segmentation fault)","""We are also having this issue both on 3.2.19 and 3.4.14 (both are WT engine). Here's the stack from 3.4.14 logs. Can you brief me on what exactly is the fix in WT-4037. It is not clear in that ticket.      Â     Please note that we are in the process of generating a core dump and will attach here when the mongod process crashes again."""
"SERVER-38323","Task","Storage",5,"Create an index builds interface for the embedded version of the server","""The IndexBuildsCoordinator is for managing index builds across a replica set. It links in networking and has a ThreadPool on which to run index builds.    The embedded server cannot link to networking code, nor can it use ThreadPools. Therefore, we must make a simple class that builds indexes via the IndexBuildsManager. There is no need for asynchronous threads on an embedded server, which is effectively a standalone without networking or asynchronously running threads.    I haven't looked into whether it is possible to just make a separate class to link into embedded, or inheritance is necessary, necessitating splitting the existing IndexBuildsCoordinator into and interface and implementation. I'd guess inheritance is necessary, since commands in the standalone library are probably all included in the greater repl inclusive libraries? Shims are also a potential tool, I haven't explored that idea, either."""
"SERVER-38312","Bug","Testing Infrastructure",1,"Unable to run jstestfuzz* tasks on variants with long names","""[The fuzzer's self-tests run before the {{jstestfuzz*}} tasks and writes the output to a filename that includes the task id|https://github.com/mongodb/mongo/blob/01d25f74348e8594e96a9e01dfca60538677d078/etc/evergreen.yml#L1922]. After the changes from [18452b9|https://github.com/mongodb/mongo/commit/18452b94f497e4f25493477ee45e759d449871e9] as part of SERVER-38116, this leads too long of a filename.        https://evergreen.mongodb.com/task_log_raw/mongodb_mongo_master_enterprise_rhel_62_64_bit_majority_read_concern_off_jstestfuzz_interrupt_replication_0_enterprise_rhel_62_64_bit_majority_read_concern_off_patch_41c44d02cf39ef581888bed68c547e4ed9b5a323_5bff8490e3c33123cb9e7dfa_18_11_29_06_19_05/0?type=T    """
"SERVER-38336","Bug","Storage",1,"Coverity analysis defect 105145: Copy without assign","""Class has user-written copy constructor but no user-written assignment operator  Defect 105145 (STATIC_C)   Checker COPY_WITHOUT_ASSIGN (subcategory none)   File:  {{/src/mongo/db/storage/kv/temporary_kv_record_store.h}}   Parse Warning (no function name available)     /src/mongo/db/storage/kv/temporary_kv_record_store.h, line: 46     {color:red}Class """"mongo::TemporaryKVRecordStore"""" has a user-written copy constructor """"mongo::TemporaryKVRecordStore::TemporaryKVRecordStore(mongo::TemporaryKVRecordStore &&)"""" but no corresponding user-written assignment operator.{color}  {code:first-line=46}     class TemporaryKVRecordStore : public TemporaryRecordStore {   """
"SERVER-38396","Task","Storage",8,"Improve the IndexBuildsCoordinator unit testing after it was made into an interface with two implementations","""SERVER-38323 added an embedded implementation, and the original unit test only tests the original implementation. This task is to cover both implementations with unit testing."""
"SERVER-38395","Task","Testing Infrastructure",1,"Python global logger is polluted when importing certain resmokelib modules","""It was discovered that the global {{logging}}Â in {{buildscripts/update_test_lifecycle.py}} gets overriden by {{buildscripts/mobile/adb_monitor.py}} because of the resmokelib imports.    The file {{buildscripts/mobile/benchrun_embedded_setup_android.py}} also has similar issue.    The {{logging}} setting should not be done in the global scope but in the {{main}} function:  """
"SERVER-38478","Task","Concurrency",8,"Remove UninterruptibleLockGuard in query yield","""restoreLockState() is used by [query yielding|https://github.com/mongodb/mongo/blob/a66a5578d5b006cef85b16eac05c96b58c877ebe/src/mongo/db/query/query_yield.cpp#L92] and [transaction reaper|https://github.com/mongodb/mongo/blob/a66a5578d5b006cef85b16eac05c96b58c877ebe/src/mongo/db/transaction_reaper.cpp#L165]. To make sure they don't conflict with prepared transactions on stepdown and shutdown, we need to guarantee they only restore IS or IX locks or they restore locks that won't conflict with transactions."""
"SERVER-38477","Task","Storage",13,"Index build lock acquisitions should be interruptible","""Index build avoids interruptions in several places, especially for background index build. They will conflict with prepared transactions on stepdown and shutdown. We can either make index build interruptible, or use IX or IS locks instead of X or S locks.    Here's a list of all occurrences of UninterruptibleLockGuard for index build.  [src/mongo/db/catalog/multi_index_block_impl.cpp:156|https://github.com/mongodb/mongo/blob/86fab3ee0e1570c6743b314fddc0af418bba9015/src/mongo/db/catalog/multi_index_block_impl.cpp#L156]  [src/mongo/db/commands/create_indexes.cpp:325|https://github.com/mongodb/mongo/blob/86fab3ee0e1570c6743b314fddc0af418bba9015/src/mongo/db/commands/create_indexes.cpp#L325]  [src/mongo/db/index_builder.cpp:195|https://github.com/mongodb/mongo/blob/86fab3ee0e1570c6743b314fddc0af418bba9015/src/mongo/db/index_builder.cpp#L195]  [src/mongo/db/index_builder.cpp:299|https://github.com/mongodb/mongo/blob/86fab3ee0e1570c6743b314fddc0af418bba9015/src/mongo/db/index_builder.cpp#L299]  """
"SERVER-38509","Improvement","Testing Infrastructure",2,"Handle degraded mode for test history in generate_resmoke_suites","""Evergreen is going to implement a """"degraded"""" mode if it cannot respond to test history queries due to load. In that mode, queries to the test history will return HTTP 503 (See https://jira.mongodb.org/browse/EVG-5633). We should detect this condition when attempting to split up test suites and divide up the suites randomly (we may want to define an expansion on the project of how much to divide up in this situation, that would allow us to change the value without needing to commit new code)."""
"SERVER-38532","Improvement","Indexing|Logging",1,"Add index ns and name to ""build index done"" log line","""A lack of final log details make it hard to reconstruct index build timelines from {{mongod}} logs using {{grep}} etc.    When a build starts we get a log like:    but when it ends, only:      Can we add at least the {{ns}} and index name to the """"build index done"""" so that we can use grep for analysis?  """
"SERVER-38531","Improvement","Testing Infrastructure",3,"Increase parallelism in test lifecycle update script and perform more rollups server-side","""The {{update_test_lifecycle.py}} script only parallelizes the requests across tasks for a given batch of tests.    We should be able to parallelize all requests to improve the overall run time.    Additionally we can take advantage of the {{group_num_days}} parameters of the Evergreen API to fetch stats results already aggregated for the reliable and unreliable periods."""
"SERVER-38562","Task","Storage",5,"Implement IndexBuildsCoordinator::voteCommitIndexBuilds","""Consider moving the [Client::setLastOpToSystemLastOpTime|https://github.com/mongodb/mongo/blob/597b4748fc36210c61cf4d6c086d364013df740a/src/mongo/db/commands/vote_commit_index_builds_command.cpp#L77-L80] logic into the function logic.    A flag must be set in-memory on the index build whether or not to proceed without voting or end the thread after voting successfully or finding the flag set after the fact. commitIndexBuild, if the build has already reached the 'committing' phase, will set the flag and start a new asynchronous thread for the commit; else, the in-memory flag will be set such that the index build discovers it later and bypasses voting, proceeding straight to commit. This is necessary because stalling commit, and thereby stalling replication on the secondary, cannot be permitted to take as long as a network call can potentially take -- a matter of seconds, presumably. The alternative would be to make the voteCommitIndexBuild command sent by the secondary have a short enough timeout that we don't mind stalling replication for that amount of time: but this is risky given that determining a reasonable time for all network and replication latencies might be impossible.    The index build thread would exist after finishing voting, as opposed to waiting on a condition variable for the commitIndexBuild signal, as ReplIndexBuildState is currently expecting with its condition variable already set up and waiting to be used -- need to change that.    The flag must also be initialized correctly on index build recovery, depending on the persisted state."""
"SERVER-38615","Bug","Testing Infrastructure",1,"The psutil module should be installed on all platforms","""We are now using {{psutil}} in {{buildscripts/tests/test_evergreen_resmoke_job_count.py}} and this runs on every platform (see SERVER-38115)."""
"SERVER-38589","Bug","Packaging",2,"service mongod stop may produce No /usr/bin/mongod found running; none killed.","""A fix should be applied to https://github.com/mongodb/mongo/blob/6c8dc5e004bf2c91df10975adef861bcf00af6cd/debian/init.d to prevent this error when stopping mongod."""
"SERVER-38667","Task","Storage",5,"Notify IndexBuildsCoordinator of replica set member stepup and stepdown","""Add step-up and step-down hooks for the IndexBuildsCoordinator.     There's a field on the mongod implementation that indicates primary/secondary state https://github.com/mongodb/mongo/blob/5eca4a77da863bd4e68bf4eb7c2d0c920982f8b9/src/mongo/db/index_builds_coordinator_mongod.h#L137    And there are interface functions for setting primary and secondary, which look like they've been implemented in the mongod and embedded already. https://github.com/mongodb/mongo/blob/5eca4a77da863bd4e68bf4eb7c2d0c920982f8b9/src/mongo/db/index_builds_coordinator.h#L197-L198    We'll need to call the state change functions in here https://github.com/mongodb/mongo/blob/4d09b2e0a605aefd7adefda28e01e309bbf30883/src/mongo/db/repl/replication_coordinator_external_state_impl.cpp#L483 and it looks like stepdown hooks go in here maybe https://github.com/mongodb/mongo/blob/4d09b2e0a605aefd7adefda28e01e309bbf30883/src/mongo/db/repl/replication_coordinator_impl.cpp#L2800 -- check in with someone from repl to find out their preferences for new hook additions."""
"SERVER-38710","New Feature","Testing Infrastructure",2,"Support dependencies when generating evergreen tasks","""Support depends_on and requires when generating tasks"""
"SERVER-38749","Bug","Testing Infrastructure",1,"Concurrent stepdown suites on 3.6 branch still use 5-second election timeout","""The changes from [3aa3155|https://github.com/mongodb/mongo/commit/3aa315557bef775c5291068e365a59a3a810fc41] as part of SERVER-30642 were ineffective at increasing the election timeout for the {{concurrency_sharded_with_stepdowns*.yml}} test suites because the JavaScript version of the stepdown thread reconfigures the replica set and sets a 5 second election timeout by default. We should additionally additional set {{electionTimeoutMS=1 day}} as part of the stepdown options specified to [the {{ContinuousStepdown.configure()}} function|https://github.com/mongodb/mongo/blob/r3.6.9/jstests/concurrency/fsm_libs/cluster.js].    *Note*: This is no longer an issue for the 4.0 or master branches because they've switched to using the Python version of the stepdown thread, which doesn't reconfigure the replica set."""
"SERVER-38779","Improvement","Storage",8,"Build a mechanism to periodically cleanup old WT sessions from session cache","""The way session cache is maintained, idle sessions keep accumulating in the session cache.  If the workload doesn't use all the idle sessions, the oldest sessions stay open forever. In some cases these sessions might hold some resources inside WiredTiger, which can cause problems. eg: dhandles that never close in WiredTiger.    This ticket is to build a mechanism around the session cache, to cleanup old sessions that have been idle for too long.    More details in the linked tickets."""
"SERVER-38822","Bug","Testing Infrastructure",1,"Linux Repeated Execution variant does not repeat the tests","""The change for SERVER-36613 introduced a second definition of the {{test_flags}} expansions for {{linux-64-repeated-execution}} buildvariant:      The second definition overrides the first."""
"SERVER-38818","Improvement","Testing Infrastructure",3,"Better handle dependencies between generated tasks","""With generated tasks, dependencies between tasks gets complicated. The dependencies should exist on the generator tasks and when generating a task should query the evergreen api to determine which dependent tasks were generated and add all of them as dependencies on the tasks being created."""
"SERVER-38817","Improvement","Testing Infrastructure",3,"Use generate.tasks on all resmoke tasks","""Migrate any resmoke tasks that are not using generate.tasks to use generate.tasks."""
"SERVER-38816","Improvement","Testing Infrastructure",2,"Use generate.tasks for required tasks over target runtime","""Apply generate.tasks to resmoke tasks on required builders that have an average runtime greater than the target runtimes.     The target runtimes are:  * RHEL 6.2: 10 minutes  * Enterprise Windows 2008R2: 20 mins  * Linux DEBUG: 15 mins"""
"SERVER-38886","Improvement","Storage",1,"refactor RecordStore::validate implementations","""All storage engines currently have one that's almost identical. The storage-engine independent iteration should be factored out, and only the storage-engine specific part should be left. There also is an implementation in {{RecordStoreValidateAdaptor::traverseRecordStore}}."""
"SERVER-38931","Task","Storage|Testing Infrastructure",3,"Apply relevant changes to snapshot_read_kill_operations.js to 4.0 branch","""SERVER-37009 fixed an error in the [{{snapshot_read_kill_operations.js}}|https://github.com/mongodb/mongo/blob/0ffb6bc78dc1219692b294215c97d48a7e9f1fdd/jstests/concurrency/fsm_workloads/snapshot_read_kill_operations.js] test that caused the {{killSessions}} portion of the test to silently fail to kill the session if the corresponding session document did not exist. This fix uncovered other issues with the test that have been fixed in master but not backported to 4.0. All relevant fixes to this test, including the fix in SERVER-37009, should be applied to the 4.0 version of this test.    One possible way to do this would be to replace the 4.0 version of the test with the corresponding version in master, being mindful of any changes made to the test that are specific to master."""
"SERVER-38927","Task","Catalog|Storage",3,"Cache collection 'temp' status on Collection object","""We cache things like validators and 'capped' status on the Collection object so we do not need to go all the way to storage and open up a storage transaction every time we want to check the value.     As far as I know, 'temp' cannot change with 'collMod', but it can with 'renameCollection', and on rollback.    This is needed for SERVER-38139 to ban temporary collections in transactions without having to consult the storage engine an extra time for every transaction statement."""
"SERVER-39007","Task","Testing Infrastructure",1,"Switch to use rhel62-large distro for concurrency* tasks on Enterprise RHEL 6.2 (InMemory) builder","""We're seeing OOM failures with the InMemory storage engine [occur consistently|https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_inmem_concurrency_replication_2f67f3c66271e724b48afa2db88e8b6c3317f6ab_19_01_11_18_02_54] after the changes from [2f67f3c|https://github.com/mongodb/mongo/commit/2f67f3c66271e724b48afa2db88e8b6c3317f6ab] as part of SERVER-33161. Changing to the {{rhel62\-large}} distro for the {{concurrency*}} tasks on the Enterprise RHEL 6.2 (InMemory) builder is a stopgap for getting the build back to being green until whether the increased memory consumption can be declared as """"expected""""."""
"SERVER-39004","Improvement","WiredTiger",5,"Introduce a quota mechanism for the overflow file","""We don't currently have a quota mechanism to prevent {{WiredTigerLAS.wt}} from growing and eventually running out of disk space.    It would help to have such a configuration in place so that once a file reaches a configured size, we reboot mongod process, which will effectively clean up the {{WiredTigerLAS.wt}} file"""
"SERVER-39068","Task","Storage",8,"Replication of simultaneous index builds startIndexBuild and commitIndexBuild oplog entries","""A temporary command, twoPhaseCreateIndexes, already exists. SERVER-39066 sets up the OpObserver and oplog.cpp. Wait for SERVER-37643 to set up builders in the IndexBuildsCoordinator/Manager.    Then, set up a code path into the Coordinator/Manager that will do a two phase index build, and have the {{twoPhaseCreateIndexes}} command call it. The [{{twoPhaseIndexBuild}}|https://github.com/mongodb/mongo/blob/e990d25622d96897d78e72b362db61f2a4f9d99c/src/mongo/db/repl_index_build_state.h#L88] flag in the {{ReplIndexBuildState}} object should be set. A startIndexBuild oplog entry should optionally (based on the Coordinator's twoPhaseIndexBuild setting) be written in the same WUOW as the index catalog entry initialization write: this should parallel the oplog write on commit seen [here|https://github.com/mongodb/mongo/blob/e990d25622d96897d78e72b362db61f2a4f9d99c/src/mongo/db/catalog/multi_index_block.cpp#L664-L666] and [here|https://github.com/mongodb/mongo/blob/e990d25622d96897d78e72b362db61f2a4f9d99c/src/mongo/db/commands/create_indexes.cpp#L402-L406]. The {{startIndexBuild}} oplog entry should start an index build, which I think is [already hooked up|https://github.com/mongodb/mongo/blob/e990d25622d96897d78e72b362db61f2a4f9d99c/src/mongo/db/repl/oplog.cpp#L280-L292], just inactive and not tested. The commitIndexBuild oplog entry should optionally be swapped out with the createIndexes oplog entry currently written on index commit, based on the {{twoPhaseCreateIndexes}} setting. Secondaries don't do anything on receipt of commitIndexBuild, and we will leave that to implement in a separate patch."""
"SERVER-39064","Task","Storage",3,"Storage interface changes for specifying durable_timestamp","""The storage interface must allow specifying a {{durable_timestamp}}Â when committing a prepared transaction."""
"SERVER-39106","Bug","Replication|Storage",3,"GlobalLock acquisition should throw when ticket acquisition times out if there is a max lock timeout and no deadline","""If global lock acquisition times out acquiring a ticket, then the constructor will not throw, but the resource will be unlocked. This leads to invariant failure when we attempt to acquire the global lock [here|https://github.com/mongodb/mongo/blob/89c3502129303b41b8d35bf5d64eb0a242f061da/src/mongo/db/transaction_participant.cpp#L788] then call {{canAcceptWritesForDatabase()}} [here|https://github.com/mongodb/mongo/blob/89c3502129303b41b8d35bf5d64eb0a242f061da/src/mongo/db/transaction_participant.cpp#L800], which invariants that the lock is held [here|https://github.com/mongodb/mongo/blob/89c3502129303b41b8d35bf5d64eb0a242f061da/src/mongo/db/repl/replication_coordinator_impl.cpp#L1947].    If the caller did not provide a deadline, then they are not checking for lock acquisition failure, so the lock acquisition should throw. Consider applying the following patch:  """
"SERVER-39094","Task","Testing Infrastructure",1,"Update jasper_process.py in resmoke to reflect Jasper RPC changes","""The current implementation for [{{jasper_process.py}}|https://github.com/mongodb/mongo/blob/master/buildscripts/resmokelib/core/jasper_process.py] has [some lines of code|https://github.com/mongodb/mongo/blob/master/buildscripts/resmokelib/core/jasper_process.py#L63-L67] for handling RPC errors from Jasper when it does things like signal a process that has already terminated somehow.    This is not reflective of how Jasper actually handles such errors after the completion of MAKE-525, and therefore should be changed to check for these conditions in the [{{val}} case|https://github.com/mongodb/mongo/blob/master/buildscripts/resmokelib/core/jasper_process.py#L59-L62].    Not doing this causes a test failure in {{replica_sets_kill_secondaries_jscore_passthrough}}, in which resmoke exhibits inconsistent behavior where it sends {{SIGKILL}}/{{SIGTERM}} to processes, and then expects the same processes to be alive at a later time. The full description of this problem can be found at [this comment|https://jira.mongodb.org/browse/MAKE-523?focusedCommentId=2117281&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-2117281]."""
"SERVER-39087","Task","Storage",13,"move initial sync index creation logic into IndexBuildsCoordinator","""Index creation during initial sync should be delegated to the IndexBuildsCoordinator"""
"SERVER-39086","Task","Storage",8,"Move startup recovery index creation logic into IndexBuildsCoordinator","""Index creation during startup recovery should be delegated to the IndexBuildsCoordinator"""
"SERVER-39085","Task","Storage",8,"move secondary oplog application logic for index creation into IndexBuildsCoordinator","""During secondary oplog application, we should delegate index creation to the IndexBuildsCoordinator."""
"SERVER-39127","Improvement","Testing Infrastructure",2,"Use generate.tasks for already converted tasks on all variants","""Several tasks already use generate.tasks on required builders. These should be switched to use generate.tasks on all builders and the non-generate.tasks tasks removed."""
"SERVER-39239","Task","Storage",5,"Two-phase index builds on secondaries will wait for the commitIndexBuild oplog entry before committing.","""The secondary's index build thread should spin in a loop before finishing up and committing the index.    The secondary will do nothing while spinning in a loop. It should drop all locks. SERVER-39458 will add functionality to the loop, of periodically reacquiring the lock and running side table draining.    On receipt of the commitIndexBuild oplog entry, the secondary should signal the index build to commit with a timestamp passed in the oplog entry. The oplog applier thread will need to drop locks to allow the index build to take the X lock for index commit -- I believe this should be safe, since 'c' oplog entries are applied serially, and so dropping the lock is OK. IndexBuildsCoordinator::commitIndexBuild will need to fetch a Future from the index build to return to the oplog applier thread to wait upon for commit completion.    SERVER-39533 will be done after this ticket and will hook up the abortIndexBuild oplog entry so that alternatively the index build on the secondary can be aborted while it's spinning in that loop waiting for commit. In case that's a design consideration.    Do not try to act on abort/commit signals earlier in the index build on secondaries than that spinning loop that waits for commit/abort. This is the simple first iteration implementation, we'll make it fancier in a subsequent ticket."""
"SERVER-39225","Task","Storage",3,"Update kill_rooted_or.js / IndexBuildsCoordinator to gracefully handle index build already in-progress errors","""""""The Coordinator registers each index spec by name that comes in, before the indexes are set up in the index catalog as in-progress. Then subsequent requests with the same index names hit an error in the Coordinator. Without the Coordinator, spec requests are normally automatically filtered out if they're already found to be built or building via the index catalog.    The concurrency test has 10 threads, potentially all running the same createIndexes requests at the same time, so """"There's already an index with name..."""" errors make sense -- previously they'd get filtered out and the redundant commands return OK.    Perhaps a new error code IndexBuildAlreadyInProgressForName to fix tests, and we can consider the final behavior changes later, and easily identify the changes by error code if we want to undo them.  This is related to our problem regarding spec checking being scattered all over the place at different levels of the code."""""""
"SERVER-39224","Task","Storage",5,"Explore why queryoptimizer3.js fails using the IndexBuildsCoordinator, then fix it","""""""So that test has one thread repeatedly recreate a collection with indexes, bulk insert a bunch of docs and do a single index table scan. And a second thread drop the collection repeatedly, to check that index table scan cursors get aborted on dropCollection."""""""
"SERVER-39279","Bug","Internal Code",2,"Race between ServiceContext::ClientDeleter and waitForClientsToFinish()","""{{ServiceContext::ClientDeleter::operator()(Client* client)}} accesses the serviceContext outside the serviceContext's mutex to call the registered observers on {{onDestroy()}}, while the serviceContext may have already been destroyed by [service_context_test_fixture|https://github.com/mongodb/mongo/blob/e12dcc7fdbdb44fb7806dfb42a49bd740f361d82/src/mongo/db/service_context_test_fixture.cpp#L52], since the client vector is empty, checked by [waitForClientsToFinish|https://github.com/mongodb/mongo/blob/cdf319123d8e5d3cd169e2a11aec6aea0b951bf1/src/mongo/db/service_context.cpp#L344].  {code:c++}  void ServiceContext::ClientDeleter::operator()(Client* client) const {      ServiceContext* const service = client->getServiceContext();      {          stdx::lock_guard<stdx::mutex> lk(service->_mutex);          invariant(service->_clients.erase(client));          if (service->_clients.empty()) {              service->_clientsEmptyCondVar.notify_all();          }      }      // The serviceContext may have already been destroyed.      onDestroy(client, service->_clientObservers);      delete client;  }  {code}"""
"SERVER-39313","New Feature","Testing Infrastructure",3,"Create burn_in_tests metric tracking script","""A script to track the effectiveness of burn_in_tests will be manually run. This script should be called buildscripts/metrics/burn_in_tests.py.    The script will invoke the task history of patch and mainline builds over a user specific period (default 4 weeks). It will provide the following, for builds which ran burn_in_tests:   * Number of patch builds   * Number of failing tasks   * Number of failing burn_in_tests tasks   * Number of patch builds where only burn_in_tests failed   * Number of tasks generated   * Number of tests executed   * Number of times task exceeded the expected run time    +AWS costs+   Computing the AWS costs for associated to each burn_in task (and the sub-tasks it spawned) will be useful in understanding what additional cost there is to run burn_in more than twice. These costs can be computed using task history time for all generated tasks and the main burn_in task weighted with the type of distro the tasks ran on. Contrasting this result with prior burn_in tasks runs will provide some metric for the increased cost.    The aws costs for burn_in can be computed with the following splunk query:  {{index=evergreen stat = task-end-stats task = burn_in_tests* project = mongodb-mongo-master | timechart span=1h avg(cost)}}"""
"SERVER-39311","New Feature","Testing Infrastructure",2,"Enable burn_in repetition count","""Update evergreen.yml to use burn_in_tests_gen task and update burn_in_test.py to pass the new repeatTests options to resmoke"""
"SERVER-39308","New Feature","Testing Infrastructure",3,"Update burn_in_tests.py to generate sub-tasks JSON file","""Modify burn_in_tests.py to support the following new option  * --generateTasksFile (default False): Generate the JSON tasks file which is used in generate.tasks  """
"SERVER-39307","New Feature","Testing Infrastructure",2,"Update burn_in_tests.py to support new resmoke repeatTests* options","""Modify burn_in_tests.py to support the following new options  * --repeatTestMin (default 2): The minimum number of times a test will run  * --repeatTestMax (default 1000): The maximum number of times a test will run  * --repeatTestTimeSecs (default 600): The time used to compute the number of repetitions a test will have, when there is a test or task history available    Note - The last 2 repeat option values will be passed to resmoke in a different ticket (SERVER-39311), which will enable burn_in testing to increase the repetition count."""
"SERVER-39305","New Feature","Testing Infrastructure",2,"Update resmoke to support new repeatTests options","""Add the following new options to control test repetition:  * --repeatTestsTimeSecs  * --repeatTestsMax    These options help repeat a test until the {{\-\-repeatTestsTimeSecs}} is reached. The default value, None, indicates no time limit specified and resmoke uses the {{\-\-repeatTests}} value. An additional parameter, {{\-\-repeatTestsMax}}, which would be used in conjunction with {{\-\-repeatTests}} (minimum number of repetitions) to bound the {{\-\-repeatTestsTimeSecs}} between these values. The test runs as follows:  * At least {{\-\-repeatTests}} times  * Stops repeating when either the {{\-\-repeatTestsTimeSecs}} or {{\-\-repeatTestsMax}} is reached    See SERVER-38911 for a proof of concept of this option."""
"SERVER-39304","New Feature","Testing Infrastructure",1,"Add new required variant linux-64-required-duroff to evergreen.yml","""In order to ensure that new tests are correctly tagged, there are currently 3 build variants which execute burn_in_tests in a distinct configuration, using the expansion macro ${burn_in_tests_build_variant}:  * enterprise-rhel-62-64-bit-required-majority-read-concern-off  * enterprise-rhel-62-64-bit-required-inmem  * rhel-62-64-bit-required-mobile    To catch tests requiring the requires_journaling tag, we can add another build variant, {{linux-64-required-duroff}}, in evergreen.yml, similar to enterprise-rhel-62-64-bit-required-majority-read-concern-off.  """
"SERVER-39355","Bug","Storage",1,"Collection drops can block the server for long periods","""Hi, sorry but we've just had another occurrence today (still running 3.4.13) so there's still an issue here. We've modified our code to drop collection to sleep 10 sec between each deletion (to give mongo some time to recover after the """"short"""" global lock and not kill the platform) but unfortunately this wasn't enough and it killed the global performance:    !https://jira.mongodb.org/secure/attachment/203795/screenshot-4.png!    After investigation I found that this was cause by some collection deletion. I tried to upload the diagnostic.data but the portal specified earlier doesn't accept files any more. I can upload it if you give another portal.    Here is the log from the drop queries: [^mongo_drop_log.txt], we can see here that they are spaced by 10sec (+drop duration) and that the drop take A LOT of time (all these collections were empty or had 5 records at most). They had some indexes though, which are not shown here but probably had to be destroyed at the same time. I don't know if it's a checkpoint global lock issue again but it's definitely still not possible to drop collection in a big 3.4.13 mongo without killing it. For the record we have ~40k namespaces, this has not changed much since the db.stats I reported above.    And before you say this is probably fixed in a more recent version, we'll need better proof than last time considering the high risk of upgrading..."""
"SERVER-39377","Task","Storage",8,"Make efficient hot backup work with enableMajorityReadConcern=false","""Currently hot backup does not work with server with enableMajorityReadConcern=false. This ticket is to modify the mechanism and enable the testing for those variants."""
"SERVER-39428","Improvement","Storage",8,"Record all indexing errors during simultaneous index builds for later constraint checking","""Hybrid index builds only record duplicate key conflicts in a side table for later resolution. With simultaneous index builds, both primary and secondary need to record conflicts in case the secondary becomes primary, so it becomes responsible for constraint checking.     Today, secondaries also ignore other types of indexing errors to maintain idempotency, and they can guarantee errors will be resolved because the primary cannot send the """"createIndexes"""" oplog entry unless they are.     With simultaneous indexes, secondaries *cannot* ignore indexing errors and must also record conflicts in a side table because if a secondary becomes primary, it needs to guarantee that all indexing errors are resolved."""
"SERVER-39419","Bug","Storage",3,"Stepdown interrupting a running dropDatabase command could leave unclean state","""As the comment in the linked BF describes, a stepdown could interrupt aÂ  running dropDatabase and leave the dropPending true on that database."""
"SERVER-39414","Improvement","Testing Infrastructure",5,"Use task tags in evergreen.yml","""Switch evergreen.yml to use task tags for specifying which tests to run on which variant.    This will involve some investigation to determine a good way of selecting which tags to use and then switching etc/evergreen.yml to use those tags.    ----    As a MongoDB engineer,  I want to be able to specify the build variants a task run on via tasks,  so that I don't have to manually add a task to all buildvariants it should run on.    ----    AC: The tasks run of each build variant should not change."""
"SERVER-39413","Improvement","Testing Infrastructure",2,"Write script to analyze evergreen task tag usage","""A script to analyze task tag usage in evergreen.yml would be useful in watching for misuse of the tags. The scripts should provide the following information:  * A list of tags being used by tasks.  * Given a tag, provide a list of tasks marked with that tag."""
"SERVER-39476","Task","Packaging",1,"Increase macOS Min Target to 10.12 For MongoDB 4.2","""Our current macOS policy is that we will support \{$latest} - \{$latest-2} with each new MongoDB release. So with the upcoming MongoDB 4.2 release, macOS 10.14/Mojave is the latest and 10.12/Sierra is the minimum we will support."""
"SERVER-39458","Task","Storage",8,"Add continuous draining on secondary's index build thread while it awaits a commitIndexBuild oplog entry","""During secondary oplog application, the IndexBuildsCoordinator should periodically drain the side tables while awaiting the commitIndexBuild or abortIndexBuild oplog entries from the primary."""
"SERVER-39455","Improvement","Testing Infrastructure",2,"lint the evergreen.yml file","""It can be really easy to introduce subtle bugs in the the evergreen.yml file (See SERVER-38822) as an example. Using a yaml linter could catch some of these issues. We should create an evergreen task to run one and as part of a required builder.    This one https://github.com/adrienverge/yamllint seems to work and can be installed as a pip module. We would want to run it in relaxed mode.    """
"SERVER-39452","Task","Storage",5,"Add rollback via refetch logic for startIndexBuild, abortIndexBuild, commitIndexBuild","""Read the relevant design documents sections for details / edge case handling."""
"SERVER-39451","Task","Storage",8,"Add recover to a stable timestamp logic for startIndexBuild, abortIndexBuild, commitIndexBuild","""Before entering rollback, [we abort all active index builds|https://github.com/mongodb/mongo/blob/6abbac58cc5b5f4b66b50ada20e70fdf96301571/src/mongo/db/repl/bgsync.cpp#L635]. After rolling back, we should add logic to [reconcileCatalogAndIdents()|https://github.com/mongodb/mongo/blob/c046a5896652acea84c9db1d9346a43b2745a37b/src/mongo/db/storage/storage_engine_impl.cpp#L324] to restart all unfinished two-phase builds."""
"SERVER-39520","Improvement","Catalog|Indexing|Storage",13,"Only use collection MODE_X locks for collection drops","""Ensure that no places rely on database intent locks to ensure a Collection pointer renames valid.  Check that collection drop doesn't block on open transactions involving other collections in the same database."""
"SERVER-39519","Improvement","Catalog|Indexing|Storage",8,"Only use Collection MODE_X locks for view creation/drop","""Test that creating/dropping views does not block on open transactions on collections in the same database."""
"SERVER-39518","Improvement","Catalog|Indexing|Storage",8,"Only use collection MODE_X locks for collection rename","""Ensure ordering in canonical ResourceId ordering of the collection locks to avoid deadlock.  Check that renaming a collection will not block on open transactions on different collections in the same database."""
"SERVER-39517","Improvement","Catalog|Indexing|Storage",5,"Only use Collection MODE_X for index creation and drop","""Add tests that creating and deleting indexes on one collection doesn't block on open transactions on a different collection in the same database."""
"SERVER-39516","Improvement","Catalog|Storage",8,"Use database MODE_IX lock for creating collections","""Remove the database {{MODE_X}} lock for collection creation.  Add tests that verify that collection creation no longer blocks on open transactions accessing different collections in the same database."""
"SERVER-39515","Improvement","Catalog|Storage",5,"Remove the KVDatabaseCatalogEntry, KVDatabaseCatalogEntryBase and DatabaseCatalogEntry classes","""Change the KVStoreEngine::DBMap to be just a set of database names.  Move the code from the removed classes into KVCatalog."""
"SERVER-39514","Improvement","Catalog|Storage",8,"Remove the KVDatabaseCatalogEntryBase::CollectionMap class","""Make the UUIDCatalog own the CollectionCatalogEntry objects instead"""
"SERVER-39512","Improvement","Catalog|Storage",3,"Make the Database class thread-safe","""After the earlier tickets in the epic, it should now be possible to make the Database class mostly immutable and make all methods thread-safe. This is required to allow for adding, removing and renaming collections without exclusive database locks."""
"SERVER-39509","Improvement","Catalog|Storage",3,"UUIDCatalog should maintain an ordered map of dbname/UUID pairs","""This replaces the current {{_orderedCollections}} map that scales poorly.  Also should improve iterating over a database in UUID order."""
"SERVER-39507","Improvement","Catalog|Storage",5,"Remove Database::CollectionMap and use UUIDCatalog instead","""Collection objects must now be owned by the UUIDCatalog."""
"SERVER-39505","Improvement","Catalog|Storage",3,"Make ViewCatalog a decoration","""The ViewCatalog also should own its own DurableViewCatalogImpl"""
"SERVER-39504","Improvement","Catalog|Storage",5,"Have Database use the UUIDCatalog for name lookup","""This requires that the UUIDCatalog maintains an extra map."""
"SERVER-39565","Task","Testing Infrastructure",0,"Add 'requires_document_locking' tag to read_at_cluster_time_outside_transactions.js test","""The {{jstests/replsets/read_at_cluster_time_outside_transactions.js}} test currently fails against the ephemeralForTest storage engine but isn't meant to work against it anyway."""
"SERVER-39584","Bug","Performance",1,"compile task in performance Evergreen project doesn't install Python dependencies","""    https://evergreen.mongodb.com/task/performance_linux_wt_standalone_compile_6089c4c1d8f166b6b61cec980672779b7cedc303_19_02_14_04_49_50"""
"SERVER-39579","Bug","Performance",1,"""compile mongodb"" function in sys-perf doesn't install Python dependencies","""    https://evergreen.mongodb.com/task/sys_perf_compile_linux_64_amzn_compile_83336cb56b269195110253918d226cbba4377a03_19_02_14_04_10_32"""
"SERVER-39578","Bug","Testing Infrastructure",1,"""check binary version"" function in etc/evergreen.yml depends on PyYAML but doesn't install it","""[The {{""""check binary version""""}} function depends on PyYAML being available|https://github.com/mongodb/mongo/blob/7951290075a7f8ecadebf789503ec05a7b10da3c/etc/evergreen.yml#L450-L470]. The Evergreen command is run [as part of the """"do setup"""" function|https://github.com/mongodb/mongo/blob/7951290075a7f8ecadebf789503ec05a7b10da3c/etc/evergreen.yml#L1099] prior to [the """"run tests"""" function running the command for the {{""""install pip requirements""""}} function|https://github.com/mongodb/mongo/blob/7951290075a7f8ecadebf789503ec05a7b10da3c/etc/evergreen.yml#L1392]. This leads to an {{ImportError}}.        https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_concurrency_replication_causal_consistency_7951290075a7f8ecadebf789503ec05a7b10da3c_19_02_14_00_07_46"""
"SERVER-39655","Bug","Storage",1,"Statistics retrieval from WiredTiger uses wrong type","""The {{WiredTigerOperationStats::fetchStats}} method uses a {{uint32_t}} when retrieving statistics from WiredTiger, but the actual statistics are signed values, we should switch to an {{int32_t}}.    This is follow on from SERVER-39026, and the [code change|https://github.com/mongodb/mongo/commit/6a9a5855048df1f4796a4032276d01318c398691] should be similar."""
"SERVER-39654","Bug","Storage",8,"Storage statistics not logged for a slow transaction","""Slow operations extract the operation statistics from the storage engine and report them as part of logging and profiling. For a slow transaction, these storage statistics are meant to be cumulative over the operations performed in that transaction.    At the moment, we do not get any storage information for the slow transaction. It looks like we are not collecting the storage stats at the correct place, we might be collecting them too late, past the point where other metrics for a transaction get accumulated. """
"SERVER-39723","Task","Storage",1,"Change listIndexes command behavior to show in-progress index builds","""The change in SERVER-25175 intended to fix a problem with initial sync as it interacts with its sync source to procure a list of indexes for each collection to build. If a background index build is in progress on the sync source and the sync source is a primary node, the background index build may or may not complete successfully. (Some examples of how it could fail are: 1. the node steps down 2. the background index build is interrupted by a killOp command 3. the background index build discovers some collection data that violates a constraint on the index being built, if the index spec has constraints.) The initial syncing node will proceed to attempt to build this same index, which might complete successfully even if the index build on the primary does not. This could leave the initial syncing node with an index that no other node has.    The initial sync process uses the listIndexes command on the sync source to obtain lists of indexes per collection. The attempt to fix this problem changed the listIndexes command to not report in-progress index builds (this will only affect background index builds, as foreground index builds lock the collection for their entirety and thus can never be observed in-progress by the listIndexes command). The fix had the intended effect for initial sync sync sources that are in primary state, but it also unintentionally changed the behavior of initial sync sync sources that are in secondary state. For such sources, background index builds are destined to complete successfully (since they already have done so on a primary node), and therefore cannot fail. Such index builds also write their oplog entries prior to completion, and therefore the only indication that an initial syncing node has to build the index is the listIndexes command response. Hiding in-progress index builds from initial sync for sync sources that are in secondary state could result in missing indexes for the initial sync. This code change restores the old behavior where listIndexes shows in-progress index builds."""
"SERVER-39705","Bug","Storage",8,"IndexBuildInterceptor does not faithfully preserve multikey when a document generates no keys","""IndexBuildInterceptor makes an incorrect assumption that a document [must generate keys|https://github.com/mongodb/mongo/blob/04882fa7f5210cfb14918ecddbbc5acbd88e86b6/src/mongo/db/index/index_build_interceptor.cpp#L384-L386] to be [considered multikey|https://github.com/mongodb/mongo/blob/04882fa7f5210cfb14918ecddbbc5acbd88e86b6/src/mongo/db/index/index_build_interceptor.cpp#L389-L397].    In particular, [sparse compound indexes|https://docs.mongodb.com/manual/core/index-sparse/#sparse-compound-indexes] may not generate keys, but will consider a document to be multikey\[1\]. MongoDB's validation code is strict and will compare an [index's multikey to the multikey output of every document|https://github.com/mongodb/mongo/blob/04882fa7f5210cfb14918ecddbbc5acbd88e86b6/src/mongo/db/catalog/private/record_store_validate_adaptor.cpp#L110-L117].    \[1\] Consider the index {{\{a: 1, b: """"2dsphere""""\}}} ({{2dsphere}} makes an index """"auto-sparse""""). Consider the document {{\{_id: 1, a: [1,2]\}}}. Because {{b}} is omitted, the sparse-ness will result in no index keys being generated. However, because {{a}} is an array, that field of the compound index will be considered to be multikey."""
"SERVER-39883","Bug","Testing Infrastructure",3,"Powercycle doesn't actually wait for the mongod process to exit during shutdown_mongod","""The """"shutdown_mongod"""" action runs the {{\{shutdown: 1, force: true\}}} command and then (on Linux) waits for {{psutil}} to say no processes with the name """"mongod"""" exist. [The {{wait_for_mongod_shutdown()}} function then sleeps an arbitrary extra 5 seconds|https://github.com/mongodb/mongo/blob/8b4f0a7893a329b0c0370385180d6a13077a8f22/pytests/powertest.py#L1481-L1483] in order to wait for any pending I/O to finish. It possible for 5 seconds to not be long enough where a file will disappear when running {{rsync}} or the mongod process will fail to start."""
"SERVER-39934","Bug","Concurrency",8,"CurOp::completeAndLogOperation should not hang waiting for global lock","""When logging a command (either slow or when forced logging is enabled) CurOp::completeAndLogOperation attempts to take a global lock to obtain storage statistics.  If something else has a Global X lock (or an enqueued Global X lock), this lock acquisition will stall behind that operation.  This introduces an undesirable dependency on the global lock for otherwise lock-free operations such as $currentOp.    We should give this acquisition a very short deadline and elide the storage stats when it is not available."""
"SERVER-39929","Bug","Build",3,"Drivers-nightly latest fails to compile on Windows","""  See https://evergreen.mongodb.com/version/drivers_nightly_6403ca518b832a49d66352620a23606348595fac.  """
"SERVER-39899","New Feature","Testing Infrastructure",3,"Enable the initial sync fuzzer in Evergreen","""Create a new {{initial_sync_fuzzer.yml}} resmoke.py YAML suite file that causes mongod processes to use an effectively infinite number of initial sync attempts.    {code:yaml|title=buildscripts/resmokeconfig/suites/initial_sync_fuzzer.yml}  test_kind: js_test    selector:    roots:    - jstestfuzz/out/*.js    executor:    archive:      tests: true    config:      shell_options:        nodb: ''        readMode: commands        global_vars:          TestData:            # TODO: logComponentVerbosity?            setParameters:              numInitialSyncAttempts: 10000000      Configure the new {{initial_sync_fuzzer_gen}} Evergreen task to run on all of the build variants the existing {{rollback_fuzzer_gen}} Evergreen task runs on with the exception of the """"Enterprise RHEL 6.2 (inMemory)"""" and """"Linux (ephemeralForTest)"""" build variants. Since the initial version of the initial sync fuzzer is meant to only be targeting the interaction between initial sync and prepared transactions, we can only run it against the WiredTiger storage engine.    - Enterprise RHEL 6.2  - Enterprise RHEL 6.2 (majority read concern off)  - Windows 2008R2 DEBUG  - macOS  - Enterprise RHEL 6.2 DEBUG Code Coverage  - ASAN Enterprise SSL Ubuntu 16.04 DEBUG  - UBSAN Enterprise Ubuntu 16.04 DEBUG"""
"SERVER-39952","Task","Build",2,"Switch master to use newer amazon linux 1 distro","""After newer Amazon Linux 1 distro has been added to Evergreen, we need to switch master branch to use that instead of the old distro."""
"SERVER-39948","Improvement","Storage",2,"Remove some simultaneous index build related fields from createIndexes cmd logging","""    I think [this|https://github.com/mongodb/mongo/blob/56efcffbcba956aa24518c71d100ecffee965058/src/mongo/db/catalog/multi_index_block.cpp#L804-L824] is where the logging is coming from, but should double check."""
"SERVER-39957","Bug","Storage",3,"Two phase drop by rename should delay the second phase until drop optime is both checkpointed and majority committed","""Currently, the old two-phase drop (by rename) executes the second phase (drop of the WT table file) when majority commit point moves past drop optime. If majority commit point is ahead of checkpoint and a crash happens after the second phase drop, on restart, the server will find the metadata of the collection still in the catalog (because it loads last checkpoint) but the actual WT file gets dropped.    On restart, the server can detect that this is from an unclean shutdown by examining the *mongod.lock* file. Then it can safely remove the metadata of those collections which do not have WT table files.    However, instead of crashing after the second phase drop, opening up backup cursor would cause similar issue which is harder to solve: there is also an inconsistency between WT table files and the catalog. But since we don't copy *mongod.lock* during backup, then the server does not trigger the code which reconciles the catalog. Then it tries to open a WT file which does not exist and hit [this fassert|https://github.com/mongodb/mongo/blob/6f3c3df4fc0abda76fd97e970ced4a01f0c48007/src/mongo/db/storage/wiredtiger/wiredtiger_record_store.cpp#L666].    To fix this problem, we should delay the second phase until drop optime is checkpointed."""
"SERVER-39988","Improvement","Build",2,"Remove integration_tests from the compile phase and move execution to a new on-box ! phase","""We currently build the integration tests as part of the {{compile}}, then pull them into the {{artifacts.tgz}} and then run them on remote machines.    Instead, we should treat them like {{dbtest}} and the unit tests and compile and run them in their own on-box {{integration_tests!}} phase which has {{compile_integration_tests}} and {{run_integration_tests}} sub-phases.  """
"SERVER-39982","Bug","Storage",5,"modify StorageTimestampTests::InitialSyncSetIndexMultikeyOnInsert to build indexes in background","""In recent weeks, the test StorageTimestampTests::InitialSyncSetIndexMultikeyOnInsert has been failing intermittently with the following invariant message:        A workaround is proposed in SERVER-39981.    However, we should investigate why this test case does not pass consistently with background/hybrid index builds and determine if there is an underlying issue in the index build machinery."""
"SERVER-40034","Task","Testing Infrastructure",1,"Set setup_group_can_fail_task to true for compile-related task groups","""The Evergreen team introduced the {{setup_group_can_fail_task}} option in EVG-5759 to make it possible for commands which fail in the {{setup_group}} list to cause the task to fail. This is desirable because it is otherwise possible for a transient network error to occur when cloning the enterprise module and for the {{compile}} task to successfully build the server without the enterprise module.    """
"SERVER-40063","Bug","Testing Infrastructure",1,"jstestfuzz_sharded_continuous_stepdown.yml is running with a 1-node CSRS on the 3.6 branch","""The changes from [2394d07|https://github.com/mongodb/mongo/commit/2394d07abe45037f44e0cdff7a56abb92e86f0a6] as part of backporting SERVER-30979 to the 3.6 branch didn't include the additional change made in [0aeb5ce|https://github.com/mongodb/mongo/commit/0aeb5ce7e8d4a190dac43fd110533eef149f7880#diff-15a75fc99f070098f4435d551de52f44] as part of SERVER-32468 to set {{num_nodes=3}} for the CSRS."""
"SERVER-40180","Improvement","Testing Infrastructure",1,"resmoke.py should escape null bytes in the output of subprocesses","""mongod is very happy to write null bytes to its stdout when they come from user input. Take setting {{logComponentVerbosity=\{write: 1\}}} and including a null byte in an update command as one example. mongod writing a null byte to its stdout causes resmoke.py to write a null bytes to its stdout. ([resmoke.py currently only attempts to deal with how the server doesn't necessarily write valid UTF-8 to its logs|https://github.com/mongodb/mongo/blob/r4.1.9/buildscripts/resmokelib/core/pipe.py#L55-L59], see SERVER-7506). This has an unfortunate consequence with tools like {{grep}} which treat output containing a null byte as binary rather than text.    In order to make it so {{grep \-\-text}} doesn't need to be specified when engineers are filtering out log messages from the server that pass through resmoke.py, we should have resmoke.py escape {{b""""\0""""}} as ."""
"SERVER-40245","Task","Testing Infrastructure",1,"Use attach.artifacts to link to GitHub wiki from every task page","""[The {{attach.artifacts}} command|https://github.com/evergreen-ci/evergreen/wiki/Project-Commands#attach-artifacts] takes a JSON file containing an array of links to include in the """"Files"""" section of the task page. A link to https://github.com/mongodb/mongo/wiki/Running-Tests-from-Evergreen-Tasks-Locally should be added for any Evergreen task that calls the """"run tests"""" function.    """
"SERVER-40241","New Feature","Testing Infrastructure",3,"Have resmoke.py log an invocation for local usage","""The changes from SERVER-28785 made it so resmoke.py writes its own command line arguments to stdout. This enables Server engineers to avoid doing mental bash evaluation to determine what command line arguments the """"run tests"""" will synthesize and pass to resmoke.py. The command line arguments for [this aggregation task|https://evergreen.mongodb.com/task/mongodb_mongo_master_enterprise_rhel_62_64_bit_aggregation_6f083bd87264e9d9c3d637fae62103c36a65316a_19_03_11_19_56_34] include a large number of details and metadata that are specific to how we run tests in Evergreen.        A more compact form for an engineer to run would look like:        The changes from this ticket should add a new log message after [this line|https://github.com/mongodb/mongo/blob/r4.1.9/buildscripts/resmoke.py#L147] containing the simplified resmoke.py invocation.    {code:python}  self._resmoke_logger.info(""""verbatim resmoke.py invocation: %s"""", """" """".join(sys.argv))  if config.EVERGREEN_TASK_ID:      args = ...      self._resmoke_logger.info(""""resmoke.py invocation for local usage: %s"""", """" """".join(args))  {code}    It must therefore do the following:    * Always log the program name as \{{buildscripts/resmoke.py}} even though in Evergreen we run the wrapper script \{{buildscripts/evergreen_run_tests.py}}.  * Always log the non-generated version of the test suite name. The \{{buildscripts/evergreen_generate_resmoke_tasks.py}} script generates new resmoke.py YAML suite files in order to be able to dynamically split the test suite into multiple Evergreen task which may run concurrently. Handling this behavior can be achieved by propagating \{{self.config_options.suite}} as a new \{{\-\-originSuite}} command line option to resmoke.py through [the \{{_generate_resmoke_args()}} function|https://github.com/mongodb/mongo/blob/r4.1.9/buildscripts/evergreen_generate_resmoke_tasks.py#L289-L294].  ** The sub-suite definitions are uploaded to Evergreen in the \{{*_gen}} task as """"Generated Task Config"""" but we don't really want engineers to have to worry about using them.  * Always remove the command line options not seen in the compact form above. The implementation should explicitly list the command line options to remove so that new ones still appear by default. It might be possible to be a little clever about trying to remove all the options from [the \{{evergreen_options}} group|https://github.com/mongodb/mongo/blob/r4.1.9/buildscripts/resmokelib/parser.py#L255] so that new ones added to that section never appear."""
"SERVER-40339","Bug","Testing Infrastructure",3,"Resmoke doesn't always show output from failing python unittests","""See: https://evergreen.mongodb.com/task_log_raw/mongodb_mongo_master_enterprise_rhel_62_64_bit_buildscripts_test_patch_e35e8076dbddc863205cf24517e1b16dc9104d07_5c9a4bcad1fe075bd51ca0ad_19_03_26_15_57_00/0?type=T    The test_burn_in_tests.py failed with -2 but no output is shown."""
"SERVER-40421","New Feature","Concurrency|Testing Infrastructure",1,"Add failpoint to skip doing retries on WiredTiger prepare conflicts","""An operation within WiredTiger that attempts to get or set a value which has been prepared by another transaction may have a {{WT_PREPARE_CONFLICT}} error returned. (Note that until SERVER-40176 is addressed, this also applies to operations which may scan over such data.) The MongoDB layer then enqueues these operations to be retried after a prepared transaction has committed or aborted. In order to allow the rollback fuzzer to generate randomized insert, update, and delete operations that may prepare conflcits without hanging, it would be useful to add a failpoint to [the {{wiredTigerPrepareConflictRetry()}} function|https://github.com/mongodb/mongo/blob/a3c7bdb31e949cfd11c2c9e24f9a04dfd6c22ba1/src/mongo/db/storage/wiredtiger/wiredtiger_prepare_conflict.h#L56] where it doesn't do any retry logic and instead has the command fail with a {{WriteConflict}} error response."""
"SERVER-40418","Improvement","Testing Infrastructure",3,"Refactor test_adb_monitor to not use files when testing","""We should mock out all the file creation, and subprocess invocations, when testing adb_monitor."""
"SERVER-40415","Bug","Testing Infrastructure",1,"Tempfile cleanup from test_adb_monitor.py","""Tempfiles are not cleaned up in test_adb_monitor.py    The code which creates a named temp file, should also specify a temp_dir:      Should be:  """
"SERVER-40486","Improvement","Testing Infrastructure",2,"Remove Test Lifecycle code","""We are no longer using the Test Lifecycle code. We should go through and remove the code executing it. This would include the references in etc/evergreen.yml, buildscripts/fetch_test_lifecycle.py, buildscripts/update_test_lifecycle.py, and any other related files.    ----    As a server engineer,  I want the test lifecycle code removed.  so that I don't have to spend time maintaining it.    ----    AC  * etc/evergreen should not run any test_lifecycle related tasks.  * test lifecycle scripts (and tests for them) should be removed from mongo repository."""
"SERVER-40470","Improvement","Storage",3,"Use roundup_timestamp API instead of round_to_oldest","""WT-4640 is deprecating the {{round_to_oldest}} API in favour of {{roundup_timestamp=(read=true)}}. Server needs to use the new API before {{round_to_oldest}} is removed from WiredTiger. This ticket is to address this change."""
"SERVER-40469","Task","Replication|Testing Infrastructure",1,"Remove the expectPreparedTxnsDuringRollback parameter to the RollbackTest constructor","""This is follow-up work around SERVER-40468 after the corresponding changes are made to the rollback fuzzer. The {{expectPreparedTxnsDuringRollback}} parameter also isn't aptly named because any outstanding transactions would prevent the ability to run the data consistency checks, not just prepared transactions."""
"SERVER-40468","Improvement","Replication|Testing Infrastructure",1,"Allow RollbackTest fixture to skip collection validation when restarting node","""The validate command requires a collection X lock, which cannot be acquired if there are outstanding transactions running on the server. The rollback fuzzer may attempt to call {{RollbackTest#restartNode()}} after having started transactions on the server. [The {{RollbackTest#restartNode()}} method|https://github.com/mongodb/mongo/blob/4459b439700f096a7b6287fdddde592db8934fe2/jstests/replsets/libs/rollback_test.js#L391] should therefore take an optional {{skipValidation}} boolean and pass it through to [the {{rst.stop()}} call|https://github.com/mongodb/mongo/blob/4459b439700f096a7b6287fdddde592db8934fe2/jstests/replsets/libs/rollback_test.js#L422].    The {{RollbackTest#transitionToSteadyStateOperations()}} method should also be updated to take an optional {{skipDataConsistencyChecks}} boolean (wrapped in an object so it's effectively a named parameter) that acts as a syntactic alternative to the usage of {{expectPreparedTxnsDuringRollback}} as a way to avoid calling {{checkDataConsistency()}}."""
"SERVER-40518","Bug","Testing Infrastructure",1,"backup_restore*.js tests send SIGTERM to resmoke.py and may leak mongo shell running FSM workloads","""Sending a SIGTERM to resmoke.py causes the Python process to immediately exit (it doesn't register a handler for that signal like the server does) without waiting for any of the mongo shell processes it spawned to also exit (though they also receive the SIGTERM signal). This means if resmoke.py was in the midst of spawning a mongo shell process to run an FSM workload, then it could continue to run (i.e. be leaked) even after the parent resmoke.py process exits."""
"SERVER-40532","Bug","Testing Infrastructure",1,"""check binary version"" doesn't pip install evgtest-requirements.txt when bypass_compile is true","""We're seeing patch builds failing in Evergreen due to missing python modules when bypass_compile is not set to false.    [Check binary version|https://github.com/mongodb/mongo/blob/73e719a1ee1c174a3131e19b537b3ae8aa958dad/etc/evergreen.yml#L498-L526] is what should be pip installing the requirements but it exits early on patch builds."""
"SERVER-40550","Bug","Testing Infrastructure",2,"Refactor job.py to support mock of time.time","""The tests for {{buildscripts/resmokelib/testing/job.py}} mock out {{time.time}}. However this is faulty, as {{time.time}} can be called from other logging calls, while the test is active. A cleaner solution would be to move {{time.time}} into a helper function which is then mocked:        Test code in {{buildscripts/tests/resmokelib/testing/test_job.py}}  """
"SERVER-40592","Bug","Testing Infrastructure",2,"Uncaught exception in resmoke.py job thread due to logkeeper unavailability when tearing down fixture","""[The call to {{teardown_fixture()}}|https://github.com/mongodb/mongo/blob/a13c018b51465b04027adee28fd79fd82ed4575b/buildscripts/resmokelib/testing/job.py#L102] should happen in a try/except block.  * If there's a {{buildscripts.resmokelib.errors.LoggerRuntimeConfigError}} exception, then we should record the message with {{self.logger.error(""""%s"""", err)}} to the task logs.  * If there's some other kind of exception, then we should record the message and Python stacktrace with {{self.logger.exception(""""Encountered an error when tearing down %s: %s"""", self.fixture, err)}} to the task logs. We should additionally set the {{teardown_flag}}.    """
"SERVER-40589","Bug","Testing Infrastructure",1,"find command should validate  $_internalReadAtClusterTime is not null","""The 'find' command will hit an invariant failure [here|https://github.com/mongodb/mongo/blob/a13c018b51465b04027adee28fd79fd82ed4575b/src/mongo/db/commands/find_cmd.cpp#L330] when a command sends a null timestamp, Timestamp(0,0)."""
"SERVER-40663","Task","Performance",1,"Reduce Frequency of Sys Perf WT_Develop variants until adoption can occur","""In a recent performance meeting we discussed that there was limited value for the storage engines team to have the wt_develop variants running. We believe there may be value in:   - A new adoption push in which we train the team on Perf Discovery   - Value for the perf build baron    Until we flesh that out, we want to reduce the spend by setting the frequency of these tests to run on a weekly (instead of daily) basis.    Acceptance Criteria:   - Update Sys Perf Master Evergreen Project to change all wt_develop variants to run weekly   - Send an email to perf interest and downstream changes (if applicable)"""
"SERVER-40702","Improvement","Testing Infrastructure",1,"resmoke.py should wait for subprocesses it spawned to exit on KeyboardInterrupt","""[resmoke.py doesn't wait for the job threads running tests to exit when they are interrupted by the user|https://github.com/mongodb/mongo/blob/b6d336bee9c7adb334333bcb22c432d376458af3/buildscripts/resmokelib/testing/executor.py#L145-L185]. It instead relies on the SIGINT being received by all the processes in the process group to exit on their own quickly. While this may reduce the likelihood a user would interrupt resmoke.py multiple times due to it taking longer to exit, it also means that processes spawned by resmoke.py may outlive the resmoke.py Python process. This behavior has caused failures in the {{backup_restore*.js}} tests which spawns its own resmoke.py subprocess in order to run FSM workloads against a {{ReplSetTest}} instance.    We should call {{thr.join()}} even after a {{KeyboardInterrupt}} exception occurs. However, it would be convenient for users if we also logged a message (say after 2 seconds of waiting for the thread) that they can use ctrl-\ to send a SIGQUIT to all of the processes to get them to exit on Linux or ctrl-c again to get them to exit on Windows as the Job object has {{JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE}} set. Sending a SIGQUIT is an easy way to ensure resmoke.py exits even if the mongod process is hung.    {code:python}  def _run_tests(self, test_queue, setup_flag, teardown_flag):      """"""""""""Start a thread for each Job instance and block until all of the tests are run.      Returns a (combined report, user interrupted) pair, where the      report contains the status and timing information of tests run      by all of the threads.      """"""""""""        threads = []      interrupt_flag = threading.Event()      user_interrupted = False      try:          # Run each Job instance in its own thread.          for job in self._jobs:              thr = threading.Thread(                  target=job, args=(test_queue, interrupt_flag), kwargs=dict(                      setup_flag=setup_flag, teardown_flag=teardown_flag))              # Do not wait for tests to finish executing if interrupted by the user.              thr.daemon = True              thr.start()              threads.append(thr)              # SERVER-24729 Need to stagger when jobs start to reduce I/O load if there              # are many of them.  Both the 5 and the 10 are arbitrary.              # Currently only enabled on Evergreen.              if _config.STAGGER_JOBS and len(threads) >= 5:                  time.sleep(10)            joined = False          while not joined:              # Need to pass a timeout to join() so that KeyboardInterrupt exceptions              # are propagated.              joined = test_queue.join(TestSuiteExecutor._TIMEOUT)      except (KeyboardInterrupt, SystemExit):          interrupt_flag.set()          user_interrupted = True      else:          # Only wait for all the Job instances if not interrupted by the user.          self.logger.debug(""""Waiting for threads to complete"""")          for thr in threads:              thr.join()          self.logger.debug(""""Threads are completed!"""")  {code}"""
"SERVER-40690","Task","Testing Infrastructure",2,"Create more lightweight Enterprise Windows build variant that's required for patch builds","""The Enterprise Windows 2008R2 build variant runs a number of Evergreen tasks that appear (at least empirically) to be redundant with the list of tasks that run on some of the required Linux build variants. We should create a new variant with a task list similar to the following    {code:yaml}  - name: enterprise-windows-64-2k8-required    display_name: """"! Enterprise Windows 2008R2""""    modules:    - enterprise    run_on:    - windows-64-vs2017-test    expansions: ...  # Use an anchor/alias to avoid duplicating these    display_tasks:    - *dbtest    tasks:    - name: compile_TG      requires:      - name: burn_in_tests_gen      - name: verify_pip      distros:      - windows-64-vs2017-compile    - name: burn_in_tests_gen    - name: verify_pip    - name: buildscripts_test    - name: dbtest_TG      distros:      - windows-64-vs2017-compile    - name: noPassthrough_gen  {code}    and update the {{display_name}} of the {{enterprise\-windows\-64\-2k8}} build variant to be """"* Enterprise Windows 2008R2"""" in order to reflect it has a {{batchtime}} of 1 hour but isn't a required build variant. The definition of the {{required}} alias in the Evergreen database must also be updated to include {{enterprise\-windows\-64\-2k8\-required}} rather than {{enterprise\-windows\-64-2k8}}.  """
"SERVER-40749","Bug","Testing Infrastructure",1,"Include execution in generated task configuration file name","""The execution number should be included in the generated.task configuration file name. Otherwise, later execution can overwrite the file, but since evergreen will no-op on reruns of generate.task, we don't actually use the new configuration. By including the execution number, we will still have the original configuration for subsequent executions of the sub-tasks."""
