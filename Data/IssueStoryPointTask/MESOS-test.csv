"issuekey","type","components","storypoint","title","description_text"
"MESOS-8838","Bug","agent",2,"Consider validating that resubscribing resource providers do not change their name or type","""The agent currently uses a resource provider's name and type to construct e.g., paths for persisting resource provider state and their recovery. With that we should likely prevent resource providers from changing that information since we might otherwise be unable to recover them successfully."""
"MESOS-8837","Task","test",1,"Add test of resource provider manager recovery","""See https://reviews.apache.org/r/66546/."""
"MESOS-8835","Bug","test",1,"mesos-tests takes a long time to execute no tests","""Executing {{mesos-tests}} takes substantially more time than running {{stout-tests}} or {{libprocess-tests}} when no tests are executed, e.g., for a release build with debug symbols        Looking at where time is spent with {{perf}} points to two {{parameters}} functions in {{src/tests/resources_tests.cpp}}. These functions construct large collections of {{Resource}} when registering {{Resources_Contains_BENCHMARK_Test}} and {{Resources_Scalar_Arithmetic_BENCHMARK_Test}}, i.e., these functions will be executed even if the corresponding test is filtered out."""
"MESOS-8851","Improvement","libprocess",3,"Introduce a push-based gauge.","""Currently, we only have pull-based gauges which have significant performance downsides.    A push-based gauge differs from a pull-based gauge in that the client is responsible for pushing the latest value into the gauge whenever it changes. This can be challenging in some cases as it requires the client to have a good handle on when the gauge value changes (rather than just computing the current value when asked).    It is highly recommended to use push-based gauges if possible as they provide significant performance benefits over pull-based gauges. Pull-based gauge suffer from delays getting processed on the event queue of a Process, as well as incur computation cost on the Process each time the metrics are collected. Push-based gauges, on the other hand, incur no cost to the owning Process when metrics are collected, and instead incur a trivial cost when the Process pushes new values in."""
"MESOS-8848","Task","master",2,"Per Framework Offer metrics","""Metrics regarding number of offers (sent, accepted, declined, rescinded) on a per framework basis."""
"MESOS-8845","Task","master",2,"Per Framework Operation metrics","""Metris for number of operations sent via ACCEPT calls by framework."""
"MESOS-8844","Task","master",2,"Per Framework EVENT metrics","""Metrics for number of events sent by the master to the framework."""
"MESOS-8843","Task","master",2,"Per Framework CALL metrics","""Metrics about number of different kinds of calls sent by a framework to master."""
"MESOS-8841","Bug","allocation|master",2,"Flaky `MasterAllocatorTest/0.SingleFramework`","""      """
"MESOS-8871","Bug","agent",3,"Agent may fail to recover if the agent dies before image store cache checkpointed.","""    This may happen if the agent dies after the file is created but before the contents are persisted on disk."""
"MESOS-8870","Bug","master",2,"Master does not correctly reconcile dropped operations after agent failover","""When an operation does not reach the agent before an agent failover, the master currently does not detect the dropped operation when the agent reregisters and sends the list of its operation in an {{UpdateSlaveMessage}}."""
"MESOS-8866","Bug","cmake",1,"CMake builds are missing byproduct declaration for jemalloc.","""The {{jemalloc}} dependency is missing a byproduct declaration in the CMake configuration. As a result, building Mesos with enabled {{jemalloc}} using CMake and Ninja will fail."""
"MESOS-8865","Bug","java api",1,"Suspicious enum value comparisons in scheduler Java bindings","""Clang reports suspicious comparisons of enum values in the scheduler Java bindings,      While the current implementation might just work since the different enum values might by accident map onto the same integer values (needs to be confirmed), this seems brittle and against the type safety the languages offers. We should fix this code."""
"MESOS-8877","Bug","docker",3,"Docker container's resources will be wrongly enlarged in cgroups after agent recovery","""Reproduce steps:    1. Run `mesos-execute --master=10.0.49.2:5050 --task=[file:///home/qzhang/workspace/config/task_docker.json] --checkpoint=true` to launch a Docker container.    2. When the Docker container is running, we can see its resources in cgroups are correctly set, so far so good.    3. Restart Mesos agent, and then we will see the resources of the Docker container will be wrongly enlarged.  """
"MESOS-8876","Bug","docker",2,"Normal exit of Docker container using rexray volume results in TASK_FAILED.","""In the fix to  MESOS-8488, we reap the Docker container process directly in Docker executor, and it will wait for `docker run` to return for at most 3 seconds. However, in some cases, the `docker run` command will indeed need more than 3 seconds to return, e.g., the Docker container uses an external rexray volume (see the attached task json as an example), for such container, there will be about 5 seconds between container process exits and the `docker run` returns (I suspect Docker daemon was doing some stuff related to rexray volume during this time), so we will reap this container, and send a {{TASK_FAILED}}."""
"MESOS-8874","Bug","test",1,"ResourceProviderManagerHttpApiTest.ResubscribeResourceProvider is flaky.","""This test is flaky on CI:      This is different from https://issues.apache.org/jira/browse/MESOS-8315."""
"MESOS-8873","Bug","test",2,"StorageLocalResourceProviderTest.ROOT_ZeroSizedDisk is flaky.","""This test is flaky on CI:  """
"MESOS-8872","Bug","test",1,"OperationReconciliationTest.AgentPendingOperationAfterMasterFailover is flaky.","""This test is flaky on CI:  """
"MESOS-8892","Bug","test",1,"MasterSlaveReconciliationTest.ReconcileDroppedOperation is flaky","""This was observed on a Debian 9 SSL/GRPC-enabled build. It appears that a poorly-timed {{UpdateSlaveMessage}} leads to the operation reconciliation occurring before the expectation for the {{ReconcileOperationsMessage}} is registered:      Full log is attached as {{MasterSlaveReconciliationTest.ReconcileDroppedOperation.txt}}."""
"MESOS-8907","Bug","fetcher",3,"Docker image fetcher fails with HTTP/2.","""    Note that curl is saying the HTTP version is """"HTTP/2"""". This happens on modern curl that automatically negotiates HTTP/2, but the docker fetcher isn't prepared to parse that.    """
"MESOS-8906","Bug","storage",2,"`UriDiskProfileAdaptor` fails to update profile selectors.","""The {{UriDiskProfileAdaptor}} ignores the polled profile matrix if the polled one has the same size as the current one: https://github.com/apache/mesos/blob/1.5.x/src/resource_provider/storage/uri_disk_profile.cpp#L282-L286  {code:cxx}    // Profiles can only be added, so if the parsed data is the same size,    // nothing has changed and no notifications need to be sent.    if (parsed.profile_matrix().size() <= profileMatrix.size()) {      return;    }  {code}  However, this prevents the profile selector from being updated, which is not the desired behavior."""
"MESOS-8917","Bug","agent|containerization|libprocess|stout",3,"Agent leaking file descriptors into forked processes","""If not all file descriptors are carefully {{open}}'ed with {{O_CLOEXEC}} the Mesos agent might leak them into forked processes e.g., executors. This presents a potential security issue as such processes can interfere with the agent.    The current approach is to fix all invocations of {{open}} to always set {{O_CLOEXEC}}, but this approach breaks down when using 3rdparty libraries as there is no reliable way to patch unbundled dependencies.    It seems a more reliable approach would be to {{close}} all but a whitelisted set of file descriptors when after {{fork}}, but before the {{exec*}}. It should be possible to assemble such a whitelist for the typical use cases (e.g., in for the Mesos containerizer's  {{launch}}) and pass it to a modified functions to start subprocess. We might need to audit uses of raw {{fork}} in the code."""
"MESOS-8916","Epic","allocation",5,"Allocation logic cleanup.","""The allocation logic has grown organically and is now very hard to read and maintain. This epic will track cleanups to improve the readability of the core allocation logic:    * Add a function for returning the subset of frameworks that are capable of receiving offers from the agent. This moves the capability checking out of the core allocation logic and means the loops can just iterate over a smaller set of framework candidates rather than having to write 'continue' cases. This covers the GPU_RESOURCES and REGION_AWARE capabilities.  * Similarly, add a function that allows framework capability based filtering of resources. This pulls out the filtering logic from the core allocation logic and instead the core allocation logic can just all out to the capability filtering function. This covers the SHARED_RESOURCES, REVOCABLE_RESOURCES and RESERVATION_REFINEMENT capabilities. Note that in order to implement this one, we must refactor the shared resources logic in order to have the resource generation occur regardless of the framework capability (followed by getting filtered out via this new function if the framework is not capable).  * Update the scalar quantity related functions to also strip static reservation metadata. Currently there is extra code in the allocator across many places (including the allocation logic) to perform this in the call-sites.  * Track across allocation cycles or pull out the following into functions: quantity of quota that is currently """"charged"""" to a role, amount of """"headroom"""" that is needed/available for unsatisfied quota guarantees.  * Pull out the resource shrinking function."""
"MESOS-8924","Improvement","libprocess",5,"Refactor the libprocess gRPC warpper.","""Refactor {{process::grpc::client::Runtime}} for better naming and interface, and fewer synchronizations."""
"MESOS-8921","Bug","build",3,"Autotools don't work with newer OpenJDK versions","""There are three distinct issues with modern Java and Linux versions:    1. Mesos configure script expects `libjvm.so` at `$JAVA_HOME/jre/lib/<arch>/server/libjvm.so`, but in the newer openjdk versions, `libjvm.so` is found at `$JAVA_HOME/lib/server/libjvm.so`.    2. On some distros (e.g., Ubuntu 18.04), JAVA_HOME env var might be missing. In such cases, the configure is able to compute it by looking at `java` and `javac` paths and succeeds. However, some maven plugins require JAVA_HOME to be set and could fail if it's not found.    Because configure scripts generate an automake variable `JAVA_HOME`, we can simply invoke maven in the following way to fix this issue:     These two behaviors were observed with OpenJDK 1.11 on Ubuntu 18.04 but I suspect that the behavior is present on other distros/OpenJDK versions.    3. `javah` has been removed as of OpenJDK 1.10. Instead `javac -h` is to be used as a replacement. See [http://openjdk.java.net/jeps/313] for more details."""
"MESOS-8919","Improvement","master",2,"Per Framework SUBSCRIBE metrics.","""Per Framework SUBSCRIBE metrics."""
"MESOS-8932","Bug","master",2,"Quota guarantee metric does not handle removal correctly.","""The quota guarantee metric is not removed when the quota gets removed:  https://github.com/apache/mesos/blob/1.6.0/src/master/allocator/mesos/metrics.cpp#L165-L174    The consequence of this is that the metric will hold the initial value that gets set and all subsequent removes / sets will not be exposed via the metric."""
"MESOS-8936","Task","allocation",5,"Implement a Random Sorter for offer allocations.","""The only sorter that Mesos supports today is the DRF sorter. But, there are cases when DRF sorting causes offer fragmentation when dealing with non-revocable resources and multiple frameworks. One of the options to improve this situation is to introduce a new random sorter instead of DRF sorter.    See [https://docs.google.com/document/d/1uvTmBo_21Ul9U_mijgWyh7hE0E_yZXrFr43JIB9OCl8/edit#heading=h.nfye94rqpotp] for additional details.          """
"MESOS-8935","Bug","allocation",3,"Quota limit ""chopping"" can lead to cpu-only and memory-only offers.","""When we allocate resources to a role, we'll """"chop"""" the available resources of the agent up to the quota limit for the role (per MESOS-7099). This prevents the role from exceeding its quota limit.    This has the unintended consequence of creating cpu-only and memory-only offers.    Consider agents with 10 cpus and 100 GB mem and roles with quota guarantee/limit of 5 cpus, 10 GB mem. The following allocations will occur:    agent 1:   r1 -> 5 cpus 10GB mem   r2 -> 5 cpus 10GB mem   r3 -> 0 cpus 10GB mem (quota allocates even if it can make progress towards a single resource and MESOS-1688 allows this)   r4 -> 0 cpus 10GB mem   ...   r10 -> 0 cpus 10GB mem    agent 2:   r3 -> 5 cpus 0GB mem (r3 is already at its 10GB mem limit)   r4 -> 5 cpus 0GB mem   r11 -> 0 cpus 10GB mem   ...   r20 -> 0 cpus 10GB mem    Here, roles 3-20 receive memory only and cpu only offers. This gets further exacerbated if DRF chooses the same ordering between roles across cycles. """
"MESOS-8942","Task","HTTP API",2,"Master streaming API does not send (health) check updates for tasks.","""Currently, Master API subscribers get task status updates when task state changes (the actual logic is [slightly more complex|https://github.com/apache/mesos/blob/d7d7cfbc3e5609fc9a4e8de8203a6ecb11afeac7/src/master/master.cpp#L10794-L10841]). We use task status updates to deliver health and check information to schedulers, in which case task state does not change. Hence these updates are filtered out and the subscribers do not get any task health updates.    Here is a test that confirms the described behaviour: https://gist.github.com/rukletsov/c079d95479fb134d137ea3ae8b7ae874"""
"MESOS-8943","Task","storage",5,"Add metrics about CSI calls.","""We should add metrics for CSI calls so operators can be alerted on flapping CSI plugins."""
"MESOS-8961","Task","containerization",1,"Output of tasks gets corrupted if task defines the same environment variables as the executor container","""The issue is easily reproducible if one launches a task group and the taks nested container defines the same set of environment variables as the executor. In those circumstances, the following [snippet is activated|https://github.com/apache/mesos/blob/285d82080748cd69044c226950274c7046048c4b/src/slave/containerizer/mesos/launch.cpp#L1057]:        But this is not the only time that this file writes into {{cout}}.    This may be a bad idea because applications which consume the standard output of a task may end up being corrupted by the container manager output. In these cases, writing to {{cerr}} should be the right approach. """
"MESOS-8975","Task","HTTP API",5,"Problem and solution overview for the slow API issue.","""Collect data from the clusters regarding {{state.json}} responsiveness, figure out, where the bottlenecks are, and prepare an overview of solutions."""
"MESOS-8985","Bug","HTTP API",2,"Posting to the operator api with 'accept recordio' header can crash the agent","""It's possible to crash the mesos agent by posting a reasonable request to the operator API.  h3. Background:    Sending a request to the v1 api endpoint with an unsupported 'accept' header:    Results in the following friendly error message:    h3. Reproducible crash:    However, sending the same request with 'application/recordio' 'accept' header:    causes the agent to crash (no response is received).    Crash log is shown below, full log from the agent is attached here:  """
"MESOS-8987","Bug","master|security",5,"Master asks agent to shutdown upon auth errors.","""The Mesos master sends a {{ShutdownMessage}} to an agent if there is an [authentication|https://github.com/apache/mesos/blob/d733b1031350e03bce443aa287044eb4eee1053a/src/master/master.cpp#L6532-L6543] or an [authorization|https://github.com/apache/mesos/blob/d733b1031350e03bce443aa287044eb4eee1053a/src/master/master.cpp#L6622-L6633] error during agent registration.     Upon receipt of this message, the agent kills alls its tasks and commits suicide. This means that transient auth errors can lead to whole agents being killed along with it's tasks.    I think the master should stop sending the {{ShutdownMessage}} in these cases, or at least let the agent retry the registration a few times before asking it to shutdown."""
"MESOS-8995","Task","storage",5,"Add SLRP unit tests for missing profiles.","""We need to add unit tests to verify SLRP correctness when the set of known profiles are shrunk by the disk profile adaptor. Here lists a couple scenarios worth testing:   1. `CREATE_VOLUME` should succeed if it is submitted before the profile becomes stale.  2. `CREATE_VOLUME` should be dropped if it is submitted after the profile becomes stale.  3. `DESTROY_VOLUME` should free spaces for a stale profile."""
"MESOS-9000","Bug","HTTP API",3,"Operator API event stream can miss task status updates.","""As of now, the master only sends TaskUpdated messages to subscribers when the latest known task state on the agent changed:        The latest state is set like this:        However, the `TaskStatus` message included in an `TaskUpdated` event is the event at the bottom of the queue when the update was sent.    So we can easily get in a situation where e.g. the first TaskUpdated has .status.state == TASK_STARTING and .state == TASK_RUNNING, and the second update with .status.state == TASK_RUNNNING and .state == TASK_RUNNING would not get delivered because the latest known state did not change.    This implies that schedulers can not reliably wait for the status information corresponding to specific task state, since there is no guarantee that subscribers get notified during the time when this status update will be included in the status field."""
"MESOS-8999","Improvement","libprocess",3,"Add default bodies for libprocess HTTP error responses.","""By default on error libprocess would only return a response  with the correct status code and no response body.    However, most browsers do not visually indicate the response  status code, so if any error occurs anyone using a browser will only  see a blank page, making it hard to figure out what happened."""
"MESOS-8998","Improvement","build|cmake",2,"Allow for unbundled libevent in CMake builds to work around 2.1.x SSL issues.","""On macOS and Ubuntu 17, libevent-openssl >= 2.1.x is broken in conjunction with libprocess.    We tried to pinpoint the issue but so far with no success. For enabling CMake SSL builds on those systems , we have to support prior libevent versions. Allowing building against a preinstalled libevent version paves that way."""
"MESOS-9009","Task","containerization",5,"Support for creation non-existing host paths in a whitelist as source paths","""Docker creates a directory specified in {{docker run}}'s {{--volume}}/{{-v}} option as the source path that will get bind-mounted into the container, if the source location didn't originally exist on the host.    Unlike Docker, UCR bails on launching containers if any of their host mount paths doesn't originally exist. While this is more secure and eliminates unnecessary side effects, it breaks transparent compatibility when trying to migrate from Docker.    As a trade-off, we should allow host path creation in a restricted manner, by introducing a new Mesos agent flag ({{--host_path_volume_force_creation}}) as a colon-separated whitelist (similar to the format of POSIX's {{$PATH}} environment variable), under whose items' subdirectories the host paths are allowed to be created.  """
"MESOS-9008","Bug","fetcher",1,"Fetcher fails to extract some archives containing hardlinks","""We recently switched the infrastructure to e.g., extract archives to libarchive which seems to have narrower support for e.g., hardlinks in archives, see e.g., [https://github.com/libarchive/libarchive/wiki/Hardlinks] upstream (likely outdated).    In a particular case, we tried to extract https://artifacts.elastic.co/downloads/kibana/kibana-5.6.5-linux-x86_64.tar.gz which the fetcher successfully extracted in e.g., 1.6.0, but which now leads to failures like  """
"MESOS-9014","Bug","test",1,"MasterAPITest.SubscribersReceiveHealthUpdates is flaky","""This test fails flaky on CI. Log attached.  """
"MESOS-9010","Bug","storage",2,"`UPDATE_STATE` can race with `UPDATE_OPERATION_STATUS` for a resource provider.","""Since a resource provider and its operation status update manager run in different actors, for a completed operation, its `UPDATE_OPERATION_STATUS` call may race with an `UPDATE_STATE`. When the `UPDATE_STATE` arrives to the agent earlier, the total resources will be updated, but the terminal status of the completed operation will be ignored since it is known by both the agent and the resource provider. As a result, when the `UPDATE_OPERATION_STATUS` arrives later, the agent will try to apply the operation, but this is incorrect since the total resources has already been updated."""
"MESOS-9015","Improvement","allocation",5,"Allow resources to be removed when updating the sorter.","""Currently we do not allow resource conversions to change the resource quantities when updating the sorter; we only allow the metadata for their consumed resources to be modified.    However, this restricts Mesos from supporting operations that remove resources. For example, when a CSI volume with a stale profile is destroyed, it would be better to convert it into an empty resource since the disk space is no longer available. See https://issues.apache.org/jira/browse/MESOS-8825.    To make the allocator more flexible, we should allow resource conversions to remove resources when updating the sorter."""
"MESOS-9025","Bug","containerization",3,"The container which joins CNI network and has checkpoint enabled will be mistakenly destroyed by agent","""Reproduce steps:    1) Run {{mesos-execute}} to launch a task which joins a CNI network {{net1}} and has checkpoint enabled:    2) After task is in the {{TASK_RUNNING}} state, restart the agent process, and then in the agent log, we will see the container is destroyed.    And {{mesos-execute}} will receive a {{TASK_GONE}} for the task:  """
"MESOS-9027","Bug","containerization|gpu",2,"GPU Isolator still depends on cgroups/devices agent flag given cgrous/all is supported.","""GPU Isolator still depends on cgroups/devices agent flag given cgrous/all is supported."""
"MESOS-9035","Task","containerization",5,"Implement `linux/seccomp` isolator","""The main purpose of this isolator is to prepare `ContainerSeccompProfile` for a containerizer launcher. `ContainerSeccompProfile` message is generated by the isolator from a JSON-file that contains declaration of Seccomp filter rules.    In addition, seccomp isolator should check for a Seccomp support by the Linux kernel."""
"MESOS-9034","Task","containerization",5,"Implement a wrapper class for `libseccomp` API","""The main purpose of this class is to provide translation of `SeccompProfile` protobuf into invocations of `libseccomp` API. The main user of this class is a containerizer launcher."""
"MESOS-9033","Task","containerization",3,"Add Seccomp-related protobufs","""Define `SeccompProfile` and `SeccompInfo` messages and then update appropriate messages, including `LinuxInfo` and `ContainerLaunchInfo`."""
"MESOS-9032","Task","containerization",5,"Update build scripts to support `seccomp-isolator` flag and `libseccomp` library","""This ticket consists of the following subtasks:  1) Bundle `libseccomp` tarball as a third-party library  2) Add build rules to compile `libseccomp` library and to link Mesos agent against this library  3) Add `enable-seccomp-isolator` flag to the build scripts"""
"MESOS-9039","Bug","cni",2,"CNI isolator recovery should wait until unknown orphan cleanup is done","""Currently, CNI isolator will cleanup unknown orphaned containers in an asynchronous way (see [here|https://github.com/apache/mesos/blob/1.6.0/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L439] for details) during recovery, that means agent recovery can finish while the cleanup of unknown orphaned containers is still ongoing which is not ideal. So we need to make CNI isolator recovery waits until unknown orphan cleanup is done."""
"MESOS-9049","Bug","agent",2,"Agent GC could unmount a dangling persistent volume multiple times.","""When the agent GC an executor dir and the sandbox of one of its run that contains a dangling persistent volume, the agent might try to unmount the persistent volume twice, which leads to an {{EINVAL}} when trying to unmount the target for the second time.    Here is the log from a failure run of {{GarbageCollectorIntegrationTest.ROOT_DanglingMount}}:  """
"MESOS-9055","Improvement","libprocess",3,"Make gRPC call deadline configurable.","""Currently, the deadline for a gRPC call to become terminal is hard-coded to 5 seconds. This would cause problems on slow machines. Ideally, we should make this deadline configurable."""
"MESOS-9066","Task","storage",3,"Changing `CREATE_VOLUME` and `CREATE_BLOCK` to `CREATE_DISK`.","""Mesos 1.5 introduced four new operations for better storage support through CSI. These operations are:      * CREATE_VOLUME converts RAW disks to MOUNT or PATH disks.    * DESTROY_VOLUME converts MOUNT or PATH disks back to RAW disks.    * CREATE_BLOCK converts RAW disks to BLOCK disks.    * DESTROY_BLOCK converts BLOCK disks back to RAW disks.    However, the following two issues are raised for these operations:    1. """"Volume"""" is overloaded and leads to conflicting/inconsistent naming.  2. The concept of """"PATH"""" disks does not exist in CSI, which could be problematic.    To address this, we could change CREATE_VOLUME/CREATE_BLOCK to CREATE_DISK, and DESTROY_VOLUME/DESTROY_BLOCK to DESTROY_DISK, and make CREATE_DISK support only MOUNT and BLOCK disks."""
"MESOS-9070","Task","containerization",3,"Support systemd and freezer cgroup subsystems bind mount for container with rootfs.","""From MESOS-8327, cgroup subsystems are bind mounted to the container's rootfs, but systemd and freezer cgroup are not bind mounted yet since they are not subsystems under the cgroup isolator but from the linux launcher.    Some applications (e.g., dockerd) may check the /proc/self/cgorup for enabled subsystems and check them at /proc/self/mountinfo to make sure there are those mounts. Here is an example:      The first one is a task without image, the second one is a task using debian image. So any app relies on systemd and freezer cgroup would may fail:      So, we should consider add systemd and freezer cgroup bind mount at the cgroup isolator and make a *NOTE* for this behavior."""
"MESOS-9088","Improvement","allocation",3,"`createStrippedScalarQuantity()` should clear all metadata fields.","""Currently `createStrippedScalarQuantity()` strips resource meta-data and transforms dynamic reservations into a static reservation. However, no current code depends on the reservations in resources returned by this helper function. This leads to boilerplate code around call sites and performance overhead."""
"MESOS-9099","Task","allocation",3,"Add allocator quota tests regarding reserve/unreserve already allocated resources.","""Add allocator quota tests regarding reserve/unreserve already allocated resources:    - Reserve already allocated resources should not affect quota headroom;  - The same applies to unreserve allocated resources."""
"MESOS-9104","Improvement","allocation",3,"Refactor capability related logic in the allocator.","""- Add a function for returning the subset of frameworks that are capable of receiving offers from the agent. This moves the capability checking out of the core allocation logic and means the loops can just iterate over a smaller set of framework candidates rather than having to write 'continue' cases. This covers the GPU_RESOURCES and REGION_AWARE capabilities.    - Similarly, add a function that allows framework capability based filtering of resources. This pulls out the filtering logic from the core allocation logic and instead the core allocation logic can just all out to the capability filtering function. This covers the SHARED_RESOURCES, REVOCABLE_RESOURCES and RESERVATION_REFINEMENT capabilities. Note that in order to implement this one, we must refactor the shared resources logic in order to have the resource generation occur regardless of the framework capability (followed by getting filtered out via this new function if the framework is not capable)."""
"MESOS-9106","Task","containerization",3,"Add seccomp filter into containerizer launcher.","""Containerizer launcher should create an instance of the `SeccompFilter` class, which will be used to setup/load a Seccomp filter rules using the given `ContainerSeccompProfile` message."""
"MESOS-9105","Task","containerization",5,"Implement Docker Seccomp profile parser.","""The parser should translate Docker seccomp profile into the `ContainerSeccompProfile` protobuf message."""
"MESOS-9112","Bug","build|cli|python api",2,"mesos-style reports violations on a clean checkout","""When running {{support/mesos-style.py}} on a clean checkout of e.g., {{e879e920c35}} Python style violations are reported,        I would expect a clean checkout to not report any violations."""
"MESOS-9116","Bug","agent|containerization",8,"Launch nested container session fails due to incorrect detection of `mnt` namespace of command executor's task.","""Launch nested container call might fail with the following error:    This happens when the containerizer launcher [tries to enter|https://github.com/apache/mesos/blob/077f122d52671412a2ab5d992d535712cc154002/src/slave/containerizer/mesos/launch.cpp#L879-L892] `mnt` namespace using the pid of a terminated process. The pid [was detected|https://github.com/apache/mesos/blob/077f122d52671412a2ab5d992d535712cc154002/src/slave/containerizer/mesos/containerizer.cpp#L1930-L1958] by the agent before spawning the containerizer launcher process, because the process was running back then.    The issue can be reproduced using the following test (pseudocode):    When we start the very first nested container, `getMountNamespaceTarget()` returns a PID of the task (`sleep 1000`), because it's the only process whose `mnt` namespace differs from the parent container. This nested container becomes a child of PID 1 process, which is also a parent of the command executor. It's not an executor's child! It can be seen in attached `pstree.png`.    When we start a second nested container, `getMountNamespaceTarget()` might return PID of the previous nested container (`echo echo`) instead of the task's PID (`sleep 1000`). It happens because the first nested container entered `mnt` namespace of the task. Then, the containerizer launcher (""""nanny"""" process) attempts to enter `mnt` namespace using the PID of a terminated process, so we get this error."""
"MESOS-9124","Bug","allocation|master",3,"Agent reconfiguration can cause master to unsuppress on scheduler's behalf.","""When agent reconfiguration was enabled in Mesos, the allocator was also updated to remove all offer filters associated with an agent when that agent's attributes change. In addition, whenever filters for an agent are removed, the framework is unsuppressed for any roles that had filters on the agent.    While this ensures that schedulers will have an opportunity to use resources on an agent after reconfiguration, modifying the scheduler's suppression may put the scheduler in an inconsistent state, where it believes it is suppressed in a particular role when it is not."""
"MESOS-9123","Improvement","allocation",5,"Expose quota consumption metrics.","""Currently, quota related metrics exposes quota guarantee and allocated quota. We should expose """"consumed"""" which is allocated quota plus unallocated reservations. We already have this info in the allocator as `consumedQuotaScalarQuantities`, just needs to expose it."""
"MESOS-9122","Improvement","master",5,"Parallel serving of '/state' requests in the Master.","""To reduce the impact of '/state'-related workloads on the Master actor and to increase the average response time when multiple '/state' requests are in the Master's mailbox, accumulate '/state' requests and process them in parallel while blocking the master actor only once."""
"MESOS-9125","Bug","network",1,"Port mapper CNI plugin might fail with ""Resource temporarily unavailable""","""https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/isolators/network/cni/plugins/port_mapper/port_mapper.cpp#L345    Looks like we're missing a `-w` for the iptable command. This will lead to issues like      This becomes more likely if there are many concurrent launches of Mesos contianers that uses port mapper on the box."""
"MESOS-9130","Bug","resource provider|storage",1,"Test `StorageLocalResourceProviderTest.ROOT_ContainerTerminationMetric` is flaky.","""This test is flaky and can fail with the following error:    The actual error is the following:    The root cause is that the SLRP calls {{ListVolumes}} and {{GetCapacity}} when starting up, and if the plugin container is killed when these calls are ongoing, gRPC will return an {{OS Error}} which will lead the SLRP to fail.    This flakiness will be fixed once we finish https://issues.apache.org/jira/browse/MESOS-8400."""
"MESOS-9137","Bug","build",2,"GRPC build fails to pass compiler flags","""The GRPC build integration fails to pass compiler flags down from the main build into the GRPC component build. This can make the build fail in surprising ways.    For example, if you use {{CXXFLAGS=""""-fsanitize=thread"""" CFLAGS=""""-fsanitize=thread""""}}, the build fails because of the inconsistent application of these flags across bundled components.    In this build log, libprotobuf was built using the correct flags, which then causes GRPC to fail because it is missing the flags:  """
"MESOS-9143","Bug","test",2,"MasterQuotaTest.RemoveSingleQuota is flaky.",""""""
"MESOS-9151","Bug","containerization",8,"Container stuck at ISOLATING due to FD leak","""When containers are launching on a single agent at scale, one container stuck at ISOLATING could occasionally happen. And this container becomes un-destroyable due to containerizer destroy always wait for isolate() finish to continue.    We add more logging to debug this issue:     which shows that the await() in CNI::attach() stuck at the second future (io::read() for stdout).    By looking at the df of this stdout:      We found              pipe 275230 is held by the agent process and the sleep process at the same time!    The reason why the leak is possible is because we don't use `pipe2` to create a pipe with `O_CLOEXEC` in subprocess:  https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess_posix.cpp#L61    Although we do set cloexec on those fds later:  https://github.com/apache/mesos/blob/1.5.x/3rdparty/libprocess/src/subprocess.cpp#L366-L373    There is a race where a fork happens after `pipe()` call, but before cloexec is called later. This is more likely on a busy system (this explains why it's not hard to repo the issue when launching a lot of containers on a single box)."""
"MESOS-9149","Bug","build",2,"Failed to build gRPC on Linux without OpenSSL.","""Building Mesos on Ubuntu 16.04 without SSL headers installed yields the following message:  """
"MESOS-9156","Bug","agent",1,"StorageLocalResourceProviderProcess can deadlock","""On fatal conditions the {{StorageLocalResourceProviderProcess}} triggers its {{fatal}} function which causes its {{Driver}} process to be torn down. Invocations of {{fatal}} need to be properly {{defer}}'d and must never execute on the {{Driver}} process.    We saw an invocation of {{fatal}} deadlock in our internal CI since its invocation in {{StorageLocalResourceProviderProcess::sendResourceProviderStateUpdate}} wasn't explicitly {{defer}}'d, and by accident was executing on the {{Driver}}'s process."""
"MESOS-9160","Bug","build",1,"Failed to compile gRPC when the build path contains symlinks.","""  Apparently, in GRPC makefile, it uses realpath (no symlinks) when computing the build directory, whereas, Mesos builds use `abspath` (doesn't resolve symlinks). So there is a target mismatch if any directory in your Mesos path is a symlink."""
"MESOS-9158","Improvement","master",5,"Parallel serving of state-related read-only requests in the Master.","""Similar to MESOS-9122, make all read-only master state endpoints batched."""
"MESOS-9166","Improvement","storage",1,"Ignore pre-existing CSI volumes known to SLRP.","""A pre-existing CSI volume can be known to an SLRP if there is a CSI volume state checkpoint for the volume, but the corresponding resource checkpoint is missing. This typically happen when the agent ID changes which would make the SLRP lose all its resource provider state. In such a case, we should not treat these volumes as unmanaged pre-existing volumes (i.e., disk resources without profiles). For now we should not report them, and consider designing a way to recover these volumes."""
"MESOS-9164","Bug","libprocess",3,"Subprocess should unset CLOEXEC on whitelisted file descriptors.","""The libprocess subprocess API accepts a set of whitelisted file descriptors that are supposed to  be inherited to the child process. On windows, these are used, but otherwise the subprocess API just ignores them. We probably should make sure that the API clears the {{CLOEXEC}} flag on this descriptors so that they are inherited to the child."""
"MESOS-9162","Bug","containerization",5,"Unkillable pod container stuck in ISOLATING","""We have a simple test that launches a pod with two containers (one writes in a file and the other reads it). This test is flaky because the container sometimes fails to start.  Marathon app definition:        During the test, Marathon tries to launch the pod but doesn't receive a {{TASK_RUNNING}} for the first container and so after 2min decides to kill the pod which also fails.     Agent sandbox (attached to this ticket minus docker layers, since they're too big to attach) shows that one of the containers wasn't started properly - the last line in the agent log says:    Until then the log looks pretty unspektakular.     Afterwards, Marathon tries to kill the container repeatedly, but doesn't succeed - the executor receives the reuests but doesn't send anything back:      Relevant Ids for grepping the logs:  """
"MESOS-9170","Bug","build",2,"Zookeeper doesn't compile with newer gcc due to format error","""RR: https://reviews.apache.org/r/68370/"""
"MESOS-9177","Bug","master",3,"Mesos master segfaults when responding to /state requests.",""""""
"MESOS-9186","Bug","cli",2,"Failed to build Mesos with Python 3.7 and new CLI enabled","""I've tried to build Mesos with the flag 'enable-new-cli' and Python 3.7 and it failed with this error message:  """
"MESOS-9185","Bug","agent|containerization",3,"An attempt to remove or destroy container in composing containerizer leads to segfault.","""`LAUNCH_NESTED_CONTAINER` and `LAUNCH_NESTED_CONTAINER_SESSION` leads to segfault in the agent when the parent container is unknown to the composing containerizer. If the parent container cannot be found during an attempt to launch a nested container via `ComposingContainerizerProcess::launch()`, the composing container returns an error without cleaning up the container. On `launch()` failures, the agent calls `destroy()` which accesses uninitialized `containerizer` field."""
"MESOS-9190","Bug","storage",2,"Test `StorageLocalResourceProviderTest.ROOT_CreateDestroyDiskRecovery` is flaky.","""The test is flaky in 1.7.x:    This is because of `DESTRY_DISK` races with a profile poll. If the poll finishes first, SLRP will start reconciling storage pools, and drop certain incoming operations during reconciliation."""
"MESOS-9196","Bug","containerization",5,"Removing rootfs mounts may fail with EBUSY.","""We observed in production environment that this      Consider fixing the issue by using detach unmount when unmounting container rootfs. See MESOS-3349 for details.    The root cause on why """"Device or resource busy"""" is received when doing rootfs unmount is still unknown.    _UPDATE_: The production environment has a cronjob that scan filesystems to build index (updatedb for mlocate). This can explain the EBUSY we receive when doing `unmount`.    _UPDATE_: Splunk that's scanning `/var/lib/mesos` could also be a source of triggers."""
"MESOS-9201","Documentation","documentation",1,"Add docs for UPDATE_OPERATION_STATUS event","""We need to add the {{UPDATE_OPERATION_STATUS}} event to the docs for the v1 scheduler API event stream."""
"MESOS-9213","Improvement","master|metrics",1,"Avoid double copying of master->framework messages when incrementing metrics.","""When incrementing metrics, we currently do stuff like    which is not efficient. We should update such callsites to avoid gratuitous conversions which could degrade performance when many events are being sent."""
"MESOS-9217","Bug","test",2,"LongLivedDefaultExecutorRestart is flaky.",""""""
"MESOS-9221","Bug","containerization",5,"If some image layers are large, the image pulling may stuck due to the authorized token expired.","""The image layer blobs pulling happen asynchronously but in the same libprocess process. There is a chance that one layer get the token then the thread switch to another layer curling which may take long. When the original layer curling resumes, the token already expired (e.g., after 60 seconds).        The impact is the task launch stuck and all subsequent task using this image would also stuck because it waits for the same image pulling future to become ready.    Please note that this issue is not likely to be reproduced, unless on a busy system using images containing large layers."""
"MESOS-9225","Bug","modules",2,"Github's mesos/modules does not build.","""The examples modules repo at GitHub.com named mesos/modules does currently not build against the latest Apache Mesos. We should update that system. """
"MESOS-9224","Improvement","HTTP API",5,"De-duplicate read-only requests to master based on principal.","""""""Identical"""" read-only requests can be batched and answered together. With batching available (MESOS-9158), we can now deduplicate requests based on principal."""
"MESOS-9223","Improvement","agent|storage",3,"Storage local provider does not sufficiently handle container launch failures or errors","""The storage local resource provider as currently implemented does not handle launch failures or task errors of its standalone containers well enough, If e.g., a RP container fails to come up during node start a warning would be logged, but an operator still needs to detect degraded functionality, manually check the state of containers with {{GET_CONTAINERS}}, and decide whether the agent needs restarting; I suspect they do not have always have enough context for this decision. It would be better if the provider would either enforce a restart by failing over the whole agent, or by retrying the operation (optionally: up to some maximum amount of retries)."""
"MESOS-9229","Task","build|test",2,"Install Python3 on ubuntu-16.04-arm docker image","""With the upgrade to Python 3 in the Mesos codebase builds which rely on docker images started to fail since they were missing a `python3` installation.    We fixed those issues for most of the Docker images in https://issues.apache.org/jira/browse/MESOS-8957. We still miss Python 3 on the Ubuntu-16.04-arm image which can be found in `support/mesos-build/ubuntu-16.04-arm`."""
"MESOS-9266","Bug","build",2,"Whenever our packaging tasks trigger errors we run into permission problems.","""As shown in MESOS-9238, failures within our packaging cause permission failures on cleanup.        We should clean that up."""
"MESOS-9265","Task","libprocess",8,"Analyse and pinpoint libprocess SSL failures when using libevent 2.1.8.","""Mesos SSL based on libevent >2.1.5beta fails to function properly. Depending on the underlying open SSL version, failures happen on accept or on receive.  The issue has been properly described https://issues.apache.org/jira/browse/MESOS-7076. We landed a workaround by bundling libevent 2.0.22. This ticket is meant to track further analysis of the true reason for the issue - so we fix it, instead of relying on a hard to maintain bandaid. """
"MESOS-9258","Improvement","HTTP API",5,"Prevent subscribers to the master's event stream from leaking connections","""Some reverse proxies (e.g., ELB using an HTTP listener) won't close the upstream connection to Mesos when they detect that their client is disconnected.    This can make Mesos leak subscribers, which generates unnecessary authorization requests and affects performance.    We should evaluate methods (e.g., heartbeats) to enable Mesos to detect that a subscriber is gone, even if the TCP connection is still open.  """
"MESOS-9275","Improvement","resource provider|storage",8,"Allow optional `profile` to be specified in `CREATE_DISK` offer operation.","""This will allow the framework to """"import"""" pre-existing volumes reported by the corresponding CSI plugin.    For instance, the LVM CSI plugin might detect some pre-existing volumes that Dan has created out of band. Currently, those volumes will be represented as RAW """"disk"""" resource with a volume ID, but no volume profile by the SLRP. When a framework tries to use the RAW volume as either MOUNT or BLOCK volume, it'll issue a CREATE_DISK operation. The corresponding SLRP will handles the operation, and validate against a default profile for MOUNT volumes. However, this prevents the volume to have a different profile that the framework might want.    Ideally, we should allow the framework to optionally specify a profile that it wants the volume to have during CREATE_DISK because it might have some expectations on the volume. The SLRP will validate with the corresponding CSI plugin using the ValidateVolumeCapabilities RPC call to see if the profile is applicable to the volume."""
"MESOS-9274","Bug","java api|scheduler driver",3,"v1 JAVA scheduler library can drop TEARDOWN upon destruction.","""Currently the v1 JAVA scheduler library neither ensures {{Call}} s are sent to the master nor waits for responses. This can be problematic if the library is destroyed (or garbage collected) right after sending a {{TEARDOWN}} call: destruction of the underlying {{Mesos}} actor races with sending the call.    """
"MESOS-9270","Task","cni|containerization",3,"Get rid of dependency on `net-tools` in network/cni isolator.","""The `network/cni` isolator has a dependency on `net-tools`. The last release of `net-tools` was released in 2001. The tools were deprecated many years ago (see [Debian|https://lists.debian.org/debian-devel/2009/03/msg00780.html], [RH|https://bugzilla.redhat.com/show_bug.cgi?id=687920], and [LWN|https://lwn.net/Articles/710533/]) and no longer installed by default.    [https://github.com/apache/mesos/blob/983607e/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L2248]"""
"MESOS-9278","Task","agent",3,"Add an operation status update manager to the agent","""Review here: https://reviews.apache.org/r/69505/"""
"MESOS-9281","Bug","storage",5,"SLRP gets a stale checkpoint after system crash.","""SLRP checkpoints a pending operations before issuing the corresponding CSI call through {{slave::state::checkpoint}}, which writes a new checkpoint to a temporary file then do a {{rename}}. However, because we don't do any {{fsync}}, {{rename}} is not atomic w.r.t. system crash. As a result, if the operation is processed during a system crash, it is possible that the CSI call has been executed, but the SLRP gets back a stale checkpoint after reboot and totally doesn't know about the operation.    To address this problem, we need to ensure the followings before issuing the CSI call:   1. The temp file is synced to the disk.   2. The rename is committed to the disk.    A possible solution is to do an {{fsync}} after writing the temp file, and do another {{fsync}} on the checkpoint dir after the {{rename}}."""
"MESOS-9283","Bug","containerization",3,"Docker containerizer actor can get backlogged with large number of containers.","""We observed during some scale testing that we do internally.    When launching 300+ Docker containers on a single agent box, it's possible that the Docker containerizer actor gets backlogged. As a result, API processing like `GET_CONTAINERS` will become unresponsive. It'll also block Mesos containerizer from launching containers if one specified `--containers=docker,mesos` because Docker containerizer launch will be invoked first by the composing containerizer (and queued).    Profiling results show that the bottleneck is `os::killtree`, which will be invoked when the Docker commands are discarded (e.g., client disconnect, etc.).    For this particular case, killtree is not really necessary because the docker command does not fork additional subprocesses. If we use the argv version of `subprocess` to launch docker commands, we can simply use os::kill instead. We confirmed that, by switching to os::kill, the performance issues goes away, and the agent can easily scale up to 300+ containers."""
"MESOS-9292","Improvement","allocation",1,"Rejected quotas request error messages should specify which resources were overcommitted.","""If we reject a quota request due to not having enough available resources, we fail with the following error:      but we don't print *which* resource was not available. This can be confusing to operators when the quota was attempted to be set for multiple resources at once."""
"MESOS-9295","Bug","containerization",5,"Nested container launch could fail if the agent upgrade with new cgroup subsystems.","""Nested container launch could fail if the agent upgrade with new cgroup subsystems, because the new cgroup subsystems do not exist on parent container's cgroup hierarchy."""
"MESOS-9302","Bug","build",2,"Mesos fails to build on Fedora 28","""Trying to compile a fresh Mesos checkout on a Fedora 28 system with the following configuration flags:    and the following compiler    fails the build due to two warnings (even though --disable-werror was passed):  """
"MESOS-9306","Bug","agent|containerization",5,"Mesos containerizer can get stuck during cgroup cleanup","""I observed a task group's executor container which failed to be completely destroyed after its associated tasks were killed. The following is an excerpt from the agent log which is filtered to include only lines with the container ID, {{d463b9fe-970d-4077-bab9-558464889a9e}}:    The last log line from the containerizer's destroy path is:    (that is the second such log line, from {{LinuxLauncherProcess::_destroy}})  Then we just see    repeatedly, which occurs because the agent's {{GET_CONTAINERS}} call is being polled once per minute. This seems to indicate that the container in question is still in the agent's {{containers_}} map.    So, it seems that the containerizer is stuck either in the Linux launcher's {{destroy()}} code path, or the containerizer's {{destroy()}} code path."""
"MESOS-9305","Improvement","containerization",3,"Create cgoup recursively to workaround systemd deleting cgroups_root.","""This is my case:    My cgroups_root of mesos-slave is some_user/mesos under /sys/fs/cgroup。    It happens that this some_user dir may be gone for some unknown reason, in which case I can no longer create any cgroup and any task will fail.    So I would like to change          to    in CgroupsIsolatorProcess::prepare in src/slave/containerizer/mesos/isolators/cgroups/cgroups.cpp.    However, I'm not sure if there's any potential problem doing so. Any advice?     """
"MESOS-9308","Bug","storage",1,"URI disk profile adaptor could deadlock.","""The loop here can be infinit:  https://github.com/apache/mesos/blob/1.7.0/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L61-L80    """
"MESOS-9314","Improvement","HTTP API",5,"Consider introducing a ScalarResourceQuantity protobuf message.","""As part of introducing quota limits, we're adding a new master::Call for updating quota. This call can take a simplified message that expresses scalar resource quantities:        This greatly simplified the validation code, as well as the UX of the API when it comes to knowing what kind of data to provide.    Ideally, the new quota paths can use this message in lieu of Resource objects, but we'll have to explore backwards compatibility (e.g. registry data)."""
"MESOS-9317","Bug","master",5,"Some master endpoints do not handle failed authorization properly.","""When we authorize _some_ actions (right now I see this happening to create / destroy volumes, reserve / unreserve resources) *and* {{authorizer}} fails (i.e. returns the future in non-ready state), an assertion is triggered:    This is due to incorrect assumption in our code, see for example [https://github.com/apache/mesos/blob/a063afce9868dcee38a0ab7efaa028244f3999cf/src/master/master.cpp#L3752-L3763]:    Futures returned from {{await}} are guaranteed to be in terminal state, but not necessarily ready! In the snippet above, {{!authorization.get()}} is invoked without being checked ⇒ assertion fails.    Full stack trace:  """
"MESOS-9324","Bug","allocation",3,"Resource fragmentation: frameworks may be starved of port resources in the presence of large number roles with quota.","""In our environment where there are 1.5k frameworks and quota is heavily utilized, we would experience a severe resource fragmentation issue. Specifically, we observed a large number of port-less offers circulating in the cluster. Thus frameworks that need port resources are not able to launch tasks even if their roles have quota (because currently, we can only set quota for scalar resources, not port range resources).    While most of the 1.5k frameworks do not suppress today and we believe the situation will significantly improve once they do. Still, I think there are some improvements the Mesos allocator can make to help.    h3. How resource becomes fragmented  The origin of these port-less offers stems from quota chopping. Specifically, when chopping an agent to satisfy a role’s quota, we will also hand out resources that this role does not have quota for (as long as it does not break other role’s quota). These “extra resources” certainly includes ALL the remaining port resources on the agent. After this offer, the agent will be left with no port resources even though it still has CPUs and etc. Later, these resources may be offered to other frameworks but they are useless due to no ports. Now we have some “bad offers” in the cluster.    h3. How resource fragmentation prolonged  A resource offer, once it is declined (e.g. due to no ports), is recovered by the allocator and offered to other frameworks again. Before this happens, it is possible that this offer might be able to merge with either the remaining resources or other declined resources on the same agent. However, it is conceivable that not uncommonly, the declined offer will be hand out again *as-is*.  This is especially probable if the allocator makes offers faster than the framework offer response time. As a result, we will observe the circulation of bad offers across different frameworks. These bad offers will exist for a long time before being consolidated again. For how long? *The longevity of the bad offer will be roughly proportional to the number of active frameworks*. In the worse case, once all the active frameworks have (hopefully long) declined the bad offer, the bad offer will have nowhere to go and finally start to merge with other resources on that agent.    Note, since the allocator performance has greatly improved in the past several months. The scenario described here could be increasingly common. Also, as we introduce quota limits and hierarchical quota, there will be much more agent chopping, making resource fragmentation even worse.    h3. Near-term Mitigations  As mentioned above, the longevity of a bad offer is proportional to the active frameworks. Thus framework suppression will certainly help. In addition, from the Mesos side, a couple of mitigation measures are worth considering (other than the long-term optimistic allocation strategy):    1. Adding a defragment interval once in a while in the allocator. For example, each minute or a dozen allocation cycles or so, we will pause the allocation, rescind all the offers and start allocating again. This essentially eliminates all the circulating bad offers by giving them a chance to be consolidated. Think of this as a periodic “reboot” of the allocator.  2. Consider chopping non-quota resources as well. Right now, for resources such as ports (or any other resources that the role does not have quota for), all are allocated in a single offer. We could choose to chop these non-quota resources as well. For example, port resources can be distributed proportionally to allocated CPU resources.  3. Provide support for specifying port quantities. With this, we can utilize the existing quota or `min_allocatable_resources` APIs to guarantee a certain number of port resources."""
"MESOS-9321","Improvement","resource provider|storage",5,"Add an optional `vendor` field in `Resource.DiskInfo.Source`.","""This will allow the framework to recover volumes reported by the corresponding CSI plugin across agent ID changes.    When an agent changes its ID, all reservation information related to resources coming from a given resource provider will be lost, so frameworks needs an unique identifier to identify if a new volume associated with the new agent ID is the same volume. Since CSI volume ID are not unique across different plugins, we will need to add a new {{vendor}} field, which together with the existing {{id}} field can provide the means to globally uniquely identify this source."""
"MESOS-9320","Bug","containerization",5,"UCR container launch stuck at PROVISIONING during image fetching.","""We observed mesos containerizer stuck at PROVISIONING when launching a mesos container using docker image: `kvish/jenkins-dev:595c74f713f609fd1d3b05a40d35113fc03227c9`:    The image pulling never finishes. Insufficient image contents are still in image store staging directory /var/lib/mesos/slave/store/docker/staging/egLYqO, forever.      It is not clear yet why the SHA pulling does not finish, so we use the same image on another empty machine with UCR. The other machine has the container RUNNING correctly, and has the following staging directory before moving to the layers dir:      By comparing two cases, we can see one layer `8aabf8f13bdf0feed398c7c8b0ac24db59d60d0d06f9dc6cb1400de4df898324` is missing on the problematic agent node, and it is the last layer to fetch.    Here is the manifest as a reference:      This should not be related: when we try to find the extracted layers on the layers dir, we could only find two:      These two are base layers that were downloaded earlier from other images. We still need to figure out why there is one layer fetch not finished. (no curl process and tar process running stuck at background)"""
"MESOS-9331","Bug","stout",1,"Some library functions ignore failures from ::close which should probably be handled.","""Multiple functions e.g., in {{stout}} ignore the return value of {{::close}} with the following rationale,    I believe this is incorrect in general. Especially when not calling {{::fsync}} after a write operation, the kernel might buffer writes and only trigger write-related failures when the file descriptor is closed, see e.g., the note on error handling [here|http://man7.org/linux/man-pages/man2/close.2.html#NOTES].    We should audit our code to make sure that failures to {{::close}} file descriptors are properly propagated when needed."""
"MESOS-9333","Task","cli",3,"Document usage and build of new Mesos CLI","""Stating how to compile and use the Mesos CLI + its limitations (only Mesos containerizer, exec DEBUG follows task-user)."""
"MESOS-9332","Bug","containerization",3,"Nested container should run as the same user of its parent container by default.","""Currently when launching a debug container, by default Mesos agent will use the executor's user as the debug container's user if the `user` field is not specified in the debug container's `commandInfo` (see [this code|https://github.com/apache/mesos/blob/1.7.0/src/slave/http.cpp#L2559] for details). This is OK for the command task since the command executor's user is same with command task's user (see [this code|https://github.com/apache/mesos/blob/1.7.0/src/slave/slave.cpp#L6068:L6070] for details), so the debug container will be launched as the same user of the task. But for the task in a task group, the default executor's user is same with the framework user (see [this code|https://github.com/apache/mesos/blob/1.7.0/src/slave/slave.cpp#L8959] for details), so in this case the debug container will be launched as the same user of the framework rather than the task. So in a scenario that framework user is a normal user but the task user is root, the debug container will be launched as the normal which is not desired, the expectation is the debug container should run as the same user of the container it debugs."""
"MESOS-9334","Bug","containerization",5,"Container stuck at ISOLATING state due to libevent poll never returns.","""We found UCR container may be stuck at `ISOLATING` state:     In the above logs, the state of container `1e5b8fc3-5c9e-4159-a0b9-3d46595a5b54` was transitioned to `ISOLATING` at 09:13:23, but did not transitioned to any other states until it was destroyed due to the executor registration timeout (10 mins). And the destroy can never complete since it needs to wait for the container to finish isolating."""
"MESOS-9344","Task","cli",3,"Add test for `mesos task attach` on task launched without a TTY","""As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py"""
"MESOS-9343","Task","cli",3,"Add test(s) for `mesos task attach` on task launched with a TTY ","""As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py"""
"MESOS-9342","Task","cli",3,"Add interactive test(s) for `mesos task exec`","""As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py  This will require new helper functions to get the input/output of the command."""
"MESOS-9341","Task","cli",3,"Add non-interactive test(s) for `mesos task exec`","""As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py"""
"MESOS-9357","Bug","test",1,"FetcherTest.DuplicateFileURI fails on macos","""I see {{FetcherTest.DuplicateFileURI}} fail pretty reliably on macos, e.g., 10.14.  """
"MESOS-9366","Bug","test",3,"Test `HealthCheckTest.HealthyTaskNonShell` can hang.","""In {{HealthCheckTest.HealthyTaskNonShell}} the {{statusRunning}} future is incorrectly checked before being waited:  [https://github.com/apache/mesos/blob/d8062f231b9f27889b7cae7a42eef49e4eed79ec/src/tests/health_check_tests.cpp#L673]  As a result, if for some arbitrary reason there is only one task status update sent (e.g., {{TASK_FAILED}}), {{statusRunning->state()}} will make the test hang forever:    (The line number above are not correct because of additional logging I added to triage this error.)"""
"MESOS-9384","Improvement","agent|master",5,"Resource providers reported by master should reflect connected resource providers","""Currently, the master will remember any resource provider it saw ever and report it e.g., in {{GET_AGENTS}} responses, regardless of whether the resource provider is currently connected or not. This is not very intuitive.    The master should instead only report resource providers which are currently connected. Agents can still report even disconnected resource providers."""
"MESOS-9386","Task","containerization",5,"Implement Seccomp profile inheritance for POD containers","""Child containers inherit its parent container's Seccomp profile by default. Also, Seccomp profile can be overridden by a Framework for a particular child container by specifying a path to the Seccomp profile.    Mesos containerizer persists information about containers on disk via `ContainerLaunchInfo` proto, which includes `ContainerSeccompProfile` proto. Mesos containerizer should use this proto to load the parent's profile for a child container. When a child inherits the parent's Seccomp profile, Mesos agent doesn't have to re-read a Seccomp profile from the disk, which was used for the parent container. Otherwise, we would have to check that a file content hasn't changed since the last time the parent was launched."""
"MESOS-9395","Bug","storage",5,"Check failure on `StorageLocalResourceProviderProcess::applyCreateDisk`.","""Observed the following agent failure on one of our staging clusters:  """
"MESOS-9399","Task","cli",2,"Update 'mesos task list' to only list running tasks","""Doing a {{mesos task list}} currently returns all tasks that have ever been run (not just running tasks). The default behavior should be to return only the running tasks and offer an option to return all of them. To tell them apart, there should be a state field in the table returned by this command."""
"MESOS-9406","Improvement","build|cmake",2,"Allow for optionally unbundled leveldb from CMake builds.","""Following the example of unbundled libevent and libarchive, we should allow for unbundled leveldb if the user wishes so.    For leveldb, this task is not as trivial as one would hope due to the fact that we link leveldb statically. This forces us to satisfy leveldb's strong dependencies against gpertools (tcmalloc) as well as snappy.  Alternatively, we would resort into linking leveldb dynamically, solving these issues.  """
"MESOS-9411","Bug","libprocess",5,"Validation of JWT tokens using HS256 hashing algorithm is not thread safe.","""from the [OpenSSL documentation|https://www.openssl.org/docs/man1.0.2/crypto/hmac.html]:    {quote}  It places the result in {{md}} (which must have space for the output of the hash function, which is no more than {{EVP_MAX_MD_SIZE}} bytes). If {{md}} is {{NULL}}, the digest is placed in a static array. The size of the output is placed in {{md_len}}, unless it is {{NULL}}. Note: passing a {{NULL}} value for {{md}} to use the static array is not thread safe.  {quote}    We are calling {{HMAC()}} as follows:        Given that this code does not run inside a process, race conditions could occur."""
"MESOS-9419","Bug","master",3,"Executor to framework message crashes master if framework has not re-registered.","""If the executor sends a framework message after a master failover, and the framework has not yet re-registered with the master, this will crash the master:        This is because Framework::send proceeds if the framework is disconnected. In the case of a recovered framework, it will not have a pid or http connection yet:    https://github.com/apache/mesos/blob/9b889a10927b13510a1d02e7328925dba3438a0b/src/master/master.hpp#L2590-L2610        The executor to framework path does not guard against the framework being disconnected, unlike the status update path:    https://github.com/apache/mesos/blob/9b889a10927b13510a1d02e7328925dba3438a0b/src/master/master.cpp#L6472-L6495    vs.    https://github.com/apache/mesos/blob/9b889a10927b13510a1d02e7328925dba3438a0b/src/master/master.cpp#L8371-L8373    It was reported that this crash didn't occur for the user on 1.2.0, however the issue appears to present there as well, so we will try to backport a test to see if it's indeed not occurring in 1.2.0."""
"MESOS-9427","Documentation","allocation|documentation",5,"Revisit quota documentation.","""At this point the quota documentation in the docs/ folder has become rather stale. It would be good to at least update any inaccuracies and ideally re-write it to better reflect the current thinking."""
"MESOS-9434","Bug","agent|resource provider",2,"Completed framework update streams may retry forever","""Since the agent/RP currently does not GC operation status update streams when frameworks are torn down, it's possible that active update streams associated with completed frameworks may remain and continue retrying forever. We should add a mechanism to complete these streams when the framework becomes completed.    A couple options which have come up during discussion:  * Have the master acknowledge updates associated with completed frameworks. Note that since completed frameworks are currently only tracked by the master in memory, a master failover could prevent this from working perfectly.  * Extend the RP API to allow the GC of particular update streams, and have the agent GC streams associated with a framework when it receives a {{ShutdownFrameworkMessage}}. This would also require the addition of a new method to the status update manager."""
"MESOS-9462","Bug","containerization",3,"Devices in a container are inaccessible due to `nodev` on `/var/run`.","""A recent [patch|https://reviews.apache.org/r/69086/] (commit ede8155d1d043137e15007c48da36ac5fa0b5124) changes the behavior of how standard device nodes (e.g., /dev/null, etc.) are setup. It uses bind mount (from host) now (instead of mknod).    The devices nodes are created under `/var/run/mesos/containers/<container_id>/devices`, and then bind mounted to the container root filesystem. This is problematic for those Linux distros that mount `/var/run` (or `/run`) as `nodev`. For instance, CentOS 7.4:      As a result, the `/dev/null` devices in the container will inherit the `nodev` from `/run` on the host      This will cause """"Permission Denied"""" error when a process in the container tries to open the device node.    You can try to reproduce this issue using Mesos Mini      And the, go to Marathon UI (http://localhost:8080), and launch an app using the following config      You'll see the task failed with """"Permission Denied"""".    The task will run normally if you use `mesos/mesos-mini:master-2018-12-01`"""
"MESOS-9473","Task","master",8,"Add end to end tests for operations on agent default resources.","""Making note of particular cases we need to test:  * Verify that frameworks will receive OPERATION_GONE_BY_OPERATOR for operations on agent default resources when an agent is marked gone  * Verify that frameworks will receive OPERATION_GONE_BY_OPERATOR when they reconcile operations on agents which have been marked gone"""
"MESOS-9472","Task","master",3,"Unblock operation feedback on agent default resources.","""# Remove {{CHECK}} marked with a TODO in {{Master::updateOperationStatus()}}.  # Update {{Master::acknowledgeOperationStatus()}}, remove the CHECK requiring a resource provider ID.  # Remove validation in {{Option<Error> validate(mesos::scheduler::Call& call, const Option<Principal>& principal)}}"""
"MESOS-9471","Task","master",3,"Master should track operations on agent default resources.","""Make {{Master::updateSlave()}} add operations that the agent sends and the master doesn't know.    Right now only operations from SLRPs are added to the master's in-memory state."""
"MESOS-9469","Bug","master",1,"Mesos does not validate framework-supplied FrameworkIDs","""Since Mesos masters do not persist frameworks (MESOS-1719) we might subscribe schedulers with self-assigned {{FrameworkIDs}}.    While we cannot confirm that used {{FrameworkIDs}} were indeed assigned by Mesos masters, we should still validate the supplied values to make sure they do not break our internal assumptions (e.g., IDs might be used to construct filesystem paths)."""
"MESOS-9474","Bug","master",3,"Master does not respect authorization result for `CREATE_DISK` and `DESTROY_DISK`.","""On our internal cluster with a custom authorizer module we observed the following problem:    The authorizer module caches authorization results, and that's why there was only one logged authorization requset. The problem is that, the logged request was for {{CREATE_MOUNT_DISK}}, and the result was {{deny}}, but despite the denial, all {{CREATE_DISK}} operations were processed, however another {{RESERVE}} operation was dropped because of this denial.    The bug is that the master pushed a authorization future in the {{futures}} vector in {{Master::accept}} for each {{DESTROY_DISK}}:   [https://github.com/apache/mesos/blob/18356bf3f4ac730b4a798261aad042555c4a4834/src/master/master.cpp#L4599-L4601]   However, the master never popped and checked the future in {{Master::_accept}}, but go ahead to process the operation:   [https://github.com/apache/mesos/blob/18356bf3f4ac730b4a798261aad042555c4a4834/src/master/master.cpp#L5706]   The future ended up mismatched with the {{RESERVE}} operation, causing it to be dropped.    Updated: a similar bug would happen if we have a validation error for {{LAUNCH_GROUP}}. See MESOS-9480."""
"MESOS-9477","Task","documentation",5,"Documentation for operation feedback","""Review: https://reviews.apache.org/r/69871/"""
"MESOS-9480","Bug","master",2,"Master may skip processing authorization results for `LAUNCH_GROUP`.","""If there is a validation error for {{LAUNCH_GROUP}}, or if there are multiple authorization errors for some of the tasks in a {{LAUNCH_GROUP}}, the master will skip processing the remaining authorization results, which would result in these authorization results being examined by subsequent operations incorrectly:  https://github.com/apache/mesos/blob/3ade731d0c1772206c4afdf56318cfab6356acee/src/master/master.cpp#L5487-L5521    """
"MESOS-9486","Improvement","master",1,"Set up `object.value` for `CREATE_DISK` and `DESTROY_DISK` authorizations.","""We should be defensive and set up {{object.value}} to the role of the resource for authorization actions {{CREATE_BLOCK_DISK}}, {{DESTROY_BLOCK_DISK}}, {{CREATE_MOUNT_DISK}} and {{DESTROY_MOUNT_DISK}} so an old-school authorizer can rely on the field to perform authorization.    This behavior is deprecated though, so will be removed once all {{*_WITH_ROLE}} authorization action aliases are removed through MESOS-7073."""
"MESOS-9485","Task","test",5,"Unit test for master operation authorization.","""We should create a unit test for MESOS-9474 and MESOS-9480. To make the test easier, we might want to consider using operation feedback once MESOS-9472 is done."""
"MESOS-9495","Bug","test",1,"Test `MasterTest.CreateVolumesV1AuthorizationFailure` is flaky.","""  This is because we authorize the retried registration before dropping it.    Full log: [^mesos-ec2-centos-7-CMake.Mesos.MasterTest.CreateVolumesV1AuthorizationFailure-badrun.txt]"""
"MESOS-9497","Improvement","HTTP API|master",5,"Parallel reads for expensive master v1 read-only calls.","""Similar to MESOS-9158 - we should make the operator API calls which serve master state perform computation of multiple such responses in parallel to reduce the performance impact on the master actor.    Note that this includes the initial expensive SUBSCRIBE payload for the event streaming API, which is less straightforward to incorporate into the parallel serving logic since it performs writes (to track the subscriber) and produces an infinite response, unlike the other state related calls."""
"MESOS-9501","Bug","containerization",5,"Mesos executor fails to terminate and gets stuck after agent host reboot.","""When an agent host reboots, all of its containers are gone but the agent will still try to recover from its checkpointed state after reboot.    The agent will soon discover that all the cgroup hierarchies are gone and assume (correctly) that the containers are destroyed.    However, when trying to terminate the executor, the agent will first try to wait for the exit status of its container:  https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/containerizer.cpp#L2631    Agent dose so by `waitpid` on the checkpointed child process pid. If, after the agent host reboot, a new process with the same pid gets spawned, then the parent will wait for the wrong child process. This could get stuck until the wrongly waited-for  process is somehow exited, see `ReaperProcess::wait()`: https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L88-L114    This will block the executor termination as well as future task status update (e.g. master might still think the task is running)."""
"MESOS-9502","Bug","containerization",8,"IOswitchboard cleanup could get stuck due to FD leak from a race.","""Our check container got stuck during destroy which in turned stucks the parent container. It is blocked by the I/O switchboard cleanup:    1223 18:04:41.000000 16269 switchboard.cpp:814] Sending SIGTERM to I/O switchboard server (pid: 62854) since container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e is being destroyed  ....  1227 04:45:38.000000  5189 switchboard.cpp:916] I/O switchboard server process for container 4d4074fa-bc87-471b-8659-08e519b68e13.16d02532-675a-4acb-964d-57459ecf6b67.check-e91521a3-bf72-4ac4-8ead-3950e31cf09e has terminated (status=N/A)    Note the timestamp.    *Root Cause:*  Fundamentally, this is caused by a race between *.discard()* triggered by Check Container TIMEOUT and IOSB extracting ContainerIO object. This race could be exposed by overloaded/slow agent process. Please see how this race be triggered below:    # Right after IOSB server process is running, Check container Timed out and the checker process returns a failure, which would close the HTTP connection with agent.  # From the agent side, if the connection breaks, the handler will trigger a discard on the returned future and that will result in containerizer->launch()'s future transitioned to DISCARDED state.  # In containerizer, the DISCARDED state will be propagated back to IOSB prepare(), which stop its continuation on *extracting the containerIO* (it implies the object being cleaned up and FDs(one end of pipes created in IOSB) being closed in its destructor).  # Agent starts to destroy the container due to its discarded launch result, and asks IOSB to cleanup the container.  # IOSB server is still running, so agent sends a SIGTERM.  # SIGTERM handler unblocks the IOSB from redirecting (to redirect stdout/stderr from container to logger before exiting).  # io::redirect() calls io::splice() and reads the other end of those pipes forever.    This issue is *not easy to reproduce unless* on a busy agent, because the timeout has to happen exactly *AFTER* IOSB server is running and *BEFORE* IOSB extracts containerIO."""
"MESOS-9504","Improvement","allocation",3,"Use ResourceQuantities in the allocator and sorter to improve performance.","""In allocator and sorter, we need to do a lot of quantity calculations. Currently, we use the full {{Resources}} type with utilities like {{createScalarResourceQuantities()}}, even though we only care about quantities. Replace {{Resources}} with {{ResourceQuantities}}.    See:    https://github.com/apache/mesos/blob/386b1fe99bb9d10af2abaca4832bf584b6181799/src/master/allocator/sorter/drf/sorter.hpp#L444-L445  https://reviews.apache.org/r/70061/    With the addition of ResourceQuantities, callers can now just do {{ResourceQuantities.fromScalarResources(r.scalars())}} instead of using {{Resources::createStrippedScalarQuantity()}}, which should actually be a bit more efficient since we only copy the shared pointers rather than construct new `Resource` objects."""
"MESOS-9507","Bug","containerization",5,"Agent could not recover due to empty docker volume checkpointed files.","""Agent could not recover due to empty docker volume checkpointed files. Please see logs:    This might happen after hard reboot. Docker volume isolator uses `state::checkpoint()` function which creates a temporary file, then writes the data, then renames the temporary file to destination file. This function is atomic and supports `fsync` for the data. However, Docker volume isolator does not use `fsync` option for performance reasons, hence the data might be lost if page cache is not synced before reboot.    Basically the docker volume is not mounted yet, so the docker volume isolator should skip recovering this volume."""
"MESOS-9509","Task","executor",5,"Benchmark command health checks in default executor","""TCP/HTTP health checks were extensively scale tested as part of https://mesosphere.com/blog/introducing-mesos-native-health-checks-apache-mesos-part-2/.     We should do the same for command checks by default executor because it uses a very different mechanism (agent fork/execs the check command as a nested container) and will have very different scalability characteristics.    We should also use these benchmarks as an opportunity to produce perf traces of the Mesos agent (both with and without process inheritance) so that a thorough analysis of the performance can be done as part of MESOS-9513."""
"MESOS-9508","Bug","build",2,"Official 1.7.0 tarball can't be built on Ubuntu 16.04 LTS.","""I installed Ubuntu 16.04.5 LTS in a VM and precisely followed the steps in [http://mesos.apache.org/documentation/latest/building/] to build Mesos (fetching the 1.7.0 release). Nevertheless what I get is the following error message:  {code:sh}  make[4]: Entering directory '/home/max/mesos-1.7.0/build/3rdparty/grpc-1.10.0'    DEPENDENCY ERROR    The target you are trying to run requires an OpenSSL implementation.  Your system doesn't have one, and either the third_party directory  doesn't have it, or your compiler can't build BoringSSL.  """
"MESOS-9514","Bug","build",2,"Reviewboard bot fails on verify-reviews.py.","""Seeing this on our Azure based Mesos CI for review requests.        This is happening pretty much exactly since we landed https://github.com/apache/mesos/commit/3badf7179992e61f30f5a79da9d481dd451c7c2f#diff-0bcbb572aad3fe39e0e5c3c8a8c3e515"""
"MESOS-9513","Task","agent",5,"Investigate command health check performance","""Users have reported performance issues caused by too many command health checks performed too quickly by the default executor. Use of the agent's LAUNCH_NESTED_CONTAINER_SESSION call can impact the containerizer's ability to successfully launch containers in general.    We need to investigate this issue and decide on a path forward to improve the performance of command health checks."""
"MESOS-9517","Bug","resource provider|storage",8,"SLRP should treat gRPC timeouts as non-terminal errors, instead of reporting OPERATION_FAILED.","""1. framework executes a CREATE_DISK operation.  2. The SLRP issues a CreateVolume RPC to the plugin  3. The RPC call times out  4. The agent/SLRP translates non-terminal gRPC timeout errors (DeadlineExceeded) for """"CreateVolume"""" calls into OPERATION_FAILED, which is terminal.  5. framework receives a *terminal* OPERATION_FAILED status, so it executes another CREATE_DISK operation.  6. The second CREATE_DISK operation does not timeout.  7. The first CREATE_DISK operation was actually completed by the plugin, unbeknownst to the SLRP.  8. There's now an orphan volume in the storage system that no one is tracking.    Proposed solution: the SLRP makes more intelligent decisions about non-terminal gRPC errors. For example, timeouts are likely expected for potentially long-running storage operations and should not be considered terminal. In such cases, the SLRP should NOT report OPERATION_FAILED and instead should re-issue the **same** (idempotent) CreateVolume call to the plugin to ascertain the status of the requested volume creation.    Agent logs for the 3 orphan vols above:  """
"MESOS-9519","Bug","build",2,"Unable to build Mesos with CMake on Ubuntu 14.04.","""Running the following command to build Mesos on Ubuntu 14.04 will lead to the error shown below:      The reason is that gRPC's CMake rules does not disable ALPN on systems with OpenSSL 1.0.1."""
"MESOS-9523","Improvement","allocation",5,"Add per-framework allocatable resources matcher/filter.","""Currently, Mesos has a single global flag `min_allocatable_resources` that provides some control over the shape of the offer. But, being a global flag, finding a one-size-fits-all shape is hard and less than ideal. It will be great if frameworks can specify different shapes based on their needs.     In addition to extending this flag to be per-framework. It is also a good opportunity to see if it can be more than `min_alloctable` e.g. providing more predicates such as max,  (not) contain and etc. """
"MESOS-9525","Task","agent",3,"Agent capability for operation feedback on default resources","""We should add an agent capability to prevent the master from sending operations on agent default resources which request feedback to older agents which are not able to handle them."""
"MESOS-9532","Bug","containerization",2,"ResourceOffersTest.ResourceOfferWithMultipleSlaves is flaky.","""    caused by this commit https://github.com/apache/mesos/commit/07bccc6377a180267d4251897a765acba9fa0c4d"""
"MESOS-9533","Bug","cni|containerization",2,"CniIsolatorTest.ROOT_CleanupAfterReboot is flaky.","""    It was from this commit https://github.com/apache/mesos/commit/c338f5ada0123c0558658c6452ac3402d9fbec29"""
"MESOS-9538","Task","agent",3,"Agent `ReconcileOperations` handler should handle operation affecting default resources","""{{Slave::reconcileOperations()}} has to be updated to send {{OPERATION_DROPPED}} for unknown operations that don't have a resource provider ID."""
"MESOS-9537","Bug","storage",3,"SLRP sends inconsistent status updates for dropped operations.","""The bug manifests in the following scenario:  1. Upon receiving profile updates, the SLRP sends an {{UPDATE_STATE}} to the agent with a new resource version.  2. At the same time, the agent sends an {{APPLY_OPERATION}} to the SLRP with the original resource version.  3. The SLRP asks the status update manager (SUM) to reply with an {{OPERATION_DROPPED}} to the framework because of the resource version mismatch. The status update is required to be acked. Then, it simply discards the operation (i.e., no bookkeeping).  4. The agent finds a missing operation in the {{UPDATE_STATE}} so it sends a {{RECONCILE_OPERATIONS}}.  5. The SLRP asks the SUM to reply with an {{OPERATION_DROPPED}} to the agent (without a framework ID set) because it no longer knows about the operation.  6. The SUM returns an error because the latter {{OPERATION_DROPPED}} is inconsistent with the earlier one since it does not have a framework ID."""
"MESOS-9540","Improvement","storage",2,"Support `DESTROY_DISK` on preprovisioned CSI volumes.","""Currently the experimental {{DESTROY_DISK}} operation only applies to {{BLOCK}} or {{MOUNT}} disk resources. We should consider supporting {{DESTROY_DISK}} on {{RAW}} disk resources with source IDs as well. This could be handy in e.g., the following scenario:    1. The framework issued a {{CREATE_DISK}}.  2. The SLRP received {{CREATE_DISK}} and translated to a {{CreateVolume}} CSI call.  3. While the {{CreateVolume}} is ongoing, the agent was restarted with a new agent ID, causing the SLRP to lose its bookkeeping and start with a new RP ID as well, and hence the {{CREATE_DISK}} operation was lost.  4. The {{CreateVolume}} call succeeded and the new SLRP picked it up as a preprovisioned CSI volume.    In the above case, the framework should be able to choose to either """"import"""" the CSI volume through {{CREATE_DISK}}, or directly reclaim the space through {{DESTROY_DISK}}. Currently the framework needs to always import the CSI volume before reclaiming the space."""
"MESOS-9542","Bug","master",5,"Hierarchical allocator check failure when an operation on a shutdown framework finishes","""When a non-speculated operation like e.g., {{CREATE_DISK}} becomes terminal after the originating framework was torn down, we run into an assertion failure in the allocator.    With non-speculated operations like e.g., {{CREATE_DISK}} it became possible that operations outlive their originating framework. This was not possible with speculated operations like {{RESERVE}} which were always applied immediately by the master.    The master does not take this into account, but instead unconditionally calls {{Allocator::updateAllocation}} which asserts that the framework is still known to the allocator.    Reproducer:   * register a framework with the master.   * add a master with a resource provider.   * let the framework trigger a non-speculated operation like {{CREATE_DISK.}}   * tear down the framework before a terminal operation status update reaches the master; this causes the master to e.g., remove the framework from the allocator.   * let a terminal, successful operation status update reach the master   * 💥     To solve this we should cleanup the lifetimes of operations. Since operations can outlive their framework (unlike e.g., tasks), we probably need a different approach here."""
"MESOS-9544","Bug","storage",5,"SLRP does not clean up destroyed persistent volumes.","""When a persistent volume created on a {{ROOT}} disk is destroyed, the agent will clean up its data: https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4397  However, this is not the case for PVs on SLRP disks. The agent relies on the SLRP to do the cleanup:  https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/slave/slave.cpp#L4472  But SLRP simply updates its metadata and do nothing:  https://github.com/apache/mesos/blob/f44535bca811720fc272c9abad2bc78652d61fe3/src/resource_provider/storage/provider.cpp#L2805    This would lead to data leakage if the framework does not call `CREATE_DISK` but just unreserve it."""
"MESOS-9555","Bug","allocation|master",3,"Allocator CHECK failure: reservationScalarQuantities.contains(role).","""We recently upgraded our Mesos cluster from version 1.3 to 1.5, and since then have been getting periodic master crashes due to this error:    Full stack trace is at the end of this issue description. When the master fails, we automatically restart it and it rejoins the cluster just fine. I did some initial searching and was unable to find any existing bug reports or other people experiencing this issue. We run a cluster of 3 masters, and see crashes on all 3 instances.    Right before the crash, we saw a {{Removed agent:...}} log line noting that it was agent 9b912afa-1ced-49db-9c85-7bc5a22ef072-S6 that was removed.    I saved the full log from the master, so happy to provide more info from it, or anything else about our current environment.    Full stack trace is below.  """
"MESOS-9554","Bug","allocation",5,"Allocator might skip allocations because a single framework is incapable of receiving certain resources.","""Currently in the hierarchical allocator allocation loops we compute {{available}} resources by taking into account the capabilities of the current framework. Further down in the loop we might then {{break}} out of the iteration under the assumption that no other framework can receive the resources in question.    This is only correct if all considered frameworks have identical capabilities."""
"MESOS-9557","Bug","master",2,"Operations are leaked in Framework struct when agents are removed","""Currently, when agents are removed from the master, their operations are not removed from the {{Framework}} structs. We should ensure that this occurs in all cases."""
"MESOS-9560","Bug","test",5,"ContentType/AgentAPITest.MarkResourceProviderGone/1 is flaky","""We observed a segfault in {{ContentType/AgentAPITest.MarkResourceProviderGone/1}} on test teardown.  """
"MESOS-9565","Task","test",5,"Unit tests for creating and destroying persistent volumes in SLRP.","""The plan is to add/update the following unit tests to test persistent volume destroy:   * CreateDestroyDisk   * CreateDestroyDiskWithRecovery   * CreateDestroyPersistentMountVolume   * CreateDestroyPersistentMountVolumeWithRecovery   * CreateDestroyPersistentMountVolumeWithReboot   * CreateDestroyPersistentBlockVolume   * DestroyPersistentMountVolumeFailed   * DestroyUnpublishedPersistentVolume   * DestroyUnpublishedPersistentVolumeWithRecovery   * DestroyUnpublishedPersistentVolumeWithReboot"""
"MESOS-9564","Bug","agent|modules",5,"Logrotate container logger lets tasks execute arbitrary commands in the Mesos agent's namespace","""The non-default {{LogrotateContainerLogger}} module allows tasks to configure sandbox log rotation (See http://mesos.apache.org/documentation/latest/logging/#Containers ).  The {{logrotate_stdout_options}} and {{logrotate_stderr_options}} in particular let the task specify free-form text, which is written to a configuration file located in the task's sandbox.  The module does not sanitize or check this configuration at all.    The logger itself will eventually run {{logrotate}} against the written configuration file, but the logger is not isolated in the same way as the task.  For both the Mesos and Docker containerizers, the logger binary will run in the same namespace as the Mesos agent.  This makes it possible to affect files outside of the task's mount namespace.    Two modes of attack are known to be problematic:  * Changing or adding entries to the configuration file.  Normally, the configuration file contains a single file to rotate:    It is trivial to add text to the {{logrotate_stdout_options}} to add a new entry:    * Logrotate's {{postrotate}} option allows for execution of arbitrary commands.  This can again be supplied with the {{logrotate_stdout_options}} variable.      Some potential fixes to consider:  * Overwrite the .logrotate.conf files each time. This would give only milliseconds between writing and calling logrotate for a thirdparty to modify the config files maliciously. This would not help if the task itself had postrotate options in its environment variables.  * Sanitize the free-form options field in the environment variables to remove postrotate or injection attempts like }\n/path/to/some/file\noptions{.  * Refactor parts of the Mesos isolation code path so that the logger and IO switchboard binary live in the same namespaces as the container (instead of the agent). This would also be nice in that the logger's CPU usage would then be accounted for within the container's resources."""
"MESOS-9574","Bug","agent",2,"Operation status update streams are not properly garbage collected.","""After successfully handling the acknowledgment of a terminal operation status update for an operation affecting agent's default resources, the agent should garbage collect the corresponding operation status update stream."""
"MESOS-9573","Bug","agent",2,"Agent should not try to recover operation status update streams that haven't been created yet.","""If the agent fails over after having checkpointed a new operation but before the operation status update stream is created, the recovery process will fail.    This happens because agent will try to recover the operation status update streams even if it hasn't been created yet.    In order to prevent recovery failures, the agent should obtain the ids of the streams to recover by walking the directory in which operation status updates streams are stored.    The agent should also garbage collect streams if the checkpointed state doesn't contain a corresponding operation."""
"MESOS-9568","Bug","storage",2,"SLRP does not clean up mount directories for destroyed MOUNT disks.","""When staging or publishing a CSI volume, SLRP will create the following mount points for these operations:    These directories are cleaned up when the volume is unpublished/unstaged. However, their parent directory, namly {{<work_dir>/csi/<rp_type>/<rp_name>/mounts/<volume_id>}} is never cleaned up."""
"MESOS-9578","Task","documentation",1,"Document per framework minimal allocatable resources in framework development guides","""With MESOS-9523 we introduced fields into {{FrameworkInfo}} to give frameworks a way to express their resource requirements. We should document this feature in the framework development guide(s)."""
"MESOS-9581","Bug","build",1,"Mesos package naming appears to be undeterministic.","""Transcribed from slack; https://mesos.slack.com/archives/C7N086PK2/p1550158266006900    It appears there are a number of RPM packages called “mesos-1.7.1-2.0.1.el7.x86_64.rpm” in the wild.    I’ve caught specimens with build dates February 1st, 7th and 13th. While it’s somewhat troubling in itself, none of these packages is the one referred to in Yum repository metadata (repos.mesosphere.com), which is a package built today on the 14th, so I can’t install Mesos right now.    Could it be that your pipeline is creating a new package with the same verson and release in every nightly build?    Repository metadata  """
"MESOS-9582","Bug","reviewbot",2,"Reviewbot jenkins jobs stops validating any reviews as soon as it sees a patch which does not apply","""The reviewbot Jenkins setup fetches all Mesos reviews since some time stamp, filters that list down to reviews which need to be validated, and then one by one validates each of the remaining review requests.    In doing that it applies patches with {{support/apply-reviews.py}} which is invoked by shelling out wth a function {{shell}} in {{support/verify-reviews.py}}. If that function sees any error from the shell command {{exit(1)}} is called which immediately terminates the Jenkins job.    As {{support/apply-reviews.py}} can fail if a patch does not apply cleanly anymore this means that any review requests which cannot be applied can largely disable reviewbot.    We should avoid calling {{exit}} in low-level functions in {{support/verify-reviews.py}} and instead bubble the error up to be handled at a larger scope. It looks like the script was alreadt designed to handle exceptions which might work much better here."""
"MESOS-9594","Bug","storage|test",3,"Test `StorageLocalResourceProviderTest.RetryRpcWithExponentialBackoff` is flaky.","""Observed on ASF CI:    Full log:  [^RetryRpcWithExponentialBackoff-badrun.txt] """
"MESOS-9605","Bug","build",2,"mesos/mesos-centos nightly docker image has to include the SHA of the build.","""As a snapshot build, we need to identify the exact HEAD of the branch build. Our current snapshot builds lack this information due to the way the build is setup.    The current build identifies e.g. when running the agent like this;      Note we lack a user in the first output line and the GIT sha altogether. Only tagged builds should commonly lack the SHA as it is not needed.      """
"MESOS-9607","Bug","agent|resource provider",2,"Removing a resource provider with consumers breaks resource publishing.","""Currently, the agent publishes all resources considered """"used"""" via the resource provider manager whenever it is asked to publish a subportion. If a resource provider with active users (e.g., tasks or even just executors) was removed, but a user stays around this will fail _any resource publishing_ on that node since a """"used"""" resource provider is not subscribed.    We should either update the agent code to just deltas, or provide a workaround of the same effect in the resource provider manager."""
"MESOS-9613","Improvement","containerization",3,"Support seccomp `unconfined` option for whitelisting.","""Support seccomp `unconfined` option for whitelisting. Authorization needs to be implemented for this protobuf option."""
"MESOS-9612","Bug","agent",5,"Resource provider manager assumes all operations are triggered by frameworks","""When the agent tries to apply an operation to resource provider resources, it invokes {{ResourceProviderManager::applyOperation}} which in turn invokes {{ResourceProviderManagerProcess::applyOperation}}. That function currently assumes that the received message contains a valid {{FrameworkID}},      Since {{FrameworkID}} is not a trivial proto types, but instead one with a {{required}} field {{value}}, the message composed with the {{frameworkId}} below cannot be serialized which leads to a failure below which in turn triggers a {{CHECK}} failure in the agent's function interfacing with the manager.    A typical scenario where we would want to support operator API calls here is to destroy leftover persistent volumes or reservations."""
"MESOS-9610","Bug","fetcher",3,"Fetcher vulnerability - escaping from sandbox","""I have noticed that there is a possibility to exploit fetcher and  overwrite any file on the agent host.    scenario to reproduce:    1) prepare a file with any content and name a file like """"../../../etc/test"""" and archive it. We can use python and zipfile module to achieve that:    2) prepare a service that will use our artifact (exploit.zip)    3) run service    at the end in /etc we will get our file. As you can imagine there is a lot possibility how we can use it.          """
"MESOS-9616","Bug","allocation",2,"`Filters.refuse_seconds` declines resources not in offers.","""The [documentation|http://mesos.apache.org/documentation/latest/scheduler-http-api/#accept] of {{Filters.refuse_seconds}} says:  {quote}  Also, any of the offer’s resources not used in the ACCEPT call (e.g., to launch a task or task group) are considered declined and might be reoffered to other frameworks, meaning that they will not be reoffered to the scheduler for the amount of time defined by the filter.  {quote}  Consider an {{ACCEPT}} call with just a {{CREATE}} operation, but no {{LAUNCH}} or {{LAUNCH_GROUP}}. The {{CREATE}} call will generate a persistent volume resource that is *not* in the offer's resources, but it will still not be reoffered to the scheduler for the amount of time defined by the filter.    Also, the term *used* is vague here. If we have an {{ACCEPT}} call with a {{CREATE}} on a disk followed by a {{DESTROY}} on the created persistent volume, would the disk be considered *used*?"""
"MESOS-9618","Improvement","webui",3,"Display quota consumption in the webui.","""Currently, the Roles table in the webui displays allocation and quota guarantees / limits. However, quota """"consumption"""" is different from allocation, in that reserved resources are always considered consumed against the quota.    This discrepancy has led to confusion from users. One exampled occurred when an agent was added with a large reservation exceeding the memory quota guarantee. The user sees memory chopping in offers, and since the scheduler didn't want to use the reservation, it can't launch its tasks.    If consumption is shown in the UI, we should include a tool tip that indicates how consumed is calculated so that users know how to interpret it."""
"MESOS-9627","Task","storage|test",3,"Test CSI v1 in SLRP unit tests.","""We could add a command line flag in the test CSI plugin to switch to either v0 and v1, and parameterize the CSI version in SLRP unit tests."""
"MESOS-9626","Task","storage",3,"Make SLRP pick the appropriate CSI versions for plugins.","""To detect the CSI version supported by a plugin, we could call {{v1.Probe}}, and fallback to {{v0.Probe}} if the previous call fails.    Alternatively, we could introduce a field in {{CSIPluginInfo}} to specify the plugin version to use."""
"MESOS-9625","Task","storage",3,"Make `DiskProfileAdaptor` agnostic to CSI spec version.","""To support multiple CSI versions, the {{DiskProfileAdaptor}} module needs to be decoupled from CSI version. Mainly, we'll have to introduce a version-agnostic {{VolumeCapability}}."""
"MESOS-9624","Task","storage",3,"Bundle CSI spec v1.0 in Mesos.","""We need to bundle both CSI v0 and v1 in Mesos. This requires some redesign of the source code filesystem layout."""
"MESOS-9622","Task","storage",5,"Refactor SLRP with a CSI volume manager.","""To support both CSI v0 and v1, SLRP needs to be agnostic to CSI versions. This could be achieved by refactoring all CSI volume management code into a CSI volume manager that can be implemented with CSI v0 and v1. Also, the volume state proto needs to be agnostic to CSI spec version as well.    Design doc: https://docs.google.com/document/d/1LPy839zwFw6UcRhmr65iKeMaHcoj6uUX25yJVbMknlY/edit#heading=h.1iswiwd3imin"""
"MESOS-9621","Bug","build",1,"Mesos failed to build due to error LNK2019 on Windows using MSVC.","""Issue description:    Mesos failed to build due to error    Could you please take a look?    Reproduce steps:    ErrorMessage:          """
"MESOS-9619","Bug","allocation",3,"Mesos Master Crashes with Launch Group when using Port Resources","""Original Issue: [https://lists.apache.org/thread.html/979c8799d128ad0c436b53f2788568212f97ccf324933524f1b4d189@%3Cuser.mesos.apache.org%3E]     When the ports resources is removed, Mesos functions normally (I'm able to launch the task as many times as possible, while it always fails continually).    Attached is a snippet of the mesos master log from OFFER to crash.    """
"MESOS-9632","Task","storage",5,"Refactor SLRP with a CSI service manager.","""The CSI volume manager relies on service containers, which should be agnostic to CSI versions. As the first step of MESOS-9622, we should first refactor SLRP with a CSI service manager that manages service container lifecycles before refactoring it with a CSI volume manager."""
"MESOS-9639","Task","storage",1,"Make CSI plugin RPC metrics agnostic to CSI versions.","""Currently SLRP provides per-CSI-call metrics, e.g.:    If we are to continue to provide such fine-grained metrics, when operators upgrade their CSI plugins to CSI v1, then SLRP would report another set of metrics for v1, which would be inconvenient to operators.    Also the fine-grained metrics are not very useful for operators, as most information are highly correlated to per-operation metrics. So most likely operators would simply aggregate the per-CSI-call metrics for monitoring CSI plugins, and use per-operation metrics to monitor volume creation/destroy/etc.    So instead of provide such fine-grained metrics, we could just provide a set of aggregated rpc metrics that are agnostic to CSI versions, such as:  """
"MESOS-9637","Bug","HTTP API|master",1,"Impossible to CREATE a volume on resource provider resources over the operator API","""Currently the master HTTP handler for operator API {{CREATE}} requests strips away the whole {{DiskInfo}} in any passed resources to calculate the consumed resources.    This is incorrect for resource provider disk resources where the {{DiskInfo}} contains information unrelated to the persistence. The handler should remove exclusively information created by the operation."""
"MESOS-9655","Improvement","storage",2,"Improving SLRP tests for preprovisioned volumes.","""We should improve SLRP tests for preprovisioned volumes:  1. Test that {{CREATE_DISK}} fails if the specified profile is unknown.  2. Update test {{AgentRegisteredWithNewId}} to ensure that a recovered published volumes can be consumed by new tasks."""
"MESOS-9661","Bug","storage",1,"Agent crashes when SLRP recovers dropped operations.","""MESOS-9537 is fixed by persisting dropped operations in SLRP, but the recovery codepath doesn't account for that:  [https://github.com/apache/mesos/blob/master/src/resource_provider/storage/provider.cpp#L1278]  Which caused the agent to crash with the following message during SLRP recovery:  """
"MESOS-9667","Bug","agent",3,"Check failure when executor for task using resource provider resources subscribes before agent is registered","""When an executor for a task using resource provider resources subscribes before the agent has registered with the master, we trigger a fatal assertion,    The reason for this failure is that we attempt to publish resources to the resource provider via the resource provider manager, but the resource provider manager is only created once the agent has registered with the master.    As a workaround one can terminate the executors and their tasks, and let the framework relaunch the tasks (provided it supports that).    A possible workaround could be to prevent such executors from subscribing until the resource provider manager is available."""
"MESOS-9672","Bug","containerization",2,"Docker containerizer should ignore pids of executors that do not pass the connection check.","""When recovering executors with a tracked pid we first try to establish a connection to its libprocess address to avoid reaping an irrelevant process:    https://github.com/apache/mesos/blob/4580834471fb3bc0b95e2b96e04a63d34faef724/src/slave/containerizer/docker.cpp#L1019-L1054    If the connection fails to establish, we should not track its pid: https://github.com/apache/mesos/blob/4580834471fb3bc0b95e2b96e04a63d34faef724/src/slave/containerizer/docker.cpp#L1071    One trouble this might cause is that if the pid is being used by another executor, this could lead to duplicate pid error and lead the agent into a crash loop:    https://github.com/apache/mesos/blob/4580834471fb3bc0b95e2b96e04a63d34faef724/src/slave/containerizer/docker.cpp#L1066-L1068"""
"MESOS-9677","Task","build",3,"RPM packages should be built with launcher sealing","""We should consider enabling launcher sealing in the Mesos RPM packages. Since this feature is built conditionally, it is hard to write e.g., module code against Mesos packages since required functions might be missing (e.g., [https://github.com/dcos/dcos-mesos-modules/commit/8ce70e6cc789054831daa3058647e326b2b11bc9] cannot be linked against the default RPM package anymore). The RPM's target platform centos7 should include a recent enough kernel for this."""
"MESOS-9691","Bug","allocation",3,"Quota headroom calculation is off when subroles are involved.","""Quota """"availableHeadroom"""" calculation:    https://github.com/apache/mesos/blob/6276f7e73b0dbe7df49a7315cd1b83340d66f4ea/src/master/allocator/mesos/hierarchical.cpp#L1751-L1754    is off when subroles are involved.    Specifically, in the formula       -The """"allocated resources"""" part is hierarchical-aware and aggregate that across all roles, thus allocations to subroles will be counted multiple times (in the case of """"a/b"""", once for """"a"""" and once for """"a/b"""").- Looks like due to the presence of `INTERNAL` node, `roleSorter->allocationScalarQuantities(role)` is *not* hierarchical. Thus this is not an issue.    (If role `a/b` consumes 1cpu and `a` consumes 1cpu, if we query `roleSorter->allocationScalarQuantities(""""a"""");` It will return 1cpu, which is correct. In the sorter, there are four nodes, root, `a` (internal, 1cpu), `a/.` (leaf, 1cpu), `a/b` (leaf, 1cpu). Query `a` will return `a/.`)    The """"total reservations""""  is correct, since today it is """"flat"""" (reservations made to """"a/b"""" are not counted to """"a""""). Thus all reservations are only counted once -- which is the correct semantic here. However, once we fix MESOS-9688 (which likely requires reservation tracking to be hierarchical-aware), we need to ensure that the accounting is still correct.    -The """"allocated reservations"""" is hierarchical-aware, thus overlap accounting would occur.- Similar to the `""""allocated resources""""` above, this is also not an issue at the moment.    Basically, when calculating the available headroom, we need to ensure """"single-counting"""". Ideally, we only need to look at the root's consumptions."""
"MESOS-9688","Bug","allocation",3,"Quota is not enforced properly when subroles have reservations.","""Note: the discussion here concerns quota enforcement for top-level role, setting quota on sublevel role is not supported.    If a subrole directly makes a reservation, the accounting of `roleConsumedQuota` will be off:    https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/hierarchical.cpp#L1703-L1705    Specifically, in this formula:  `Consumed Quota = reservations + allocation - allocated reservations`    The `reservations` part does not account subrole's reservation to its ancestors. If a reservation is made directly for role """"a/b"""", its reservation is accounted only for """"a/b"""" but not for """"a"""". Similarly, if a top role ( """"a"""") reservation is refined to a subrole (""""a/b""""), the current code first subtracts the reservation from """"a"""" and then track that under """"a/b"""".    We should make it hierarchical-aware.    The """"allocation"""" and """"allocated reservations"""" are both tracked in the sorter where the hierarchical relationship is considered -- allocations are added hierarchically."""
"MESOS-9693","Task","containerization",3,"Add master validation for SeccompInfo.","""1. if seccomp is not enabled, we should return failure if any fw specify seccompInfo and return appropriate status update.  2. at most one field of profile_name and unconfined should be set. better to validate in master"""
"MESOS-9692","Bug","allocation",2,"Quota may be under allocated for disk resources.","""Due to a bug in the resources chopping logic:  https://github.com/apache/mesos/blob/1915150c6a83cd95197e25a68a6adf9b3ef5fb11/src/master/allocator/mesos/hierarchical.cpp#L1665-L1668        When chopping different resources with the same name (e.g. vanilla disk and mount disk), we only include one of the resources. For example, if a role has a quota of 100disk, and an agent has 50 vanilla disk and 50 mount disk, the offer will only contain 50 disk (either vanilla or the mount type). The correct behavior should be that both disks should be offered.    Since today, only disk resources might have the same name but different meta-data (for unreserved/nonrevocable/nonshared resources -- we only chop this), this bug should only affect disk resources today.    The correct code should be:      """
"MESOS-9695","Improvement","containerization",2,"Remove the duplicate pid check in Docker containerizer","""In `DockerContainerizerProcess::_recover`, we check if there are two executors use duplicate pid, and error out if we find duplicate pid (see [here|https://github.com/apache/mesos/blob/1.7.2/src/slave/containerizer/docker.cpp#L1068:L1078] for details). However I do not see the value this check can give us but it will cause serious issue (agent crash loop when restarting) in rare case (a new executor reuse pid of an old executor), so I think we'd better to remove it from Docker containerizer."""
"MESOS-9697","Bug","build",3,"Release RPMs are not uploaded to bintray","""While we currently build release RPMs, e.g., [https://builds.apache.org/view/M-R/view/Mesos/job/Packaging/job/CentOS/job/1.7.x/], these artifacts are not uploaded to bintray. Due to that RPM links on the downloads page [http://mesos.apache.org/downloads/] are broken."""
"MESOS-9696","Bug","test",1,"Test MasterQuotaTest.AvailableResourcesSingleDisconnectedAgent is flaky","""The test {{MasterQuotaTest.AvailableResourcesSingleDisconnectedAgent}} is flaky, especially under additional system load."""
"MESOS-9701","Improvement","allocation",3,"Allocator's roles map should track reservations.","""Currently, the allocator's {{roles}} map only tracks roles that have allocations or framework subscriptions:    https://github.com/apache/mesos/blob/1.7.2/src/master/allocator/mesos/hierarchical.hpp#L531-L535    And we separately track a map of total reservations for each role:    https://github.com/apache/mesos/blob/1.7.2/src/master/allocator/mesos/hierarchical.hpp#L541-L547    Confusingly, the {{roles}} map won't have an entry when there is a reservation for a role but no allocations or frameworks subscribed. We should ensure that the map has an entry when there are reservations. Also, we can consolidate the reservation information and framework ids into the same map, e.g.:    """
"MESOS-9704","Improvement","containerization",3,"Support docker manifest v2s2 config GC.","""After docker manifest v2s2 support, layer GC is still properly supported.    However, the manifest config is not garbage collected. Need to add the config dir to the checkpointed LAYERS_FILE to support config GC."""
"MESOS-9710","Task","allocation",3,"Add tests to ensure random sorter performs correct weighted sorting.","""We added tests for the weighted shuffle algorithm, but didn't test that the RandomSorter's sort() function behaves correctly.    We should also test that hierarchical weights in the random sorter behave correctly."""
"MESOS-9712","Bug","storage",3,"StorageLocalResourceProviderTest.CsiPluginRpcMetrics is flaky.","""From an internal CI run:  """
"MESOS-9711","Bug","agent",1,"Avoid shutting down executors registering before a required resource provider.","""If an HTTP-based executor resubscribes after agent failover before a resource provider exposing some of its resources has subscribed itself the agent currently does not know how to inform the resource provider about the existing resource user and shuts the executor down.    This is not optimal as the resource provider might subscribe soon, but we fail the task nevertheless.    We should consider improving on that, e.g., by deferring executor subscription until all providers have resubscribed or their registration timeout is reached, see MESOS-7554."""
"MESOS-9727","Bug","agent|executor",1,"Heartbeat calls from executor to agent are reported as errors","""These HEARTBEAT calls and events were added in MESOS-7564.     HEARTBEAT calls are generated by the executor library, which does not have access to the executor's Framework/Executor IDs.  The library therefore uses some dummy values instead, because HEARTBEAT calls do not really require required fields.  When the agent receives these dummy values, it returns a 400 Bad Request.  It should return 202 Accepted instead."""
"MESOS-9733","Bug","allocation",3,"Random sorter generates non-uniform result for hierarchical roles.","""In the presence of hierarchical roles, the random sorter shuffles roles level by level and then pick the active leave nodes using DFS:    https://github.com/apache/mesos/blob/7e7cd8de1121589225049ea33df0624b2a1bd754/src/master/allocator/sorter/random/sorter.cpp#L513-L529    This makes the result less random because subtrees are always picked together. For example, random sorting result such as `[a/., c/d, a/b, …]` is impossible.   """
"MESOS-9759","Improvement","allocation",2,"Log required quota headroom and available quota headroom in the allocator.","""This would ease the debugging of allocation issues."""
"MESOS-9750","Bug","agent|executor",2,"Agent V1 GET_STATE response may report a complete executor's tasks as non-terminal after a graceful agent shutdown","""When the following steps occur:  1) A graceful shutdown is initiated on the agent (i.e. SIGUSR1 or /master/machine/down).  2) The executor is sent a kill, and the agent counts down on {{executor_shutdown_grace_period}}.  3) The executor exits, before all terminal status updates reach the agent. This is more likely if {{executor_shutdown_grace_period}} passes.    This results in a completed executor, with non-terminal tasks (according to status updates).    When the agent starts back up, the completed executor will be recovered and shows up correctly  as a completed executor in {{/state}}.  However, if you fetch the V1 {{GET_STATE}} result, there will be an entry in {{launched_tasks}} even though nothing is running.      This happens because we combine executors and completed executors when constructing the response.  The terminal task(s) with non-terminal updates appear under completed executors.  https://github.com/apache/mesos/blob/89c3dd95a421e14044bc91ceb1998ff4ae3883b4/src/slave/http.cpp#L1734-L1756"""
"MESOS-9766","Bug","libprocess",3,"/__processes__ endpoint can hang.","""A user reported that the {{/\_\_processes\_\_}} endpoint occasionally hangs.    Stack traces provided by [~alexr] revealed that all the threads appeared to be idle waiting for events. After investigating the code, the issue was found to be possible when a process gets terminated after the {{/\_\_processes\_\_}} route handler dispatches to it, thus dropping the dispatch and abandoning the future."""
"MESOS-9765","Bug","test",1,"Test `ROOT_CreateDestroyPersistentMountVolumeWithReboot` is flaky.","""Observed a failure on test {{CSIVersion/StorageLocalResourceProviderTest.ROOT_CreateDestroyPersistentMountVolumeWithReboot/v0}} with the following error:      The problem is that the task was OOM-killed:      This might happen to other SLRP root tests as well."""
"MESOS-9774","Task","libprocess",8,"Design client side SSL certificate verification in Libprocess.","""Notes from an offline discussion with [~vinodkone], [~tillt], [~jgehrcke], [~CarlDellar].  * Authentication can happen at the transport and/or at the application layer. There is no real benefit in doing it at both layers.  * Authentication at the application layer allows for subsequent authorization.  * We would like to have an option to mutually authenticate all components in a Mesos cluster, including external tooling, regardless at which layer, to secure communication channels.  * Mutual authentication at the transport layer everywhere can be hard because some components can't or don't want to provide certificates, e.g., a Lua HTTP client reading master's state.  * Theoretically, some components, e.g., Mesos masters and agents, can form an ensemble inside which all connections are authenticated on both sides at the transport layer (TLS certificate verification). Practically, it may then be hard to implement communication with the components outside such ensemble, e.g., frameworks, executors, since at least two types of connections/sockets should be distinguished: with and without client certificate verification (Libprocess can't do it now), or all the traffic between the ensemble and outside components should go via a proxy.  * An alternative is to combine server side TLS certificate verification with the client side application layer authentication. For that to be secure, we need to implement client authentication for Mesos components, e.g., master with agent, replica with other replica (see MESOS-9638). Plus relax certificate verification option in Libprocess for outgoing connections only. For non-streaming connections a secret connection identifier should be passed by the client to prove they are the entity that has been previously authenticated.  * Whatever path we choose, truly secure communication channels will become when separate certificates for Mesos components are used, either signed by a different root CA or using a specific CN/SAN, which can't be obtained by everyone.    What needs to be done:  * Introduce or adjust the Libprocess flag for verifying certificates for outgoing connections only.  * Verify how replicas in the master's replicated log discover other replicas and what harm a rogue replica can do if it tries to join the quorum. Estimate whether master's replicated log can use its own copy of Libprocess.  * Implement Mesos master authentication with Mesos agents, MESOS-9638."""
"MESOS-9778","Improvement","allocation",1,"Randomized the agents in the second allocation stage.","""Agents are currently randomized before the 1st  allocation stage (the quota allocation stage) but not in  the 2nd stage. One perceived issue is that resources on  the agents in the front of the queue are likely to be mostly  allocated in the 1st stage, leaving only slices of resources  available for the second stage. Thus we may see consistently  low quality offers for role/frameworks that get allocated first  in the 2nd stage.    Consider randomizing the agents in the second allocation stage."""
"MESOS-9779","Bug","agent|HTTP API|resource provider",1,"`UPDATE_RESOURCE_PROVIDER_CONFIG` agent call returns 404 ambiguously.","""The {{UPDATE_RESOURCE_PROVIDER_CONFIG}} API call returns 404 if the specified resource provider does not exist. However, libprocess also returns 404 when the `/api/v1` route is not set up. As a result, a client will get confused when receiving 404 and wouldn't know the actual state of the resource provider config. We should not overload 404 with different errors.    The other codes for client errors returned by this call are:  * 400 if the request is not well-formed.  * 403 if the call is not authorized.    To avoid ambiguity, we could keep 404 to represent that the requested URI does not exist, and use 409 to indicate that based on the current the current agent state, the update request cannot be done because the specified resource provider config does not exist, similar to what a PATCH command would return if certain elements do not exist in the requsted resource (https://www.ietf.org/rfc/rfc5789.txt):    Adapting 409 also makes {{UPDATE_RESOURCE_PROVIDER_CONFIG}} symmetric to {{ADD_RESOURCE_PROVIDER_CONFIG}}."""
"MESOS-9782","Bug","allocation",1,"Random sorter fails to clear removed clients.","""In `RandomSorter::SortInfo::updateRelativeWeights()`, we do not clear the stale `clients` and `weights` vector if the state is dirty. This would result in an allocator crash due to including removed framework and roles in a sorted result e.g. check failure would occur here (https://github.com/apache/mesos/blob/62f0b6973b2268a3305fd631a914433a933c6757/src/master/allocator/mesos/hierarchical.cpp#L1849)."""
"MESOS-9785","Bug","HTTP API",2,"Frameworks recovered from reregistered agents are not reported to master `/api/v1` subscribers.","""Currently when an operator subscribes to the {{/api/v1}} master endpoint, it would receive a {{SUBSCRIBED}} event carrying information about all known frameworks, including registered ones and unregistered ones. If an unregistered framework reregisters later, a {{FRAMEWORK_UPDATED}} event would be sent to the operator.    However, if an operator subscribes to the {{/api/v1}} master endpoint after a master failover but before any of the frameworks and agents reregisters, {{SUBSCRIBED}} would contain no recovered framework information. When a agent with running tasks reregisters later, unregistered frameworks of those tasks will be recovered, but no {{FRAMEWORK_ADDED}} will be sent to the operator, so the operator will receive {{TASK_ADDED}} for those tasks with unknown framework IDs."""
"MESOS-9788","Task","containerization",8,"Configurable IPC namespace and shared memory in `namespaces/ipc` isolator","""See [design doc|https://docs.google.com/document/d/10t1jf97vrejUWEVSvxGtqw4vhzfPef41JMzb5jw7l1s/edit?usp=sharing] for the background of this improvement and how we are going to implement it."""
"MESOS-9794","Task","containerization",8,"Design doc for container debug endpoint.","""Design doc for container debug endpoint."""
"MESOS-9802","Improvement","allocation",2,"Remove quota role sorter in the allocator.","""Remove the dedicated quota role sorter in favor of using the same sorting between satisfying guarantees and bursting above guarantees up to limits. This is tech debt from when a """"quota role"""" was considered different from a """"non-quota"""" role. However, they are the same, one just has a default quota.    The only practical difference between quota role sorter and role sorter now is that quota role sorter ignores the revocable resources both in its total resource pool as well as role allocations. Thus when using DRF, it does not count revocable resources which is arguably the right behavior.    By removing the quota sorter, we will have all roles sorted together. When using DRF, in the 1st quota guarantee allocation stage, its share calculation will also include revocable resources."""
"MESOS-9803","Bug","storage",2,"Memory leak caused by an infinite chain of futures in `UriDiskProfileAdaptor`.","""Before MESOS-8906, {{UriDiskProfileAdaptor}} only update its promise for watchers if the polled profile matrix becomes larger in size, and this prevents the following code in the {{watch}} function from creating an infinite chain of futures when the profile matrix keeps the same:  https://github.com/apache/mesos/blob/fa410f2fb8efb988590f4da2d4cfffbb2ce70637/src/resource_provider/storage/uri_disk_profile_adaptor.cpp#L159-L160    However, the patch of MESOS-8906 removes the size check in the {{notify}} function to allow profile selectors to be updated. As a result, once the watch function is called, the returned future will be chained with a new promise every time a poll is made, hence creating a memory leak.    A jemalloc call graph for a 2hr trace is attached."""
"MESOS-9807","Improvement","allocation",5,"Introduce a `struct Quota` wrapper.","""We should introduce:    struct Qutota {    ResourceQuantities guarantees;    ResourceLimits limits;  }    There are a couple of small hurdles. First, there is already a struct Quota wrapper in """"include/mesos/quota/quota.hpp"""", we need to deprecate that first. Second, `ResourceQuantities` and `ResourceLimits` are right now only used in internal headers. We probably want to move them into public header, since this struct will also be used in allocator interface which is also in the public header. (Looking at this line, the boundary is alreayd breached: https://github.com/apache/mesos/blob/master/include/mesos/allocator/allocator.hpp#L41)"""
"MESOS-9806","Improvement","allocation",5,"Address allocator performance regression due to the addition of quota limits.","""In MESOS-9802, we removed the quota role sorter which is tech debt.    However, this slows down the allocator. The problem is that in the first stage, even though a cluster might have no active roles with non-default quota, the allocator will now have to sort and go through each and every role in the cluster. Benchmark result shows that for 1k roles with 2k frameworks, the allocator could experience ~50% performance degradation.    There are a couple of ways to address this issue. For example, we could make the sorter aware of quota. And add a method, say `sortQuotaRoles`, to return all the roles with non-default quota. Alternatively, an even better approach would be to deprecate the sorter concept and just have two standalone functions e.g. sortRoles() and sortQuotaRoles() that takes in the role tree structure (not yet exist in the allocator) and return the sorted roles.    In addition, when implementing MESOS-8068, we need to do more during the allocation cycle. In particular, we need to call shrink many more times than before. These all contribute to the performance slowdown. Specifically, for the quota oriented benchmark `HierarchicalAllocator_WithQuotaParam.LargeAndSmallQuota/2` we can observe 2-3x slowdown compared to the previous release (1.8.1):    Current master:    QuotaParam/BENCHMARK_HierarchicalAllocator_WithQuotaParam.LargeAndSmallQuota/2  Benchmark setup: 3000 agents, 3000 roles, 3000 frameworks, with drf sorter  Made 3500 allocations in 32.051382735secs  Made 0 allocation in 27.976022773secs    1.8.1:  HierarchicalAllocator_WithQuotaParam.LargeAndSmallQuota/2  Made 3500 allocations in 13.810811063secs  Made 0 allocation in 9.885972984secs"""
"MESOS-9819","Task","agent|master",5,"Update the agent's behavior when marked GONE","""Currently, when an agent is marked GONE, the master sends a {{ShutdownMessage}} to the agent, which causes it to shutdown all frameworks and then terminate.    As part of the agent draining work, we would like to change this behavior so that instead of terminating, the agent will sleep indefinitely once all frameworks are shut down. This will avoid the issue of a flapping agent process when the agent is managed by an init service like systemd."""
"MESOS-9818","Task","agent",2,"Implement minimal agent-side draining handler","""To unblock other work that can be done in parallel, this ticket captures the implementation of a handler for the {{DrainSlaveMessage}} in the agent which will:  * Checkpoint the {{DrainInfo}}  * Populate a new data member in the agent with the {{DrainInfo}}  """
"MESOS-9817","Task","master",3,"Add minimum master capability for draining and deactivation states","""Since we are adding new fields to the registry to represent agent draining/deactivation, we cannot allow downgrades of masters while such features are in use.      A new minimum capability should be added to the registry with the appropriate documentation:  https://github.com/apache/mesos/blob/663bfa68b6ab68f4c28ed6a01ac42ac2ad23ac07/src/master/master.cpp#L1681-L1688  http://mesos.apache.org/documentation/latest/downgrades/"""
"MESOS-9816","Task","master",3,"Add draining state information to master state endpoints","""The response for {{GET_STATE}} and {{GET_AGENTS}} should include the new fields indicating deactivation or draining states:      The {{/state}} and {{/state-summary}} handlers should also expose this information."""
"MESOS-9815","Task","documentation|master",5,"Deprecate maintenance primitives","""The existing maintenance primitives should be marked deprecated in the protobuf definitions, and the documentation should be mostly removed with links which redirect to the new agent draining feature.    The {{updateMaintenanceSchedule()}} handler code path should also be updated to verify that the new agent draining feature is not in use before allowing maintenance schedules to be created."""
"MESOS-9814","Task","master",8,"Implement DrainAgent master/operator call with associated registry actions","""We want to add several calls associated with agent draining:      Each field will be persisted in the registry:  """
"MESOS-9813","Improvement","allocation",3,"Track role consumed quota for all roles in the allocator.","""We are already tracking role consumed quota for roles with non-default quota in the allocator. We should expand that to track all roles' consumptions which will then be exposed through metrics later."""
"MESOS-9831","Bug","master",2,"Master should not report disconnected resource providers.","""MESOS-9384 attempted to make the master to garbage-collect disconnected resource providers. However, if there are disconnected resource providers but none of the connected ones changes, the following code snippet would make the master ignore the agent update and skip the garbage collection:  https://github.com/apache/mesos/blob/2ae1296c668686d234be92b00bd7abbc0a6194b0/src/master/master.cpp#L8186-L8234  The condition to ignore the agent update will be triggered in one of the following conditions:  1. The resource provider has no resource, so the agent's total resource remains the same.  2. When the agent restarts and reregisters, its resource provider resources will be reset.    As a result, the master will still keep records for the disconnected resource providers and report them."""
"MESOS-9823","Task","agent",3,"Agent should modify status updates while draining","""While it's draining, the agent should decorate TASK_KILLING and TASK_KILLED status updates with REASON_AGENT_DRAINING. It should also convert TASK_KILLED to TASK_GONE_BY_OPERATOR in the {{mark_gone}} case, ensuring that TASK_GONE_BY_OPERATOR is the state checkpointed to disk."""
"MESOS-9822","Task","agent",2,"Agent recovery code for task draining","""In the case where the agent crashes while it's in the process of killing tasks due to agent draining, it must recover the checkpointed {{DrainInfo}} and kill any tasks which did not have KILL events sent to them already."""
"MESOS-9821","Task","agent",2,"Agent kills all tasks when draining","""The agent's {{DrainSlaveMessage}} handler should kill all tasks when draining is initiated, specifying a kill policy with a grace period equal to the minimum of the task's grace period and the min_grace_period specified in the drain message."""
"MESOS-9820","Improvement","allocation",5,"Add `updateQuota()` method to the allocator.","""This is the method that underlies the `UPDATE_QUOTA` operator call. This will allow the allocator to set different values for guarantees and limits.    The existing `setQuota` and `removeQuota` methods in the allocator will be deprecated. This will likely break many existing allocator tests. We should fix and refactor tests to verify the bursting up to limits feature."""
"MESOS-9836","Bug","containerization",5,"Docker containerizer overwrites `/mesos/slave` cgroups.","""The following bug was observed on our internal testing cluster.    The docker containerizer launched a container on an agent:    After the container was launched, the docker containerizer did a {{docker inspect}} on the container and cached the pid:   [https://github.com/apache/mesos/blob/0c431dd60ae39138cc7e8b099d41ad794c02c9a9/src/slave/containerizer/docker.cpp#L1764]   The pid should be slightly greater than 13716.    The docker executor sent a {{TASK_FINISHED}} status update around 16 minutes later:    After receiving the terminal status update, the agent asked the docker containerizer to update {{cpu.cfs_period_us}}, {{cpu.cfs_quota_us}} and {{memory.soft_limit_in_bytes}} of the container through the cached pid:   [https://github.com/apache/mesos/blob/0c431dd60ae39138cc7e8b099d41ad794c02c9a9/src/slave/containerizer/docker.cpp#L1696]    Note that the cgroup of {{cpu.shares}} was {{/mesos/slave}}. This was possibly because that over the 16 minutes the pid got reused:    It was highly likely that the container itself exited around 06:09:35, way before the docker executor detected and reported the terminal status update, and then its pid was reused by another forked child of the agent, and thus {{cpu.cfs_period_us}}, {{cpu.quota_us}} and {{memory.soft_limit_in_bytes}} of the {{/mesos/slave}} cgroup was mistakenly overwritten."""
"MESOS-9835","Bug","test",2,"`QuotaRoleAllocateNonQuotaResource` is failing.","""    The test is failing because:    After agent3 is added, it misses a settle call where the allocation of agent3 is racy.  In addition, after https://github.com/apache/mesos/commit/7df8cc6b79e294c075de09f1de4b31a2b88423c8  we now offer nonquota resources on an agent (even that means """"chopping"""") on top of role's satisfied guarantees, the test needs to be updated in accordance with the behavior change."""
"MESOS-9843","Task","containerization",3,"Implement tests for the `containerizer/debug` endpoint.","""Implement tests for container stuck issues and check that the agent's `containerizer/debug` endpoint returns a JSON object containing information about pending operations."""
"MESOS-9837","Task","containerization",5,"Implement `FutureTracker` class along with helper functions.","""Both `track()` and `pending_futures()` helper functions depend on the `FutureTracker` actor.  `FutureTracker` actor must be available globally and there must be only one instance of this actor."""
"MESOS-9846","Task","webui",3,"Update UI for agent draining","""We should expose the new agent metadata in the web UI:  * Drain info  * Deactivation state    It may also be worth exposing unreachable and gone agents in some way, so that agents do not simply disappear from the UI when they transition to unreachable and/or gone, during or after maintenance."""
"MESOS-9845","Task","documentation",5,"Add docs for automatic agent draining","""Will probably require:  * A separate page describing the feature (in lieu or superceding the maintenance doc)  * Updates to the API docs, for master and agent APIs.  Any GET_STATE or similar call changes will also be included."""
"MESOS-9847","Bug","executor",5,"Docker executor doesn't wait for status updates to be ack'd before shutting down.","""The docker executor doesn't wait for pending status updates to be acknowledged before shutting down, instead it sleeps for one second and then terminates:        This would result in racing between task status update (e.g. TASK_FINISHED) and executor exit. The latter would lead agent generating a `TASK_FAILED` status update by itself, leading to the confusing case where the agent handles two different terminal status updates."""
"MESOS-9849","Task","scheduler driver",5,"Add support for per-role REVIVE / SUPPRESS to V0 scheduler driver.","""Unfortunately, there are still schedulers that are using the v0 bindings and are unable to move to v1 before wanting to use the per-role REVIVE / SUPPRESS calls.    We'll need to add per-role REVIVE / SUPPRESS into the v1 scheduler driver."""
"MESOS-9852","Bug","allocation|master",3,"Slow memory growth in master due to deferred deletion of offer filters and timers.","""The allocator does not keep a handle to the offer filter timer, which means it cannot remove the timer overhead (in this case memory) when removing the offer filter earlier (e.g. due to revive):    https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1338-L1352    In addition, the offer filter is allocated on the heap but not deleted until the timer fires (which might take forever!):    https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1321  https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1408-L1413  https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L2249    We'll need to try to backport this to all active release branches."""
"MESOS-9856","Bug","allocation",3,"REVIVE call with specified role(s) clears filters for all roles of a framework.","""As pointed out by [~asekretenko], the REVIVE implementation in the allocator incorrectly clears decline filters for all of the framework's roles, rather than only those that were specified in the REVIVE call:    https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1392    This should only clear filters for the roles specified in the REVIVE call."""
"MESOS-9861","Bug","metrics",1,"Make PushGauges support floating point stats.","""Currently, PushGauges are modeled against counters. Thus it does not support floating point stats. This prevents many existing PullGauges to use it. We need to add support for floating point stat."""
"MESOS-9860","Task","agent",2,"Agent should erase DrainInfo when draining complete","""When the agent is in the DRAINING state and it sees that all terminal acknowledgements for completed operations and tasks have been received, it should clear the checkpointed {{DrainInfo}} from disk and from memory so that it no longer believes it is DRAINING. It will then be ready to receive new tasks/operations if it is reactivated."""
"MESOS-9871","Task","master",3,"Expose quota consumption in /roles endpoint.","""As part of exposing quota consumption to users and displaying quota consumption in the ui, we will need to add it to the /roles endpoint (which is currently what the ui uses for the roles table)."""
"MESOS-9874","Task","containerization",3,"Add environment variable `MESOS_ALLOCATION_ROLE` to the task/container.","""Set this env var as the role from the task resource. Here is an example:  https://github.com/apache/mesos/blob/master/src/master/readonly_handler.cpp#L197    We probably want to set this env from executors, by adding this env to CommandInfo.    Mesos and docker containerizers should be supported."""
"MESOS-9875","Bug","agent",8,"Mesos did not respond correctly when operations should fail","""For testing persistent volumes with {{OPERATION_FAILED/ERROR}} feedbacks, we sshed into the mesos-agent and made it unable to create subdirectories in {{/srv/mesos/work/volumes}}, however, mesos did not respond any operation failed response. Instead, we received {{OPERATION_FINISHED}} feedback.    Steps to recreate the issue:    1. Ssh into a magent.   2. Make it impossible to create a persistent volume (we expect the agent to crash and reregister, and the master to release that the operation is {{OPERATION_DROPPED}}):   * cd /srv/mesos/work (if it doesn't exist mkdir /srv/mesos/work/volumes)   * chattr -RV +i volumes (then no subdirectories can be created)    3. Launch a service with persistent volumes with the constraint of only using the magent modified above.              Logs for the scheduler for receiving `OPERATION_FINISHED`:    (Also see screenshot)         2019-06-27 21:57:11.879 [12768651|rdar://12768651] [Jarvis-mesos-dispatcher-105] INFO c.a.j.s.ServicePodInstance - Stored operation=4g3k02s1gjb0q_5f912b59-a32d-462c-9c46-8401eba4d2c1 and feedback=OPERATION_FINISHED in podInstanceID=4g3k02s1gjb0q on serviceID=yifan-badagents-1         * 2019-06-27 21:55:23: task reached state TASK_FAILED for mesos reason: REASON_CONTAINER_LAUNCH_FAILED with mesos message: Failed to launch container: Failed to change the ownership of the persistent volume at '/srv/mesos/work/volumes/roles/test-2/19b564e8-3a90-4f2f-981d-b3dd2a5d9f90' with uid 264 and gid 264: No such file or directory"""
"MESOS-9882","Bug","flaky",1,"Mesos.UpdateFrameworkV0Test.SuppressedRoles is flaky.","""Observed in CI, log attached.      """
"MESOS-9887","Bug","agent|containerization",8,"Race condition between two terminal task status updates for Docker/Command executor.","""h2. Overview    Expected behavior:   Task successfully finishes and sends TASK_FINISHED status update.    Observed behavior:   Task successfully finishes, but the agent sends TASK_FAILED with the reason """"REASON_EXECUTOR_TERMINATED"""".    In normal circumstances, Docker executor [sends|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/docker/executor.cpp#L758] final status update TASK_FINISHED to the agent, which then [gets processed|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5543] by the agent before termination of the executor's process.    However, if the processing of the initial TASK_FINISHED gets delayed, then there is a chance that Docker executor terminates and the agent [triggers|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L6662] TASK_FAILED which will [be handled|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5816-L5826] prior to the TASK_FINISHED status update.    See attached logs which contain an example of the race condition.  h2. Reproducing bug    1. Add the following code:    to the [`ComposingContainerizerProcess::status`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L578]   and to the [`DockerContainerizerProcess::status`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/docker.cpp#L2167].    2. Recompile mesos    3. Launch mesos master and agent locally    4. Launch a simple Docker task via `mesos-execute`:    h2. Race condition - description    1. Mesos agent receives TASK_FINISHED status update and then subscribes on [`containerizer->status()`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5754-L5761].    2. `containerizer->status()` operation for TASK_FINISHED status update gets delayed in the composing containerizer (e.g. due to switch of the worker thread that executes `status` method).    3. Docker executor terminates and the agent [triggers|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L6662] TASK_FAILED.    4. Docker containerizer destroys the container. A registered callback for the `containerizer->wait` call in the composing containerizer dispatches [lambda function|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L368-L373] that will clean up `containers_` map.    5. Composing c'zer resumes and dispatches `[status()|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L579]` method to the Docker containerizer for TASK_FINISHED, which in turn hangs for a few seconds.    6. Corresponding `containerId` gets removed from the `containers_` map of the composing c'zer.    7. Mesos agent subscribes on [`containerizer->status()`|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5754-L5761] for the TASK_FAILED status update.    8. Composing c'zer returns [""""Container not found""""|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/containerizer/composing.cpp#L576] for TASK_FAILED.    9. `[Slave::_statusUpdate|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5826]` stores TASK_FAILED terminal status update in the executor's data structure.    10. Docker containerizer resumes and finishes processing of `status()` method for TASK_FINISHED. Finally, it returns control to the `Slave::_statusUpdate` continuation. This method [discovers|https://github.com/apache/mesos/blob/0026ea46dc35cbba1f442b8e425c6cbaf81ee8f8/src/slave/slave.cpp#L5808-L5814] that the executor has already been destroyed."""
"MESOS-9894","Bug","build",1,"Mesos failed to build due to fatal error C1083 on Windows using MSVC.","""Mesos failed to build due to fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory on Windows using MSVC. It can be first reproduced on 6a026e3 reversion on master branch. Could you please take a look at this isssue? Thanks a lot!    Reproduce steps:    1. git clone -c core.autocrlf=true https://github.com/apache/mesos D:\mesos\src  2. Open a VS 2017 x64 command prompt as admin and browse to D:\mesos  3. cd src  4. .\bootstrap.bat  5. cd ..  6. mkdir build_x64 && pushd build_x64  7. cmake ..\src -G """"Visual Studio 15 2017 Win64"""" -DCMAKE_SYSTEM_VERSION=10.0.17134.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=""""C:\gnuwin32\bin"""" -T host=x64  8. msbuild Mesos.sln /p:Configuration=Debug /p:Platform=x64 /maxcpucount:4 /t:Rebuild         ErrorMessage:    D:\Mesos\src\include\mesos/docker/spec.hpp(29): fatal error C1083: Cannot open include file: 'mesos/docker/spec.pb.h': No such file or directory    D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory    D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory          """
"MESOS-9893","Bug","containerization",3,"`volume/secret` isolator should cleanup the stored secret from runtime directory when the container is destroyed","""`volume/secret` isolator writes secret into a file (its filename is a UUID) under `/run/mesos/.secret` when launching container, but it does not clean up that file when the container is destroyed. Over time, the `/run/mesos/.secret` directory may take up all disk space on the partition."""
"MESOS-9901","Bug","json api",3,"jsonify uses non-standard mapping for protobuf map fields.","""Jsonify current treats protobuf as a regular repeated field. For example, for the schema         it will produce:        This output cannot be parsed back to proto messages. We need to specialize jsonify for Maps type to get the standard output:    """
"MESOS-9906","Bug","libprocess",1,"Libprocess tests hangs on arm","""  https://builds.apache.org/job/Mesos-Buildbot-ARM/BUILDTOOL=cmake,COMPILER=clang,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--disable-java%20--disable-python%20--disable-parallel-test-execution,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1%20%20MESOS_TEST_AWAIT_TIMEOUT=60secs%20JOBS=16%20GTEST_FILTER=-DiskQuotaTest.SlaveRecovery,label_exp=arm/lastBuild/console"""
"MESOS-9907","Task","master",1,"Retain agent draining start time in master","""The master should store in memory the last time that a {{DrainSlaveMessage}} was sent to the agent so that this time can be displayed in the web UI. This would help operators determine the expected time at which the agent should transition to DRAINED.    We should update the webui to use that time as a starting point and the {{DrainConfig}}'s {{max_grace_period}} to calculate the expected maximum time until the agent is drained."""
"MESOS-9908","Improvement","containerization",5,"Introduce a new agent flag and support docker volume chown to task user.","""Currently, docker volume is always mounted as root, which is not accessible by non-root task users. For security concerns, there are use cases that operator may only allow non-root users to run as container user and docker volume needs to be supported for those non-root users.    A new agent flag is needed to make this support configurable, because chown-ing a docker volume may be limited to some use case - e.g., multiple non-root users on different hosts sharing the same docker volume simultaneously. Operators are expected to turn on this flag if their cluster's docker volume is not shared by multiple non-root users."""
"MESOS-9909","Bug","cni|containerization",2,"Mesos agent crashes after recovery when there is nested container joins a CNI network","""Reproduce steps:    1. Use `mesos-execute` to launch a task group with checkpoint enabled. The task in the task group joins a CNI network `net1` and has health check enabled, and the health check will succeed for the first time, fail for the second time, and succeed for the third time, ... The reason that we do health check in this way is that we want to keep generating status updates for this task after recovery.     2. Restart Mesos agent, and then we will see Mesos agent crashes when it handles `TASK_RUNNING` status update triggered by the health check.     """
"MESOS-9919","Task","agent|containerization",5,"Health check performance decreases on large machines","""In recent testing, it appears that the performance of Mesos command health checks decreases dramatically on nodes with large numbers of cores and lots of memory. This may be due to the changes in the cost of forking the agent process on such nodes. We need to investigate this issue to understand the root cause."""
"MESOS-9917","Improvement","allocation",8,"Store a role tree in the allocator.","""Currently, the client (role and framework) tree for the allocator is stored in the sorter abstraction. This is not ideal. The role/framework tree is generic information that is needed regardless of the sorter used. The current sorter interface and its associated states are tech debts that contribute to performance slowdown and code convolution.     We should store a role/framework tree in the allocator."""
"MESOS-9922","Bug","flaky",1,"MasterQuotaTest.RescindOffersEnforcingLimits is flaky","""Showed up on ASF CI:  https://builds.apache.org/view/M-R/view/Mesos/job/Mesos-Buildbot/6657/BUILDTOOL=cmake,COMPILER=clang,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-parallel-test-execution=no,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1%20MESOS_TEST_AWAIT_TIMEOUT=60secs,OS=centos:7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!ubuntu-4)&&(!H21)&&(!H23)&&(!H26)&&(!H27)/consoleFull        If I understand correctly, the offer with resources on a second slave went to the fiirst framework, because the allocator wasn't aware about the second framework when the second slave was added.  """
"MESOS-9925","Bug","containerization",2,"Default executor takes a couple of seconds to start and subscribe Mesos agent","""When launching a task group, it may take 6 seconds for default executor to start and subscribe Mesos agent:    This is obviously too long which may affect the performance of launching task groups."""
"MESOS-9930","Bug","allocation",3,"DRF sorter may omit clients in sorting after removing an inactive leaf node.","""The sorter assumes inactive leaf nodes are placed in the tail in the children list of a node.  However, when collapsing a parent node with a single """"."""" virtual child node, its position may fail to be updated due to a bug in `Sorter::remove()`:        This bug would manifest, if  (1) we have a/b and a/.  (2) deactivate(a),  i.e. a/. becomes inactive_leaf  (3) remove(a/b)  When these happens, a/. will collapse to `a` as an inactive_leaf, due to the bug above, however, it will not be placed at the end, resulting in all the clients after `a` not included in the sort().    Luckily, this should never happen in practice, because only frameworks will get deactivated, and frameworks don’t have sub clients.  """
"MESOS-9932","Improvement","allocation|scheduler api",3,"Removal of a role from the suppression list should be equivalent to REVIVE.","""[~timcharper] and [~zen-dog] pointed out that removal of a role from the suppression list (e.g. via UPDATE_FRAMEWORK) does not clear filters. This means that schedulers have to issue a separate explicit REVIVE for the roles they want to remove.    It seems like these are not the semantics we want, and we should instead be clearing filters upon removing a role from the suppression list."""
"MESOS-9935","Bug","containerization",2,"The agent crashes after the disk du isolator supporting rootfs checks.","""This issue was broken by this patch:  https://github.com/apache/mesos/commit/8ba0682521c6051b42f33b3dd96a37f4d46a290d#diff-33089e53bdf9f646cdb9317c212eda02    A task can be launched without disk resource. However, after this patch, if the disk resource does not exist, the agent crashes - because the info->paths only add an entry 'path' when there is a quota and the quota comes from the disk resource.    """
"MESOS-9934","Bug","master",3,"Master does not handle returning unreachable agents as draining/deactivated","""The master has two code paths for handling agent reregistration messages, one culminating in {{Master::___reregisterSlave}} and the other in {{Master::}}{{__reregisterSlave}}. The two paths are not continuations of each other.  Looks like we missed the double-underscore case in the initial implementation.  This is the path that unreachable agents take, when/if they come back to the cluster.  The result is that when unreachable agents are marked for draining, they do not get sent the appropriate message unless they are forced to reregister again (i.e. restarted manually)."""
"MESOS-9938","Documentation","documentation",3,"Standalone container documentation","""We should add documentation for standalone containers."""
"MESOS-9949","Task","allocation|master",5,"Track allocated/offered in the allocator's role tree.","""Currently the allocator's role tree only tracks the reserved resources for each role subtree. For metrics purposes, it would be ideal to track offered / allocated as well.    This requires augmenting the allocator's structs and recoverResources to hold the two categories independently and transition from offered -> allocated as applicable when recovering resources. This might require a slight change to the recoverResources interface."""
"MESOS-9948","Improvement","master",3,"master::Slave::hasExecutor occupies 37% of a 150 second perf sample.","""If you drop the attached perf stacks into flamescope, you can see that mesos::internal::master::Slave::hasExecutor occupies 37% of the overall samples!    This function does 3 hashmap lookups, 1 can be eliminated for a quick win. However, the larger improvement here will come from eliminating many of the calls to this function.    This was reported by [~carlone]."""
"MESOS-9952","Bug","test",1,"ExampleTest.DiskFullFramework is slow","""Executing {{ExampleTest.DiskFullFramework}} on my setup takes almost 18s in a not optimized build. This is way too long for a default-enabled test."""
"MESOS-9956","Bug","storage",2,"CSI plugins reporting duplicated volumes will crash the agent.","""The CSI spec requires volumes to be uniquely identifiable by ID, and thus SLRP currently assumes that a {{ListVolumes}} call does not return duplicated volumes. However, if a SLRP uses a non-conforming CSI plugin that reports duplicated volumes, these volumes would corrupt the SLRP checkpoint and cause the agent to crash at the next reconciliation:    MESOS-9254 introduces periodic reconciliation which make this problem much easier to manifest."""
"MESOS-9958","Bug","build|cli|release",1,"New CLI is not included in distribution tarball","""The files needed to build the new CLI are not included in distribution tarballs. This makes it impossible to build the CLI from released tarballs, and users have instead build directly from the git sources."""
"MESOS-9961","Bug","agent",2,"Agent could fail to report completed tasks.","""When agent reregisters with a master, we don't report completed executors for active frameworks. We only report completed executors if the framework is also completed on the agent:    https://github.com/apache/mesos/blob/1.7.x/src/slave/slave.cpp#L1785-L1832"""
"MESOS-9964","Improvement","containerization",8,"Support destroying UCR containers in provisioning state","""Currently when destroying a UCR container, if the container is in provisioning state, we will wait for the provisioner to finish provisioning before we start destroying the container, see [here|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/containerizer.cpp#L2685:L2693] for details. This may cause the container stuck at destroying, and more seriously it may cause the subsequent containers created from the same image stuck at provisioning state, because if the first container was stuck at pulling the image somehow, the subsequent containers have to wait for the puller to finish the pulling, see [here|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/provisioner/docker/store.cpp#L341:L345] for details.    So we'd better to support destroying the container in provisioning state so that the subsequent containers created from the same image will not be affected."""
"MESOS-9965","Bug","agent",1,"agent should not send `TASK_GONE_BY_OPERATOR` if the framework is not partition aware.","""The Mesos agent should not send `TASK_GONE_BY_OPERATOR` if the framework is not partition-aware. We should distinguish the framework capability and send different updates to legacy frameworks.    The issue is exposed from here:  https://github.com/apache/mesos/blob/f0be23765531b05661ed7f1b124faf96744aa80b/src/slave/slave.cpp#L5803    An example to follow:  https://github.com/apache/mesos/blob/f0be23765531b05661ed7f1b124faf96744aa80b/src/master/master.cpp#L9921"""
"MESOS-9966","Bug","containerization",3,"Agent crashes when trying to destroy orphaned nested container if root container is orphaned as well","""Noticed an agent crash-looping when trying to recover. It recognized a container and its nested container as orphaned. When trying to destroy the nested container, the agent crashes. Probably when trying to [get the sandbox path of the root container|https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/containerizer.cpp#L2966].    """
"MESOS-9968","Bug","HTTP API|libprocess",1,"WWWAuthenticate header parsing fails when commas are in (quoted) realm","""This was discovered when trying to launch the {{[nvcr.io/nvidia/tensorflow:19.08-py3|http://nvcr.io/nvidia/tensorflow:19.08-py3]}} image using the Mesos containerizer. This launch fails with    This is because the [header tokenization in libprocess|https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/http.cpp#L640] can't handle commas in quoted realm values."""
"MESOS-9971","Bug","build",1,"'dist' and 'distcheck' cmake targets are implemented as shell scripts, so fail on Windows/MSVC.","""Mesos failed to build due to error MSB6006: """"cmd.exe"""" exited with code 1 on Windows using MSVC. It can be first reproduced on {color:#24292e}e0f7e2d{color} reversion on master branch. Could you please take a look at this isssue? Thanks a lot!    Reproduce steps:    1. git clone -c core.autocrlf=true [https://github.com/apache/mesos] D:\mesos\src   2. Open a VS 2017 x64 command prompt as admin and browse to D:\mesos   3. cd src   4. .\bootstrap.bat   5. cd ..   6. mkdir build_x64 && pushd build_x64   7. cmake ..\src -G """"Visual Studio 15 2017 Win64"""" -DCMAKE_SYSTEM_VERSION=10.0.17134.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=""""C:\gnuwin32\bin"""" -T host=x64   8. msbuild Mesos.sln /p:Configuration=Debug /p:Platform=x64 /maxcpucount:4 /t:Rebuild         ErrorMessage:    67>PrepareForBuild:           Creating directory """"x64\Debug\dist\dist.tlog\"""".         InitializeBuildStatus:           Creating """"x64\Debug\dist\dist.tlog\unsuccessfulbuild"""" because """"AlwaysCreate"""" was specified.    67>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(209,5): error MSB6006: """"cmd.exe"""" exited with code 1. [D:\Mesos\build_x64\dist.vcxproj]  67>Done Building Project """"D:\Mesos\build_x64\dist.vcxproj"""" (Rebuild target(s)) -- FAILED.     """
"MESOS-9975","Bug","allocation",2,"Sorter may leak clients allocations.","""In MESOS-9015, we allowed resource quantities to change when updating an existing allocation. When the allocation is updated to empty, however, we forget to remove the client in the map in the `sorter::update()` if the `newAllocation` is `empty()`.    https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/sorter/drf/sorter.hpp#L382-L384    The above case could happen, for example, when a CSI volume with a stale profile is destroyed, it would be better to convert it into an empty resource since the disk space is no longer available. """
"MESOS-9978","Bug","build",1,"Nvml isolator cannot be disabled which makes it impossible to exclude non-free code","""We currently do not allow disabling of the link against {{libnvml}} which is probably not under a free license. This makes it hard to include Mesos at all in distributions requiring only free licenses, see e.g., https://bugzilla.redhat.com/show_bug.cgi?id=1749383.    We should add a configuration time flag to disable this feature completely until we can provide a free replacement."""
"MESOS-10007","Bug","executor|libprocess",1,"Command executor can miss exit status for short-lived commands due to double-reaping.","""Hi,    While testing Mesos to see if we could use it at work, I encountered a random bug which I believe happens when a command exits really quickly, when run via the command executor.    See the attached test case, but basically all it does is constantly start """"exit 0"""" tasks.    At some point, a task randomly fails with the error """"Failed to get exit status for Command"""":               I've had a look at the code, and I found something which could potentially explain it - it's the first time I look at the code so apologies if I'm missing something.     We can see the error originates from `reaped`:    [https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L1017]         Looking at the code, we can see that the `status_` future can be set to `None` in `ReaperProcess::reap`:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L69]                        So we could have this if the process has already been reaped (`kill -0` will fail).         Now, looking at the code path which spawns the process:    `launchTaskSubprocess`    [https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L724]         calls `subprocess`:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L315]         If we look at the bottom of the function we can see the following:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462]                        So at this point we've already called `process::reap`.         And after that, the executor also calls `process::reap`:    [https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L801]                        But if we look at the implementation of `process::reap`:    [https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L152]              We can see that `ReaperProcess::reap` is going to get called asynchronously.         Doesn't this mean that it's possible that the first call to `reap` set up by `subprocess` ([https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462)|https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462]    will get executed first, and if the task has already exited by that time, the child will get reaped before the call to `reap` set up by the executor ([https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L801]) gets a chance to run?         In that case, when it runs         would return false, `reap` would set the future to None which would result in this error.     """
"MESOS-10010","Task","libprocess",5,"Implement an SSL socket for Windows, using OpenSSL directly",""""""
"MESOS-10017","Improvement","libprocess",1,"Log all reverse DNS lookup failures in 'legacy' TLS (SSL) hostname validation scheme.","""There were being logged at VLOG(2):    https://github.com/apache/mesos/blob/1.9.0/3rdparty/libprocess/src/openssl.cpp#L859-L860    In the same spirit as MESOS-9340, we'd like to log all networking related errors as warnings and include any relevant information (IP address, etc)."""
"MESOS-10026","Improvement","HTTP API",13,"Improve v1 operator API read performance.","""Currently, the v1 operator API has poor performance relative to the v0 json API. The following initial numbers were provided by [~Will Mahler] from our state serving benchmark:       |OPTIMIZED - Master (baseline)| | | | |  |Test setup|1000 agents with a total of 10000 running tasks and 10000 completed tasks|10000 agents with a total of 100000 running tasks and 100000 completed tasks|20000 agents with a total of 200000 running tasks and 200000 completed tasks|40000 agents with a total of 400000 running tasks and 400000 completed tasks|  |v0 'state' response|0.17|1.66|8.96|12.42|  |v1 x-protobuf|0.35|3.21|9.47|19.09|  |v1 json|0.45|4.72|10.81|31.43|      There is quite a lot of variance, but v1 protobuf consistently slower than v0 (sometimes significantly so) and v1 json is consistently slower than v1 protobuf (sometimes significantly so).    The reason that the v1 operator API is slower is that it does the following:    (1) Construct temporary unversioned state response object by copying in-memory un-versioned state into overall response object. (expensive!)  (2) Evolve it to v1: serialize, de-serialize into v1 overall state object. (expensive!)  (3) Serialize the overall v1 state object to protobuf or json.  (4) Destruct the temporaries (expensive! but is done after response starts serving)    On the other hand, the v0 jsonify approach does the following:    (1) Serialize the in-memory unversioned state into json, by traversing state and accumulating the overall serialized json.    This means that v1 has substantial overhead vs v0, and we need to remove it to bring v1 on-par or better than v0. v1 should serialize directly to json (straightforward with jsonify) or protobuf (this can be done via a io::CodedOutputStream)."""
"MESOS-10038","Task","agent",5,"Implement agent code to listen on a domain socket","""On an agent with executor domain sockets enabled, we need to implement code such that the agent listens for incoming connections on its domain sockets, and creates `Connection` objects through which executor <-> agent v1 communication can happen.    The existing implementation of the I/O switchboard might give some inspiration on how this can be implemented."""
"MESOS-10041","Bug","libprocess",1,"Libprocess SSL verification can leak memory","""In {{process::network::openssl::verify()}}, when the SSL hostname validation scheme is set to """"openssl"""", the function can return without freeing an {{X509}} object, leading to a memory leak."""
"MESOS-10048","Task","containerization",5,"Update the memory subsystem in the cgroup isolator to set container's memory resource limits and `oom_score_adj`","""Update the memory subsystem in the cgroup isolator to set container’s memory resource limits and `oom_score_adj`"""
"MESOS-10063","Task","executor",2,"Update default executor to call `LAUNCH_CONTAINER` to launch nested containers","""The default executor will be updated to use the LAUNCH_CONTAINER call instead of the LAUNCH_NESTED_CONTAINER call when launching nested containers. This will allow the default executor to set task limits when launching its task containers."""
"MESOS-10064","Task","stout",3,"Accommodate the ""Infinity"" value in JSON","""See [here|https://docs.google.com/document/d/1iEXn2dBg07HehbNZunJWsIY6iaFezXiRsvpNw4dVQII/edit?ts=5de78977#heading=h.ejuvxat6x3eb] for what need to be done for this ticket."""
"MESOS-10080","Task","containerization",5,"Cgroups isolator: update cleanup logic to support nested cgroups","""Update Cgroups isolator to cleanup a nested cgroup for a nested container taking into account hierarchical layout of cgroups. Lowest nested cgroups should be destroyed first."""
"MESOS-10079","Task","containerization",5,"Cgroups isolator: recover nested cgroups","""Update recovery of Cgroups isolator to recover nested cgroups for those nested containers, which were launched in nested cgroups."""
"MESOS-10077","Task","containerization",3,"Cgroups isolator: allow updating and isolating resources for nested cgroups","""Allow Cgroups isolator to update and isolate resources for nested cgroups."""
"MESOS-10076","Task","containerization",3,"Cgroups isolator: create nested cgroups","""Update Cgroups isolator to create nested cgroups for a nested container, which supports nested cgroups, during container launch preparation."""
"MESOS-10096","Bug","agent|master",3,"Reactivating a draining agent leaves the agent in draining state.","""When reactivating an agent that's in the draining state, the master erases it from its draining maps, and erases its estimated drain time.    However, it doesn't send any message to the agent, so if the agent is still draining and waiting for tasks to terminate, it will stay in that state, ultimately making any tasks that then get launched get DROPPED due to the agent still being in a draining state.    Seems like we should either:    * Disallow the user from reactivating if still in draining, or  * Send a message to the agent, and have the agent move itself out of draining."""
"MESOS-10095","Improvement","agent|master",1,"Agent draining logging makes it hard to tell which tasks did not terminate.","""When draining an agent, it's hard to tell which tasks failed to terminate.    The master prints a count of the tasks remaining (only as VLOG(1) however), but not the IDs:        The agent does not print how many or which ones.    It would be helpful to at least see which tasks need to be drained when it begins, and possibly, upon each check, which ones remain."""
"MESOS-10094","Bug","master",1,"Master's agent draining VLOG prints incorrect task counts.","""This logic is printing the framework counts of these maps rather than the task counts:    https://github.com/apache/mesos/blob/4575c9b452c25f64e6c6cc3eddc12ed3b1f8538b/src/master/master.cpp#L6318-L6319        Since these are {{hashmap<FrameworkID, hashmap<TaskID, Task>>}}."""
"MESOS-10097","Bug","master",1,"After HTTP framework disconnects, heartbeater idle-loops instead of being deleted.","""In some cases, Master closes connection of HTTP framework without deleting the heartbeater:  https://github.com/apache/mesos/blob/65e18bef2c5ff356ef74bac9aa79b128c5b186d9/src/master/master.cpp#L3323  https://github.com/apache/mesos/blob/65e18bef2c5ff356ef74bac9aa79b128c5b186d9/src/master/master.cpp#L10910    It can be argued that this does not constitute a leak, because old heartbeaters are deleted on reconnection/removal.    However, this means that for each disconnected framework there is a ResponseHeartbeaterProcess that performs an idle loop."""
"MESOS-10098","Bug","agent",3,"Mesos agent fails to start on outdated systemd.","""Mesos agent refuses to start due to a failure caused by the systemd-specific code:      It turns out that some versions of systemd do not set environment variables `LISTEN_PID`, `LISTEN_FDS` and `LISTEN_FDNAMES` to the Mesos agent process, if its systemd unit is [ill-formed|https://github.com/dcos/dcos/pull/6886/files]. If this happens, `listenFdsWithName` returns an empty list, therefore leading to the error above.    After fixing the problem with the systemd unit, systemd sets the value for `LISTEN_FDNAMES` taken from the `FileDescriptorName` field. In our case, the env variable is set to `systemd:dcos-mesos-slave`. Since the value is expected to be equal to """"systemd:unknown"""" (for the compatibility with older systemd versions), the mismatch of values happens and we see the same error message.     """
"MESOS-10109","Bug","allocation",1,"After failover, master crashes on re-adding an agent with maintenance schedule set.","""Stacktrace:      This immediately follows re-adding an agent after master failover.    The issue was introduced by this patch:  https://reviews.apache.org/r/71428  which didn't account for the fact that `addSlave()` takes as an argument per-framework used resources that potentially can contain frameworks that were not added to allocator yet.    (Note that when master re-registers an agent, it first calls addSlave(), and only then calls addFramework() for the frameworks recovered from the agent.)"""
"MESOS-10117","Task","containerization",3,"Update the `usage()` method of containerizer to set resource limits in the `ResourceStatistics` protobuf message","""In the `ResourceStatistics` protobuf message, there are a couple of issues:   # There are already `cpu_limit` and `mem_limit_bytes` fields, but they are actually CPU & memory requests when resources limits are specified for a task.   # There is already `mem_soft_limit_bytes` field, but this field seems not set anywhere.    So we need to update this protobuf message and also the related containerizer code which set the fields of this protobuf message."""
"MESOS-10120","Bug","libprocess",1,"Authorization for /logging/toggle and  /metrics/snapshot is skipped on Windows.","""Due to path::join without specifying a separator being used to join an URI when looking for the authorization callback:  https://github.com/apache/mesos/blob/5e5783d748af17dfb1502df5870a5397879c82f1/3rdparty/libprocess/src/process.cpp#L3845"""
"MESOS-10126","Bug","containerization",3,"Docker volume isolator needs to clean up the `info` struct regardless the result of unmount operation","""Currently when [DockerVolumeIsolatorProcess::cleanup()|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/isolators/docker/volume/isolator.cpp#L610] is called, we will unmount the volume first, but if the unmount operation fails we will not remove the container's checkpoint directory and NOT erase the container's `info` struct from `infos`. This is problematic, because the remaining `info` in the `infos` will cause the reference count of the volume is larger than 0, but actually the volume is not being used by any containers. And next time when another container using this volume is destroyed, we will NOT unmount the volume since its reference count will be larger than 1 (see [here|https://github.com/apache/mesos/blob/1.9.0/src/slave/containerizer/mesos/isolators/docker/volume/isolator.cpp#L631:L651] for details) which should be 2, so we will never have chance to unmount this volume.    We have this issue since Mesos 1.0.0 release when Docker volume isolator was introduced."""
