"issuekey","type","components","storypoint","title","description_text"
"STL-39","New Feature","Valdator Server",1,"Startup without initializing genesis data can be confusing","""During development, it is common to startup a single validator.  This requires two steps prior to staring the validator: a) create a validator key with 'sawtooth admin keygen' and b) create data for the genesis block with 'sawtooth admin genesis'.    If a user using the single validator forgets step (b), the validator will currently startup and start peering.  The validator is essentially waiting for a genesis block before it will start publishing it's own blocks.  Peering happens in one thread while the journal is running in it's own threads.  (This maybe an imprecise description of the validator's behavior, as the implementation is fairly detailed.)    Current output in this situation looks like this:            If the user mistakenly forgot the step and is not starting another validator, this debug output does not help.  In fact, the 'None' statements look buggy as it looks like str(a) where 'a is None' in python slipped through and got logged.  (This is intentional currently, so it's not actually buggy.)    Recommend we have the genesis code identify this situation and log an INFO-level message with: 'No genesis block is present, resulting in no initial chain head; block publishing is disabled until a chain head is updated' or similar and removing the logging statements where 'None' occurs (or changing them for clarity)."""
"STL-35","New Feature","Go SDK|Packaging",1,"Research Go SDK Distribution Model","""Output of this task should be additional stories for distributing the SDK """
"STL-34","Bug","XO Family",2,"xo create's --wait option does not wait","""When creating a game with xo, it returns before the game is committed, even with --wait:    Expected behavior is for the CLI to pause until either an error is returned or the transaction has been committed to a winning block."""
"STL-82","New Feature","Documentation|Validator Global State",1,"Migrate Namespace/Address Documentation to Public Sphinx Doc","""Migrate this document into the appropriate place in the public facing documentation:"""
"STL-66","New Feature","Validator Global State",3,"Implement State Delta Registration with Fork Resolution","""State Export clients may send {{last_known_block_ids}} which may no longer exist on the chain (e.g. the connected validator had a fork that was discarded).      Implement a way for the validator and subscriber to negotiate for a known block id."""
"STL-65","New Feature","Settings Family",2,"Implement Settings Key Naming Strategy","""Addresses are made up of 70 hexadecimal digits, with the first 6 making up a transaction family namespace, and the remaining 64 make up the address of the data within the namespace.  With on-chain settings, the namespace is 000000.  The remainder of the data is made up from a series of hashes of key parts.         In order to allow users to to query for groups of settings, without having to query the whole of the the settings, there needs to be a correspondence between the key parts and the address.           The remaining 32 bytes/64 hex characters of a setting address can be broken up into 4 parts, each corresponding to a part of the setting key.  Each part of the key is hashed with sha256 and the first 16 characters of the hash are used.  For keys with less than 4 parts, the remaining hashes are produced using empty strings.  For keys with more than 4 parts, the last part will be the remaining subkey (that is, 3 individual parts, and the 4 made up of the remain subkey).      Breaking it into 4 parts limits the granularity of subqueries, for deeply nested settings, but a consistent position for the keys is necessary for subgroup queries to be predictable, without the application developer requiring knowledge of how deeply nested the settings are."""
"STL-64","New Feature","Validator Global State",3,"Implement State Delta Subscription Catch-up","""State Export clients can send \{\{last_known_block_ids}} which may be older than the current chain head.  Provide a way to send the successive events between the block id sent and the chain head."""
"STL-61","New Feature","Documentation|sawtooth-core|XO Family",1,"Write Transaction Family Spec for XO","""We do not currently have a transaction family specification for xo in our docs.  Since we use xo as our example transaction family for tutorial purposes, it would make sense to also provide a transaction family specification.  Following the format of the Sawtooth Config or Validator Registry transaction families, write a transaction family spec for the XO Family."""
"STL-56","New Feature","Valdator Server|Validator Txn Execution",1,"Add configuration option to select between schedulers","""Add command line option and file-based configuration to switch between schedulers. Update sawtooth cluster command to enable switching scheduler."""
"STL-55","New Feature","sawtooth-core|Validator Txn Execution",5,"Implement state handling in parallel scheduler branch","""Finish implementation of state handling in the parallel scheduler branch, with appropriate integration with the context manager. At the end of this effort, the parallel scheduler should work well enough that it can be used in place of the serial scheduler. """
"STL-93","New Feature","XO Family",3,"Update xo tests","""The xo TP and smoke tests don't test very much right now. xo smoke, for instance, verifies that nothing breaks when xo is run, but doesn't do any state checking at all [1]. They should be more extensive.    [1] In this sense it's a true smoke test: """"The phrase smoke test comes from electronic hardware testing. You plug in a new board and turn on the power. If you see smoke coming from the board, turn off the power. You don't have to do any more testing."""""""
"STL-114","New Feature","Continuous Integration|Documentation",1,"Publish stable docs as default, with docs from master at a sub-url","""Currently, our published documentation is not in-sync with our stable debian packages and stable docker images.  This is causing app developer confusion.  Instead, we should publish stable documentation at the same time we publish the debian/docker artifacts.  We should also generate master as we do now, but in a different URL (similar to what we do for 0.7)."""
"STL-104","Bug","XO Family",1,"Fix xo state format to handle hash collisions","""Currently, the xo examples do not handle possible hash collisions.  Instead, we have warnings like the following in xo python:    Since we use xo for our tutorials, we should provide a correct implementation which stores a list of games at the address instead of a single game, thus handling hash collisions properly.  Note that this will require an update to all xo TP implementations as well as any impacted documentation."""
"STL-103","Bug","Documentation",2,"In docs, code blocks which include source files have many empty lines","""In our documentation, use of literalinclude results in empty lines at the end of the block (see attached image).   """
"STL-97","New Feature","Documentation|REST API",1,"Clarify Python REST instructions are for Python 3","""The instructions for submitting a transaction to the REST API from a python client are Python 3 specific, but do not state the Python version requirement.  In Python 2.7, these instructions result in an error:    The page should clearly indicate that it is for Python 3."""
"STL-160","Improvement","sawtooth-core",3,"Update Poet WaitTimer and WaitCertificate Tests","""Create mock_poet_enclave_wait_timer and mock_poet_enclave to allow faster testing without using sleeps.     include Jamie on the review."""
"STL-159","Improvement","Go SDK",2,"Handle Validator Disconnects","""Handle validator disconnect events in the Go SDK correctly.    When the validator is restarted, the Go SDK does not re-connect.  Expected behavior is that a restart of the validator should not require restarting the TPs such as intkey go."""
"STL-157","New Feature","Documentation|Javascript SDK",1,"App Dev Guide: Writing Transaction Processors-JavaScript","""Partially done fro NML-2436.    Use specially written JavaScript code fragments to insert into tutorial template already created.      """
"STL-155","New Feature","Documentation|Python SDK",1,"Complete rebasing of Python Txn Tutorial","""The Python tutorial/guide for writing a transaction processor (NML-2432) is mainly complete, but is divided into two PR's due to a process anomaly.     Get the commits in the PR's merged into a single PR and clean up.     """
"STL-153","Improvement","Validator Txn Execution",5,"Create yaml based scheduler test","""With this test (framework) it will be possible to test correctness on any scheduler implementation by writing a yaml file using particular data (The test framework will create transactions and batches and mark transaction as valid or invalid as specified by the yaml). The test of correctness is that the correct merkle root is available in the scheduler for the block production/validation component to query."""
"STL-148","Improvement","PoET",1,"Remove Signup Information Verification from PoET Enclave Simulator","""Signup information is verified in the validator registry transaction processor, removing the need to verify it in the enclave."""
"STL-147","Improvement","PoET",3,"Make Setting the IAS REPORT_PUBLIC_KEY_PEM Configurable for PoET Consensus","""Currently, the validator registry transaction processor and the PoET enclave simulator use a predetermined IAS verification key to check attestation verification report signature.    This IAS verification key needs to be configurable."""
"STL-146","Improvement","PoET",1,"Make Setting the IAS Enclave Basename Configurable for PoET Consensus","""Currently, the validator registry transaction processor and the PoET enclave simulator use a predetermined enclave basename to check against the attestation verification report. The SGX enclave checks against its own enclave basename value.    This enclave basename value needs to be configurable."""
"STL-144","Improvement","PoET",1,"Make Setting the IAS Enclave Measurement Configurable for PoET consensus","""Currently, the validator registry transaction processor and the PoET enclave simulator use a predetermined enclave measurement to check against the attestation verification report. The SGX enclave checks against its own enclave measurement value.    This enclave measurement value needs to be configurable."""
"STL-143","Improvement","PoET",2,"Remove Dependency on kwargs in PoET Enclave Simulator","""The PoET simulator enclave still is dependent upon the old configuration model (i.e., passing all configuration via kwargs dictionary).  This dependency needs to be removed and the PoET simulator needs to be updated to use either the new config transaction family or the new local configuration being work on by [~dan.middleton@intel.com]."""
"STL-135","Task","PoET",2,"PoET Block Publisher - claim readiness if the wait timer has expired","""Check that PoET Block Publisher claims readiness only when it is time to claim the current candidate blocks - the wait timer has expired.  check_publish_block()  return self._wait_timer.has_expired(now=time.time())"""
"STL-134","Task","PoET",2,"PoET Fork Resolver - current & new fork heads are not PoET blocks","""Test conditions:  Current fork head is not a PoET block  New fork head is not a Poet block   - new fork head builds directly on top of it (i.e., the new fork head has the current fork head as its previous block)   - new fork head does not build directly on top of it"""
"STL-133","Task","PoET",5,"PoET Block Publisher - initialize_block() Failure & Success Cases","""- Verify that it returns False if the validator does not have an entry in the validator registry   - Verify that it returns False if the PoET public key in the validator registry is not current   - Verify that it returns False if the K (block claim count) test fails   - Verify that it returns False if the validator sign up info was committed too late   - Verify that it returns False if the C policy (block claim delay) test fails   - Verify that it returns False if the Z policy (block claim frequency) test fails   - Verify that it returns True if the validator passes all the tests above"""
"STL-132","Task","PoET",3,"PoET Fork Resolver - current & new fork heads are both PoET blocks ","""- current fork head and new fork head previous blocks are the same (i.e., they both build on the same block)   - current fork wait certificate duration < new fork wait certificate duration   - current fork wait certificate duration > new fork wait certificate duration   - current fork wait certificate duration == new fork wait certificate duration   - current fork header signature > new fork header signature   - new fork header signature >= current fork header signature   - current fork head and new fork head previous blocks are not the same   - current fork consensus state aggregate local mean > new fork consensus state aggregate  local mean + new fork wait certificate local mean   - current fork consensus state aggregate local mean < new fork consensus state aggregate  local mean + new fork wait certificate local mean   - current fork consensus state aggregate local mean == new fork consensus state aggregate  local mean + new fork wait certificate local mean   - current fork header signature > new fork header signature   - new fork header signature >= current fork header signature"""
"STL-131","Task","PoET",1,"Refactor - create MockConsensusState()","""Create a MockConsensusState() to eliminate duplicate code in most tests in test_consensus."""
"STL-185","Bug","Python SDK|sawtooth-core",1,"xo create should send a friendly response after waiting","""Creating a game with xo currently prints out a raw JSON response from the REST API. It should instead wait for the block to commit, and then print a friendlier XO-specific confirmation."""
"STL-188","Improvement","PoET",2,"Fix PoET Block Publisher State Persistence","""The PoET block publisher uses one or more class variables to persist state across creation of PoET block publisher objects.  While this works, it is quite ugly when it comes to devising unit tests.  This """"global"""" state needs to be persisted in a better way.  Of highest importance is keeping track of the current PoET public key, which can probably be persisted in the PoET Key Store, an already existing global state persistence used by the PoET block publisher."""
"STL-209","Improvement","PoET",1,"Remove PoET as a Service","""The PoET as a Service code should be removed from the repo."""
"STL-198","New Feature","Documentation",2,"App Dev Guide's Transaction Processor Tutorial lacks xo CLI examples","""The tutorial as it currently exists covers the creation of a transaction processor, for which we have a full implementation to serve as an example.    Instead of ending as it does currently, the tutorial should continue by showing some client examples as well, including game play using the xo CLI.    This could potentially change the tutorial from a transaction processor tutorial to a more comprehensive application development tutorial covering transaction processor and client-side transaction submission and state viewing."""
"STL-197","New Feature","Documentation",2,"App Dev Guide's Transaction Processor Tutorial Python is vagrant specific","""The """"Transaction Processor Tutorial Python"""" contains vagrant-specific instructions, which are invalid in this context (vagrant is a core guide topic).  This includes path references to /project, for example.  This needs to be cleaned up so it makes sense in terms of the environment setup provided in the Application Development Guide."""
"STL-196","Bug","Documentation",1,"App Dev Guide has duplicate sections for transaction family tutorial","""The Application developer guide has both a top-level section called 'Transaction Family Tutorial' as well as the Python SDK/Transaction Processor Tutorial Python.  There should only be one TF tutorial section for python."""
"STL-195","New Feature","Documentation",3,"App Dev Guide flow is confusing when using docker","""The Environment Setup section describes how to startup a sawtooth environment easily with Docker.  This is probably the easiest/best option for most application developers.    The """"Getting Started with Hyperledger Sawtooth"""" section describes things in a way that would be more appropriate for native Ubuntu installs or vagrant.  For example, when using Docker, you do not manually startup the validator or worry about genesis block creation as it is done for you in the docker compose file.    Some of this content should be rewritten and generalized.  Other portions of the content should move into the Ubuntu install section of the Environment Setup guide.  (Similar to how we show docker compose up/down, we could show basic operations when installing via deb packages.)"""
"STL-194","New Feature","Documentation",2,"App Dev Guide should not reference git or require git clone of sawtooth-core repository","""Currently, the Application Developer's guide's Environment Setup section for Docker requires the user to clone the git repository.    Instead, the guide should provide a link to download the compose file from the documentation.  The cleanest approach would probably be to copy the yaml file into the app dev guide directory within the Makefile, then link to that file.    This may be a bit tricky for PDF generation.  In that instance, we will want to link to the file on github or in-line the file within the PDF text itself. Sphinx-doc allows for format conditionals."""
"STL-230","New Feature","Documentation",1,"App Dev Guide's docker install section needs more detail","""The current docker installation section provides a bare minimum of links and help.    While we do not want to document docker itself, we should provide more assistance in finding the right information to get docker and docker compose installed such that the rest of our docker-compose instructions will work."""
"STL-228","New Feature","Documentation|Packaging",1,"Rename compose file sawtooth-demo.yaml to sawtooth-default.yaml","""The word 'demo' should be avoided in this context; this is our recommended starting point for app developers  ."""
"STL-225","New Feature","Documentation",2,"App Dev Guide needs pointers to pages describing docker proxy configuration","""The application developer guide should provide links to resources which help the app developer configure proxies when using docker-compose.    This should cover at least Windows and MacOS proxy configuration, and perhaps Linux."""
"STL-223","New Feature","sawtooth-core|Validator Global State",5,"Provide State Delta Export Proxy Over Web Sockets","""Extend the Rest API component to provide State Export Deltas over a WebSocket.     Proposed solution here would be to have the REST API component subscribe to all state delta events.      Any client connecting via a websocket would interact in the following way:        # Send last known block id and address prefixes   # Receive State Delta Event    The rest api would manage the filtering, and the catch-up, but the clients should not need to send unregister messages, as this is simply a consequence of the WS closing."""
"STL-219","Improvement","Javascript SDK",2,"Test Javascript SDK client modules in browser","""Test the Javascript SDK Client module in the browser, using at least one of Browserify or Webpack.     * Ensure that the client functionality works via the REST API.   * Verify that the signing libraries work in browser """
"STL-213","New Feature","Packaging",1,"Docker compose_client_1 image should initialize user keys","""After starting a network with:        and logging into compose_client_1:        the user is required to run 'sawtooth keygen' prior to using commands like 'xo':        It would be a better experience if keys were pre-generated.    Presumably we would not want these keys to be distributed with the image, but rather generated when doing the docker-compose up.  """
"STL-242","Improvement","Settings Family",1,"Rename Config Transaction Family","""Rename {{sawtooth-config}} family to something more representative of the fact that it is a reference implementation."""
"STL-248","Task","Supply_Chain",1,"Complete Supplychain Transaction Processor Permission enforcement.","""Review the permission checks and update the transaction handler to enforce proper permission enforcement in the Transaction Handler. The code below has TBD comments in many of the places to update.                    class RecordHandler(object):   @classmethod   def apply(cls, transaction, state):   payload = json.loads(transaction.payload.decode())    LOGGER.debug(""""apply payload: %s"""", repr(payload))    tnx_action = payload.get('Action', None)   txnrecord_id = payload.get('record_id', None)    header = TransactionHeader()   header.ParseFromString(transaction.header)   tnx_originator = addressing.get_agent_index(   addressing.get_agent_id(header.signer_pubkey))   # Retrieve the stored record data if an ID is provided.   record_id = txnrecord_id   record_store_key = record_id    record_store = state_get_single(state, record_store_key)   # TBD: the originator should be a registered agent   # TBD: if it's not Create then the record should exist     # Check Action   if tnx_action == 'Create':   if txnrecord_id is None:   raise InvalidTransaction(   'Record id expected for CreateRecord')    record_store = \{}   cls.create_record(tnx_originator, record_id, payload,   state, record_store)   # TBD: If there are parents they should not be final and they should   # have accepted applications from the txn originator.   # TBD: If it is an ownership transfer then then the record should   # not have custodians.   # TBD: What if the sensor is already registered? Should we require   # an unregister operation?    elif tnx_action == """"CreateApplication"""":   if txnrecord_id is None:   raise InvalidTransaction(   'Record id expected for create_application')    cls.create_application(tnx_originator, record_id, payload,   state, record_store)   # TBD: Check for existing application - for now only one at a   # time.   # TBD: applicationtype is owner or custodian, terms should be   # defined   # TBD: If app for ownership, then there should be no custodians    elif tnx_action == """"AcceptApplication"""":   if txnrecord_id is None:   raise InvalidTransaction(   'Record id expected for accept_application')    cls.accept_application(tnx_originator, record_id, payload,   state, record_store)   # TBD: must be the record holder   # TBD: there must be an open application    elif tnx_action == """"RejectApplication"""":   if txnrecord_id is None:   raise InvalidTransaction(   'Record id expected for reject_application')    cls.reject_application(tnx_originator, record_id, payload,   state, record_store)   # TBD: must be the record holder   # TBD: there must be an open application    elif tnx_action == """"CancelApplication"""":   if txnrecord_id is None:   raise InvalidTransaction(   'Record id expected for cancel_application')    cls.cancel_application(tnx_originator, record_id, payload,   state, record_store)   # TBD: must be the application creator   # TBD: there must be an open application    elif tnx_action == """"Finalize"""":   if txnrecord_id is None:   raise InvalidTransaction(   'Record id expected for Finalize')    cls.finalize_record(tnx_originator, record_id, payload,   state, record_store)   # TBD: must be the record owner. The only way a custodian can   # finalize a record is by making a child.   # TBD: record must not be final already   else:   raise InvalidTransaction('Action \{} is not valid'.   format(tnx_action))     # Store the record data back   state_put_single(state, record_store_key, record_store)    @classmethod   def create_record(cls, originator, record_id, payload, state, my_store):   sensor_id = payload.get('Sensor', None)   sensor_idx = None   if sensor_id != None:   sensor_idx = addressing.get_sensor_index(sensor_id)    record_info = \{}   # Owner set below   record_info['CurrentHolder'] = originator   # Custodians set below   record_info['Parents'] = payload.get('Parents', None)   record_info['Timestamp'] = payload.get('Timestamp')   record_info['Sensor'] = sensor_idx   record_info['Final'] = False   record_info['ApplicationFrom'] = None   record_info['ApplicationType'] = None   record_info['ApplicationTerms'] = None   record_info['ApplicationStatus'] = None   record_info['EncryptedConsumerAcccessible'] = None   record_info['EncryptedOwnerAccessible'] = None   my_store['RecordInfo'] = record_info    my_store['StoredTelemetry'] = payload.get('Telemetry', \{})   my_store['DomainAttributes'] = payload.get('DomainAttributes', \{})   # Determine if this record has parents   has_parents = record_info['Parents'] != None and \   len(record_info['Parents']) > 0     # If there are parents update Owner and Custodian depending on the   # ApplicationType   if has_parents:   # Use the first parent   # TBD: how to handle multiple parents? If there are no custodians   # then it seems straight forward to transfer to the new owner.   # Maybe if there are custodians then all but the first need to be   # held by the owner and we handle it based on the first parent.   # One thing that could be useful here is to be able to combine   # multiple parents and keep the owner/custodians the same - right   # now we can only add a custodian or pop the stack.   parent_id = record_info['Parents'][0]   parent_store = state_get_single(state, parent_id)   if parent_store['RecordInfo']['ApplicationType'] == """"Owner"""":   # Transfer ownership - in this case there should be   # no custodians.   assert len(parent_store['RecordInfo']['Custodians']) == 0   record_info['Owner'] = originator   record_info['Custodians'] = []   else:   # Transfer custodianship   record_info['Owner'] = \   parent_store['RecordInfo']['Owner']   record_info['Custodians'] = \   list(parent_store['RecordInfo']['Custodians'])     # Check the next to last element of the Custodians array. If it   # is the new holder, then this is a 'pop' operation. It's also   # a pop if here is one custodian and the applicant is the   # owner.   is_pop = False   if len(record_info['Custodians']) > 1 and \   record_info['Custodians'][-2] == originator:   is_pop = True   elif len(record_info['Custodians']) == 1 and \   record_info['Owner'] == originator:   is_pop = True    if is_pop:   record_info['Custodians'].pop()   else:   record_info['Custodians'].append(originator)   else:   # No parents, just create a new record   record_info['Owner'] = originator   record_info['Custodians'] = []     # If there are parents mark them as final.   if has_parents:   for parent in record_info['Parents']:   parent_store = state_get_single(state, parent)   parent_store['RecordInfo']['Final'] = True   state_put_single(state, parent, parent_store)     # Remove the record from the former owner - even if this   # is a custodian transfer we need to store the new   # record ID with the owner.   AgentHandler.remove_record_owner(state,   parent_store['RecordInfo'][""""Owner""""],   parent)     # Remove the previous holder   AgentHandler.remove_record_holder(state,   parent_store['RecordInfo'][""""CurrentHolder""""],   parent)     # Remove the accepted application from the new owner   AgentHandler.remove_accepted_application(state,   parent_store['RecordInfo']['ApplicationFrom'],   parent)     # Record the owner of the new record in the agent   AgentHandler.add_record_owner(state, record_info[""""Owner""""], record_id,   record_info[""""Owner""""] == record_info[""""CurrentHolder""""])   # Record the new record holder in the agent   AgentHandler.add_record_holder(state, record_info[""""CurrentHolder""""],   record_id)     # Register the sensor   if sensor_id != None:   if state_get_single(state, sensor_idx) != None:   sensor_store = state_get_single(state, sensor_idx)   else:   sensor_store = \{}   sensor_store[""""Record""""] = record_id   sensor_store[""""Name""""] = sensor_id   state_put_single(state, sensor_idx, sensor_store)    @classmethod   def create_application(cls, originator, record_id,   payload, state, my_store):   record_info = my_store['RecordInfo']   # Agent ID who initiated the application   record_info['ApplicationFrom'] = originator   # custodian or owner   record_info['ApplicationType'] = payload['ApplicationType']   # Should be encrypted?   record_info['ApplicationTerms'] = payload['ApplicationTerms']   # To indicate acceptance (or not) of the application.   record_info['ApplicationStatus'] = """"Open""""     # Record the new application in the current holder   AgentHandler.add_open_application(state,   record_info['ApplicationFrom'],   record_info['CurrentHolder'],   record_id)    @classmethod   def accept_application(cls, originator, record_id, payload, state,   my_store):   # Mark the application as accepted. After this the new   # owner/custodian is able to make a new record with this   # record as the parent.   record_info = my_store['RecordInfo']   record_info['ApplicationStatus'] = """"Accepted""""     # Record the accepted application in the new holder   AgentHandler.remove_open_application(state,   record_info['ApplicationFrom'],   record_info['CurrentHolder'],   record_id)   AgentHandler.add_accepted_application(state,   record_info['ApplicationFrom'],   record_id,   record_info['Sensor'])    @classmethod   def reject_application(cls, originator, record_id, payload, state,   my_store):   # Mark the application as rejected.   record_info = my_store['RecordInfo']   record_info['ApplicationStatus'] = """"Rejected""""     # Record the rejected application in the agent   AgentHandler.remove_open_application(state,   record_info['ApplicationFrom'],   record_info['CurrentHolder'],   record_id)    @classmethod   def cancel_application(cls, originator, record_id, payload, state,   my_store):   # Mark the application as cancelled.   record_info = my_store['RecordInfo']   record_info['ApplicationStatus'] = """"Cancelled""""     # Record the cancelled application in the agent   AgentHandler.remove_open_application(state,   record_info['ApplicationFrom'],   record_info['CurrentHolder'],   record_id)    @classmethod   def finalize_record(cls, originator, record_id, payload, state, my_store):   record_info = my_store['RecordInfo']   # TBD: check that there are no custodians before finalizing   record_info['Final'] = True   # Remove the record from the agent   # TBD: handle any pending applications   assert record_info['Owner'] == originator   assert record_info['CurrentHolder'] == originator   AgentHandler.remove_record_owner(state, originator, record_id)   AgentHandler.remove_record_holder(state, originator, record_id)"""
"STL-247","New Feature","Settings Family",1,"Rename the config transaction family to settings transaction family","""The term 'config' is overloaded, in that we have used it for two purposes: 1) off-chain file-based configuration; and 2) on-chain configuration.    To assist with this, we could rename the config transaction family to 'settings'.  In usual conversation, 'config' would refer to file-based configuration and 'settings' to on-chain settings.    This should be discussed and approved by the maintainers prior to implementation."""
"STL-246","Bug","Documentation|Validator Journal",1,"It is the BlockPublisher that sends the block out through Gossip, not Consensus.BlcokPublisher","""The journal diagram in [http://intelledger.github.io/architecture/journal.html] has a bug, that diagram shows an arrow from Consensus.BlcokPublisher to Gossip, but it is  is the BlockPublisher that sends the block out through Gossip."""
"STL-245","Task","PoET",1,"PoET Block Publisher - finalize_block() success case ","""Check that PoET block finalze_block() only finalizes a block to be claimed when the candidate block is good and should be generated.     Check if the wait_certificate was correctly created."""
"STL-260","New Feature","REST API|Settings Family",2,"Finalize design of api-key based authorization","""* What format will the key be in?   * What endpoints and query parameters will be added?   * What configuration options will be available?"""
"STL-259","Task","OMI Summer Lab",2,"Package OMI domain-specific Javascript","""Package javascript library for in-browser use as appropriate to execute on the plan for deployment."""
"STL-258","Task","OMI Summer Lab",5,"Package OMI transaction processor","""Package transaction processor in deb or docker as appropriate to execute on plan for deployment.    This includes appropriate documentation."""
"STL-256","Task","OMI Summer Lab",2,"Plan & document training and training collateral","""What do the participants need to know? How do they bring up and interact with a tests system?          Generate any tutorials documentation and guides the OMI Summer Lab participants may need in order to use the OMI TP.           """
"STL-255","Task","OMI Summer Lab",2,"OMI Plan for deployment","""Create plan and collateral necessary to support the OMI Summer labs. This may include standing up servers to host validators, creating Docker compose files to allow the participants to run validators with the OMI TP (potentially local copies they have modified).       """
"STL-254","Task","OMI Summer Lab",5,"OMI Domain Specific Javascript Library","""Write a domain specific JS library to enable easy access to the validator data store and submission of Transactions to the validator. This should include a tutorial and sufficient documentation to enable   easy use. This library should run in the broswer environment or under node.js"""
"STL-253","Task","OMI Summer Lab",5,"Implement the OMI transaction Processor","""This can be done on top of the supplychain transaction processor or as it's own TP, which ever path is more expedient.          """
"STL-252","Task","Documentation|OMI Summer Lab",2,"Write OMI TP spec/defintion","""    Use Cases      - Cataloging, attributing and distributing live DJ mixes           -- Register Artist/DJ          -- Register Song/Mix/Asset          -- Reference Other Songs/Assets in New Asset      - Commercializing mixtapes built from original material and back catalogs           -- Register Artist          -- Register Song/Asset          -- Reference Other Songs/Assets in New Asset      - Compensating musicians for visual works using their songs as data           -- Provide Payment information for Songs/Assets (Who/How Much)      - Identifying individuals for their contribution to single tracks in new works           -- Register Individuals          -- Reference Individuals as Contributors In Songs/Assets                    Transaction Actions      - Register Entity/Identity (Artist / DJ / Individuals)           -- Fields               --- Name      - Update by Owner/Registrar      - Register Asset (Song/Mix/Compilation/Visual Work)           -- Fields (Not all are req'd)               -- 'Owner' of Asset (Could be label/PRO/etc.)              -- Title of Work              -- Type (Recording/Composition/Mix/Visual/etc.)              -- ISRC, ISWC, IPI, ISNI              -- Label              -- Songwriter / Composer              -- Publisher              -- Reference Entities as Contributors In Songs/Assets              -- Reference Other Assets in New Asset                   --- Entirely new works have no references                  --- Derivatives have a single reference                  --- Mixes/Compilations have multiple references              -- Provide Payment Information for Songs/Assets                   --- Contact Information                  --- OR                  -- Addresses and Royalty Split/Amounts              -- splits to contributors and publishers.      -- Update Fields by Owner/Registrar"""
"STL-281","New Feature","Continuous Integration|Javascript SDK",3,"Publish JS SDK to NPM registry","""In order to simplify development for app developers, publish the Javascript SDK to the NPM registry.  This will allow user to simple run {{npm install -S sawtooth-sdk}} """
"STL-279","New Feature","Continuous Integration",1,"Update pylint support to 1.7.x and un-pin current pylint=1.6.5 in docker images","""The release of pylint 1.7.x broke builds.  As a workaround, we pinned the docker pip installs at 1.6.5 so builds continue to work.    Linting in the vagrant environment is currently broken as it installs 1.7.x which the code does not pass.    At the completion of this task:    - sawtooth-core will support the latest pylint 1.7.x  - pylint will no longer be pinned to a specific version withing docker or vagrant  """
"STL-277","New Feature","Python SDK",3,"Implement basic intkey show, list, inc, dec, set CLI commands","""The intkey command currently lacks the ability to generate specific inc/dec/set transactions as well as the ability to view the current intkey namespace.    Propose the following be added to the intkey CLI:    intkey inc NAME  intkey dec NAME  intkey set NAME VALUE  intkey list  intkey show NAME    """
"STL-272","Task","Supply_Chain",1,"Update the Supplychain TP to use ProtoBuf for global state representation","""Currently the Supplychain TP uses ProtoBuf as global state storage. Protobuf is a better choice due to it's conciseness and repeat-ability. Updated as per comment below. """
"STL-271","Task","Supply_Chain",1,"Update the Supplychain TP to handle Hash collisions. ","""Currently the the supplychain TP assumes all hashes are unique, and will overwrite items in the case of a collision. It needs to be updated to follow the standard pattern of using a ordered dictionary or list at each address to allow for collisions. """
"STL-270","Task","Supply_Chain",2,"Supplychain Transaction Processor Spec","""Write a transaction processor spec"""
"STL-269","Task","Supply_Chain",5,"Add Supplychain rest interface","""Add a domain specific rest api. Implement in accordance with the Spec writtent in STL-295    Associated python client issue is STL-491."""
"STL-311","Improvement","Validator Global State",1,"Add Previous Block Id to State Delta Event Message","""In order to improve fork resolution and lost events, include the {{previous_block_id}} in the StateDeltaEvent protobuf message.  This will allow subscribers/clients to request the previous block's state deltas if the haven't missed or dropped the original event. """
"STL-310","Improvement","Hyperledger",3,"Enter Documentation Epics & Stories into JIRA","""We need to fill out the JIRA epics and stories we have planned for documentation.  At a minimum this include architecture and app dev guide."""
"STL-308","New Feature","Documentation|REST API",2,"Document Apache proxy REST API setup","""In the System Administrator's Guide, document the process of putting the REST API behind an Apache proxy for authorization."""
"STL-307","Task","Documentation",1,"Resolve Fossology documentation licensing","""HL Fossology report flagged unlicensed documentation files.  Have pointed out to HL that other major Apache-licensed projects do not license individual document files. Awaiting response.  Once resolved, issue JIRA item(s) as needed."""
"STL-305","Improvement","REST API",2,"Extend integration test to include HTTPS proxy","""Extend the integration test built in STL-301 to include verifying support for proxies running HTTPS."""
"STL-304","Improvement","REST API",2,"Update XO CLI to support Basic Auth","""Add support to the XO CLI for a {{--user}} and {{--password}} optional arguments for specifying basic auth login information."""
"STL-303","Improvement","Python SDK|REST API",2,"Update Intkey CLI to support Basic Auth","""Add support to the Intkey CLI for a {{--user}} and {{--password}} optional arguments for specifying basic auth login information."""
"STL-302","Improvement","REST API|sawtooth CLI",2,"Update sawtooth CLI to support HTTP Basic Auth","""Add support to the sawtooth CLI for a {{--user}} and {{--password}} optional arguments for specifying basic auth login information.    This assumes that HTTP basic auth has been configured on a proxy."""
"STL-301","Improvement","REST API",5,"Implement integration test for REST API behind Apache Proxy","""Implement an integration test for the REST API, such that an instance of the REST API sits behind an Apache server, which acts as a proxy.      Use this test to verify:  * the a the rest api properly responds and produces links that are relative to the proxy, not localhost  * the rest api functions behind a proxy with HTTP basic auth"""
"STL-297","Improvement","Supply_Chain|Validator Networking",5,"Design a mechanism for permissioned access to a network based on validator keys and on-chain config","""Validators should be able to determine whether messages delivered to them should be handled or dropped based on a set of identities stored in the config namespace. Permissioning rules could determine the roles an identity is able to play on the network (e.g. connect, peer, publish blocks, forward blocks/batches). Message handlers should examine incoming messages against the permissioning rules and the current configuration and either permit, drop, or respond with an error to the sending node. In certain cases (e.g. attempting to peer without permission), it may make sense to forcibly close the connection."""
"STL-291","Improvement","Validator Networking",1,"Move Curve ZMQ keys out of core.py to local config files","""The hardcoded 'public network instance' Curve ZMQ keys should be moved out of the code into a local config file (depends on the completion of local config stories). Moving the keys to local config would allow a basic configuration of a secured network based on sideband sharing of a single network keypair to all participating nodes.    Make sure that the choice of configuration plays nicely with the planned design for a stronger 'per-node' key scheme."""
"STL-289","Task","Supply_Chain",3,"Create Supplychain python client and CLI","""Create a command line cli to allow for submission SC transactions and query of SC State. This should also include a document that provides detailed instructions on the usages and walks thru an example use case. """
"STL-286","Bug","Python SDK",1,"xo show returns 404 when game doesn't exist","""xo show gives an unfriendly message when a game does not exist:    % ./bin/xo show game000  Error: Error 404: Not Found    Instead, this should give """"Error: no such game: game000""""."""
"STL-317","Task","Supply_Chain",1,"Refactor the Application to allow it to be tracked","""Encapsulate the Application to be an object in the Record, that is either not set or the current object application.  Possible distinct object in it's own namespace, that is referenced by both the Agent(applicant) and the Record.  Extend the Record to track the history of applications and the results of those applications.           """
"STL-316","Task","Supply_Chain",1,"Update Supply chain records to use native addressing","""Supplychain records should use serial numbers or some other natural addressing scheme to look up items. In the case of raw materials this should be a lot number or maybe even the sensor tag or nfc tag attached to the item upon harvesting. """
"STL-322","Bug","Documentation|Settings Family",1,"Update Namespaces and Address Config example","""The config transaction family setting address scheme needs to be updated to match the more complex method of converting a key to an address.    Suggest using the intkey family scheme as the simple example.    See the config transaction family spec for details on the scheme"""
"STL-327","New Feature","PoET",1,"Fix --enclave_module command-line argument in PoET CLI","""PoET CLI commands/sub-commands should not have underscores.  Specifically `–poet_enclave` should be `--poet-enclave`.  Fix PoET CLI as well as all scripts and documentation."""
"STL-335","Improvement","Documentation|sawtooth-core",2,"App Dev Guide: section ""Using Sawtooth with Docker""","""Complete """"Using Sawtooth with Docker"""" section with roughly the following TOC:    *Using Sawtooth with Docker*    """
"STL-334","Improvement","Documentation",1,"App Dev Guide: Introductory text for ""Installing and Running Sawtooth"" section","""The """"Installing and Running Sawtooth"""" section needs introductory text prior to the sections specific to docker and ubuntu.  This introductory text should describe what the section covers and the choices available to the reader."""
"STL-342","Improvement","sawtooth-core",2,"Refactor XO Python for improved documentation readability","""The current structure of XO python is very difficult to consume, from a learning perspective.  Refactor the transaction handler to  improve readability for a new sawtooth app developer."""
"STL-340","Task","Burrow EVM",3,"Research all the Ethereum/Burrow layers and decide where to ultimately position Sawtooth","""* Research how Solidity contracts are actually used by developers   * Research and identify all the layers between front-end developers, Solidity developers, Ethereum and Burrow, and the EVM   * Decide where it is best to position the Sawtooth Burrow-EVM integration"""
"STL-338","Improvement","Burrow EVM",2,"Handle Ethereum nonces correctly","""Currently they are ignored except when creating contract accounts. Below is a partial list of the additional work that needs to be done:   * Whenever a contract is called, the nonce of the account associated with the contract must be incremented.   * If a transaction submitted contains a nonce that does not match the (sender’s?) account, the transaction is invalid.   * Burrow stores nonces as an int64 but since the nonce is a monotonically increasing value starting at 0, it probably makes more sense for this to be a uint64. Currently a type cast is performed when translating between the transaction payload and the EVM call."""
"STL-379","Improvement","Go SDK",2,"Update Go SDK to pass error information in responses","""Update the Go SDK to take the current exception message and set the values on the TpProcessResponse message. This depends on STL-369's completion."""
"STL-378","Improvement","Javascript SDK",2,"Update JavaScript SDK to pass error information in responses","""Update the Python SDK to take the current exception message and set the values on the TpProcessResponse message. This depends on STL-369's completion."""
"STL-377","Improvement","Java SDK",1,"Update Java SDK to pass error information in responses","""Update the Java SDK to take the current exception message and set the values on the TpProcessResponse message. This depends on STL-369's completion."""
"STL-374","Improvement","sawtooth-core",2,"Enter System Admin's Guide TOC into Epic description","""System Administartor's Guide     -Recommended Set-up   -Diagrams & Descriptions   -Networking  -Deployment   -Upgrading   -SGX   -Installing from Repository    -Ansible    -Install files & locations   -Validator Key Definition   -Starting with System D   -Validator Genesis  -Log Configuration   -Overview   -Default   -Configuration  -Examples  -Using a Proxy Server to Authorize the REST API   -Forwarding URL Info with Headers   -Apache Proxy Setup Guide  -Monitoring & Troubleshooting  """
"STL-373","Improvement","Documentation",2,"Enter Architecture Doc stories into JIRA based on TOC","""Create stories for building content for the Architecture Document based upon the agreed-upon ToC."""
"STL-372","Improvement","Documentation",2,"Enter Architecture Doc TOC into Epic description","""Determine the key entries and structure of the Architecture Document ToC and add to the Architecture Documentation Epic as predecessor to subsequent stories to create content."""
"STL-371","Improvement","sawtooth-core",3,"Cache Invalid Transactions for Client Communication","""Cache invalid transactions to be reported back to the transaction submitter when requesting batch status."""
"STL-370","Improvement","Python SDK",2,"Update Python SDK to pass error information in responses","""Update the Python SDK to take the current exception messages and set the values on the TpProcessResponse message. This depends on STL-369's completion."""
"STL-369","Improvement","Validator Txn Execution",1,"Update TpProcessResponse to include Error message","""Add fields to the TpProcessResponse to include    * {{string message}}  * {{bytes data}}    where message is the string message of the exception, and data is extended exception information.  Both fields are optional."""
"STL-368","Improvement","Documentation",2,"Review App Dev Guide TOC","""Review the TOC for the App Dev Guide with the team and integrate any changes into the epic."""
"STL-367","Improvement","REST API|sawtooth-core",2,"Send Errors Back via REST API","""Currently, if a transaction fails, the REST API reports that the transaction is PENDING. This is misleading to the end-user.  This should return a response with more information:  """
"STL-366","Improvement","Documentation",2,"Enter App Dev Guide stories into JIRA based on TOC","""Based on the TOC described in STL-329, open up JIRA stories based on section.  Work with the team to figure out the right granularity of the JIRA stories."""
"STL-356","New Feature","Continuous Integration|PoET",3,"Create docker Container for Building PoET SGX Enclave","""A docker container needs to be created for building the PoET SGX enclave.  This includes automatic installation of all necessary required components (i.e., PoET SGX SDK, etc.)"""
"STL-354","New Feature","Packaging|PoET",1,"Update All PoET SGX Code License Headers","""All current PoET SGX enclave code has the standard Intel license text header.  The header needs to be updated to be open source friendly version of Intel license header."""
"STL-353","New Feature","Packaging|PoET",1,"Supply All Code/Licenses to Tom for Verification","""In order to get approval for releasing the PoET SGX code for open source, all licenses for libraries and tools used need to be submitted to IP/license verifiction system."""
"STL-352","New Feature","PoET",2,"Update PoET SGX Build to Use Existing Debian Packages","""The PoET SGX enclave code requires the json-c and cryptopp libraries as dependencies.  Currently the code is manually built and libraries and headers are copied to a defined directory.  There exist pre-built Debian packages for crypto++ and json-c.  Migrate the code and build for PoET SGX to use these libraries."""
"STL-350","New Feature","PoET",2,"Add PoET SGX Enclave Code to sawtooth-core Repository","""The PoET SGX enclave code currently lives in a separate repository.  It should be moved to the sawtooth-core repository under `consensus/poet` according to the repository layout specified in Google doc."""
"STL-387","Bug","Documentation",1,"[Documentation] Creating and Submitting Transactions - Unable to submit transactions","""Following: [https://intelledger.github.io/app_developers_guide/intro_to_sawtooth.html#creating-and-submitting-transactions] on my Ubuntu 16.04.02 install, I do not know how to submit transactions using intkey load.    *tkuhrt@sawtooth*:*~*$ intkey load -f batches.intkey    [15:32:46 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:46 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    [15:32:47 WARNING load] Unable to connect to """"http://localhost:8080"""": make sure URL is correct    batches: 1001 batch/sec: 3262.4823347214206    I tried using -U with the following addresses as obtained from previous steps, but I still did not see the transactions loading:    -U tcp://127.0.0.1:40000 (currently running transaction processor)    -U tcp://127.0.0.1:8800 (currently running validator)    Which address should I send to?  Regardless of address specified, nothing happened on the validator window."""
"STL-386","Bug","Documentation",1,"[Documentation] App Developer Guide - Unable to create keys to run validator","""Using [https://intelledger.github.io/app_developers_guide/intro_to_sawtooth.html#start-validator,] I was unable to create keys due to incorrect path specified.  In addition, after Talking with Shawn Amudson, it seems that the wrong command was specified.    {{*tkuhrt@sawtooth*:*~*$ sawtooth keygen --key-dir /home/ubuntu/sawtooth/keys/ validator}}    {{Error: no such directory: /home/ubuntu/sawtooth/keys/}}         {{*tkuhrt@sawtooth*:*~*$ mkdir -p /home/tkuhrt/sawtooth/keys}}    {{*tkuhrt@sawtooth*:*~*$ sawtooth keygen --key-dir /home/tkuhrt/sawtooth/keys validator}}    {{writing file: /home/tkuhrt/sawtooth/keys/validator.priv}}    {{writing file: /home/tkuhrt/sawtooth/keys/validator.pub}}    However, after talking with Shawn, he recommended that I use the following command:    {{*tkuhrt@sawtooth*:*~*$ sudo sawtooth admin keygen}}    {{[sudo] password for tkuhrt:}}    {{writing file: /etc/sawtooth/keys/validator.priv}}    {{writing file: /etc/sawtooth/keys/validator.pub}}"""
"STL-385","Bug","Documentation",1,"[Documentation] Environment Setup - Sudo required","""Following [https://intelledger.github.io/app_developers_guide/intro_to_sawtooth.html#create-genesis-block] after setting up my environment on Ubuntu 16.04.02 using [https://intelledger.github.io/app_developers_guide/environment_setup.html#env-ubuntu,] I received the following error because I did not use sudo, which was not mentioned as being required on the page.    {{*tkuhrt@sawtooth*:*~*$ sawtooth admin genesis}}    {{Generating /var/lib/sawtooth/genesis.batch}}    {{Traceback (most recent call last):}}    {{  File """"/usr/lib/python3/dist-packages/sawtooth_cli/main.py"""", line 150, in main_wrapper}}{{    main()}}    {{  File """"/usr/lib/python3/dist-packages/sawtooth_cli/main.py"""", line 126, in main}}{{    do_admin(args)}}    {{  File """"/usr/lib/python3/dist-packages/sawtooth_cli/admin.py"""", line 25, in do_admin}}{{    do_genesis(args)}}    {{  File """"/usr/lib/python3/dist-packages/sawtooth_cli/admin_command/genesis.py"""", line 75, in do_genesis}}{{    with open(genesis_file, 'wb') as out_file:}}    {{PermissionError: [Errno 13] Permission denied: '/var/lib/sawtooth/genesis.batch'}}         {{*tkuhrt@sawtooth*:*~*$ ls -la /var/lib/sawtooth/}}    {{total 8}}    {{drwxr-xr-x  2 sawtooth sawtooth 4096 May 16 20:25 *.*}}    {{drwxr-xr-x 41 root     root     4096 Jun  5 14:37 *..*}}         {{*tkuhrt@sawtooth*:*~*$ sudo sawtooth admin genesis}}    {{Generating /var/lib/sawtooth/genesis.batch}}"""
"STL-384","Bug","Documentation",1,"[Documentation] Environment Setup - Problem adding to /etc/apt/sources.list","""Following the steps at [https://intelledger.github.io/app_developers_guide/environment_setup.html#env-ubuntu] to setup my newly created Ubuntu 16.04.02 server, I ran into the following issue:    {{*tkuhrt@sawtooth*:*~*$ sudo echo """"deb http://repo.sawtooth.me/ubuntu/0.8/stable xenial universe"""" >> /etc/apt/sources.list}}    {{-bash: /etc/apt/sources.list: Permission denied}}    Instead, I had to manually edit /etc/apt/sources.list     """
"STL-394","Bug","PoET",2,"Validator Registry does not remove Validator ID on re-enrollment","""consensus.poet.families.sawtooth_validator_registry.validator_registry.processor.handler._update_validator_state(...)    Manages structures associated with validator admission.    On re-enrolling an existing anti-sybil ID (EPID), with a new Validator ID (OPK), the previous Validator ID will not be expunged from state, and the \{anti-sybil ID: Validator ID} map will not be updated appropriately.        Correct the method to remove the old Validator ID record from state and update the \{anti-sybil ID: Validator ID} map to include the new Validator ID."""
"STL-393","New Feature","Documentation|PoET",2,"Publish PoET Specification","""Revise format / content.  Correct errors like zTest algorithm (one value is initialized and never gets updated)."""
"STL-396","New Feature","Private Transaction",3,"Write Spec for Private UTXO Transaction Processor. ","""Create a Sawtooth style Spec based onthe private UTXO transaction document. """
"STL-403","Bug","Validator Global State",2,"Update context manager to enforce address characteristics","""The context manager should enforce that gets and sets only use valid addresses: those having 70 lowercase hex characters."""
"STL-446","Improvement","sawtooth-core",2,"Determine all the validator stat metrics to collect","""The output of this work should be a list of metrics the validator could/should log.  Descriptions of the metrics can be brief.  It does not need to include OS metrics, as those are collected with telegraph outside of the validator process.    This list does not need to be 100% complete - we can add metrics later.  However, it does need to be close enough to inform the design of the stats/metrics library; in particular, we need to know the different types of metrics.  For example, we know some metrics are additive (such as number of blocks published) and some might be set (current length of chain), while others might be an average (txn rate)."""
"STL-444","Improvement","sawtooth-core",3,"Set-up LR30-SGX","""Set-up 5 machines."""
"STL-438","Improvement","sawtooth-core",1,"Remove Javascript example from “Development without an SDK”","""* Remove template, replace with a simple RST   * Evaluate whether the text needs to be tweaked accordingly"""
"STL-423","Improvement","sawtooth-core",2,"Create section for Introduction to the XO Transaction Family","""Create new or reuse material to complete the following sections:       - What is Xo (to inform users that have not heard of the game)        - How to play Xo (How to use the client)        - Xo Transaction Family Specification (include link to existing spec)"""
"STL-460","Improvement","Python SDK|sawtooth-core",3,"Python SDK: Add signing/encoder client functionality","""The client features of Python SDK should be updated to match the functionality of the JavaScript SDK. Specifically this means adding a module for creating/encoding Batches and Transactions, as well as a signing module which can sign, verify, generate private keys, and generate public keys. Decisions must made about whether or not the root level signing module already satisfies those requirements, or should be replaced by this SDK.         Add any appropriate unit tests for the new functionality."""
"STL-459","Improvement","Burrow EVM",3,"Implement Seth transaction receipts","""Create Seth transaction receipts"""
"STL-455","Improvement","Burrow EVM",5,"Design the Burrow-EVM JSON-RPC","""Design the Burrow-EVM JSON-RPC server so that it:   # Is concurrent and handles requests efficiently   # Can be easily extended with new methods, as we will most likely support multiple JSON-RPC specs as projects diverge   # Reuses existing libraries (see https://github.com/tendermint/go-rpc)"""
"STL-454","Improvement","Burrow EVM",2,"Add receipt lookup to seth","""Add features to seth to:   # Lookup a transaction receipt   # When waiting for a submitted transaction to commit, print its receipt"""
"STL-453","Improvement","Burrow EVM",1,"Update txn inputs/outputs to enable contract chaining","""Currently, contract chaining is not possible because only the address of the sender account and receiver account are passed as inputs and outputs when submitting a transaction.    To support contract chaining, it should be made possible to pass the burrow-EVM namespace prefix as inputs and output when submitting a transaction.    The inputs/outputs are set here:    [https://github.com/hyperledger/sawtooth-core/blob/master/families/burrow_evm/src/sawtooth_burrow_evm/client/client.go#L60]    [https://github.com/hyperledger/sawtooth-core/blob/master/families/burrow_evm/src/sawtooth_burrow_evm/client/client.go#L106]     """
"STL-452","Improvement","Burrow EVM",3,"Implement logging event handler within Burrow-EVM","""A set of LOG instructions are defined by the EVM which can be used to place information into a separate “substate”. It is intended that these logs be stored in a manner that is easily indexed and searched. Smart contract languages have adopted the convention of using these LOG instructions to set up event pub/sub patterns.    The LOG opcodes in the burrow-evm rely on an event-handling system that is part of the burrow project. This will likely need to be preserved in order to use the EVM as it exists in the burrow project. This event handling should be preserved if possible and used to forward events to the Sawtooth event system."""
"STL-451","Improvement","Burrow EVM",0,"Implement stored EVM return values","""Duplicate of STL-459"""
