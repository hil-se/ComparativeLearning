"issuekey","type","components","storypoint","title","description_text"
"XD-10","Story","Runtime",5,"Reactor based http ingestion","""When there is support for boostrapping a http server in the reactor project, and inbound SI adapter and associated XD source module should be created."""
"XD-7","Story","Analytics",1,"Tuple data structure","""The tuple data structure should be backward compatible in functionality for use in spring batch.  Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility."""
"XD-24","Story","Ingest",2,"Create pipes and filters DSL for ingestion","""Initial simple handcoded implementation for straight through pipe and filter model, e.g. a | b | c"""
"XD-22","Technical task","Runtime",0,"Create Module base abstractions","""A module groups together a collection of spring configuration files."""
"XD-33","Technical task","Runtime",0,"Implement LocalChannelRegistry","""This should be usable within a single JVM process. Lives within shared application context of the process. """
"XD-32","Technical task","Runtime",0,"Create base Channel Registry abstraction","""Define the ChannelRegistry interface. """
"XD-31","Story","Analytics",5,"Create field-value counters","""A field-value counter is useful for bar chart graphs, Strings on x-axis and count on y-axis.  Maps well to zset in redis. Implementations for in-memory and redis."""
"XD-30","Story","Analytics",1,"Create a simple counter service","""A simple counters can increment/decrement a number.  Implementations for in-memory and redis."""
"XD-29","Story","Analytics",5,"Create rich gauge service","""A rich gauge stores a number and also rmd, min, max. Implementations for in-memory and redis."""
"XD-28","Story","Analytics",1,"Create simple gague service","""A gauge just stores a number.  Implementations for in-memory and redis."""
"XD-37","Story","Packaging",2,"Gradle based multi-project build","""multi project build. - look to Spring Framework for source of starting point."""
"XD-45","Story","Analytics",1,"Remove the expiry of keys in Redis based repositories","""There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and/or base class."""
"XD-44","Story","Analytics",1,"Redis based repositories should use a NamingStrategy class to calculate the name of the key to use for persistence","""RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place.  One such duplication is the determination of the key name to use for persistence.  This should be abstracted out into a strategy helper class."""
"XD-43","Story","Analytics",5,"Metric repositories should support Spring Data CrudRepository interface","""This provides common CRUD behavior and a shared interface that can be useful in testing scenarios.  """
"XD-50","Story","Analytics|Runtime",2,"Add tap support to DIRT","""syntax:   """
"XD-56","Story","Analytics",2,"Switch to use Lettuce driver for Redis","""Replace the use of Jedis with Lettuce as it has higher performance"""
"XD-55","Story","Analytics",3,"SI ServiceActivator for an XD Metrics backed Field Value Counter","""A Spring Integration based @ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support."""
"XD-54","Story","Analytics",1,"XD Metrics backed Message Counter","""A Spring Integration based @ServiceActivator that counts the number of messages using the Spring XD metrics support"""
"XD-53","Story","Runtime",13,"Design and document desired high level DSL for configuring data processing in XD","""Start to explore how the DSL can cover both advanced (non-linear) spring integration flows as well as spring batch jobs."""
"XD-58","Bug","Packaging",1,"build.gradle doesn't handle a small handful of libraries","""Trying to build spring-xd for the first resulted in lots of errors inside STS (I had an empty .m2 repo)."""
"XD-59","Story","Analytics",1,"Tuple should support storing nested tuples","""Nested tuple structures shoudl be supported,  getTuple(int index), getTuple(String name)"""
"XD-60","Story","Analytics",1,"Saving a metric (Counter, Gauge..) with an existing name should throw an exception","""The difference between saving a new metric and updating an existing one needs to be defined.  Suggest that if we try to save when an existing counter is already in the database to throw exception, such as DataIntegrityViolationException."""
"XD-65","Story","Ingest",2,"Gemfire Sink to update a gemfire cache.","""Update a gemfire region."""
"XD-62","Story","Batch",5,"Use the tuple data structure to process data in a spring batch step ","""Do not require a POJO in order to do end-to-end processing in a batch step."""
"XD-61","Story","Packaging",8,"Create distributable artifact that contains server application and start/stop scripts","""The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat/jetty etc.  Now there is a launch task  task(launch, dependsOn: 'classes', type: JavaExec) {   main = 'org.springframework.xd.dirt.stream.StreamServer'   classpath = sourceSets.test.runtimeClasspath  }   The same main should be referenced in the application plugin, a task to create a .zip distributable is needed.  Ideally would be nice to  1. download .zip 2. unzip 3. cd spring-xd/bin 4. xdserver start   and gracefully shutdown later with   5. xdserver stop  I don't know if we can/should bundle redis, I think we should bundle it.  The scripts can be for unix/linux and for windows.    Discuss a brew based install as well. """
"XD-67","Story","Packaging",8,"Submit a brew-based install for Spring XD","""- Host the Spring XD distributable zip somewhere that is accessible by external http request. - Create brew formula for Spring XD install while specifying redis as dependency.  - starting up stream server upon successful brew install  couple of questions: - should we name the brew task springxd? (name not taken yet) - should we start the stream server as part of the brew install process? - should we specify redis as a recommended dependency? user can pass in 'brew install springxd --without-redis' to skip redis installation. by default, 'brew install springxd' will install redis as well."""
"XD-69","Story","Export",5,"Export of data from HDFS to MongoDB","""Based on a single process Spring Batch job, ETL of data from HDFS to MongoDB."""
"XD-68","Story","Export",8,"Export of data from HDFS to a relational database","""Based on a single process running a Spring Batch job, support the ETL of data from HDFS to a RDBMS"""
"XD-72","Story","Ingest",5,"Provide a http source","""stream should be able to ingest data from http """
"XD-71","Story","Ingest",1,"Remove UUID from Tuple class or replace with more efficient implementation","""The Java UUID class is known not to be the fasted implementation available.   See https://github.com/stephenc/eaio-uuid and http://mvnrepository.com/artifact/com.eaio.uuid/uuid for high perf impls.  """
"XD-100","Story","Analytics",2,"Rename Tuple class in spring-xd-tuple","""The Tuple classes in Reactor follow the more traditional data structure concept of Tuples, an immutable fixed length sequence of values where each value can have different types.  They are ordered and can often be access by index.  An example in a static language is the Tuple class found in .NET http://msdn.microsoft.com/en-us/library/system.tuple.aspx or in Scala http://www.tutorialspoint.com/scala/scala_tuples.htm  Using this standard definition of a Tuple, they do not support named values.  There is also a different tuple class instance for each length, e.g. Tuple<T1,T2>, Tuple<T1,T2,T3>.  The Tuple class in XD is more like a record or named tuple.   Python has a named tuple concept - http://docs.python.org/2/library/collections.html#collections.namedtuple  and http://stackoverflow.com/questions/1490413/languages-that-allow-named-tuples shows that other languages use the term 'Record' for a 'named tuple' - Haskell, Standard ML, OCaml, and F#.  http://en.wikibooks.org/wiki/F_Sharp_Programming/Tuples_and_Records#Defining_Records  So boiling it all down, to avoid conflicts of names, and also to open up the possibility of using Reactor tuples as keys (instead of strings for names), we should change the name to either NamedTuple or Record.  ATM, there is no direct relationship between Reactor's Tuple and NamedTuple (such as inheritance) and so probably Record is the way to go.   """
"XD-103","Story","Packaging",1,"Create XDAdmin server to start container launcher","""This will launch the RedisContainerLauncher, in future will be able to select from a variety of middleware options."""
"XD-102","Story","Packaging",1,"Create XDContainer class to start stream server","""Provide optional command line arg to embed the container launcher, aka - xd-admin server.    XDContainer.sh --embeddAdmin"""
"XD-119","Story","Hadoop",1,"HDFS sink should default to hdfs://localhost:8020","""The current default is hdfs://localhost:9000 but most new distributions/installs use 8020"""
"XD-117","Story","Runtime",1,"add spring-integration-groovy to container dependencies","""This will enable the use of groovy scripts within modules."""
"XD-114","Technical task","Packaging",2,"Add install script for Redis","""This assumes the redis source tar is available under $rootDir/redis/redis-2.6.13.tar.gz  The install script does the following:  - Check the platform OS & arch - unzip the tar, compile the sources"""
"XD-111","Story","Packaging",5,"Create final distribution zip across multiple projects","""The final directory structure should look like  <install-dir>/xd <install-dir>/redis <install-dir>/gemfire  inside the XD directory   /xd/bin - which has xd-container and xd-admin scripts /xd/lib  inside the gemfire directory /gemfire/bin - has the gemfire-server script /gemfire/lib   inside the redis directory is   /redis/redis-latest-v.x.y.z.tar /redis/README /readis/install-redis  - script that does the basic 4 commands to install redis.   There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and 'redis-binary' directories and creates the final layout for the distribution."""
"XD-108","Story","Packaging",1,"Build script should not package 'spring-xd-dirt' scripts ","""We are packaging separate scripts to start XDAdmin and XDContainer.  The Gradle application plugin will generate an unwanted 'spring-xd-dirt' scripts, this should be removed from the bin directory when creating a distribution zip."""
"XD-106","Story","Runtime",1,"Container server does not log a message that it has started or stopped successfully","""$ ./xd-container  processing module 'Module [name=file, type=sink]' from group 'tailtest' with index: 1 processing module 'Module [name=tail, type=source]' from group 'tailtest' with index: 0   Logging of 'processing module' should have log level, time.."""
"XD-105","Story","Packaging",1,"Add LICENSE to be included in root directory of distribution","""should contain apache licence"""
"XD-104","Story","Packaging",1,"Add README to be included in root directory of distribution","""should explain basic layout of the distribution"""
"XD-123","Bug","Packaging",3,"XD scripts lib path needs to be dynamic","""We currently have the manually created XD scripts. This makes it difficult to maintain as the lib path is error prone with the changes. We need to make sure that the properties such as lib path etc., are dynamically updated."""
"XD-122","Bug","Packaging",2,"XD scripts need to have spring-integration milestone versions updated","""Spring-integration version is changed to 3.0.0.M2 and since we manually create the XD scripts, they still point to the 3.0.0.BUILD-SNAPSHOT version.  As discussed, we also need to have a better strategy on updating the lib directory inside the XD scripts."""
"XD-143","Story","Packaging",3,"Create externalized property file to support connectivity to redis","""We need to have an externalized property file(under xd/conf/) for the xd-container & admin scripts to use as options. """
"XD-142","Story","Runtime",8,"StreamServer Context Lifecycle Issues","""The {{ModuleDeployer}} calls {{getBeansOfType}} before the context has had its {{PropertySourcesPlaceholderConfigurer}} attached. This can cause issues with {{FactoryBean}} s with placeholders in constructor args because the unresolved placeholder is used when the {{FactoryBean}} is pre-instantiated to determine the type of object it will serve up."""
"XD-141","Story","Packaging",1,"install-redis script should not use relative path to determine redis source dist","""Currently, the install-redis script uses relative path to determine redis source  dist file. Since this is error prone, we need to fix it."""
"XD-140","Story","Ingest",2,"Parameterize syslog Source; Add Support for TCP","""The syslog source currently is hard-coded to use udp on port 11111.  Need to parameterize the port and provide an option to use TCP."""
"XD-139","Story","Documentation",2,"Update README.txt to include instructions on how to build","""Building XD should not be part of the out first out of the box experience, but we should include some instructions on what targets are available, such as distXD."""
"XD-136","Story","Documentation",3,"Documentation that points on how to install hadoop","""Pointers to other documentation on how to install hadoop. """
"XD-134","Story","Documentation",2,"Investigate link checking tool for user guide","""Asciidoc/doctor might have one as part of it toolchain"""
"XD-133","Story","Packaging",1,"Fail Sonar CI build if there are any package tangles violated.","""Similar to what would show up on structure101 reports."""
"XD-132","Story","Ingest",8,"Profile support for modules","""To allow for groups of beans to be defined or not in the container that runs a module.  When deploying a stream (e.g. via the REST API), it should be possible to also provide profile names. Then those would apply to any modules within that particular stream deployment."""
"XD-128","Story","Ingest",3,"Create TCP sink module","""Based off SI tcp inbound adapter.  This will allow for event fowarding."""
"XD-126","Story","Documentation",2,"Documentation for sources, sinks, modules should define which attributes are required and which optional","""This will eventually be supplied by the admin server, but for now write it up by hand in the documentation"""
"XD-124","Story","Ingest",2,"Clean shutdown of redis in xd-container","""Need to shutdown cleanly, no exception messages are shown.  Order of components in the stream should be shut down from 'first to last'  (opposite of creation)"""
"XD-147","Story","Packaging",2,"Remove use of application plugin for redis project","""Currently redis project uses application plugin to bundle distribution. This also includes 'java plugin' which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. """
"XD-146","Technical task","Runtime",0,"Change TCP Source/Sink to Use Profiles","""Currently, the TCP source/sink use specific beans for the serializer/deserializer options; when profiles are available, they should be used to avoid having to declare a bean of each type."""
"XD-159","Story","CLI|DSL",1,"Parameter parsing does not work if an argument contains '--'.","""Parameter parsing does not work if an argument contains '--'.  For example:    Also, I was surprised that this worked..    ... but this didn't...    I think we need to tokenize the argument (with ' if contains spaces) and remove any surrounding '...' from the result. This means if someone wants a SpEL literal they would have to use something like     resulting in a SpEL literal 'Hello, world!'"""
"XD-156","Story","Packaging",2,"Create config support for Redis","""We would like to have Redis driven from a config property file under XD_HOME."""
"XD-155","Story","Runtime",5,"Add a groovy script processor module","""A processor module that accepts either the location of a groovy script resource or an inline script (string). Also some discussion about a default classpath location for scripts. """
"XD-154","Story","Runtime",2,"Provided console output of started server","""Shouldn't we have something like a ContextRefreshedEvent Listener and output some informational messages to the console, so the user knows the Container is up (Which contexts. Maybe even print a link to the docs))? Maybe even some simple ascii art (for demos)? Right now it looks somewhat barren.  Redis provides something similar.  This may even go hand in hand to provided a better configuration model (storing common config parameters centrally)   """
"XD-153","Story","Analytics",5,"create a gauge module","""Spring config for simple gauge plus message handler to process message. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse."""
"XD-152","Story","Analytics",5,"Create rich gauge module","""Spring config for rich gauge plus message handler to coerce a numeric or string payload to a double."""
"XD-151","Story","Packaging",2,"Add Redis binaries for Windows","""Presently, Spring XD does not ship Windows binaries for Redis. However, Microsoft is actively working [1] on supporting Redis on Windows. You can download Windows Redis binaries from:  https://github.com/MSOpenTech/redis/tree/2.6/bin/release   [1] http://blogs.msdn.com/b/interoperability/archive/2013/04/22/redis-on-windows-stable-and-reliable.aspx"""
"XD-150","Story","Packaging",2,"Publish Spring XD final distribution zip as part of Bamboo artifactory plugin","""Currently, Bamboo's gradle artifactory plugin has the artifacts configured to projects target(build) directory 'archives'. We need to have a way to set the final distribution archive as one of the gradle 'configurations' in our build.gradle and refer it inside bamboo artifacts."""
"XD-166","Story","Packaging",3,"Create config support based on channel registry type","""We need to have the XD container & admin reading the registry specific property based on the registry type selected.   From Mark F, on one of the code review comments:  Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry (""""redis"""", """"rabbit"""", or """"local"""" being possible values), and then we could use something like:  <import resource=""""config/${registry.type}.xml""""/>  <context:property-placeholder location=""""config/${registry.type}.properties""""/>"""
"XD-164","Story","Runtime",2,"Validate processing modules declare the required channels","""Validate that modules have required channels declared according to their type.  Currently the stream deployer accepts processors with no input, but the stream doesn't complete. We should fail earlier and more loudly."""
"XD-187","Story","Packaging",2,"Create XD script for xd-single node","""This script will launch XD admin along with the module container.  As part of this implementation, we will also remove the embedded options for XD admin & container scripts."""
"XD-186","Story","Runtime",2,"Create a pipe protocol independent StreamDeployer","""Create StreamDeployer that does not depend on an adapter implementation"""
"XD-185","Story","Runtime",2,"Refactor StreamServer to an interface and create Redis and Local implementations","""The current StreamServer depends on RedisStreamDeployer. Call this RedisStreamServer and extract interface to allow alternate implementations"""
"XD-182","Story","Packaging",3,"Create redisProtocol.xml that will load all the Redis specific implementations to suppor the XD container runtime and administration","""The redis specific beans that are defined in the current launcher.xml should move into this configuration file.   """
"XD-181","Story","Packaging",3,"Update launcher.xml to have protocol independent beans defined and an import statement to load protocol specific defintiions from a system property defined location.","""launcher.xml can make use of the system property xd.pipeProtocol inside an import statement.  This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers.   File name conventions should be used, so if the option passed in from the command line is --pipeProtocol localChannel  then the XML filename looked for has the 'Protocol' suffix applied, e.g. localChannelProtocol, and is loaded via the classpath.  Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes (an advanced task).  """
"XD-180","Story","Packaging",1,"The command line for xd-admin and xd-container to support an additional option, pipeProtocol, that is used to determine the middleware for sending admin requests and data between processing steps","""The name 'pipeProtocol' is tentative.    1. The command line scripts for xd-admin and xd-container would support a --pipeProtocol option, with the default being to use Redis.  (Otherwise use xd-singlenode). 2. The xd-admin and xd-container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app. """
"XD-179","Story","Packaging",1,"Have three startup scripts, xd-singlenode, xd-admin, and xd-container","""The xd-singlenode script will launch a main application that creates both the admin node (to process http admin requests) and the container node (to execute modules for data processing) within in the same process   the xd-admin script will launch a main application that creates only the admin node (remove current embeddedContainer options)  the xd-container script will launch a main application that creates only the container node (as it is now)"""
"XD-178","Story","Runtime",1,"DefaultContainer should have a default constructor that generates a UUID","""The current incrementAndGet approach based off redis will not easily be applicable in local model deployment"""
"XD-176","Story","Analytics",2,"Support exponential moving average in RichGauge ","""This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter """"alpha"""" to the gauge data (https://en.wikipedia.org/wiki/Exponential_moving_average). If not set it would default to the current behaviour (simple mean), otherwise it would calculate the exponential moving average in place of the mean."""
"XD-170","Story","Documentation",2,"Home wiki page improvements","""Add more structure, more easily find the reference guide.  The style that is here  https://github.com/snowplow/snowplow/wiki is nice. """
"XD-192","Story","Documentation",2,"Update getting started documentation to use xd-singlenode start script.","""With the new option of starting without requiring redis, the getting started documentation should reflect this easier way to start processing data."""
"XD-190","Improvement","Runtime",1,"Cleanup embedded container story","""The --embeddedX options are a bit confusing in code right now, as the Admin can embed the Container and vice-versa. I guess we should only keep the Admin>Container side of things."""
"XD-193","Story","Packaging",3,"Need more unique resource locations for XD internal configuration","""Currently internal config files are in META-INF/spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META-INF/spring/xd"""
"XD-210","Story","Runtime",1,"If output directory does not exist for a file sink, by default allow it to be created","""There shouldn't be a need to do a mkdir -p before sending data to a file sink."""
"XD-206","Bug","Packaging",1,"XD AdminMain & ContainerMain should check xd.home property from scripts","""Currently, the system property xd.home is set as JVM_OPTS (via SPRING_XD_ADMIN_OPTS) into xd-admin & xd-container scripts.  Inside the ContainerMain & AdminMain, we need to check if this system property is set and use it. It seems like, this check is missing now."""
"XD-201","Bug","Packaging",2,"Fix XD scripts on windows","""Currently the XD scripts are broken in windows. """
"XD-198","Improvement","Documentation|Runtime",1,"Documentation for developing streams in the IDE needs to mention including scripts dir to project classpath","""{{curl -X POST -d """"time --interval=3 | transform | log"""" http://localhost:8080/streams/test}}  results in the following stack trace in the DEBUG log. It's apparently benign, but ugly...    """
"XD-214","Story","Documentation",3,"Create documentation on the general DSL syntax","""The asciidoc wiki should have a section (included in the _Sidebar.asciidoc as well) that describes the general usage of the DSL syntax."""
"XD-212","Improvement","Runtime",2,"Add http port command line option to AdminMain","""Currently StreamServer has setPort, but no way for end user to set it. """
"XD-215","Story","Documentation",1,"Add authentication information to twittersearch source doc","""Since the changes for XD-202, twittersearch requires authentication. Need to update the docs to reflect this."""
"XD-222","Story","Documentation",1,"Add docs for Deleting a simple stream.","""curl -X DELETE http://localhost:8080/streams/ticktock"""
"XD-221","Story","Documentation",2,"Links in asciidoctor generated HTML+docbook documentation are broken","""The issue arises because the link:document[Label] asciidoc macro is meant for """"external documents"""" and creates {{<ulink>}} in docbook / {{<a href=""""document"""">}} in html, whereas we want {{<link linkend=""""anchor"""">}} / {{<a href=""""doc#anchor>}} resp. We also want it to continue working in github live view.  I guess what could work is to have the macro (either override the link macro or create our own if github supports that) that looks like : {{link:document#anchor[Label]}}  (the #anchor works out of the box in asciidoc and should work in github) but override it for the html and docbook backends to render to the correct form.  The thing is, there are several ways to create/override macros (and templates they render to), some of which make sense to our setup: - having asciidoc.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27) - having docbook.conf/html.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27) - defining macros using attributes (http://asciidoc.org/userguide.html#_setting_configuration_entries)  I tried all of those, but to no avail. These DO WORK with plain asciidoc, but not with our toolchain. Don't know if the problem is with asciidocTOR or with the gradle wrapper though.    """
"XD-226","Story","Packaging",2,"Cleanup and Optimize gradle tasks to bundle spring-xd distribution","""We need to cleanup some of the duplicate gradle tasks that bundle spring-xd distributions.   Currently, distXD does the copy of distributions from """"spring-xd-dirt"""", """"redis"""" and """"spring-xd-gemfire-server"""" projects into """"$rootDir/dist/spring-xd"""".  And, the task """"zipXD"""" makes the zip archive.  These tasks should be combined with the """"distZip"""" & """"docZip"""" tasks.  We also need to remove the duplicate artifacts configuration from these tasks."""
"XD-227","Story","Packaging",1,"Add jetty-util-6.1.26.jar and jsr311-api-1.1.1.jar as required jars so they will be on the XD classpath","""This is needed for the use of the webhdfs:// scheme to talk to HDFS over http."""
"XD-228","Bug","Documentation",1,"Missing '=' in example of http stream","""In documentation attached to M1, in Streams/Introduction section, there's  while it should be:  missing """"{{=}}"""" in {{http}}"""
"XD-245","Story","Packaging",8,"Deploy Batch Jobs on XD","""h2. Narrative As a developer, I need a way to deploy job configurations as well as the related custom code to XD.  h2.  Acceptance Criteria # Provide the ability to register jobs that have been deployed as modules via something like {{curl -d """"job"""" http://localhost:8080/streams/myJob}} where job is the name of the job definition located in /modules/job and myJob is the name of the resulting registered job # Confirm that both """"regular"""" jobs and Spring Hadoop based jobs can be packaged/run."""
"XD-244","Story","Batch",8,"Create a Trigger","""h2. Narrative As the XD system, I need to be able to execute a job (or potentially a stream) based on a given condition (time, data existence, etc).  This story is intended is for a local trigger implementation but remote triggers will also need to exist.  h2.  Acceptance Criteria # Implement the ability to register a time based trigger {{trigger <CRON_STRING>}} for example # Implement the ability to register a file existence based trigger {{trigger <PATH>}} for example # Implement the ability to execute a job via an anonymous trigger: {{job_name @ <CRON_STRING OR PATH>}} # Implement the ability to execute a job via a job via the previously registered trigger: {{job_name @ trigger_name}} """
"XD-236","Story","Analytics",8,"Create an Aggregate Counter","""An aggregate counter rolls up counts into discrete time buckets.  There is an existing POC implementation in Java based off the library https://github.com/thheller/timed-counter   The README there has a good description of the desired feature set."""
"XD-257","Story","CLI",3,"Create the base implementation for XDCommands for the shell","""This is the basic setup of the commands file - no specific command implementations"""
"XD-255","Story","CLI",3,"Set up a project for XD Shell","""Set up a basic Spring Shell project for XD Shell"""
"XD-248","Story","CLI",0,"Provide a Spring Shell implementation for XD","""Need to create a basic Spring Shell implementation to provide easier access to the XD REST API via an XD REST API Client library. """
"XD-247","Story","Runtime",2,"Need to be able to specify password for Redis","""Running on Cloud Foundry (and other managed environments) we need to be able to specify a Redis password in addition to host and port."""
"XD-291","Bug","Ingest",1,"HTTP Source still listens on port 9000 after removal.","""Steps to reproduce:  1.  curl -d """"http | log"""" http://localhost:8080/streams/testHttp  2.  curl -X DELETE http://localhost:8080/streams/testHttp  3.  curl -d """"http | log"""" http://localhost:8080/streams/testHttp  org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:9000"""
"XD-290","Story","Ingest",2,"Redis backed container's RedisQueueInboundChannelAdapter is not performant","""Currently, the RedisQueueInboundChannelAdapter has blocking operation when pulling the messages out of redis queue and this is not performant.   There are few ideas from the discussion to make it better:  1) Get more items from the redis queue per connection 2) We will also have compression of messages(at the channel registry) before being sent to the redis queue   We also need to investigate what redis connection strategy makes the RedisQueueInboundAdapter better. """
"XD-288","Story","Hadoop",0,"Support writing to HDFS using Thrift","""See https://github.com/kevinweil/elephant-bird"""
"XD-287","Story","Hadoop",0,"Support writing to HDFS using Protocol Buffers","""See https://github.com/kevinweil/elephant-bird"""
"XD-285","Story","Hadoop",0,"Provide a strategy interface to obtain the key used when writing SequenceFiles ","""the key used in writing key-value pairs should be able to be specified declaratively."""
"XD-281","Improvement","Ingest",0,"Support writing to HDFS using a custom codec","""The classname for the codec would be used to instantiate it.  Note, the ReflectionUtils or CompressionCodeFactory should be used to be efficient."""
"XD-280","Improvement","Ingest",0,"Support writing to HDFS using the Snappy codec","""snappy codec can be included in the distribution.  for more info http://blog.cloudera.com/blog/2011/09/snappy-and-hadoop/  Depends on using a file container format such as sequence or avro files."""
"XD-279","Improvement","Ingest",0,"Support writing to HDFS text file using the LZO codec","""note, the LZO codes are GPL-licensed, so can't be included in the distribution. It is splittable, which makes it a good candidate for writing without any additional data file container structure such as sequence or avro files."""
"XD-277","Improvement","Ingest",0,"Support writing to HDFS text file using the BZip2Codec","""The BZip2 codec is splittable, making it a common choice."""
"XD-276","Story","Hadoop",0,"Investigate throughput performance writing to HDFS","""This could be an optimization, to be verified, that delegating the writing operations to Reactor (e.g. with a backing ringbuffer implementation) will increase the throughput performance.  Other strategies, such as threads to handle writes to individual files concurrently, should be investigated."""
"XD-275","Story","Hadoop",0,"Support for in-memory grouping/aggregation of data before writing to HDFS","""This should be an optimization, to be verified, that aggregating data in memory, for example at the size of a HDFS block (64Mb often) will result in increased performance vs. not aggregating data for writes."""
"XD-274","Story","Hadoop",0,"Headers in a Message that will indicate which HDFS file the data should be stored in.","""Based on message processing, a header in a Message can be added that contains the output file name.  This will work together with the hdfs writer module so it can read the header and write the contents of the message to the specified file. """
"XD-273","Story","Hadoop",0,"File name should support common date and time format strings","""The file name should allow the use of date and time patterns, either JDK or Joda (TBD)."""
"XD-272","Story","Hadoop",0,"A rotation file policy based on time","""A strategy that will automaticaly roll over files based time of day.  For example  New files will be created every hour, or every 6 hours etc.  The directory for files can also be rotated so that directory structures such as  /data/{year}/{month}/{day}  can easily be supported with a minimum of configuration.  """
"XD-271","Story","Hadoop",8,"The HDFS Sink should support a number of rollover options","""A strategy to roll over files that allows the user to choose between  1) the size of the file 2) the number of events/items in the file 3) an idle timeout value that if exceeded, will close the file"""
"XD-270","Story","Hadoop",8,"The HDFS Sink should support a file naming strategy to distinguish between file currently being written and completed files","""A file that is in the process of being written to should have a customized suffix added to the name, e.g. 'temp'.  Once the file is closed, the suffix is removed and replaced with another value - default value can be dependent on the serialization format used, but can be customized"""
"XD-296","Story","Packaging",1,"Add log config file to gemfire in final distro","""The changes for XD-144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn't."""
"XD-295","Bug","Runtime",1,"redis.properties values ignored","""The container application loads {{redis.properties}}, but for some reason the values are ignored, and defaults are used instead.  Repro steps: # Unpack Spring XD 1.0.0.M1 to a machine with no running Redis instance # Change /xd/config/redis.properties to specify a different hostname # Run /xd/bin/xd-container # Observe error about inability to connect to Redis on localhost  Workaround * Pass -Dredis.hostname={desired IP} as a JVM parameter"""
"XD-306","Story","Testing",8,"User wants ability to test sources","""Examples: 1. Be able to start the rabbitmq source just by pointing to modules/source/rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in-memory queue backed channel for use with assertions to verify functionality.   2. Test for as many source types as is 'reasonable', e.g. MQTT/TCP testing might be harder than say rabbitmq. 3. Test that sending json, results in media-type header is set to json 4. Test that sending POJO,   """"  POJO 5. Test that sending Tuple, """"   Tuple 6. Test that sending raw bytes, """" raw bytes """
"XD-305","Story","Testing",8,"User wants ability to test sinks","""Handled by 1245"""
"XD-304","Story","Testing",8,"User wants ability to test processors","""Be able to point to the processor xml file, e.g. modules/processors/transformer.xml, and have access to a source channel that drives messages into the processor and a output channel where output messages are send.  The outbound channel is queue backed.  Test sending JSON to a processor module that uses Tuples. """
"XD-303","Story","Testing",8,"User wants ability to create a in-process sink or tap","""So that we can validate the message content in the stream"""
"XD-302","Story","Runtime",8,"User wants ability to create a mock source","""To send a pre-set message to process(es)"""
"XD-316","Story","Batch",5,"Create a common exception framework for XD","""Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions.  An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. """
"XD-315","Story","CLI",3,"Package Shell ""binary"" next to xd-admin and xd-container","""The shell should be an 'executable' delivered out of the box in much the same way that xd-container and xd-admin are right now. If we follow how redis/mongo distribut the shell, it sits side by side with the other binaries"""
"XD-340","Story","Hadoop",8,"Create script to extract table data from JSON based on a given HAWQ table structure","""We should be able to write a script that can examine the table structure for a given HAWQ table and then extract the data from JSON without the custom script we are using now."""
"XD-342","Story","Packaging",3,"Fix classpath error caused by multiple conflicting servlet-api jars","""There is some conflicting Servlet API jars on the claspath that needs cleanup. Building and running with xd-singlenode script gave this error:  Jun 27, 2013 3:18:16 PM org.apache.coyote.http11.AbstractHttp11Processor process SEVERE: Error processing request java.lang.NoSuchMethodError: javax.servlet.ServletContext.getEffectiveSessionTrackingModes()Ljava/util/Set;  at org.apache.catalina.connector.CoyoteAdapter.postParseRequest(CoyoteAdapter.java:674)  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:402)  at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)  at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)  at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:310)  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)  at java.lang.Thread.run(Thread.java:680) """
"XD-341","Story","Documentation|Runtime",2,"Document JMX features","""Document jmx command line options and refer to jolokia"""
"XD-343","Story","Runtime",5,"Investigate JMX object naming of deployed modules and inbound/outbound channel adapters.","""The object naming is still not ideal for XD since SI conventions add some noise. Likely  need to design and implement a custom naming strategy"""
"XD-348","Story","Runtime",1,"Trigger - Add support for fixed-delay interval","""Trigger - Add support for fixed-delay interval"""
"XD-347","Story","Performance Testing",2,"Investigate Redis connection timeout issues when running performance test","""With the performance test run, the numbers (messages sent/received per second) keep varying as there are  """"redis client connection timeout exceptions"""" (Caused by: org.jboss.netty.channel.ConnectTimeoutException: connection timed out) at both redis inbound/outbound channel adapters as I increase the total number of messages being processed (max. 10K/second). Some of the exception messages for the review: 1) With connection pool (at Redis outbound): Caused by: org.springframework.data.redis.connection.PoolException: Could not get a resource from the pool; nested exception is com.lambdaworks.redis.RedisException: Unable to connect at org.springframework.data.redis.connection.lettuce.DefaultLettucePool.getResource(DefaultLettucePool.java:95) at org.springframework.data.redis.connection.lettuce.DefaultLettucePool.getResource(DefaultLettucePool.java:36) at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:318) at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:109) at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:81) at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:53) at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:157) at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:137) at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84) at org.springframework.data.redis.core.DefaultListOperations.leftPush(DefaultListOperations.java:71) at org.springframework.data.redis.core.DefaultBoundListOperations.leftPush(DefaultBoundListOperations.java:67) at org.springframework.xd.perftest.redis.outbound.RedisQOutboundChannelAdapter.handleMessageInternal(RedisQOutboundChannelAdapter.java:71) at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73) ... 17 more Caused by: com.lambdaworks.redis.RedisException: Unable to connect at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176) at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139) at org.springframework.data.redis.connection.lettuce.DefaultLettucePool$LettuceFactory.makeObject(DefaultLettucePool.java:252) at org.apache.commons.pool.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:1181) at org.springframework.data.redis.connection.lettuce.DefaultLettucePool.getResource(DefaultLettucePool.java:93) ... 29 more Caused by: org.jboss.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:6379 at org.jboss.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:137) at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:83) at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42) 2) Without connection pool (at Redis inbound): Caused by: com.lambdaworks.redis.RedisException: Unable to connect at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176) at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139) at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:321) ... 12 more Caused by: org.jboss.netty.channel.ConnectTimeoutException: connection timed out: localhost/127.0.0.1:6379 at org.jboss.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:137) at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:83) at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42) ... 3 more"""
"XD-368","Story","Analytics",2,"Improve connection handling in RedisAggregateCounterService.","""This is currently too chatty. It should be possible to use a single connection for each """"increment"""" operation."""
"XD-419","Story","Documentation",2,"Taps introduction section should show use of shell to create a real stream and a real tap using the shell","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#taps  The existing docs should be made to show a real stream being created with filter and/or transformer and then a tap that goes to logging.    The shell syntax to also stop/undeploy a tap should be shown here as well since the lifecycle is discussed."""
"XD-418","Story","Documentation",1,"Update Sink's GemFire section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire """
"XD-417","Story","Documentation",1,"Update Sink's TCP section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp_sinks"""
"XD-416","Story","Documentation",1,"Update Sink's HDFS section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#hdfs"""
"XD-415","Story","Documentation",1," Update Sink's File section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#file_sinks"""
"XD-414","Story","Documentation",1,"Update Sink's Log section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#log_sinks"""
"XD-413","Story","Documentation",1,"Update Processors Script section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#script"""
"XD-412","Story","Documentation",1,"Update Processors JSON Field Extractor section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-field-extractor"""
"XD-411","Story","Documentation",1,"Update Processors Transform section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#transform"""
"XD-410","Story","Documentation",1,"Update Processors Filter & JSon Filed Value Filter section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#filter http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-value-filter"""
"XD-409","Story","Documentation",1,"Update Sources TCP section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp"""
"XD-408","Story","Documentation",1,"Update Source Syslog section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#syslog"""
"XD-407","Story","Documentation",1,"Update Sources Gemfire CQ section to use Shell commands instead of curl ","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire-cq"""
"XD-406","Story","Documentation",1,"Update Sources twitter search section to use Shell commands instead of curl","""See   http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#twittersearch"""
"XD-405","Story","Documentation",1,"Update Sources tail section to use Shell commands instead of curl","""http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tail"""
"XD-404","Story","Documentation",3,"Update documentation section ""Running in Distributed Mode"" to show use of RabbitMQ in addition to Redis","""The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes.  This functionality is provided by the core ChannelRegistry abstraction.  A new intro paragraph shoul convey that it isn't a 'redis' only or 'rabbitmq' only system.  There should be """"Installing RabbitMQ"""" and """"Starting RabbitMQ"""" sections to match those for Redis.  """"Starting Spring XD in Distributed Mode"""" should cover how to configure the system to select to use Redis or Rabbit."""
"XD-403","Story","Documentation",1,"Update Sources section to use Shell commands instead of curl","""See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#http    """
"XD-401","Story","Documentation",1,"Create a shell command to post data to an http port for use with the http source module","""the current streams chapter  http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams  shows using curl to post some data to a http source module,   curl -d """"hello"""" http://localhost:9000  create a shell command so curl doesn't have to be used.  https://github.com/SpringSource/rest-shell  has a command already developed for this."""
"XD-400","Story","Documentation",1,"Update Streams Chapter to use shell commands instead of curl","""the current streams chapter  http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams  shows creation and deleting streams using CURL - switch to use shell.  Also add listing of a stream.  there is also an example of creating a stream, this should be replaced as well. """
"XD-399","Story","Documentation",1,"Update Getting Started chapter to include a section on starting the shell.","""The chapter on how to start up the shell should ocme right after """"start the runtime"""" and before """"create the stream"""""""
"XD-398","Story","Documentation",1,"Update Getting Started chapter to use Shell commands instead of curl","""See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#getting-started  """
"XD-397","Story","Documentation",2,"Document Monitoring & Management Features","""This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia.  in particular showing how some existing metrics for inbound message channel adapters or the 'inbound' channel of the stream, that indicate the number of messages processed per section.    """
"XD-396","Story","Documentation",1,"Add section to documentation that shows command line options available for each server","""This should likely be in the """"start the runtime"""" section of Getting Started section."""
"XD-434","Story","Runtime",5,"Consider removing the Topic/Queues when deleting the Stream","""As a user, I'd like to have the option to delete the queues/topics so that we can include an _optional_ attribute as part of the stream destroy command to also clean-up the associated queues/topics.  *Notes:* * Spring-AMQP {{RabbitAdmin}} now has a {{getQueueProperties()}} method which returns the number of consumers so it may be possible to use it for this purpose. * Consider the possibility of _producers_ and/or _queues_ still containing data * Consider the scenario even after the topics/queues are cleaned-up, what to do with fanout exchange?  *Some Further Thoughts* * Consider using the upcoming Spring AMQP REST API {{RabbitManagementTemplate}} if the timing is not right, we could temporarily invoke the rabbit REST API directly. * Should be optional; perhaps via {{stream destroy foo --clean}} * Should this be done by the admin? Or, via a new plugin handling module undeployments - in the rabbit case, undeploying a consumer would check for us being the last consumer and remove the queue/binding/exchange, since we undeploy left->right, everything can be cleaned up on the consumer side. * Third option would be new methods on the bus {{cleanConsumer}} etc invoked by the {{StreamPlugin}} * Down side of doing it on the admin is that he wouldn't necessarily know which rabbit cluster a stream was deployed to - so it probably has to happen on the container - even so, we'd need the admin url(s) for the cluster."""
"XD-433","Story","Runtime",5,"Homogenize Container Initialization Failures ","""If Redis is not running, the container fails to initialize in {{ContainerMain.launch()}} because the connection factory attempts to eagerly connect.  If RabbitMQ is not running, the container fails to initialize in {{AbstractContainerLauncher.launch()}}.  Make the failure behavior consistent from a user perspective and add a spring-retry {{RetryTemplate}} to retry container startup."""
"XD-432","Story","Runtime",0,"User wants to configure MessageBus","""XD-162 requires registering message converters with the ChannelRegistry. End user needs to configure this statically as the Spring configuration is not exposed."""
"XD-429","Bug","Documentation",1,"Document time source","""time source is used in some examples, but it isn't documented explicitly, eg. --interval option in seconds."""
"XD-445","Story","CLI|Documentation",1,"Add support to set the read timeout for http request","""We need to have the ability to set read timeout for http request.  This is already implemented here: https://github.com/SpringSource/rest-shell/"""
"XD-436","Story","Runtime",0,"Decouple transport from DIRT","""Currently spring-xd-dirt has direct dependencies on Redis and Rabbit. Consider moving transport dependent classes to separate jars with """"runtime"""" dependencies"""
"XD-449","Story","Stream Module",1,"The user needs the ability to set up a misfire policy for a Trigger","""2 options are: 1) Fire the trigger immediate - Launch the job when trigger can gather the resources necessary start the job 2) Do nothing - Ignore this job fire time and catch   this scenario can occur if XD is down or resources (threads) are not available at the time a job is to be launched. """
"XD-475","Improvement","Analytics",8,"Avro sink for HDFS","""We need a sink that can write data in Avro serialized format. This story is for investigating what we would need to do to support that. The Spring Integration Kafka adapter provides Avro support for Kafka."""
"XD-474","Story","Stream Module",8,"Create JSON to tab-delimited text transformer script","""We need a generic script that can do JSON to tab-delimited text transformation for data written to HDFS/HAWQ external tables. Users should be able to specify columns/fields to be included."""
"XD-473","Story","Runtime",5,"Modify startup script of xdadmin/xdcontainer to allow specifying hadoop distro to use","""we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with --hadoopDistro=phd1"""
"XD-472","Story","Packaging",8,"Add spring-xd-hadoop distro specific sub-projects","""we need to modify build adding two sub-projects for spring-xd-hadoop: one for hadoop 1.1.2 and one for phd1 (Pivotal HD) to pull in transitive dependencies for correct Hadoop distro"""
"XD-471","Story","Stream Module",8,"Batching JDBC channel adapter","""we need a batching JDBC channel adapter (int-jdbc:outbound-channel-adapter is not batching statements AFAICT) """
"XD-470","Story","Analytics",3,"Create JDBC sink","""we need a JDBC sink for writing to HAWQ (using int-jdbc:outbound-channel-adapter and postgresql JDBC driver) """
"XD-469","Story","Hadoop",1,"Upgrade to spring-data-hadoop 1.0.1.RC1","""spring-data-hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros/versions and we should make use of that."""
"XD-496","Story","Analytics",3,"Disable Collection to Object conversion in DefaultTuple","""DefaultFormattingConversionService provides Collection -> Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple(List<Tuple> list) would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple."""
"XD-485","Story","CLI",5,"Create stories to enable the use of Spring Shell's 2.0 branch testing facilities ","""We need a few steps 1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch 2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes.  Create stories related to Shell release. 3. Determine and document the basic recipe for doing integration tests. 4. Create stories to provide integration tests for each existing command"""
"XD-480","Bug","Stream Module",2,"In certain scenarios a job can be redeployed more than once","""In a scenario where we are using the same job definition i.e. Job.xml and we create Job Instance Foo.  If I create and deploy Foo2 using Job.xml  I will see only 2 job definitions(correct), but I will see the job run 3 times.  If I create Foo3 & deploy, I will see 3 job definitions(correct), but the jobs will run 5 times.  """
"XD-479","Story","Runtime",8,"Add conversion support to ChannelRegistrar and ChannelRegistry ","""Implements automatic conversion. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport."""
"XD-478","Story","Analytics|Stream Module",3,"Add accepted type logic to module","""A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion.  This can be done in the module XML configuration and processed by StreamPlugin"""
"XD-504","Story","Documentation",1,"Add ""How to Build Spring-XD"" instructions to the documentation","""We need to determine where this information could fit in. It can be either in """"README"""" at the project home page or """"Getting started"""" wiki page."""
"XD-514","Story","Runtime",8,"Create proper test coverage for Controllers","""Create proper test coverage for Controllers"""
"XD-522","Bug","CLI",1,"Cannot create tap if you have already tried to create an invalid one of same name","""From the shell:    Looks like the first tap was created even though there was a parse error.  And so the second attempt to create the tap failed due to an existing tap."""
"XD-540","Story","Runtime",5,"Broadcast Undeploy Requests","""Use an 'undeploy' topic to broadcast undeploy requests to all containers.  Applies to Redis and Rabbit transports, not local.  Also, rename {{ModuleDeploymentRequest}} to {{ModuleOperationRequest}} with an enum {{DEPLOY}}, {{UNDEPLOY}}."""
"XD-585","Bug","Packaging",1,"Deploying with twittersearch source throws Jackson ClassDefNotFound exception","""The upgrade to Jackson 2.2 included the following change to the build script   Spring social twitter template depends on these classes """
"XD-584","Bug","CLI",0,"Parsing stream definition with parameter containing single quotes not working","""The documented gemfire-cq example (https://github.com/springsource/spring-xd/wiki/Sources#wiki-gemfire-cq) fails:  xd:>stream create --name cqtest --definition """"gemfire-cq --query=""""Select * from /Stocks where symbol= 'VMW'"""" | file"""" You cannot specify option 'name' when you have also specified '' in the same command xd:>stream create --name cqtest --definition """"gemfire-cq --query=Select * from /Stocks where symbol=' VMW' | file"""" 10:01:46,249  WARN Spring Shell client.RestTemplate:524 - POST request for """"http://localhost:8080/str eams"""" resulted in 400 (Bad Request); invoking error handler Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD115E:(pos 26): unexpected  data in stream definition '*' gemfire-cq --query=Select * from /Stocks where symbol='VMW' | file                           ^ """
"XD-583","Bug","Runtime",5,"Dispatcher Has No Subscriber Error when posting a message to a stream","""This has been observed intermittently with Redis transport by myself and others when sending a message to a valid stream. Not sure how to recreate it yet.  11:27:10,082 ERROR ThreadPoolTaskScheduler-1 redis.RedisQueueInboundChannelAdapter:126 - Error sending message org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'org.springframework.context.support.GenericApplicationContext@3f73865d.input'. """
"XD-592","Bug","Analytics",8,"Problems with advanced tapping","""Start of a test program that can be placed in StreamCommandTests:    In the test program see two taps. One using the older style and one using the newer style and '>' so that there is no real tap module source, the log module just gets its input channel wired directly to myhttp.1 (the output of transform).  They should be doing the same thing.  However when run the output for tap_new is missing, all I see is:    No errors are reported, there is just no output for tap_new."""
"XD-596","Story","Documentation",1,"Add CONTRIBUTING.md file","""Add CONTRIBUTING.md file, use the Spring Integration file as the basis."""
"XD-595","Story","Documentation",1,"Fix wiki documentation to use xd shell command prompt to read ""xd:>""","""We need to fix the github wiki to use the xd shell command prompt """"xd:>""""."""
"XD-594","Story","CLI",2,"Create list/delete commands for all the metrics","""We need to add list/delete commands for the metrics:  InMemoryAggregateCounter FieldValueCounter Gauge RichGauge  Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics."""
"XD-593","Story","CLI",1,"Add ""counter delete"" shell command","""Add """"counter delete"""" shell command. This also requires implementation of DELETE rest end point at CountersController."""
"XD-614","Story","Runtime",8,"Conversion Enhancements","""Content-Type during transport transit is not the same as the content-type within modules.  """"Real"""" transports always use byte[] which may contain raw byte[] from a source, a byte[] converted from a String (which may or may not already contain JSON), or a byte[] containing JSON converted by the transport on the outbound side.  The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message.  Retain any content-type header that already exists in the message, and restore it.  For Rabbit, use normal SI/Rabbit headers to convey this information.  For Redis, add the information to the byte[]."""
"XD-613","Story","Runtime",5,"Deployed streams should be restarted on container start","""When using Redis store, stored deployed streams should be deployed on container restart."""
"XD-624","Bug","Analytics",1,"Use External Connection Factory in TCP Syslog Source","""WARN log emitted because the embedded connection factory does not get an application event publisher.  Will be fixed in SI M3 (INT-3107)."""
"XD-621","Story","CLI",2,"Set Default Hadoop Name Node for Shell","""Currently, you have to set the default name node every time your start the shell. We should do 2 things:   - Provide a default Name node Set Default Hadoop Name Node for Shell: hdfs://localhost:8020 - Should we provide some form of persistence? It kind of sucks that you have to re-specify the name node every time the shell starts up   """
"XD-643","Story","Analytics",3,"Map column names with underscore to camelCase style keys for JDBC sink","""We need to add support for matching column names with underscores like """"user_name"""" and map them to camel case style keys like """"userName"""" in the JdbcMessagePayloadTransformer."""
"XD-640","Bug","Runtime",3,"Cannot start xd-container with the --hadoopDistro option","""Trying to use xd-container with PHD, and therefore need to start with --hadoopDistro. I get the following error:  $ bin/xd-container --hadoopDistro phd1 17:11:20,305 ERROR main server.ContainerMain:59 - """"--hadoopDistro"""" is not a valid option """
"XD-634","Story","Packaging",3,"Fix guava dependency for hadoop20 and phd1 ","""Spring Xd currently ships with Guava 12.0 while Hadoop 2.0.5 and Pivotal HD 1.0 depends on 11.0.2 - this could lead to classpath problems if we include both."""
"XD-631","Story","Testing",1,"Pluralize test classes in package org.springframework.xd.shell.command","""The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename *JobCommandTests* to *JobCommandsTests* as it tests class *JobCommands*. Please check all tests in that package for correct naming."""
"XD-665","Story","CLI",1,"AggregateCounter display command options with ""lastHours"" and ""lastDays""","""It would be nice to have """"lastHours"""" and """"lastDays"""" options for aggregatecounter display command."""
"XD-663","Story","CLI",3,"Use correct FS_DEFAULT_NAME_KEY constant based on Hadoop version used","""Keep getting the following warning:  WARN Spring Shell conf.Configuration:817 - fs.default.name is deprecated. Instead, use fs.defaultFS  Should switch to use the runtime value of the FS_DEFAULT_NAME_KEY constant based on Hadoop version used."""
"XD-658","Story","Runtime",2,"Update to Spring-Data-Redis 1.1.0.M2","""Remove the {{NoOpRedisSerializer}} and use the non-serialization feature of M2."""
"XD-654","Story","CLI|Runtime",0,"Support explict named channel creation with configurable settings via the REST API and Shell","""Support pubsub named channels… the story could be a bit more general though: enable channel creation (with configurable settings) via the REST API and shell  >namedchannel create foo --domain PUBSUB"""
"XD-650","Story","Packaging",5,"Eclipse build path error after running gradle -> refresh source folders in Eclipse","""After running gradle -> refresh source folders on the spring-xd-module project in Eclipse, there is an error because the {{src/test/java}} folder is missing.  Solution is to add a placeholder file."""
"XD-686","Story","Runtime",8,"Support Named Taps (or Similar)","""Provide some syntax allowing multiple tap points to be directed to a named channel.  e.g.  tap foo.4 > namedTap tap bar.2 > namedTap  or  :tap.foo > counter"""
"XD-699","Story","Batch|CLI",2,"Handling tap operations on a tap that has reference to a deleted stream","""When trying to undeploy/destroy a tap that has reference to an already deleted stream fails with the following exception: Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD116E:(pos 4): unrecognized stream reference '<stream_name at the tap defintion>'.  As expected, the StreamConfigParser's lookupStream fails to find the stream name as the stream doesn't exist in the repository.   In this scenario, what is a better way to handle the tap operations.  Should we undeploy the tap when the stream is destroyed? ( though I don't see an easy way to find the taps that use a specific stream)."""
"XD-726","Story","Testing",5,"Test sink module in isolation","""Register the module under test Send a message to the sink using a test source and verify the sink contents - this requires checking an external resource - depends on the sink """
"XD-725","Story","Testing",5,"Test processor module in isolation","""Register the module under test  and have access to a source channel that drives messages into the processor and a output channel where output messages are sent.   Examples Built-in Message conversion: send JSON to a processor module that accepts Tuples. """
"XD-724","Story","Testing",5,"Test source module in isolation","""Register the module under test and deploy the module Verify output across all transports Examples Be able to start the rabbitmq source just by pointing to modules/source/rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in-memory queue backed channel for use with assertions to verify functionality.  Test that sending json, results in media-type header is set to json Test that sending POJO -> POJO Test that sending Tuple ->  Tuple Test that sending a (JSON) String -> String Test that sending raw bytes ->  raw bytes """
"XD-752","Story","Runtime",2,"Restrict Job launcher with more than one batch job configured in job module","""Currently the Job launcher launches all the batch jobs configured in the job module.  Please refer, ModuleJobLauncher's executeBatchJob().  This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name (group name).  Also, it is understood that having multiple jobs configuration under the same config xml is uncommon."""
"XD-751","Story","Hadoop",1,"XD UI on Yarn","""Technically speaking of we want to integrate XD UI on Hadoop tools we should do it so that the proxy on resource manager works with XD UI. From Hadoop Yarn resource manager point of view this proxied url is the applications tracking url(which is registered when application is deployed)."""
"XD-750","Story","Hadoop",1,"Container and Grid Control","""1. We'll need a system which give better control of what yarn/xd containers are out there and what is a status of those containers. 2. We also need grouping of containers order to choose, prioritize and scale tasks. 3. We need heartbeating of the grid nodes. Hadoop Yarn itself doesn't give enough tools to know if container is """"alive""""."""
"XD-749","Story","Hadoop",1,"Comm protocol for appmaster","""We need to be able to talk to appmaster which will control the whole xd yarn app.  1. Choose the implementation? Thrift? Spring Int? Something else? """
"XD-748","Story","Hadoop",1,"Interacting with XD on Yarn","""1. How we talk to the XD instance(s) on Yarn 2. There is a rest interface which location can be exposed either via resource manager or appmaster 3. Technically appmaster could also expose interface which could eihter be proxy for xd rest or dedicated interface implementation(i.e. thrift or spring int)"""
"XD-747","Technical task","Hadoop",1,"Bootstrap XD on Yarn","""1. How XD Yarn application should be packaged and bootstrapped? 2. Where the code should be? Within xd itself or separate repo?"""
"XD-754","Story","Runtime",1,"Fix Class/Package Tangle Introduced by XD-353","""{{container}} and {{event}}. {{XDContainer}} references and is referenced by {{ContainerStartedEvent}} (and stopped).  https://sonar.springsource.org/drilldown/measures/7173?metric=package_cycles&rids%5B%5D=7717  """
"XD-755","Story","Runtime",3,"Reactor Environment Improvements","""Use a profile or similar to only include the {{Environment}} conditionally (currently in module-common.xml.  Also  Jon Brisbin one thing to keep in mind: we talked about having a properties file for XD that configured the RingBuffer et al in a non-default way  Jon Brisbin e.g. no event loop Dispatchers…a ThreadPoolDispatcher with a large thread pool size (50 threads? 100?)…and maybe even two RingBufferDispatchers: input and output  Jon Brisbin so we might want to change from strictly a default Environment bean to an EnvironmentFactoryBean with a specific configuration…thinking about it now I maybe should add a namespace element for the Environment"""
"XD-776","Bug","CLI",1,"Shell: Remove ""taps list"" command","""We should only allow """"tap list"""" - currently """"tap list"""" AND """"taps list"""" are allowed but """"tap list"""" does not show up under help."""
"XD-773","Bug","CLI",2,"Tab support inconsistent for http post","""When doing *xd:> http post* and press the *tab* key. One should get a list of available options. Right now nothing happens. I have to press *--* and then tab to get the options.  Interestingly, this works for *stream create* + *tab* key"""
"XD-788","Story","Acceptance Testing",8,"Add Integration Tests to run JobCommands Tests against all transports","""similar to ChannelRegistry:  - AbstractChannelRegistryTests that has the real tests - subclasses for each impl provide the registry to be tested  Thus one test can run against multiple transports."""
"XD-808","Story","Packaging",3,"Update to spring-data-hadoop 1.0.1.RELEASE","""This might mean we should adjust our hadoopDistro options to the ones supported in the new release - hadoop12 (default), cdh4, hdp13, phd1 and hadoop21"""
"XD-807","Story","CLI",2,"Shell: Standardize counter name parameter","""The parameters are not optimal for the counter name between """"Aggregate Counter"""" """"Field Value Counter""""  --counterName versus --name"""
"XD-805","Story","Runtime",8,"Get notified when created named channel ""is ready""","""For testing purposes it would be super-helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to """"Thread.sleep""""."""
"XD-804","Story","Runtime",8,"Add Named Channel API","""We need an abstraction in place to retrieve messages from a """"named channel"""" programmatically.  Right now there is no implementation agnostic way of doing this (such as receiveMessage(), queueSize()).  This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to """"temp-files"""" and non-essential sinks or sources etc. - e.g.   """
"XD-819","Improvement","Stream Module",3,"Add Service Activator Processor","""Would be nice to have a ServiceActivator Processor available so that if one had an existing Spring bean they could simply describe the bean id and method name - without going through the full complexity of creating a processing module."""
"XD-842","Story","Packaging",1,"Add back classifier = 'dist' to distZip build target","""Add back """"classifier = 'dist'"""" to distZip build target - it was was accidentally removed."""
"XD-847","Story","Packaging",5,"Revise the available hadoopDistro options","""We should adjust our --hadoopDistro options to the ones supported in the new spring-data-hadoop 1.0.1.RELEASE - hadoop12 (default), cdh4, hdp13, phd1, hadoop20  This includes updating the wiki pages"""
"XD-850","Story","Packaging",3,"JAR version mismatches","""Looks like there are some version mismatch issues with the build/packaging of the XD components. Looking in xd/lib I see the following which looks suspicious:  mqtt-client-0.2.1.jar mqtt-client-1.0.jar  jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.12.jar  spring-integration-core-3.0.0.M3.jar spring-integration-http-2.2.5.RELEASE.jar  spring-data-commons-1.6.0.M1.jar spring-data-commons-core-1.4.0.RELEASE.jar """
"XD-849","Improvement","Analytics",2,"Gemfire modules should support connection via locator","""The gemfire modules currently accept server host and port. Provide an option to specify a locator host and port"""
"XD-874","Story","Analytics",8,"For file based item reader jobs, step/job completion message should have name of file sent on named channel","""It looks like we don't handle deletion of source files currently. We should provide some support for that - Maybe there is a way to into Spring Integration's PseudoTransactionManager support:  http://docs.spring.io/spring-integration/api/org/springframework/integration/transaction/PseudoTransactionManager.html  The *File Source* should possibly also support File archival functionality (But that might also be a dedicated processor?). Not sure where we want to set the semantic boundaries for the File Source. """
"XD-873","Story","Analytics",8,"File Source - Provide option to pass on File object","""This story may need to be broken into several stories  Particularly for Batch scenarios one may not want to run a """"file-to-string-transformer"""" on the payload file in the file source but rather handle/pass the file reference itself (local SAN etc.) - e.g. in case somebody drops a 2GB or in scenarios where one wants to push those large files into HDFS and run hadoop jobs on the data.  This is important for Batch Jobs as they need to access the file itself for the reader.   We need to *keep in mind the various transports we support*. Not sure how Kryo handles file serialization. I would think we only need the File Meta Data to be persisted not the file-data itself (make that configurable??). """
"XD-872","Story","Analytics",8,"Make in-memory meta data stores persistent","""Just wanted to create story for this - so we can consider whether this should be addressed.  In at least 2 modules we use non-persisted states. We may want to consider making them persistent:   *Twitter Search* uses an in-memory *MetadataStore* that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration:  """"Create a Redis-backed MetadataStore"""" See: https://jira.springsource.org/browse/INT-3085  *File Soure*'s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in-memory Queue to keep track of duplicate files.  """
"XD-885","Story","Runtime",8,"Add Batch Job Listeners Automatically","""Add Batch Job Listeners Automatically  * Each major listener category should send notifications to own channel (StepExecution, Chunk, Item etc.) * Add attribute to disallow automatic adding of listeners"""
"XD-892","Bug","Hadoop",1,"Spring Batch Behavior change from M2 to M3","""In M3, the batch job behavior has changed. In M2, it was much easier to create an invoke a batch job. In M3, a trigger is required. Figuring that change out isn't a big deal but the behavior of this batch job in M3 throws a stack trace, yet it executes.   In M2, this same batch job runs fine with no stack trace.   Logs are attached. I can't see a difference in the container log property files from M2 to M3. Turning the log settings down will suppress the traces, but I was not expecting the traces since they did not show up in M2.  Stream Definitions:  job create --name pdfLoadBatchJob --definition """"batch-pdfload --inputPath='LOCAL_PDF_PATH' --hdfsPath='REMOTE_HDFS_PATH'""""   stream create --name pdfloadtrigger --definition """"trigger > job:pdfLoadBatchJob"""""""
"XD-901","Story","Packaging",3,"Wrong Jetty Util on classpath for WebHdfs","""We currently include jetty-util-6.1.26.jar but we need to add correct jar for different distributions - PHD uses jetty-util-7.6.10.v20130312.jar  Need to check hadoop-hdfs dependencies for the distros and add jetty-util-* to the jar copy for each distro """
"XD-897","Story","Stream Module",8,"The HDFS Sink should support copying File payloads","""We should support *java.io.file* payloads in order to support non-textual file and large text file payloads being uploaded to HDFS.   Currently text file payloads are converted to a text stream in memory and, non-String payloads are converted to JSON first, using an """"object-to-json-transformer"""".   Ultimately we need to support streams such as """"file | hdfs"""" where the actually payload being copied to HDFS is not necessarily JSON or textual.  Need to be able to support headers in the message that will indicate which HDFS file the data should be stored in.  """
"XD-904","Bug","Analytics",1,"Fix hardcoded redis port from tests","""kparikh-mbpro:spring-xd kparikh$ grep -r 6379 * | grep java spring-xd-analytics/src/test/java/org/springframework/xd/analytics/metrics/common/RedisRepositoriesConfig.java:   cf.setPort(6379); spring-xd-analytics/src/test/java/org/springframework/xd/analytics/metrics/integration/GaugeHandlerTests.java:   cf.setPort(6379); spring-xd-analytics/src/test/java/org/springframework/xd/analytics/metrics/integration/RichGaugeHandlerTests.java:   cf.setPort(6379); spring-xd-dirt/src/test/java/org/springframework/xd/dirt/listener/RedisContainerEventListenerTest.java:   cf.setPort(6379); """
"XD-912","Story","Runtime",5,"Support for registering custom message converters","""Users need to register custom message converters used by modules."""
"XD-908","Story","Analytics",3,"Add aggregate counter query by number of points","""It should be possible to supply a start or end date (or none for the present), plus a """"count"""" value for the number of points required (i.e. after or prior to the given time)."""
"XD-917","Story","DSL",8,"Make the parser aware of message conversion configuration","""Enhance the stream parser to take message conversion into account in order to validate or automatically configure converters. For example:   {noformat:nopanel=true} source   --outputType=my.Foo  | sink --inputType=some.other.Bar   is likely invalid since XD doesn't know how to convert Foo->Bar.  {noformat}"""
"XD-919","Story","Analytics",2,"Remove json parameter from twittersearch source","""json parameter is no longer required. Use --outputType=application/json instead"""
"XD-931","Story","CLI|Runtime",2,"Format option to display runtime module properties in shell","""The runtime module properties requires a format option when displayed in the Shell   Based on the PR (https://github.com/spring-projects/spring-xd/pull/340), the module properties are stored as String and displayed as is. """
"XD-930","Story","Analytics",2,"Return rounded interval values from aggregate counter queries","""The aggregate counter query result currently returns the interval that is passed in, whether it is aligned with the bucket resolution requested or not. It would be more intuitive if the time values returned are rounded (down) to the resolution of the query (i.e. whole minutes, hours, days or whatever)."""
"XD-928","Story","Testing",1,"Refactor src/test/resources in Dirt","""* In the testmodules.source ** Rename source-config to packaged-source ** Rename source-config to packaged-source-no-lib * All xml files should be prefixed with test.  i.e. testsource, testsink * Make sure all tests pass with new configuration"""
"XD-939","Story","CLI",2,"Make Runtime modules listing by ContainerId pageable","""The RuntimeContainersController (from PR#340) returns the list of runtime modules. Instead we need make it pageable."""
"XD-955","Story","Documentation",1,"Update Jobs documentation to include ""job launch"" command","""This is currently missing and probably supersedes some of the stuff that's in there now."""
"XD-981","Story","Hadoop",3,"Missing guava-11.0.2.jar dependency for hadoop distros","""We used to have a shared guava-11.0.2.jar dependency in the lib dir. That's no longer there so hadoop distros that require this now fail (at least any hadoop 2.0.x based ones)  We should also upgrade to current Hadoop versions (Hadoop 2.2 stable)"""
"XD-974","Story","Hadoop",8,"The HDFS Sink should support compressing files as they are copied","""Get a java.io.File and copy it into HDFS.  Could be text or binary.  Write compressed with Hadoop and third party codecs   see: (XD-277, XD-279)  should initially support:  - bzip2    - LZO  """
"XD-994","Story","Hadoop",8,"The HDFS Sink should support writing POJOs to HDFS using Parquet","""Writing POJOs using Kite SDK """
"XD-993","Story","Hadoop",8,"The HDFS Store Library should support compression when writing to Sequence Files","""Support for using compression when writing Sequence Files  Either block or record-based compression. """
"XD-992","Story","Hadoop",8,"The HDFS Store Library should support writing to Sequence Files","""Support for writing Sequence Files  Without Compression  Need a means to specify the key/value to be used """
"XD-991","Improvement","Hadoop",8,"The HDFS Store Library should support compression when writing text","""Need to support writing text in compressed format  should initially support:  - bzip2  - LZO"""
"XD-990","Improvement","Hadoop",8,"The HDFS Store Library should support writing text with delimiter","""Support writing lines of text separated by a delimiter  Support writing a CSV (comma-separated variables), TSV (tab-separated variables),  No compression"""
"XD-1007","Story","UI",3,"UI: User should be able to see step execution info in a table below job detail","""On clicking the job detail page, we should display all the step executions associated with the specific job execution in a table view."""
"XD-1006","Story","UI",3,"UI: User should be able to view job detail from a specific job execution at Job Executions page","""On clicking """"details"""" link on a job execution row, user should see the job details.  Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily."""
"XD-1005","Story","UI",3,"UI: User should be able to filter the list of executions on the execution tab","""On clicking the “Executions” tab, user should see the list of all batch job executions. There should be options to filter job executions by few criteria such as by “Job name”, “execution time” etc., """
"XD-998","Story","Documentation",1,"Add documentation for gemfire cache-listener source","""Need some sample usage, docs for   https://github.com/spring-projects/spring-xd/tree/master/modules/source/gemfire   """
"XD-1016","Story","Stream Module",3,"Provide an option to pretty print JSON output","""Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print=true.  This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml"""
"XD-1039","Story","Runtime",5,"Composed of Composed fails at stream deployment time","""Although composition of a module out of an already composed module seems to work at the 'module compose' level, trying to deploy a stream with that more complex module fails with   at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)  at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:312)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:724) Caused by: java.lang.IllegalArgumentException: each module before the last must provide 'output'  at org.springframework.util.Assert.notNull(Assert.java:112)  at org.springframework.xd.module.CompositeModule.initialize(CompositeModule.java:132)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:234)  at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:224)  at org.springframework.xd.dirt.module.ModuleDeployer.handleCompositeModuleDeployment(ModuleDeployer.java:180)  at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:129)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  ... 63 more"""
"XD-1041","Story","Packaging",3,"Upgrade to Spring for Apache Hadoop 1.0.2.RELEASE and Pivotal HD 1.1","""Make sure the sinks and jobs work against Pivotal HD 1.1"""
"XD-1045","Story","REST",5,"Create project for model that is common between client and server","""this would elminate dependencies that are currently in the codebase, such as:  * RESTModuleType and ModuleType enums * ModuleOption and DetailedModuleDefinitionResource.Option  """
"XD-1048","Improvement","Analytics",5,"Extend aggregate counter to dynamically aggregate by field values in addition to time.","""This would be a combination of the existing aggregate counter and field value counter functionality.  For example if the stream data was for car purchases some fields might be colour, make and model.  When analysing the aggregate data I dont just want to know how many were sold on Monday, but how many of each make or how many of each colour,  or how many of a particular colour, make AND model. This would allow a dashboard type client to 'drill down' into each dimension or combination of dimensions (in real time without executing batch queries against the raw data)  Ideally the aggregate counter would be specified as   stream create --name mytap --definition """"tap:mystream > aggregatecounter --name=mycount --fieldNames=colour,make,model""""  The keys would be dynamically created according to the field values in each record (ie in a similar way to the field value counter you would not need to predefine field values) and keys would be created for all combinations of the fields specified eg the record   { """"colour"""":""""silver"""" , """"make"""":""""VW"""" , """"model"""" : """"Golf"""" }   would increment the following key counters (in addtion to the existing time buckets)  <existing base counter - ie all fields for this time bucket> colour:black make:VW model:Golf colour:black.make:VW colour:black.model:Golf make:VW.model:Golf colour:black.make:VW.model:Golf  ie the actual keys would look something like  aggregatecounters.mycount.make:VW.model:Golf.201307 etc  This may seem like it would generate a lot of key combinations but in practice the data generated will still be massively less than the raw data, and keys will only be created if that combination occurs in a time period.  Also some fields may be dependent on each other (such as make and model in the above example) so the amount of possibilites for those composite keys would be a lot less that the number of one times the number of the other.  """
"XD-1047","Improvement","Analytics",5,"Allow Aggregate Counter to use timestamp field in data.","""Currently the aggregate counter aggerates by the current time. However the data may already have a timestamp in it (eg streams from activity events on a website).  It would be useful as an alternative approach to be able to specify this field to aggregate on.   This would have the following benefits:  1) The aggregate counts would be more accurate as they would reflect the acutal event times and not have any lag from an intermediate messgaging system they might have passed through. 2) If for whatever reason XD is down, comes back up and starts pulling queued messages from the messaging system the aggregate counter will reflect the correct event time. Currently you would get a gap and then a spike as a backlog of messages would get allocated to the current aggregate count. 3) Old data could be rerun through XD still creating the correct aggregate counts.  Configuration would be something like   stream create --name mytap --definition """"tap:mystream > aggregatecounter --name=mycount --timestampField=eventtime""""  without the timestampfield it would behave as currently. """
"XD-1061","Story","Documentation",2,"Upgrade asciidoctor-gradle-plugin to 0.7.0","""Looks like we need to spend a cycle on Asciidoc - as we still have the author-tag-issue - I thought we can simply upgrade the asciidoctor-gradle-plugin to 0.7.0 (currently 0.4.1) but that breaks the docs being generated."""
"XD-1060","Story","Hadoop",1,"Add support for Hortonworks Data Platform 2.0","""(apologies if a ticket already exists for this, but I didn't see one)  I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn't supported by Spring XD yet.  How hard would it be to add these Distro's in?  Is it just a matter of dropping in a lib folder for hadoop22 and/or hdp20, and allowing those and options to be passed in via the --hadoopDistro option?  I'm currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox  http://hortonworks.com/hadoop-tutorial/using-spring-xd-to-stream-tweets-to-hadoop-for-sentiment-analysis/  Thanks!"""
"XD-1072","Improvement","Stream Module",1,"Add bridge module","""Add a bridge module per XD-956 to support definitions like topic:foo > queue:bar . Convenient for testing for XD-1066"""
"XD-1080","Story","Batch",1,"Make deploy=false as the default when creating a new job","""The automatic deployment of the job makes it harder to understand the lifecycle of the job and also does not allow for the opportunity to define any additional deployment metadata for how that job runs, e.g is it partitioned etc."""
"XD-1097","Story","Packaging",8,"Redo Hadoop distribution dependency management","""The way we now include various Hadoop distributions is cumbersome to maintain. Need a better way of managing and isolating these dependencies on a module level rather than container level."""
"XD-1103","Bug","Analytics",5,"JDBC sink is broken - looks like some config options got booted","""The JDBC sink is broken. Simple """"time | jdbc"""" results in:  org.springframework.jdbc.BadSqlGrammarException: PreparedStatementCallback; bad SQL grammar [insert into test (payload) values(?)]; nested exception is java.sql.SQLSyntaxErrorException: user lacks privilege or object not found: TEST  Looks like some config options got clobbered during bootification. """
"XD-1105","Story","Acceptance Testing",3,"Add some test coverage to mqtt modules","""Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is  somesource | mqtt --topic=foo  with   mqtt --topics=foo | somesink   And asserting that what is emitted to somesource ends up in somesink.  """
"XD-1104","Story","Testing",5,"Create Shell Integration test fixture for jdbc related sink","""Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD.  Use of an in memory db where we expose eg a JdbcTemplate to assert state"""
"XD-1108","Story","Configuration",2,"Restore lax command line options","""Restore --foo=bar as well as --foo bar  Validation of values should be done as a separate story"""
"XD-1115","Story","Packaging",3,"We no longer validate the --hadoopDistro options in the xd scripts","""We no longer validate the --hadoopDistro options in the xd scripts. Seem sthe classes doing this validation were removed for boot.  We do this validation in the xd-shell script"""
"XD-1114","Story","Runtime",5,"Investigate dropped Module Deployment Requests","""We have observed in unit tests (see AbstractSingleNodeStreamIntegrationTests) that(Redis/SingleNode) occasionally fail. The root cause must be investigated further but there is some evidence to suggest that the control messages (ModuleDeploymentRequests) are not always received and handled by the ModuleDeployer. This does not produce an error but results in runtime stream failures. This problem may be resolved as part of the planned Deployment SPI but is being tracked here until we are certain that it has been resolved."""
"XD-1112","Story","Runtime",0,"Add port scan (and ability to disable) to container launcher","""Spring Boot support port scanning if you set server.port=0 (and disable with -1), so we could make that the default for the container node."""
"XD-1122","Story","Configuration",2,"Add jmxPort to list of coerced cmd line options","""Following merge of XD-1109.  See discussion at https://github.com/spring-projects/spring-xd/commit/eaf886eab3b2ef07da55575029ccabb2c8a36af9#commitcomment-4701947"""
"XD-1132","Improvement","Analytics",2,"JMS Module - add support for TOPICS","""As a Spring XD user I need to listen on a JMS Topic and ingest the messages, so I can process the messages.  Currently the module only allows for Queues"""
"XD-1147","Story","Runtime",0,"Allow alternate transports to be used within a stream","""Need to clarify if this means alternate transports within a stream, e.g   source |[rabbit] | processor |[redis]| sink   or specifying that a stream use an alternate transport to the one configured for the container.  """
"XD-1155","Bug","Packaging",3,"The lib directory for hadoop12 contains mix of hadoop versions","""This causes issues depending on which version of the core/common jar gets loaded first - like:  xd:>hadoop fs ls -ls: Fatal internal error java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation   at org.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:213)   at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2401)   at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2411)   at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2428)   at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)   at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)   at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:166)   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:351)   at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)   at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)   at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)   at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)   at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)   at org.apache.hadoop.fs.shell.Command.run(Command.java:154)   at org.apache.hadoop.fs.FsShell.run(FsShell.java:255)   at org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)   at org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)   at org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)   at java.lang.reflect.Method.invoke(Method.java:606)   at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)   at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)   at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)   at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)   at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:483)   at org.springframework.shell.core.JLineShell.run(JLineShell.java:157)   at java.lang.Thread.run(Thread.java:724) """
"XD-1162","Bug","Stream Module",1,"Column option of JDBC sink should not convert underscore to property name.","""Current implementation of column option of JDBC sink convert underscore to java property name. If database column contains underscore, there is no way to store data.  So JdbcMessagePayloadTransformer should not use JdbcUtils.convertUnderscoreNameToPropertyName even if column contains """"_""""."""
"XD-1161","Bug","Analytics",3,"Re-deployment of hdfs sink reuses filename of first deployment","""Need to check for existing files with the same file counter"""
"XD-1160","Story","Stream Module",8,"Standardize naming and unit for options across modules","""We should standardize on the options between modules:  idleTimeout - timeout rolloverSize - rollover  Also, need to standardize on unit used for timeout - should this be s or ms? """
"XD-1159","Improvement","Stream Module",5,"Add a MongoDB Sink","""This should be quite straightforward, since the Spring Data Mongo jars are already included. We have this working by just adding the attached sink context file and the spring-integration-mongodb jar.  (This works for JSON string streams, but a mongo converter probably needs added to support Tuple conversion) """
"XD-1170","Bug","Stream Module",2,"Splunk module is broken","""Splunk sink module doesn't work at all. It throws java.lang.VerifyError exception like following.  nested exception is java.lang.VerifyError: class org.springframework.integration.splunk.outbound.SplunkOutboundChannelAdapter overrides final method onInit.()V  This is because SplunkOutputChannelAdapter refers old spring integration jar, but recent AbstractReplyProducingMessageHandler (which SplunkOutputChannelAdapter extends) set final to onInit method. Hence it doesn't work.  SplunkOutboundChannelAdapter should be fixed to not override onInit method and replace the jar file spring-integration-splunk-1.0.0.M1.jar."""
"XD-1176","Story","Hadoop",1,"Update to spring-data-hadoop 2.0.0.M4","""Update dependencies to spring-data-hadoop 2.0.0.M4"""
"XD-1182","Story","Hadoop",3,"Update to spring-data-hadoop 2.0.0.M5","""Update to spring-data-hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring-xd-hadoop  We should also review the supported hadoop distros - think we should support anything that is current/stable: - hadoop12 - hadoop22 - phd1 (PHD 1.1) - hdp13 - hdp20 - cdh4 """
"XD-1190","Story","Configuration",5,"Setup precedence order for module properties' property resolver","""The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties:  From lowest to the highest order,  0 application.yml 1 applicaiton.yml fragment 2 property placeholders 2a  property placeholder under 'shared' config directory  2b property placeholder under module/(source/sink/processor)/config directory 3. environment variables 4. system properties 5. command line   """
"XD-1191","Bug","Analytics",3,"JDBC sink destroys existing table","""The jdbc sink deletes existing table and creates a single column payload one even if properties file has 'initializeDatabase=false'"""
"XD-1220","Story","Stream Module",5,"Batch jobs should use application.yml provided connection as default","""Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch-jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default."""
"XD-1217","Story","Stream Module",5,"twittersearch and twitterstream should support compatible formats","""Currently twitterstream emits native twitter json whereas twittersearch uses SI/Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions.  This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE: I think it's a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types."""
"XD-1241","Story","Acceptance Testing",8,"Add to Acceptance Test EC2 job a stage that uses XD distributed mode with redis","""See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements  """"Stages are comprised of one or more Jobs, which run in parallel""""  we would like the tests across the rabbit and redis transport to occur in parallel."""
"XD-1240","Story","Acceptance Testing",8,"Add to Acceptance Test EC2 CI build plan a stage that uses XD distributed mode with rabbit","""See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements  """"Stages are comprised of one or more Jobs, which run in parallel""""  we would like the tests across the rabbit and redis transport to occur in parallel. """
"XD-1228","Story","Testing",8,"Provide a easy, prescriptive means to perform unit and basic stream integration tests.","""AbstractSingleNodeStreamDeploymentIntegrationTests is the basis of 'state of the art' testing for a stream that allows you to get a reference to the input and output channel of the stream  http | filter | transform | file.  One can send messages to the channel after the http module, but before filter and one can retrieve the messages that were sent to the channel after the transform module but before file.  The current implementation inside AbstractSingleNodeStreamDeploymentIntegrationTests can be improved in terms of ease of use for end-users.    The issue is to create as simple a way as possible for a user to test their processing modules/stream definitions without having to actually do a real integration test by sending data to the input module.  Either as a separate issue or as part of this one, the documentation   https://github.com/spring-projects/spring-xd/wiki/Creating-a-Processor-Module  should be updated to explicitly show how to use this issue's test functionality. """
"XD-1245","Story","Acceptance Testing",5,"Develop basic acceptance test application to exercise based XD-EC2 deployment from CI","""Create a first pass at an acceptance test app for a stream definition of http | log.    This will involve creating two new projects in xd  1. spring-xd-integration-test 2. spring-xd-acceptance-tests  #1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules. #2 will contain tests that use #1 to test the various out of the box modules provides in XD."""
"XD-1252","Story","Stream Module",5,"Allow processor script variables to be passed as module parameters","""Currently, if we want to bind values to script variables we need to put them in a properties file like so:  xd:> stream create --name groovyprocessortest --definition """"http --port=9006 | script --location=custom-processor.groovy --properties-location=custom-processor.properties | log  Ideally it should be:  xd:> stream create --name groovyprocessortest --definition """"http --port=9006 | script --location=custom-processor.groovy --foo=bar --baz=boo | log   """
"XD-1255","Story","Acceptance Testing",5,"Create assertion to get count of messages processed by a specific module in a stream","""The modules are exposed via JMX and in turn exposed over http via jolokia.  See https://jira.springsource.org/browse/XD-343.  This issue is to develop a helper method that given a stream id and/or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g.  int originalCount = getCount(""""testStream"""", """"file"""");  //do stuff that generates 100 messages assertCount(""""testStream"""", """"file"""", 100, originalCount)  For now we can assume we know the location of where the modules are located by assuming we have only one container deployed."""
"XD-1256","Improvement","Runtime",2,"Running XD as service","""It is useful to configure operating system so that it will start Spring XD automatically on boot.  For example, in Linux it would be great if Spring XD distro contains init.d script to run it as service. A typical init.d script gets executed with arguments such as """"start"""", """"stop"""", """"restart"""", """"pause"""", etc. In order for an init.d script to be started or stopped by init during startup and shutdown, the script needs to handle at least """"start"""" and """"stop"""" arguments. """
"XD-1267","Epic","Configuration",0,"Improve configuration option handling","""There are inconsistencies in our current approach for handling module options (using property file for default vs. classes has different behavior in terms of over-riding with system properties.  Need to rationalize the behavior."""
"XD-1270","Story","Runtime",20,"Add states to the deployment of stream","""Improve how the state of the stream is managed.  A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is ‘deployed’ If one or more module deployments failed, the stream state is failed.  Any modules that were successfully deployed, are still running.    Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state.  For the individual modules that failed, we will be able to find out which ones failed.  Not yet sure if we can try to redeploy just those parts of the stream that failed.  See the [design doc|https://docs.google.com/a/gopivotal.com/document/d/1kWtoH_xEF1wMklzQ8AZaiuhBZWIlpCDi8G9_hAP8Fgc/edit#heading=h.2rk74f16ow4i] for more details.  Story points for this issue are the total of all the story points for the subtasks."""
"XD-1273","Improvement","Documentation",1,"The use of labelled modules and taps needs more explanation","""https://github.com/spring-projects/spring-xd/wiki/Taps mentions this but the explanation needs more elaboration and example, e.g.  mystream ->  """"http | flibble: transform --expression=payload.toUpperCase() | file""""  """"tap:stream:mystream.flibble > transform --expression=payload.replaceAll('A','.') | log"""");"""
"XD-1282","Story","Configuration",5,"Add caching to ModuleOptionsMetadataResolver","""Will likely involve having the module identity (type+name) be part of the OptionsMetadata identity/cache key"""
"XD-1300","Story","Configuration",3,"Handling boolean type module option properties defaults in option metadata","""There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData.   Also, when using boolean we need to have module option using primitive type boolean than Boolean type.  Currently, these are some of the module options that require this change:  """"initializeDatabase"""" in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module  """"restartable"""" in all the job modules  """"deleteFiles"""" in filejdbc, filepollhdfs job modules       """
"XD-1296","Story","Testing",2,"Few integration tests fail if JMX is enabled","""If JMX is enabled, some of the integration tests fail.  This is similar to what we see in XD-1295.  One example of this case is, the test classes that extend StreamTestSupport.  In StreamTestSupport, the @BeforeClass has this line:  moduleDeployer = containerContext.getBean(ModuleDeployer.class);  When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer (since it is of type MessageHandler) and thereby the above line to get bean by the implementing class type (ModuleDeployer) fails.  There are few other places where we use to refer the implementing classes on getBean(). Looks like we need to fix those as well.  """
"XD-1301","Bug","Runtime",5,"MBeans are not destroyed if stream is created and destroyed with no delay","""Problem: The container that the stream was deployed to, will not allow new streams to be deployed.  Once the error occurs, the only solution is to terminate the XD Container and restart it.  To reproduce create a stream foo and destroy the stream, then create the stream  foo again.  This best done programmatically, taking the same steps using the """"shell"""" may not reproduce the problem.  i.e. if you put a Sleep of 1-2 seconds between the destroy and the next create, it works fine  """
"XD-1316","Bug","UI",2,"UI:Fix E2E test warning","""When running E2E tests the following warning may be observed:   """
"XD-1314","Story","Hadoop",3,"Create XD .zip distribution for YARN","""Create XD .zip distribution for YARN that adds an additional sub-project to the spring-xd repo for building the xd-YARN.zip  Link into main build file  Produce a new artifact spring-xd-v-xyz-yarn.zip as part of the nightly CI process -- will now have 2 artifacts, main xd.zip distribution and xd-yarn.zip  Does not include any Hadoop distribution libraries  Does include spring-hadoop jars for Apache22 ‘unflavored’ """
"XD-1313","Story","Batch",0,"Commands that start a job should return a representation of the JobExecution","""See discussion at https://github.com/spring-projects/spring-xd/pull/572"""
"XD-1312","Story","Batch",5,"Job execution restart fails with NPE","""Create a job, launch it but make it fail (eg filejdbc with missing file)  job execution list => it's there, as FAILED. Good  job execution restart <theid> ==> Fails with NPE:  """
"XD-1311","Story","Batch",3,"Job execution list should mention jobs that have been deleted","""Create a job, execute it a couple of times, destroy it and then invoke job execution list.  The job name column should mention that a job is defunct (even though a job with the same name could have been re-created in the interim)"""
"XD-1310","Story","Batch",1,"Misleading error message when trying to restart a job exec","""Disregard the missing date that is caused by another problem. Here is the setup:   while the server exception is a bit better:   I'd argue we should not speak in terms of execution ids if possible, but rather in terms of job names """
"XD-1309","Story","Configuration",5,"JSR303 validation of options interferes with dsl completion","""When using a JSR303 annotated class for module options, the binding failures should be bypassed, as they interfere with completion proposals. """
"XD-1307","Story","REST",5,"Use HATEOAS Link templates","""HATEOAS 0.9 introduced some support for templated links. This should be leveraged to properly handle eg /streams/{id} instead of using string concatenation"""
"XD-1322","Story","YARN Runtime",5,"Add way to provide module config options for XD on YARN","""There seems to be some intersection with the work for this issue and the rationalization of how module properties are handled.  There will be changes to configuration/property management support such that each module (source, sink, etc) will be able to also be overridden in spring-xd.yml (or wherever -Dspring.config.location points to.  The HDFS sink module for example, will have default values based on it's OptionsMetadata and will be of the form <type>.<module>.<option>   That means in the configuration for hdfs.xml sink, there would be a config section such as    With default values defined by a HdfsSinkOptionsMetadata class.  The hdfs.xml module file would not contain any references to a properties file.  A file specified by -Dspring.config.location could override the values in a config section such as  sink:   hdfs:     hd.fs : hdfs://foobarhost:8020     hd.jt : 10.123.123.123:9000  etc.  """
"XD-1321","Story","YARN Runtime",8,"Add XD deployment for YARN","""Add YARN specific code based on Janne's prototyping  Add YARN Client and AppMaster implementations and startup config files  This includes shell scripts to deploy XD to YARN  Test working on Apache 2.2 distribution  We can modify config files, everything should be possible to override by providing command-line args or env variables. ./xd-yarn-deploy --zipFile /tmp/spring-xd-yarn.zip --config /tmp/spring-xd-yarn.yml  """
"XD-1320","Story","Batch",0,"Make Batch Job Restarts Work with Distributed Nodes ","""Job restart fails with NPE. See PR for XD-1090:  https://github.com/spring-projects/spring-xd/pull/572 """
"XD-1319","Story","Configuration",5,"Allow mixins of ModuleOptionsMetadata","""A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains (eg jdbc + hdfs).  I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like:  public class JdbcHdfsOptionsMetadata {    @OptionsMixin   private JdbcOptionsMetadata jdbc;    @OptionsMixin   private HdfsOptionsMetadata hdfs; }  this would expose eg """"driverClass"""" as well as """"rolloverSize"""" as top level options. Values could be actually injected into the fields, so that eg custom validation could occur (default validation for the mixin class would occur by default)"""
"XD-1327","Bug","Stream Module",1,"Rabbit source module with outputType fails to deploy","""To replicate the issue:  Create stream:  stream create rabbittest --definition """"rabbit --queues=test --outputType=text/plain | log""""  Stacktrace thrown:  17:59:56,436 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:191 - Caught exception while handling a request java.lang.IllegalArgumentException: Module option named outputType is already present  at org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.<init>(FlattenedCompositeModuleOptionsMetadata.java:56)  at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:49)  at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:117)  at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:73)  at org.springframework.xd.dirt.rest.XDController.save(XDController.java:227)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:601)  at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:214)  at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)  at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)  at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:690)  at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)  at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:945)  at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:876)  at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)  at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:647)  at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)  at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:114)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)  at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:131)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)  at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)  at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)  at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:97)  at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:82)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)  at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)  at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)  at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)  at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)  at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)  at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)  at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)  at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)  at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)  at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:722) """
"XD-1326","Story","CLI|YARN Runtime",8,"Provide xd-shell integration for deploying XD on YARN","""Command such as  yarn app list yarn deploy-xd --zipFile /tmp/myapp.zip --config /tmp/myconfig.yml """
"XD-1330","Story","Hadoop",3,"Enhance HadoopFileSystemTestSupport to obtain resource for a specific hadoop distro","""It looks like the HadoopFileSystemTestSupport test rule by default runs against hadoop 1.2 and we can add a way to support running the hadoop centric tests to run against a given hadoop distro.   Currently, if the test is run against a version other than 1.2, the rule says:  15:47:34,469 ERROR main hadoop.HadoopFileSystemTestSupport:95 - HADOOP_FS IS NOT AVAILABLE, SKIPPING TESTS org.apache.hadoop.ipc.RemoteException: Server IPC version 9 cannot communicate with client version 4  at org.apache.hadoop.ipc.Client.call(Client.java:1113)  at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)  at com.sun.proxy.$Proxy8.getProtocolVersion(Unknown Source)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:601)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)  at com.sun.proxy.$Proxy8.getProtocolVersion(Unknown Source)  at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)  at org.apache.hadoop.hdfs.DFSClient.createNamenode(DFSClient.java:183)  at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:281)  at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:245)  at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1446)  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:67)  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1464)  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:263)  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:124)  at org.springframework.xd.test.hadoop.HadoopFileSystemTestSupport.obtainResource(HadoopFileSystemTestSupport.java:49)  at org.springframework.xd.test.AbstractExternalResourceTestSupport.apply(AbstractExternalResourceTestSupport.java:58)  at org.junit.rules.RunRules.applyAll(RunRules.java:26)  at org.junit.rules.RunRules.<init>(RunRules.java:15)  at org.junit.runners.BlockJUnit4ClassRunner.withTestRules(BlockJUnit4ClassRunner.java:379)  at org.junit.runners.BlockJUnit4ClassRunner.withRules(BlockJUnit4ClassRunner.java:340)  at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:256)  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)  at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)  at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)  at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)  at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)  at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)  at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)  at org.junit.runners.ParentRunner.run(ParentRunner.java:309)  at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)  at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)  at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197) """
"XD-1329","Improvement","Stream Module",8,"Add a Kafka Source","""This would use the Kafka Spring Integration Extension. We have a version of this working but had to modify the adapter code as its not currently compatible with Spring Integration 4. See INTEXT-97  """
"XD-1333","Bug","Packaging",2,"Add config file fragment support configuration in XD windows bat scripts","""The external configuration fragment file support by setting spring.config.location in the XD startup scripts are not updated in xd-admin, xd-container and xd-singlenode .bat scripts.   Please refer: https://github.com/spring-projects/spring-xd/issues/582 """
"XD-1339","Improvement","Runtime",8,"Deployment manifest to support directing deployment to run on a group of servers","""Need some kind of hint that a given deployment is to be run on a group of servers.  That deployment would then be part of a partitioned work flow.  Another item on this would be the """"group"""" that the server is running in can be added to removed dynamically.  The use case on this would be if the group of servers are running a max CPU capacity we can easily add another compute node.  Likewise we can remove a server from the group if the servers are not being fully utilized.    This issue is lightly linked to: https://jira.springsource.org/browse/XD-1337"""
"XD-1337","Improvement","Runtime",8,"Stream partitioning metadata should allow updating at runtime - dynamically / anytime","""In a running system some times the algorithm for partitioning the data might overload a given server with work.  When that happens we might need to """"rebalance"""" the partitioned work / data to achieve a even balance of stream throughput across servers in a given compute group.  We can think of this dynamic rebalancing behavior as an extension of a failure use case.   In the failure scenario we need to re-partition the stream to other servers in the group.  We should allow third parties to plug-in to help with this capability.  As an example GemFire will report the new partitioning meta-data when this type failure / rebalance happens. """
"XD-1336","Improvement","Runtime",1,"Allow easy integration with other types of message transports - remove enums for transport layers","""If a third party messaging solution wants to be the transport layer in SpringXD they must currently fork the SpringXD code base and change the enums.  Example: CommonDistributedOptions.ControlTransport currently limits to the following options (rabbit, redis).  So if a third party like messaging system, like ZeroMQ, wanted to plug-in they would have to add to the enum.  Here is another example where GemFire was used as the messaging system:   https://github.com/charliemblack/spring-xd/blob/master/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/CommonDistributedOptions.java#L38  All messaging enums should be removed for an extensible model."""
"XD-1342","Improvement","Ingest",1,"Configuration for RabbitMQ message bus concurrent consumers","""By having the configuration option for concurrent consumers would help improve the performance of message consumption by the consumer modules when the ordering of the incoming messages don't matter."""
"XD-1341","Story","Batch",2,"Support oracle jdbc configuration for XD batch job repository","""Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings."""
"XD-1344","Bug","DSL",1,"Cannot undeploy stream that was created and deployed with a ""."" in the name","""eserrano-mbp:spring-xd-1.0.0.M5 eserrano$ ./shell/bin/xd-shell  _____                           __   _______ /  ___|          (-)             \ \ / /  _  \ \ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |  `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | | /\__/ / |_) | |  | | | | | (_| | / / \ \ |/ / \____/| .__/|_|  |_|_| |_|\__, | \/   \/___/       | |                  __/ |       |_|                 |___/ eXtreme Data 1.0.0.M5 | Admin Server Target: http://localhost:9393 Welcome to the Spring XD shell. For assistance hit TAB or type """"help"""". xd:>stream list   Stream Name    Stream Definition                                          Status   -------------  ---------------------------------------------------------  --------   eesstream.log  http | transform --expression=payload.toUpperCase() | log  deployed   httptest       http | file   tictac         time | log  xd:>stream   stream all         stream create      stream deploy      stream destroy     stream list        stream undeploy      xd:>stream undeploy --name  stream undeploy --name  required --name: the name of the stream to un-deploy; no default value xd:>stream undeploy --name eesstream.log  Command failed org.springframework.xd.rest.client.impl.SpringXDException: The stream named 'eesstream' is not currently deployed  xd:> """
"XD-1343","Story","Configuration",8,"Provide a conventional way to extend XD Container configuration","""Provide an easy way for users to add beans (e.g., Gemfire cache configuration) or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath*:META-INF/spring/xd/extensions or include this path in an extensible @Configuration base class.  In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix."""
"XD-1351","Story","Runtime",8,"Replace BeanDefinitionAddingBeanPostProcessor with Ordered Plugins","""This will allow us to control the order of plugins and use plugin(s) to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser"""
"XD-1348","Story","Configuration|Runtime",1,"Allow end users to configure Rabbit MQ properties on the MessageBus (for acks, txs, etc).","""This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. """
"XD-1356","Bug","Hadoop|Packaging",3,"Hadoop distro option hdp20 is broken","""Starting the shell with --hadoopDistro hdp20 causes this:  Exception in thread """"main"""" org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Unable to locate Spring NamespaceHandler for XML schema namespace [http://www.springframework.org/schema/hadoop] Offending resource: URL [jar:file:/Users/trisberg/Demo/spring-xd-1.0.0.BUILD-SNAPSHOT/shell/lib/spring-xd-shell-1.0.0.BUILD-SNAPSHOT.jar!/META-INF/spring/spring-shell-plugin.xml]  Creating a stream """"time | hdfs"""" in xd-singlenode started with --hadoopDistro hdp20 causes this:  java.lang.IllegalStateException: Can't find class used for type of option 'codec': org.springframework.data.hadoop.store.codec.Codecs """
"XD-1355","Story","Runtime",2,"Publish ContainerStoppedEvent when the container shutsdown","""When the container shuts down, ContainerStoppedEvent should be published so that appropriate listeners would act on.  Please refer to this discussion here:  https://github.com/spring-projects/spring-xd/pull/612"""
"XD-1360","Bug","REST",3,"Json information returned by curl does not reflect deployed status correctly","""The Json information returned by curl does not reflect deployed status correctly. To recreate: 1. Start xd-singlenode 2. start xd-shell In the xd-shell     (i). stream create --definition """"time | log"""" --name ticktock    (ii). stream list Note the status of the ticktock stream is deployed 3. open a new command prompt & type curl http://localhost:9393/streams/ticktock 4. Note the returned json stream:   {""""name"""":""""ticktock"""",""""deployed"""":null,""""definition"""":""""time | log"""",""""links"""":[{""""rel"""":""""self"""",""""href"""":""""http://localhost:9393/streams/ticktock""""}]} 5. I would expect the json attribute """"deployed"""" to be """"true"""", but it is null."""
"XD-1364","Story","Hadoop|Runtime",5,"Upgrade to SHDP 2.0 M6","""The YARN support in M6 changes most of the config properties, need to update XD to use new ones."""
"XD-1368","Story","Runtime",8,"Refactor container to remove shared module context as a separate context ","""The main container context becomes the shared context for modules."""
"XD-1365","Bug","Runtime",2,"StreamDeployer.deleteAll() does not handle dependency tracking","""create a composed module, use it in a stream, delete ALL streams. Try to delete the composed module => fails thinking that it's still used by the stream  """
"XD-1372","Story","Stream Module",0,"Update Reactor integration to align 1.1 changes","""Need to update the spring-xd-extension-reactor support to reflect the changes to Reactor refactorings introduced in v1.1."""
"XD-1371","Improvement","Documentation",3,"Clarify API or syntax for managing deployment parameters","""Suppose we have 3 environements of Spring XD : - Dev environment  - Test environment  - Prod environement (  Suppose whe develop the script bellow: ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// stream1 = http | filter --expression=payload.contains('toto') | file --dir=/tmp/toto  stream2 = http | filter --expression=payload.contains('titi') | file --dir=/tmp/titi //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  When we need to deploy the script in Test and Prod environements , we must modify """"dir"""" option of """"file"""" sink. This is very easy when there is not a lot of options and when we have a small factory team. But in a big factory environment this will be problematic.   In order to industrialize deployment, it would be convenient to implement in DSL a directory interface API or something equivalent like below:  Suppose we call this directory interface XDDI ... like """"XD Directory Interface"""" :-)  The script can be like that: ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////// stream1 = http | filter --expression=payload.contains('toto') | file --dir=XDDI('totoKey')  stream2 = http | filter --expression=payload.contains('titi') | file --dir=XDDI('titiKey')  //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////  The XDDI keys are defined in a centralized directory interface (admin console or XDDI.properties)  The XDDI keys/values in Dev environment: ///////////////////////////////////// totoKey=/tmp/toto titiKey=/tmp/titi /////////////////////////////////////  The XDDI keys/values in Test environment: ////////////////////////////////////////// totoKey=/tartempion/toto titiKey=/petaouchnok/titi /////////////////////////////////////////  The XDDI keys/values in Prod environment: ///////////////////////////////////////////////////// totoKey=/vavoirlabasijysuis/toto titiKey=/vavoirlabasijysuis/titi /////////////////////////////////////////////////////  When the script is deployed in Test or Prod environement, if the script contain a key that is not defined in centralized directory, the deployment fail.   This will reduce errors risks in a big factory environnement (several hundred parameters and signifiant team turnover).  """
"XD-1370","Bug","Runtime",3,"Serialization over data transport fails for classes that are module specific","""Given:    the counter example from the guide is run with redis enabled:  xd-singlenode --transport redis --store redis  When:   a stream is created  stream create --name springtweets --definition """"twittersearch --consumerKey=<your_key> --consumerSecret=<your_secret> --query=spring | file --dir=/tweets/""""  Then:  An exception is thrown:  Exception in thread """"inbound.springtweets.0-redis:queue-inbound-channel-adapter35"""" org.springframework.integration.MessageHandlingException: error occurred in message handler [springtweets.0.convert.bridge]  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:94)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:42)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:86)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:207)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)  at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:286)  at java.lang.Thread.run(Thread.java:724) Caused by: org.springframework.integration.x.bus.serializer.SerializationException: unable to deserialize [null]. Class not found.  at org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:247)  at org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumer(MessageBusSupport.java:191)  at org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumerIfNecessary(MessageBusSupport.java:168)  at org.springframework.integration.x.redis.RedisMessageBus.access$300(RedisMessageBus.java:57)  at org.springframework.integration.x.redis.RedisMessageBus$ReceivingHandler.handleRequestMessage(RedisMessageBus.java:176)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:142)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)  ... 13 more Caused by: java.lang.ClassNotFoundException: org.springframework.social.twitter.api.Tweet  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)  at java.security.AccessController.doPrivileged(Native Method)  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)  at java.lang.Class.forName0(Native Method)  at java.lang.Class.forName(Class.java:190)  at org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:241)  ... 19 more"""
"XD-1369","Story","Hadoop",5,"Using hdfs sink throwing an error","""I am trying to use HDFS as sink while creating streams and I am encountering the following error :  Please refer attached document : Exception - localhost - 8020.txt   The fs.default.name set in hadoop.properties is : fs.default.name=hdfs://localhost:8020  I have also tried the following variation in the hadoop.properties file : fs.default.name=hdfs://127.0.0.2:8020 fs.default.name=hdfs://127.0.0.2:50070  Using these values are also throwing me exceptions as mentioned in the following files attached :  Exception - 127.0.0.2 - 8020.txt  Exception - 127.0.0.2 - 50070.txt   We are using : Pivotal HD 1.0.1 spring-xd-1.0.0.M5-dist  Kindly let us know the way around for this issue.   """
"XD-1373","Bug","Runtime",5,"HSQL always started, even when using other database","""I set the config/xd-config.yml properties to use MySQL including this   profiles:     active: default,mysql  When XD ADmin starts I still see HSQL server started and localhost:9393/env shows:  """"profiles"""": [     """"adminServer"""",     """"hsqldb"""",     """"default"""" ], """
"XD-1380","Story","Stream Module",5,"Can't create http source while TCP is used as a source and sink on singlenode","""[Problem] Can't use tcp source, sink and http together on Single Node.  While creating tests for CI I tried to create the following: [Steps to Reproduce] xd:>stream create fooOut --definition """"tcp|file"""" Created new stream 'fooOut' xd:>stream create fooIn --definition """"http --port=9002|tcp"""" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Failed to bind to: 0.0.0.0/0.0.0.0:9000. Possibly the port is already in use. Even if I use different ports for the tcp I still get failures pointing to 9000. [Extra Notes] The stream below is works. xd:>stream create fooOut --definition """"tcp|file"""" Created new stream 'fooOut' xd:>stream create fooIN --definition """"time|tcp""""  *Stack Trace Attached*"""
"XD-1390","Story","Batch",3,"Investigate missing stepExecutions in JobRepository.getLastJobExecution() ","""When the job is run with its jobParameters by SimpleJobLauncher, its lastJobExecution's stepExecutions are checked for UNKNOWN status to throw JobRestartException. It looks like the stepExecutions for the lastJobExecution are never set and the collection 'stepExecutions' is not fetched from job repository.  Hence, not sure if the following condition in SimpleJobLauncher's run(final Job job, final JobParameters jobParameters) would ever get executed:  for (StepExecution execution : lastExecution.getStepExecutions()) {     if (execution.getStatus() == BatchStatus.UNKNOWN) {      //throw      throw new JobRestartException(""""Step ["""" + execution.getStepName() + """"] is of status UNKNOWN"""");     }//end if    }//end for"""
"XD-1389","Bug","Batch",5,"Sometimes getting NPE when master step runs for ftphdfs job","""Depending in the ftp server used there seems to be an error condition that generates an NullPointerException.   These are the steps to reproduce this:    Exception: """
"XD-1401","Story","Stream Module",8,"Add new reactor tcp module","""A reactor based TCP module that would support some basic CODECS.  Should evaluate if this new TCP module would subsume the current reactor-syslog module functionality or if the reactor-syslog module should be enhanced/upgradted."""
"XD-1400","Story","Runtime",8,"Containers should listen for Module deployment requests and their deletions","""The Admin leader will write each Module deployment request to a child node of /xd/deployments for a selected Container (see XD-1399). That Container-specific (persistent) child node needs to be created by the Container at the same time as it creates its ephemeral node under /xd/containers.  The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module."""
"XD-1399","Story","Runtime",8,"Admin leader should watch ZooKeeper for Stream deployment requests","""The Stream deployment requests will be written to /xd/streams/streamname and the data on that node will include the state or a boolean indicator (for now) of whether it should be deployed.  When a Stream is deployed, the leader will consult its Container cache and write the modules to the various /xd/deployments child nodes (see XD-1400)."""
"XD-1398","Story","Runtime",8,"Admin servers should write streams to and delete them from ZooKeeper","""This should also enable removal of any StreamDefinitionRepository code.  The state should be written as a data node at the stream level (e.g. /xd/streams/mystream {state=...})  For now we at least need to support the boolean --deploy=true|false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers (XD-1399)"""
"XD-1410","Story","Testing",5,"XD EC2 needs to bootstrap ZOOKEEPER at installation time.","""Startup zookeeper on EC2 cluster instances."""
"XD-1409","Story","Documentation",0,"GemFire sink properties missmatch ","""The documentation lists the gemfire-server sink module's attributes to be 'gemfireHost' and 'gemfirePort'.  In the module/code they are 'host' and 'port'.  The other attributes are correct.  """
"XD-1407","Story","Stream Module",5,"Create a throughput sink","""The throughput module would expect a payload of the type Message<byte[]> and look for the byte[] to be START or STOP strings to trigger a throughput measurement.  https://github.com/spring-projects/spring-xd/tree/master/extensions would be the place for the module to live."""
"XD-1416","Story","Runtime",8,"When there are no wiretap listeners don't publish messages","""Being able to listen to a stream at any point has a significant performance impact.  The reason for the impact is the message needs to be """"serialized + transported + deserialized"""" to other members even if there is no one listening.  This """"serialized + transported + deserialized"""" processes happens for each step in a flow - source | process | sink.  Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data.  Likewise we need to deregister the listener if the wiretap is deleted."""
"XD-1411","Story","Runtime",5,"Create xd-yarn script","""Create an xd-yarn script that is more """"Cloud Foundry"""" like -   xd-yarn push -p <path-to-unzipped-yearn-distro> xd-yarn start admin xd-yarn start container """
"XD-1424","Improvement","Documentation",0,"Docs could use link to Tuple artifacts","""The Tuple documentation, http://docs.spring.io/spring-xd/docs/1.0.0.M5/reference/html/#tuples, has no link or reference to the Jar(s) and/or Maven artifacts required to use Tuples in a project.  Took me a bit of searching to find the Maven artifacts.  Would be nice to include the jar name and maven/gradle config.    In Gradle (from the spring repo maven { url """"http://repo.spring.io/libs-snapshot"""" }): compile 'org.springframework.xd:spring-xd-tuple:1.0.0.M5'"""
"XD-1420","Story","Analytics",8,"Create a JPMML module that will evaluate a model.","""A analytical model should be evaluated as a processor in a stream.  The model evaluation will take the input variables from a Tuple and output variable will be placed into the tuple as well.  A strawman of the stream definition can be   stream create --definition """" SOURCE | jpmml ‘fraud-detection’ | PROC1 … | PROCN """" --name stream1  Using profiles and playing all implementations of the analytical model in the same module lib directory, it maybe possible to select one of multiple implementations in the form   """" SOURCE | analytic --library=jpmml --name=‘fraud-detection’ | PROC1 … | PROCN """"  such that the core module name is the same but parameterized by what library type to use.  This may be problematic in that different libraries may have incompatible dependencies.  The analytical model can define the names of input and output fields, so at a minimum a name is required, however to easily adapt a given analytic model evaluation to a specific source modules output, it seems desirable to specify which fields are to be used as input, overriding the names of the input fields could be done in a manner such as   jpmml –name=linear-regresssion –inputFields=a,b,c """
"XD-1418","Story","Analytics",8,"Create subproject spring-xd-machine-learning-analytics","""This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations.    The initial code for this has been developed in a separate github repo and is located here   https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics/src/main/java/org/springframework/xd/analytics/model  The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple.  Note, it maybe useful to consider Message<Tuple> in case any metadata outside the core 'input data' is required to help guide the evaluation.  The build.gradle file should be updated such that there is a new build artifact spring-xd-machine-learning-analytics.jar along the lines of our other build artifacts.  Open to other naming suggestions."""
"XD-1417","Story","Packaging",8,"Create RPM for distribution","""Package SpringXD into an RPM install path = /opt/pivotal/spring-xd-1.0.0.M5 with symlink /opt/pivotal/spring-xd -> current version init.d scripts to start/stop/status service springxd-admin start|stop|status service springxd-container start|stop|status user/group = springxd/pivotal Host springxd rpm in Pivotal repo yum install springxd Support RHEL/CentOS version 5 and 6? (tested on latest updates) Support for 32 and 64 bits Support Java 1.6 and 1.7 """
"XD-1428","Improvement","Packaging",3,"Log Hadoop Distro and ZK client connect info on Container startup","""It would be nice to display container config logging with the hadoop distro and zookeeper client connect being used when the container starts up. """
"XD-1436","Story","UI",5,"Misc cleanup in UI","""0. Remove home page with sign in and upper right hand corner with user login info. 1. Change the word template to modules in the tab 2. Different text for each of the tabs, “modules, definition, deployments, scheduled” 3. Definitions tab to have text along the lines """"“allows you to deploy  and undeploy batch job definitions"""" add links to help on how to do that in the CLI. 4. Deployments tab  a   creating new definitions, - parameters needs to be space on parameters,  “Job Parameters for Job XYZ” after clicking launch.  b. comment out scheduler button  c. add quick filter 5. Scheduler tab  a. comment out tab """
"XD-1435","Story","UI",1,"Improvements to Executions Tab","""1. Add quick filter 2. The table should have columns for                                           name | instance | execution id  Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch.  3. The restart action should appear only if the job is restartable and the status was failed. """
"XD-1434","Story","UI",8,"Improvements to Modules Tab","""1. Get listing of job modules 2. Remove version and action column 3. Text to say creating definitions from available modules in the UI is forthcoming, link to https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs#creating-a-job for how to do this in the command line.   4. Hardcode an association between spring xd out of the box module names and a description.  5. Add button to display the XML file that defines the job module """
"XD-1432","Story","Runtime",3,"Configure servers to use VanillaHealthEndpoint","""The standard SimpleHealthIndicator that boot performs a database test that fails in xd-container since it does not require the use of a database.  """
"XD-1438","Improvement","Documentation",0,"RabbitMQ port wrong in Docs","""The documentation (http://docs.spring.io/spring-xd/docs/1.0.0.M5/reference/html/#_using_rabbitmq) list the default RabbitMQ port as 5674.  It is 5672 and is correct in the SXD config.  """
"XD-1442","Story","Configuration|Hadoop",3,"Remove Hadoop distro Enum options","""Please see the discussion here:  https://github.com/spring-projects/spring-xd/pull/655/files#r10892925"""
"XD-1440","Story","Runtime",8,"Allow re-use of a module classloader","""See report at https://github.com/spring-projects/spring-xd/issues/661  It would be good indeed to allow this (eg by having a WeakHashMap<Classloader, type+name> map in the global context). The caveat though, is that any statics used by the module would be shared too. We can make this an opt-out though (I think that sharing by default makes sense) by having a flag in the module .properties manifest"""
"XD-1439","Story","Runtime",5,"Investigate module classloader leakage","""See report at https://github.com/spring-projects/spring-xd/issues/661  This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky."""
"XD-1448","Improvement","REST",1,"SpringXD logs error and large stack trace when metric can't be found. Distracting.","""When a REST client of SpringXD (i.e., a dashboard) attempts to query (GET) a metric (e.g., counter, gauge, etc.) that does not exist the admin sever logs an ERROR and a large stack trace (attached).  In usage of Spring XD we see this frequently because a dashboard is running but the streams and counters have not been created quite yet, or initialized by messages flowing through the streams.  With a polling dashboard this results in a lot of distracting and large stack traces in the logs that are not actually issues.    I would suggest logging a one line warning or info message instead of the error and stack trace. """
"XD-1446","Story","Hadoop",8,"Update spring-data-hadoop dependency and add new Hadoop distros","""Update to Spring for Apache Hadoop 2.0 RC3 Add support for new hadoop distros:  - Pivotal HD 2.0 (phd20) - Hortonworks HDP 2.1 (hdp21) - Cloudera CDH5 (cdh5)  """
"XD-1456","Story","Acceptance Testing",3,"Allow user to configure tests with DI ","""With the addition of sinks and sources that require connections with external entities (hadoop, JMS, JDBC, ...)  the environment setup is getting unwieldy.  * Integrate SpringJUnit4ClassRunner.class into acceptance tests. * Retrieve environment variables via Dependency injection from application.properties. * Utilize profiles for    --local single node   --local cluster   --ec2 single node   --ec2 cluster"""
"XD-1463","Story","Stream Module",1,"Delete post module and CF profile","""This would get rid of the CF specific post module, keeping the general abstraction of 'http' source across CF and non-CF environments."""
"XD-1460","Story","Configuration|Packaging",3,"Remove jmxEnabled as a cmdLine option and enable JMX by default","""After some discussion and voting, we decided to remove """"jmxEnabled"""" as a command line option and have JMX enabled by default. This can be disabled from xd-config.yml externally."""
"XD-1466","Story","Acceptance Testing",2,"Update XdEc2Validation to reference <root>/management endpoint","""change   """"/jolokia/list"""";  to   """"/management/jolokia/list"""";  etc."""
"XD-1480","Story","Runtime",2,"Merge Module.Type and ModuleType","""Also likely rename, remove, or replace that Module (maybe can be supplanted by ModuleDescriptor when used in the refactored parser).  Also, considering the """"url"""" property is not necessary (vestige of the prototype), all we'd be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself (e.g. getDescriptor(moduleKey))."""
"XD-1479","Story","Runtime",2,"DefaultContainerMatcher should make a better attempt at round-robin distribution","""Currently the index is used globally but applied to a range of candidates that can differ based on the match criteria per invocation."""
"XD-1475","Story","Runtime",8,"Improve Exception handling for ZooKeeper data access","""Currently we have many catch(Exception) blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top-level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access.  Also, we should not be re-wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler (and although I'm typically hesitant to recommend it, this might be a case where a static util method is the right approach). """
"XD-1474","Story","Runtime",8,"Refactor StreamParser to return a StreamDefinition","""  We should also consider explicit methods such as parseStream (so that parseJob and parseComposedModule are at least separate methods, if not separate parser classes that share the common parser support class that is the core of today's parser). The parsing for """"completion providers"""" should probably be spun off to its own class as well. In the end, there should be no need for a ParsingContext enum but rather, more explicitly named methods and dedicated classes if that seems like the right approach.  the StreamDefinition should be composed of """"ModuleDescriptors"""" (that name is not set in stone) and other Stream-level metadata like source/sink channels  consider merging some of StreamFactory code there, and the rest into StreamDeployer  merge ModuleDescriptor and ModuleDeploymentRequest as part of this effort (again, a new name could be considered, but ModuleDescriptor should take precedence over ModuleDeploymentRequest), and note in the process that ModuleDescriptor was originally designed to be immutable (taking constructor args), but as we migrated the prototype code into XD itself, this was violated. We may want to consider a builder approach, and we likely want to avoid the need for a ModuleDefinition within the ModuleDescriptor."""
"XD-1495","Bug","CLI",1,"xd:>runtime modules gives error from CLI","""xd:>runtime modules Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/4dc55d87-125b-4e4a-a76e-82bb6980820d/TickTock.sink.log-1/metadata  This is on OSX running in distributed mode with --transport rabbit --hadoopDistro hadoop22, redis 2.8.8, rabbit 3.2.3, hadoop 2.2.0, and zookeeper 3.4.5."""
"XD-1494","Bug","CLI",1,"OS commands no longer supports whitespace/arguments in M6","""OS commands, i.e., """"!"""" doesn't support arguments in M6; it did in M5.    The following gives an error: xd:>! ls / You cannot specify option '' more than once in a single command  No arguments or whitespace works: xd:>! ls command is:ls spring-shell.log xd-shell xd-shell.bat"""
"XD-1493","Bug","CLI",1,"xd-shell tab completion missing for http post/get","""xd-shell tab completion missing for 'http post' and 'http get' cli commands.  Typing """"xd:>http post"""" <tab> <tab> gives no suggestions event though --file or --data are required.  """
"XD-1508","Bug","Runtime",3,"All jobs end up on the same container node","""The jobs aren't spread evenly across available container nodes as they are created/deployed. I had 3 nodes but only one has the job modules.  [zk: localhost:2181(CONNECTED) 56] ls /xd/deployments/modules/621230e0-a089-4fbe-afc8-611ae527fcbc [myjob9.job.jdbchdfs-0, myjob5.job.jdbchdfs-0, myjob8.job.jdbchdfs-0, myjob4.job.jdbchdfs-0, myjob6.job.jdbchdfs-0, myjob7.job.jdbchdfs-0] [zk: localhost:2181(CONNECTED) 57] ls /xd/deployments/modules/6969579c-0cf4-4cc1-8e21-e01d73a70965 [] [zk: localhost:2181(CONNECTED) 58] ls /xd/deployments/modules/d0667cd1-a57a-4279-b7fb-dd63e4dd40d4 []  """
"XD-1507","Bug","Batch|UI",3,"Prevent submiting jobs that are not currently deployed using Admin UI","""Job modules """"Launch"""" and """"Schedule"""" command buttons are active even if the job module isn't deployed or has been destroyed.  Get errors like:  """"Yikes, something bad happened while launching job myjob4"""" """"The job named 'myjob4' is not currently deployed"""""""
"XD-1505","Bug","Documentation",1,"Documentation typo in JSON SPEL filter","""In the JSON SPEL Filter twitter example here: http://docs.spring.io/spring-xd/docs/1.0.0.M5/reference/html/#filter  """"hashTags"""" should not have a capital 'T'.  Should be """"hashtags""""."""
"XD-1502","Bug","Runtime",1,"Investigate failing LocalSingleNodeStreamDeploymentIntegrationTests","""Investigate the failing test LocalSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus:    This can be most easily reproduced on Ubuntu."""
"XD-1501","Improvement","Runtime",1,"IP address used as default data when creating paths","""Invoking {{Paths.ensurePath}} is creating a default value of the host IP address instead of the expected """"empty"""" value. """
"XD-1500","Bug","Runtime",5,"Stream deployment race condition","""When a container is started, the leader admin will scan the deployed streams to determine if any have modules that need to be deployed on the new container.   When a stream is deployed, the leader admin will select containers to deploy modules to.  If a new container and stream are deployed at the same time, there is the window for a race condition where both attempt to deploy a module to a container. This can be solved by (at least one) of the following:  * Consider using a single thread in the admin leader to handle all ZooKeeper updates. This means that the handling of new containers and stream deployment requests will not happen concurrently. * Trap the {{NodeExists}} exception when creating the {{/xd/deployments/modules/...}} node in ZooKeeper"""
"XD-1496","Bug","Runtime",5,"Exception thrown when accessing Jolokia via the management context path","""When trying to access Jolokia via the management/jolokia (http://localhost:9393/management/jolokia) I get the following exception.     {""""error_type"""":""""java.lang.IllegalArgumentException"""",""""error"""":""""java.lang.IllegalArgumentException : No type with name 'management' exists"""",""""status"""":400,""""stacktrace"""":""""java.lang.IllegalArgumentException: No type with name 'management' exists\n\tat org.jolokia.util.RequestType.getTypeByName(RequestType.java:69)\n\tat org.jolokia.request.JmxRequestFactory.createGetRequest(JmxRequestFactory.java:94)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:78)\n\tat org.jolokia.http.AgentServlet$3.handleRequest(AgentServlet.java:298)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:229)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:194)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:154)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)\n\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)\n\tat org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)\n\tat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)\n\tat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n""""}"""
"XD-1517","Story","REST",3,"Change request mapping for removing a stream deployment in XDController","""Currently _deployments - with an understore in XDController should be something else.  Need to segment up the url space better for stream/jobs to avoid a clash."""
"XD-1526","Bug","Hadoop",3,"Exception when accessing CDH4 namenode","""Get exception when accessing cdh4 from shell -  java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.  at com.google.protobuf.GeneratedMessage.getUnknownFields  most likely due to protobuf-java-2.5.0.jar being on the main classpath now   Full stack trace: """
"XD-1525","Improvement","CLI",1,"Need better error handling for module info shell command","""Would be better to provide a more useful message, e.g. """"The module name must be of the form <module-type>:<module-name>"""" xd:>module info --name time java.lang.StringIndexOutOfBoundsException: Failed to convert 'time' to type QualifiedModuleName for option 'name,' String index out of range: -1 xd:>module info --name sink/time java.lang.StringIndexOutOfBoundsException: Failed to convert 'sink/time' to type QualifiedModuleName for option 'name,' String index out of range: -1 xd:>module info --name sink:time Command failed org.springframework.xd.rest.client.impl.SpringXDException: NullPointerException  xd:>module info --name source:time Information about source module 'time':"""
"XD-1524","Story","Documentation",1,"Create small documentation section on jmx/monitoring functionalty","""Should mention jolokia, how to turn on/off boot/jolokia http metric/monitoring and jmx.  Mention the naming strategy to identify modules running in a stream."""
"XD-1520","Story","Configuration",8,"Push ${xd.stream.name} into POJO defaults","""See XD-1283. We've been waiting for 1283 to change constructs like  attr=""""${name}"""" {noformat}   Turns out we can simply push down the ${xd.stream.name} bit in the default value (most likely initialization of a field in a POJO metadata class) and it will work just fine.  We can also consider: - providing a fake value for those placeholders to use when doing """"module info"""" (ie user will see  """"<name of the stream>"""" instead of """"${xd.stream.name}""""   """
"XD-1533","Bug","Runtime",2,"Admin needs to clean up failed deployment attempts","""If a container fails to deploy a module, the admin needs to clean up the {{/xd/deployments/modules/CONTAINER-ID/module}} path so that another attempt can be made to deploy that module to that container."""
"XD-1532","Bug","Runtime",2,"Clean up MBean registration for failed module deployments","""When a module fails to deploy (for instance an http module configured with a port that is already bound) subsequent attempts to deploy the module fail due to a JMX exception:  """
"XD-1531","Story","YARN Runtime",3,"Rename xd-config.yml to servers.yml and add modules/modules.yml to spring-xd-yarn","""Make changes to XD on YARN config that correspond to XD-1499 changes"""
"XD-1530","Bug","CLI",3,"Error when removing HDFS files in shell","""I get this:    so far I have seen this with --hadoopDistro hdp13 and hadoop12  same command works fine using shell from M5 release """
"XD-1555","Story","Stream Module",2,"transform processor with script option is broken","""Creating the following stream throws exception:  stream create s1 --definition """"http | transform --script=transform.groovy | log"""" Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module transform of type processor:     valid: the 'script' and 'expression' options are mutually exclusive  The ExpressionOrScriptMixin's assertions to check if script and expression options are mutually exclusive `always` fails."""
"XD-1552","Improvement","CLI|Runtime",2,"Remove --transport option except for single node","""Since transport is now shared by Admin and Container, a command line arg is not appropriate since it allows the user to set them to different values which would break XD. The recommend way to configure transport is in servers.yml.  The command line arg is still valid for single node"""
"XD-1550","Improvement","Runtime",1,"Fix 'cannot find MessageBuilderFactory' warning","""5:27:47,887 WARN DeploymentsPathChildrenCache-0 org.springframework.integration.context.IntegrationContextUtils:195 - No 'beanFactory' supplied; cannot find MessageBuilderFactory, using default. a lot of those"""
"XD-1547","Bug","Runtime",3,"clean up dead entries in ZooKeeper /xd/deployments/modules","""When starting and stopping xd containers there are entries left in the /xd/deployments/modules directory that will cause 'runtime modules' command to fail.  xd:>runtime modules  Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b/test.sink.hdfs-1/metadata  here the """"/xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b"""" container is no longer running, but there is some data left over. """
"XD-1543","Story","Documentation",2,"Update instructions to how to setup admin to use RDBMS.","""Need to update instructions to discuss the setup of the relational database requirement for the xd-admin."""
"XD-1568","Story","Documentation",2,"Update documentation related to transport and controlTransport","""e.g. Need to update this section (maybe others): https://github.com/spring-projects/spring-xd/wiki/Running-Distributed-Mode  Remove all mentions of Control Bus, and replace any mentions of the --transport cmd line arg with the xd.transport property in yml. """
"XD-1566","Improvement","Documentation",3,"Document append configuration, else jdbchdfs writes empty file to hdfs","""Similar to XD-1565  so I'd link these 2 together.  [Steps to reproduce on Hadoop12] 1) Created Table People with columns forename,surname and address (use the result from filejdbc) 2) job create myjob --definition """"jdbchdfs --sql='select col1,col2,col3 from some_table'"""" 3)job launch myjob 4) myjob is created on hdfs but with zero bytes 5) throws an exception, stack trace attached."""
"XD-1565","Improvement","Documentation",3,"Document append support, else filepollhdfs writes empty file to hdfs","""When testing in both singlenode and cluster (redis), XD throws exception (stacktrace attached).  The file is created on hdfs, but it is empty.  [Steps to recreate Using Hadoop12] 1) job create myjob --definition """"filepollhdfs --names=forename,surname,address"""" --deploy  2) stream create csvStream --definition """"file --ref=true --dir=/tmp/dug --pattern=*.csv > queue:job:myjob"""" --deploy 3) use excel to create a 3 column spreadsheet and save as csv.  4) Copy csv to /tmp/dug directory"""
"XD-1564","Bug","Stream Module",2,"Rabbit Sink with explicit routingKey as 'string' SpEl literal expression fails","""Following stream fails to work:  tream create s3 --definition """"http | rabbit --routingKey='mytest1'"""" --deploy  Created and deployed new stream 's3' xd:>http post --data """"testing"""" > POST (text/plain;Charset=UTF-8) http://localhost:9000 testing > 500 INTERNAL_SERVER_ERROR > 500 INTERNAL_SERVER_ERROR  Error sending data 'testing' to 'http://localhost:9000'  The exception at the container log is:  07:24:57,245 ERROR pool-18-thread-4 http.NettyHttpInboundChannelAdapter:171 - Error sending message org.springframework.messaging.MessageHandlingException: Expression evaluation failed: mytest1  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)  at org.springframework.integration.handler.ExpressionEvaluatingMessageProcessor.processMessage(ExpressionEvaluatingMessageProcessor.java:76)  at org.springframework.integration.amqp.outbound.AmqpOutboundEndpoint.handleRequestMessage(AmqpOutboundEndpoint.java:196)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)  at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy109.handleMessage(Unknown Source)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)  at sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy54.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)  at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)  at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)  at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)  at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)  at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)  at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)  at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)  at sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)  at java.lang.reflect.Method.invoke(Method.java:606)  at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)  at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)  at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)  at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)  at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)  at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)  at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)  at com.sun.proxy.$Proxy111.send(Unknown Source)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)  at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)  at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)  at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)  at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.access$300(NettyHttpInboundChannelAdapter.java:69)  at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.messageReceived(NettyHttpInboundChannelAdapter.java:168)  at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)  at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)  at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)  at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)  at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)  at org.jboss.netty.handler.execution.OrderedMemoryAwareThreadPoolExecutor$ChildExecutor.run(OrderedMemoryAwareThreadPoolExecutor.java:314)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:744) Caused by: org.springframework.expression.spel.SpelEvaluationException: EL1008E:(pos 0): Property or field 'mytest1' cannot be found on object of type 'org.springframework.messaging.support.GenericMessage' - maybe not public?  at org.springframework.expression.spel.ast.PropertyOrFieldReference.readProperty(PropertyOrFieldReference.java:215)  at org.springframework.expression.spel.ast.PropertyOrFieldReference.getValueInternal(PropertyOrFieldReference.java:85)  at org.springframework.expression.spel.ast.PropertyOrFieldReference.getValueInternal(PropertyOrFieldReference.java:78)  at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)  at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)  at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:119)  ... 91 more"""
"XD-1576","Story","Configuration|Documentation",3,"Remove unused .properties files in config and update docs","""There are some properties files in the config directory that no longer are needed. We should clean that up and also remove/update any documentation references to these files"""
"XD-1575","Story","Ingest",8,"Add UDP support to reactor-syslog source module","""Currently the reactor-syslog source module only supports TCP.  Once we add UDP support, we can probably remove the existing syslog-tcp and syslog-udp modules."""
"XD-1581","Bug","Packaging",2,"XD config home should use XD_CONFIG_LOCATION if this is set","""If XD_CONFIG_LOCATION is set, then XD runtime's xd.config.home should use that. otherwise, they point to two different paths."""
"XD-1587","Improvement","Stream Module",2,"Provide module configuration templates for twitter sources","""Provide module templates including required property keys but not values for  $XD_MODULE_CONFIG/source/twitter*/twitter*.properties. Also look for any other packaged modules that have required properties that should be statically configured and we cannot provide defaults.  The Source modules document should be more clear regarding the configuration of these properties."""
"XD-1586","Bug","Runtime",3,"Stream should not be in deployed state following module failure. ","""Run singlenode. Ensure twitterstream credentials are not valid. e.g.,  no consumerKey property. This is the default state.  >stream create tweets --definition """"twitterstream | log"""" --deploy Created and deployed stream 'tweets'  Meanwhile, Singlenode throws an exception, the stacktrace below   xd:>stream list   Stream Name  Stream Definition    Status   -----------  -------------------  --------   tweets       twitterstream | log  deployed  {code} 15:54:07,298 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 - java.lang.RuntimeException: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'twitterTemplate' defined in URL [file:/Users/dturanski/spring-xd/spring-xd-1.0.0.M6/xd/modules/source/twitterstream/config/twitterstream.xml]: Could not resolve placeholder 'consumerKey' in string value """"${consumerKey}""""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'consumerKey' in string value """"${consumerKey}""""  at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:448)  at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:347)  at org.springframework.xd.dirt.server.ContainerRegistrar.access$700(ContainerRegistrar.java:93)  at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:678)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:744) Caused by: org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'twitterTemplate' defined in URL [file:/Users/dturanski/spring-xd/spring-xd-1.0.0.M6/xd/modules/source/twitterstream/config/twitterstream.xml]: Could not resolve placeholder 'consumerKey' in string value """"${consumerKey}""""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'consumerKey' in string value """"${consumerKey}"""" {/code}"""
"XD-1585","Bug","CLI",8,"Tab completion does not work for stream definition following > ",""">stream create """"tap:stream:foo >   does not suggest modules"""
"XD-1591","Improvement","Runtime",5,"Flatten out ephemeral nodes ","""Flatten out ephemeral nodes written by containers when deploying modules. For instance, instead of {{.../streams/moduleType/moduleLabel/container}} use {{.../streams/moduleType.moduleLabel.container}}.  This change allows us to derive state for a stream/job without having to traverse multiple layers of znodes. This is a big deal because: * each level of children requires a network call * Curator can only cache one level of children """
"XD-1590","Improvement","Runtime",2,"Move ephemeral nodes from /xd/streams to /xd/deployments/streams","""To have a clear separation of definition vs runtime information, move the ephemeral nodes written by containers from {{/xd/streams/stream-name}} to {{/xd/deployments/streams/stream-name}}. Same for jobs."""
"XD-1588","Bug","Configuration",5,"PropertySource leakage between runtime and modules","""in EnvironmentAwareModuleOptionsMetadataResolver::loadPropertySources, the call to merge(parentEnv) was added to inherit the active profiles of the runtime.  Sadly, it added the parentEnv property sources by side effect.  Note that the jdbc module defaults rely on this bug"""
"XD-1595","Improvement","Runtime",3,"Remove aliasHint flag usage when binding producer/consumer to MessageBus ","""The MessageBus interface uses the aliasHint flag when binding consumer/producer on a point-to-point channel.   Actually, the aliasHint is only needed when computing Source/Sink channel names in case named channel names. Otherwise, indexed channel names will be used for the input/output channel name.   The only place where aliasHint is used in the message bus is on the LocalMessageBus where it provides a way to choose the channel provider (direct/queue channel) based on the alias hint. Otherwise, it is not needed in message bus bindproducer/consumer.  We need to simplify this."""
"XD-1596","Story","Runtime",3,"Rabbit Source Should Expose More Container Options","""acknowlege-more, tx-size, prefetch-count, concurrency etc."""
"XD-1600","Improvement","Batch",1,"Validate existence of batch job at the admin side","""Since the batch job repository is not intended to be deleted, it is possible to have a batch job that already exists in the batch job repo even if the batch job definition is destroyed in XD. When a new job definition is created, we need to add a validation for the same job definition name against the batch job repository. Currently, we will only see a failure when the job is actually deployed into the container (when the batch job repository is updated during the deployment)."""
"XD-1599","Story","Packaging",3,"Change SpringSource references in pom.xml to Spring/spring.io","""This is currently in the M6 pom:    <organization>     <name>SpringSource</name>     <url>http://springsource.org</url>   </organization> """
"XD-1598","Story","Stream Module",1,"Use MessageBus Binding to start() underlying endpoint","""The messagebus implementations, upon registration of consumer and producer from/to messagebus the corresponding endpoints start. Instead of directly calling the start() on adapter/consumer we can call the corresponding Binding's start() which calls the underlying endpoint to start.  This is in-line with the way the corresponding endpoints are stopped (using Binding's stop()) during undeploy/destroy."""
"XD-1612","Improvement","REST",3,"Simplify/Refactor UI controllers","""The UI controllers in spring-xd/spring-xd-ui/app/scripts/controllers.js definitions look overly complicated to get the modularization work.   We can possibly refactor and make it look clean; especially we will follow this as the example for subsequent controllers definitions."""
"XD-1602","Improvement","Acceptance Testing",3,"JMS Source on EC2 only uses localhost for activemq broker","""[Problem] On a EC2 container jms-activemq.properties was configured to use a activemq broker on a different host, it still referred to localhost.   On my local mac, I was able to updated the jms-activemq.properties with an activemq on a different host and it worked.  [work-around] While not recommended you can set the amq.url in the jms-activemq-infrastructure-context.xml.  [Steps to reproduce] 1) Deploy a single admin/container using xd-ec2.   2) create a jms-activemq.properties file in the spring-xd-1.0.0.BUILD-SNAPSHOT/xd/ where it refers to a broker on another machine (ec2-54-221-32-82.compute-1.amazonaws.com).   3) Create a stream with JMS as its source."""
"XD-1629","Story","Runtime",3,"RabbitMessageBus should prefix all created queues with a prefix in order to support HA","""To configure Rabbit HA a naming convention should be used to identify the queue that need to be mirrored.  """
"XD-1622","Story","Batch",8,"Add support for typed Batch Steps","""This may require additional support (Jiras) for Spring Batch"""
"XD-1620","Story","Testing",1,"Fix JobCommandTests' verification of shell result table rows using specific index","""Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row (mostly first row) like this:  String id = jobExecutions.getRows().get(0).getValue(1);   displayJobExecution(id);  It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. """
"XD-1613","Bug","DSL",2,"Parser fails on + after literal within an expression","""This fails:   But this works:  """
"XD-1632","Improvement","Testing",1,"Use unique queue names in shell tests","""There seems to be some cross talk among the shell integration tests.  It looks like the same singlenode application might get shared among the test classes when they run in parallel.  Using unique queue names across the tests seem to fix the issue for now."""
"XD-1630","Story","Packaging",2,"Packaging of lib directory for shell contains many jars that are not used","""Between M5 and M6 the size of the shell/lib directory went up ~50 MB.  Investigate and remove jars from being packaged that are not used."""
"XD-1641","Bug","Runtime",2,"Upon a container departure, redeployment of batch job fails on an existing container","""When there are multiple containers (A, B and C) and a batch job is deployed into one of the containers A. When the container A goes down, the admin server tries re-deploy the job module that was deployed in container A into other matching container. But, when the re-deployment happens, it tries to update the distributed job locator as if a new job is being deployed and following exception is thrown:  17:13:38,811 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 -  java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists  at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:411)  at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:355)  at org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:349)  at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:695)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:253)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)  at java.util.concurrent.FutureTask.run(FutureTask.java:166)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)  at java.util.concurrent.FutureTask.run(FutureTask.java:166)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:722) Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists  at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:167)  at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103)  at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1514)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:252)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)  at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:699)  at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)  at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)  at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)  at org.springframework.boot.SpringApplication.run(SpringApplication.java:311)  at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)  at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:241)  at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:186)  at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:176)  at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:166)  at org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:230)  at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:399)  ... 20 more Caused by: org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists  at org.springframework.xd.dirt.plugins.job.DistributedJobLocator.addJob(DistributedJobLocator.java:114)  at org.springframework.xd.dirt.plugins.job.BatchJobRegistryBeanPostProcessor.postProcessAfterInitialization(BatchJobRegistryBeanPostProcessor.java:106)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:421)  at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.postProcessObjectFromFactoryBean(AbstractAutowireCapableBeanFactory.java:1698)  at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:164)  ... 36 more"""
"XD-1637","Improvement","Packaging|UI",1,"Re-enable JSHint during grunt build","""JSHint should be enabled in grunt build. There are few minor issues and needs to be fixed.  """
"XD-1636","Improvement","Configuration",1,"servers.yaml's 'xd: -> transport: rabbit' overrides xd-singlenode's default of local transport","""When working w/ SXD xd-singlenode, out of the box, it defaults to using all embedded components (transport, analytics, hsqldb, & zookeeper), which is easy and a great way to get going.  This is also great for development.  When I then started trying out the M6 distributed mode I set my transport to rabbit in servers.yaml (now that the --transport option is gone).  Rabbit is my preferred transport here.  I then went back to running the singlenode, for simplicity, and then got an exception saying that the singlenode couldn't contact RabbitMQ/AMQP (I was no longer running rabbit).  I then had to add the '--transport local' flag back to xd-singlenode.    Having the --transport option on xd-singlenode but not on xd-container is confusing.  Also I would expect xd-singlenode to default to local transport unless I specify another option in --transport.  -Derek"""
"XD-1635","Bug","Documentation",2,"Documentation: Hovering over some of the examples corrupts the text","""If you mouse over any of the examples in the documentation, the grey boxes, containing code, shell commands, etc., typically in the upper right hand corner a label for the type of code/example will appear.  E.g., 'Ruby', 'Javascript' ,etc.    1) The labels that appear seem to be random and incorrect.  Shell scripts show as 'Ruby' and 'Javascript'.  2) More importantly, on some of the examples the label appears in front of and part of the example, corrupting the example.  To see this hover your mouse over the two examples, grey boxes, here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#_xd_shell_in_distributed_mode  There may be more but this is the ones I noticed.    -Derek"""
"XD-1642","Story","Runtime",1,"Fail fast admin server if admin's embedded tomcat couldn't start","""During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn't running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message."""
"XD-1653","Story","Runtime",8,"Add More Sophisticated Retry Configuration to the Rabbit MessageBus","""XD-1019 added simple (stateless) retry to the message bus.  Use stateful retry and an {{AmqpRejectAndDontRequeueRecoverer}} enabling failed messages to be requeued on the broker until successful (perhaps because another instance can handle the message); also provides a mechanism to route failed messages to a dead-letter exchange.  Requires setting the message id header in bus-generated messages.  Also add profiles and properties for common retry/backoff policies."""
"XD-1651","Story","Hadoop",5,"Update HDFS sink to use unique id (GUID) as part of file name","""HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id (GUID) - like base-path/logfile-GUID-1.txt """
"XD-1650","Story","Hadoop",8,"Update HDFS sink to accept a partition strategy","""Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data.  The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation.  Partition configuration could be made available to the sink using a  --format parameter:  that could then be used in XML config like:  Similar to the time source."""
"XD-1656","Bug","Testing",1,"The type StubDatasetOperations must implement the inherited abstract method DatasetOperations.getDatasetDescriptor(Class<T>)","""StubDatasetOperations class needs to be either declared asbtract or implemente inherited methods from DatasetOperations"""
"XD-1654","Improvement","Stream Module",1,"Change twittersearch default outputType to be application/json","""The current output type is a Java object - this raises issues wrt to consumers in other JVM that to no have the spring social tweet object in the main container classpath.  See https://jira.spring.io/browse/XD-1370  Will also create another issue to update twittersearch to generate the raw twitterstream output vs. the structure of the spring social tweet object """
"XD-1663","Improvement","Runtime",2," Tap naming consistency for stream taps","""Currently, when creating the taps for streams, the name of the pub/sub channel inside the message bus would be   """"tap:<name-of-the-stream>.<module-name>.<module-index>  For instance, the following stream with name """"test"""":  http | transform --expression=payload.toLowerCase() | file  will have the exchanges as  'topic.tap:test.http.0', 'topic.tap:test.transform.1' when using rabbit message bus.  Though, the stream config parser takes care of translating what user would provide in the DSL (for example: tap:stream:test.transform.1 to use the message bus exchange topic.tap:test.transform.1), it would be better we have the consistency inside the message bus channel name as well.  Also, this would be in sync with how we name taps for jobs. (tap:job:*)"""
"XD-1668","Story","UI",5,"Modularize angular app modules based on the functionality","""When adding streams page to the UI (from XD-1667), it is necessary to modularize the angular app modules based on the functionality/components (job, stream, auth etc.,).   As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality."""
"XD-1667","Story","UI",2,"Add Steams page to show job triggers","""The streams page needs to be added to the UI at least to show the job triggers that are created while scheduling XD jobs."""
"XD-1670","Bug","Runtime",2,"NPE when a container departs","""When a container departs the cluster the admin will try to redeploy any modules that container was running. If the stream was *destroyed* and the container exited before it had the chance to clean up its deployments under {{/xd/deployments/modules}} (for example, with {{kill -9}}) the following NPE occurs:    If the stream was *undeployed* the following stack appears:   In short, this logic makes the assumption that the stream is still present and deployed. It needs to take into account the fact that neither assumption can be made."""
"XD-1675","Bug","Batch",5,"FilePollHdfs is not writing results to hdfs","""XD Deployment  Description:  XD Cluster (1 Container) Environment:  EC2 Type Of Test:  Manual Test Test Failed On  filepollhdfs (only test that was run) Build Used  Built May 7, 10:29 UTC  From the shell, attempted to create filepollhdfs however no results were written to hdfs (hadoop22).    The commands executed were the following: job create myjob --definition """"filepollhdfs  --names=forename,surname,address"""" --deploy stream create mystream --definition """"file --dir=67fc27a6-224d-4c67-a02a-40730bcf8906 --pattern='*.out' > queue:job:myjob"""" --deploy  No warnings nor exceptions were displayed till I changed the log4j.logger.org.springframework to INFO and restarted the container.  Then when I copied the sample file to the monitored directory the log reported: 21:30:07,605  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer:118 - deployed SimpleModule [name=file, type=source, group=mystream, index=0 @61612c7c] Exception in thread """"inbound.job:myjob-redis:queue-inbound-channel-adapter1"""" org.springframework.messaging.core.DestinationResolutionException: failed to look up MessageChannel with name 'errorChannel' in the BeanFactory (and there is no HeaderChannelRegistry present).  at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:108)  at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:44)  at org.springframework.integration.channel.MessagePublishingErrorHandler.resolveErrorChannel(MessagePublishingErrorHandler.java:111)  at org.springframework.integration.channel.MessagePublishingErrorHandler.handleError(MessagePublishingErrorHandler.java:78)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)  at java.lang.Thread.run(Thread.java:724) Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'errorChannel' is defined  at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)  at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)  When using the attached sample file, you need to rename the file to try2.out."""
"XD-1677","Story","Stream Module",1,"Add ""log-full-message"" Property to the Log Sink","""Allows looking at message headers without turning on debugging."""
"XD-1683","Bug","Stream Module",5,"syslog-tcp throws exception when receiving syslog data","""XD Deployment  Description XD Cluster (1 Container) Environment EC2 Type Of Test Manual test via shell Test Failed On syslog-tcp (only test that was run) Build Used Built May 7, 10:29 UTC  [Setting up the Environment] * Used the wiki instructions to setup the syslog on the ec2 instance.  * Deploy the stream below: stream create mystream --definition """"syslog-tcp | file --binary=true --mode=REPLACE"""" --deploy  * On the EC2 Instance execute the line below: logger -p local3.info -t TESTING """"Test Syslog Message""""  [What occurred] Stream fails to process inbound syslog information and throws the exception below:   Exception in thread """"inbound.mystream.0-redis:queue-inbound-channel-adapter17"""" org.springframework.messaging.core.DestinationResolutionException: failed to look up MessageChannel with name 'errorChannel' in the BeanFactory (and there is no HeaderChannelRegistry present).  at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:108)  at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:44)  at org.springframework.integration.channel.MessagePublishingErrorHandler.resolveErrorChannel(MessagePublishingErrorHandler.java:111)  at org.springframework.integration.channel.MessagePublishingErrorHandler.handleError(MessagePublishingErrorHandler.java:78)  at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)  at java.lang.Thread.run(Thread.java:724) Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'errorChannel' is defined  at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)  at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)  ... 5 more"""
"XD-1695","Story","REST",8,"Research how to secure Admin's REST endpoints","""As a user, I'd like to have the option to provide security configurations so that I can access REST endpoints in a secured manner.   Ideally, all the listed [REST|https://github.com/spring-projects/spring-xd/wiki/REST-API#xd-resources] endpoints needs to be wrapped within a security layer.   *Scope of this spike:*  * Research Spring Security and Spring Boot and the OOTB features  * Design considerations and approach for XD * Developer experience ** How users will be configuring security credentials? ** How DSL shell will be handled? ** How Admin UI will be handled?"""
"XD-1686","Story","Runtime",1,"Pluralization of admin nodes leadership selector group path (/xd/admin)","""Currently, the admin nodes that participate in the leadership election are grouped under /xd/admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to /xd/admins."""
"XD-1710","Story","Acceptance Testing",5,"ProcessorTest.testfailedSink needs to use http as its test source","""Also check the JMX output to see that the filter rejected the entry."""
"XD-1709","Bug","Batch",1,"Handling JobExecution stop action if the JobExecution is COMPLETED","""Currently, the flag """"stoppable"""" on JobExecutionInfoResource is used to find if the jobExecution can be stopped.  Since this flag is set to true even if the JobExecution status is COMPLETED, the jobExecution can still say it can be stopped."""
"XD-1707","Bug","Runtime",1,"The Dynamic Router example in the docs throws an exception with Rabbit Transport","""The example in the M6 documentation for the Dynamic Router (here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#dynamic-router) for the SpEL-Based Routing throws an exception when processing the message (from the HTTP post) saying """"No bean named 'queue:foo' is defined"""", when using RabbitMQ as the transport.  I do not know a workaround.  Steps to reproduce: 1) Run RabbitMQ locally 2) Run xd-singlenode --transport rabbit 3) xd:>stream create f --definition """"queue:foo > transform --expression=payload+'-foo' | log"""" --deploy  xd:>stream create b --definition """"queue:bar > transform --expression=payload+'-bar' | log"""" --deploy  xd:>stream create r --definition """"http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'"""" --deploy  4) xd:>http post --data """"a""""  5) This should give a stacktrace: Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'queue:foo' is defined  at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)  at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)  at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)  at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)  ... 83 more """
"XD-1705","Story","Packaging",3,"Add defaultYarnClasspath entry for phd20, cdh5 and hdp21","""Each Hadoop distro uses different settings for """"yarn.application.classpath"""" and we should provide some starting points for the distros we support running XD on YARN for.  We should add a commented out stub """"defaultYarnClasspath"""" entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros. """
"XD-1704","Story","Documentation",5,"Create doc section about quotes handling","""Document the different """"onion layers"""" that come in play with regard to quoting and escaping (shell, xd-parser, SpEL expressions in some cases) and provide practical examples to common scenarios  """
"XD-1701","Bug","Stream Module",3,"hdfs sink loads Codecs class during 'module info --name sink:hdfs' command","""The hdfs sink metadata causes loading of  org.springframework.data.hadoop.store.codec.Codecs class during 'module info --name sink:hdfs' command since the type is a specific Spring Hadoop class  options.codec.description = compression codec alias name options.codec.type = org.springframework.data.hadoop.store.codec.Codecs options.codec.default =  Don't think we want to tie the sink module to specific Spring Hadoop classes during runtime of the admin, we can't be sure that admin has hadoop classes on classpath in all environments and there is no way of specifying the hadoop distro for admin.  Wouldn't it be better to have this option as a String to be passed in to the module's context that could then load the class"""
"XD-1719","Bug","Runtime",1,"ZooKeeper Job deployments path state is not updated after successful deployment","""After successful job deployment, the Job deployments path in ZK doesn't get updated with the data {""""state"""": """"deployed""""}  Though this data is not used for deployed instance repository (org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository) to check for the deployment status, it may be better to have this state updated like stream deployment path."""
"XD-1718","Story","Acceptance Testing",3,"Twitter Search test uses case sensitive search when it should be case insensitive.","""The TwitterSearch does a case insensitive search.  Tests need to do a insensitive check for the keywords in the search result."""
"XD-1716","Story","Documentation",1,"Document that modules can reference property values in servers.yml","""Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml (or individual modules).   The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq/mqtt where using the server config values as defaults is useful and that they can still be overridden. """
"XD-1715","Story","Documentation",3,"Create documentation section for the shell","""Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes.  Include some discussion of basic commands to manipulate streams, jobs and list modules.  How to pass in a file that can be executed when the shell starts up.  Also point to spring-shell ref docs for extensibility in terms of adding custom commands."""
"XD-1712","Story","Acceptance Testing",3,"StreamUtil Cleanup","""Update StreamUtils based on Code Review comments."""
"XD-1724","Improvement","CLI",2,"CLI error when not specifying module type in module commands is cryptic an not helpful","""All of the CLI module commands that require the module name (e.g., 'module display source:mqtt') require that you preface the name with the module type.  If you forget to do this, e.g., 'module display mqtt', you get a fairly cryptic exception which can confuse end users.  The exception is:  java.lang.StringIndexOutOfBoundsException: Failed to convert 'mqtt' to type QualifiedModuleName for option 'name,' String index out of range: -1"""
"XD-1723","Improvement","Documentation",1,"'--type=' not supported by module delete as shown in documentation examples","""In the Module Composition example here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#composing-modules on of the examples is """"module delete --name foo --type sink"""" which fails as the '--type' argument is not supported by the CLI.    There are 3 other references to the '--type' argument in the documentation which may not be supported by the CLI anymore. """
"XD-1728","Story","UI",2,"Add Support for Bold/Strong Fonts ","""Hitting this issue in Chrome:  http://stackoverflow.com/questions/22891611/google-font-varela-round-doesnt-support-font-weight-in-chrome  Looks like Chrome has some issues with making text bold if the font does not explicitly support it. """
"XD-1735","Bug","Acceptance Testing",5,"FileJdbcTest & JdbcHdfsTest failing","""JdbcHdfsTest, FileJdbcTest works for singlenode but not for admin & Container on the same machine."""
"XD-1733","Story","YARN Runtime",3,"Investigate fall through of server.yml values when running in YARN","""We don't support using @Configuration for modules ATM.  The current code was committed during the same time as improvements to handling module configuration.  We should switch the reactor-ip.xml to include all bean definitions and remove referencing @Configuration classes or see how to add support for @Configuration.    Another short term hack is to put the prefix 'sink.reactor-ip' in all @Value used in NetServerInboundChannelAdapterConfiguration."""
"XD-1740","Story","Runtime",1,"ZooKeeper Admin server node data to have admin server host address","""It would be useful to store admin server ip address in ZooKeeper leadership group node (/xd/admins) to identify admin server and it's admin port.  """
"XD-1739","Bug","Runtime",2,"Container reconnection to ZK fails intermittently","""As reported by Matt Stine:  After closing and reopening a laptop, the following stack trace appears in the container log:    This can occur if ZK does not remove the ephemeral node before the container creates a new one. This can be fixed in the following ways:  * Remove the existing ephemeral node if it already exists * Register containers with a new UUID upon every new connection  For now I'll implement the first solution."""
"XD-1745","Improvement","Hadoop",5,"Support for hadoop name node HA configuration","""Hadoop supports namenode HA with two name nodes running, one being active and other in standby. If the active name node fails the standby name node has all the data readily available and can start serving requests. In this configuration name node url is no longer a host:port url but a logical name that translates to any active name node at runtime.   This is to ensure spring xd stream can handle a name node failure, for instance when writing a hdfs sink, seamlessly"""
"XD-1742","Improvement","Stream Module",1,"Remove toStringTransformer from tcp Source; Add Binary Support to the http Source","""The TCP source unconditionally converts to String. This prevents binary transfers.  Remove the transformer; if the user wants a String; (s)he can use  {{tcp --outputType=text/plain;charset=UTF-8}} (assuming the byte stream has valid UTF-8 encoding).  Another option would be to add a {{--binary}} option, but since conversion can already handle it, it's probably better to use that.  On the other hand, a {{--binary}} option would enable backwards compatibility.  The http source also unconditionally converts to String."""
"XD-1741","Improvement","Runtime",1,"Register StringToByteArrayMessageConverter","""The converter was not configured, therefore String to byte[] for --outPutType application/octet-stream fails for a String payload."""
"XD-1751","Story","Configuration",8,"Modules that use tomcat connection pool need to expose configurations","""filejdbc, hdfsjdbc, jdbchdfs & jdbc modules each support a tomcat connection  pool.  At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file. We need to allow the user to configure them via yml, property file and environment variables. """
"XD-1750","Bug","CLI",2,"Exception handling at Module info command","""When not prefixing with appropriate module type, module info command throws StringIndexOutOfBoundsException:   xd:>module info file java.lang.StringIndexOutOfBoundsException: Failed to convert 'file' to type QualifiedModuleName for option 'name,' String index out of range: -1"""
"XD-1748","Story","Runtime",1,"Update to Spring Integration 4.0.1","""Add messages store optimization to the `hdfs-dataset`"""
"XD-1757","Bug","Configuration",2,"Resolve runtime module option properties using module metadata","""Since the module metadata properties are resolved at runtime (when the module gets deployed), we can resolve the module options values that are already resolved in there.  For example, currently the """"runtime modules"""" command for """"log"""" module would show this:  runtime modules  [7m[27;32m  Module            Container Id                          Options   ----------------  ------------------------------------  --------------------------------------------------------   s1.source.http-0  633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {port=9000}   s1.sink.log-1     633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {name=${xd.stream.name}, expression=payload, level=INFO}  In this case, we can resolve the module option """"name"""" from the module metadata.   """
"XD-1756","Story","Hadoop|Packaging",3,"Update spring-data-hadoop version to 2.0.0.RC4","""Update spring-data-hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration."""
"XD-1758","Bug","Stream Module",2,"JMS Source (ActiveMQ) failing to use jmsUrl environment variable","""Deployed on: SingleNode Ec2, SingleNode Mac SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab  [Description] JMS Source (Activemq) tried to access a broker on localhost.   The current deployment uses the following to set the JMS Broker: * export amq_url=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616  [Analysis] After reviewing the configuration of the jms-activemq-infrastructure-context.xml, it was noted that the brokerUrl environment variable has been changed from amq.url to amqUrl.  While the jms-activemq.properties has not been changed (still amq.url).   After setting the following, the test still failed: * export amqUrl=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616  After going into the jms-activemq-infrastructure-context.xml and replacing the amqUrl with amq.url, the jms source (activemq) returned to normal operation.   [Incident] Acceptance tests reported a failure on Saturday Morning's build that the JMS Source failed."""
"XD-1767","Story","UI",3,"JobExecution restart action should depend on job deployment status","""At the JobExecution page, if the job execution is failed and restartable, then we should enable the """"restart"""" action only if the job is deployed.  Please see https://github.com/spring-projects/spring-xd/pull/884 for the discussion related to this."""
"XD-1766","Story","Acceptance Testing",2,"Failing tcp to file in script tests","""build 22-May-2014 08:45:04 Creating stream tcptofile with definition 'tcp+--port%3D21234+--socketTimeout%3D2000+%7C+file+--dir%3D%2Ftmp%2Fxdtest%2Fbasic' ... build 22-May-2014 08:45:04 {""""name"""":""""tcptofile"""",""""deployed"""":null,""""definition"""":""""tcp --port=21234 --socketTimeout=2000 | file --dir=/tmp/xdtest/basic"""",""""links"""":[{""""rel"""":""""self"""",""""href"""":""""http://127.0.0.1:9393/streams/tcptofile""""}]} build 22-May-2014 08:45:04  build 22-May-2014 08:45:11 Destroying stream tcptofile ... build 22-May-2014 08:45:11  build 22-May-2014 08:45:11  build 22-May-2014 08:45:11 Expected blahblah does not match actual value (98,108,97,104,98,108,97,104) simple 22-May-2014 08:45:11 Failing task since return code of [/bin/sh /tmp/XD-SCRIPTS-RS-513-ScriptBuildTask-7280766559152712153.sh] was 1 while expected 0 simple 22-May-2014 08:45:11 Finished task 'Run basic_stream_tests'  See https://build.spring.io/download/XD-SCRIPTS-RS/build_logs/XD-SCRIPTS-RS-513.log"""
"XD-1765","Story","Documentation",1,"Update documentation to list supported Hadoop distributions","""After spring hadoop 2.0 RC4 update."""
"XD-1760","Improvement","Runtime",8,"Support in-memory transport for co-located modules","""We are looking to speed up the message passing from source to sink  and wondering if we could use a in-memory transport whenever we know that source and sink modules are co-located on the same container. Currently we do not see a straight forward way of doing it  Option 1 : Create a composite module and let users deploy a composite module by itself or in other words deploy a stream with one module  Option 2 : Let users define a transport as in-memory when defining a stream. This could be used along with the deployment manifest feature enforcing co-location of a source and sink module, with in-memory transport  cc @adenissov """
"XD-1774","Story","UI",3,"UI Automatically close notification messages","""* Automatically close notification messages * Polish UI"""
"XD-1771","Story","Acceptance Testing",5,"Update twitterSearchTest to handle the latest release of twitterSearch","""The changes to twitterSearch means that it will send multiple messages during the duration of the test. To support these changes: 1) Remove assertReceived.  Since the number of messages is indeterminate 2) Change file sink that captures the results to append mode.  Because each message will overwrite the previous messages result."""
"XD-1770","Bug","Runtime",1,"Handle NPE while deploying stream module at the Container","""When trying to deploy a stream module, the ContainerRegistrar throws NPE if the deployment loader couldn't load a non-null stream based on the stream name.  07:10:29,902 ERROR DeploymentsPathChildrenCache-0 server.ContainerRegistrar:450 - Exception deploying module java.lang.NullPointerException  at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:549)  at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:436)  at org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:96)  at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:803)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)  at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)  at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)  at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)  at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)  at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)  at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)  at java.util.concurrent.FutureTask.run(FutureTask.java:262)  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)  at java.lang.Thread.run(Thread.java:744)"""
"XD-1769","Story","UI",3,"User should be able to provide job deployment properties","""At the job definitions page, user should be able to provide the job deployment manifest (module count, criteria etc.,)"""
"XD-1768","Story","UI",3,"User should be able to specify deploy properties for Jobs","""When clicking deploy from the job definitions page, user should be able to specify the deployment manifest (module count, module criteria etc.,)"""
"XD-1778","Story","Batch",2,"Check job ""restartable"" flag for JobExecution restart action","""job create bogus --definition """"jdbchdfs --sql='select * from bogus' --restartable=false"""" job deploy bogus job launch bogus  http://localhost:9393/admin-ui/#/jobs/executions  click """"Restart Job Execution"""" on the failed job execution  get message """"Job was relaunched""""  container log has:  12:36:27,231 ERROR task-scheduler-10 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: org.springframework.batch.core.repository.JobRestartException: JobInstance already exists and is not restartable"""
"XD-1777","Story","Runtime",20,"Restore deployment properties for orphaned modules","""As part of XD-1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container.  However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for.  This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn't have enough containers to support the stream."""
"XD-1786","Story","Runtime",5,"Support Partitioning/Bus Properties in the RedisMessageBus","""PR: https://github.com/spring-projects/spring-xd/pull/926"""
"XD-1791","Story","Batch",5,"New job that executes a Spark job","""Create OOTB batch job that executes a job on Spark as a tasklet  could be something along this:  job create yarnJob --definition """"sparkjob --master=spark://localhost:7077 --class=SimpleApp"""" """
"XD-1805","Story","Stream Module",8,"Support the ability to create module definitions in Groovy","""XML is currently required for module definitions. XD should also support Java @Config and Groovy bean definitions and potentially, SI DSLs. """
"XD-1812","Story","Runtime",2,"Support Bus Producer Properties for Dynamic Producers","""Pass module properties from stream plugin to {{MessageBusAwareChannelResolver}}.  Disallow partitioning properties."""
"XD-1823","Story","UI",8,"Investigate need for UI Pagination","""This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller (making the respective service call).  This would also necessitate some form of improved state management for the UI. E.g.  * User is on page 5 of the listing of Job Executions * User views details * User presses the back-button (on the screen) * The the listing of Job Executions *should* be still on page 5  """
"XD-1817","Bug","Runtime",5,"ContainerListener to redeploy modules based on stream order.","""When redeploying in the case of a container failure the modules are now redeployed in a random order.  The list of modules in the failed container needs to be sorted based on its position in a given stream and then redeployed."""
"XD-1831","Improvement","UI",2,"Mask Database Passwords in REST Controllers and Admin UI","""When deploying a batch job, the UI displays the database password found in the server.yml in plain text to the user.  At the very least, this should be displayed in a password field so it's masked out and have it masked out in the resulting definition at the bottom of the page.  Ideally, we wouldn't provide the password on that page at all and only accept overriding options (if the user wants a password other than the configured one, enter it…otherwise, we'll use what we have).  I'm finding that this occurs in other places as well.  A full pass though of the UI should be done to mask out passwords (or eliminate their display all together)."""
"XD-1838","Bug","Acceptance Testing",3,"FileSourceTest needs to apply label to source and sink","""* Currently Acceptance FileSource Acceptance Tests are failing ** This is because the sink that tests the result for the file source test is a filesink.  Both use the """"file"""" token.  Thus causing a failure * SimpleFileSource and SimpleFileSink needs to support a label method. * Update testFileSource to use the labels."""
